<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>DEV Community</title>
    <author>DEV Community</author>
    <description>A constructive and inclusive social network for software developers. With you every step of your journey.</description>
    <link>https://dev.to</link>
    <language>en</language>
    <item>
      <title>Rsync ile CentOS8 Üzerinde Dizin Senkronizasyonu </title>
      <author>Zeki Ahmet Bayar</author>
      <pubDate>Thu, 30 Sep 2021 10:38:42 +0000</pubDate>
      <link>https://dev.to/aciklab/rsync-ile-centos8-uzerinde-dizin-senkronizasyonu-294a</link>
      <guid>https://dev.to/aciklab/rsync-ile-centos8-uzerinde-dizin-senkronizasyonu-294a</guid>
      <description>&lt;p&gt;Rsync ile dizin senkronizasyonu yapmak için öncelikle sunucular arasında parolasız erişimi sağlamak gerekmektedir. &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#1-ssh-paketinin-kurulmas%C4%B1"&gt;
  &lt;/a&gt;
  1- SSH Paketinin Kurulması
&lt;/h3&gt;

&lt;p&gt;Senkronizasyon yapılacak sunucuların her ikisine gidilerek,&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;yum &lt;span class="nb"&gt;install &lt;/span&gt;ssh
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;komutu ile SSH paketi kurulur.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#2-ssh-konfig%C3%BCrasyon-ayarlar%C4%B1n%C4%B1n-yap%C4%B1lmas%C4%B1"&gt;
  &lt;/a&gt;
  2- SSH Konfigürasyon Ayarlarının Yapılması
&lt;/h3&gt;

&lt;p&gt;Her ikisi sunucuda da,&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; ~
&lt;span class="nb"&gt;mkdir&lt;/span&gt; .ssh
&lt;span class="nb"&gt;touch&lt;/span&gt; .ssh/config .ssh/known_hosts .ssh/authorized_keys
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;komutları ile gerekli SSH dizin yapısı oluşturulur. Daha sonra ilk sunucuda,&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;nano .ssh/config
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;ile konfigürasyon dosyası açıldıktan sonra,&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;Host server02 &lt;span class="c"&gt;# İkinci sunucu kısa adı&lt;/span&gt;
    HostName 10.0.0.2 &lt;span class="c"&gt;# İkinci sunucu ip adresi&lt;/span&gt;
    User root &lt;span class="c"&gt;# Rsync için kullanılacak user&lt;/span&gt;
    Port 22
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;ikinci sunucunun bilgileri girilir. Aynı işlem ikinci sunucuda da, birinci sunucunun bilgileri girilerek uygulanır.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#3-ssh-anahtar%C4%B1n%C4%B1n-olu%C5%9Fturulmas%C4%B1"&gt;
  &lt;/a&gt;
  3- SSH Anahtarının Oluşturulması
&lt;/h3&gt;

&lt;p&gt;SSH anahtarı oluşturmak için her iki sunucuda da,&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;ssh-keygen
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;komutu çalıştırılır. Gelen sorular boş geçilerek anahtar ikilisi oluşturulur.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;ssh-keygen
Enter file &lt;span class="k"&gt;in &lt;/span&gt;which to save the key &lt;span class="o"&gt;(&lt;/span&gt;/root/.ssh/id_rsa&lt;span class="o"&gt;)&lt;/span&gt;:
Enter same passphrase again:
Your public key has been saved &lt;span class="k"&gt;in&lt;/span&gt; /root/.ssh/id_rsa.pub.
SHA256:&lt;span class="k"&gt;***************************&lt;/span&gt; username@pc
+---[RSA 3072]----+
|..B+             |
|                 |
|o.o..            |
|+&lt;span class="k"&gt;*&lt;/span&gt; ..         o  |
|+.o.    S.   &lt;span class="nb"&gt;.&lt;/span&gt; + |
|.  ... oo o &lt;span class="nb"&gt;.&lt;/span&gt; E o|
|..         &lt;span class="nb"&gt;.&lt;/span&gt; &lt;span class="nb"&gt;.&lt;/span&gt; + |
|+o +sssss     o  |
|+o&lt;span class="k"&gt;*&lt;/span&gt;o.  &lt;span class="k"&gt;*&lt;/span&gt;+o       |
+----[SHA256]-----+
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Her iki sunucuda da anahtar ikilisi oluşturulduktan sonra birinci sunucuda,&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;ssh-copy-id server02
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;ve ikinci sunucuda,&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;ssh-copy-id server01
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;komutları çalıştırılarak anahtarlar birbirlerine kopyalanır. Kopyalama işleminden sonra birinci sunucuda,&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;ssh server02
&lt;span class="nb"&gt;exit&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;ve ikinci sunucuda,&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;ssh server01
&lt;span class="nb"&gt;exit&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;komutları çalıştırıldığında parolasız erişimin sağlandığı görülmelidir. SSH bağlantılarının sonlandırılması için her bağlantı sonrasında exit komutu çalıştırılmalıdır.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#4-dosya-senkronizasyonu-beti%C4%9Finin-sisteme-yerle%C5%9Ftirilmesi"&gt;
  &lt;/a&gt;
  4- Dosya Senkronizasyonu Betiğinin Sisteme Yerleştirilmesi
&lt;/h3&gt;

&lt;p&gt;Gerekli dizini oluşturmak için,&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;&lt;span class="nb"&gt;mkdir&lt;/span&gt; /usr/share/rsync
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;komutu kullanıldıktan sonra,&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;nano /usr/share/rsync/file-sync.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;komutu ile ilgili dosya açılır ve içerisine,&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;&lt;span class="k"&gt;while &lt;/span&gt;inotifywait &lt;span class="nt"&gt;-r&lt;/span&gt; &lt;span class="nt"&gt;-e&lt;/span&gt; modify,attrib,close_write,move,create,delete /bu/sunucuda/esitlenecek/dizin/&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do
    &lt;/span&gt;rsync &lt;span class="nt"&gt;-aAXru&lt;/span&gt; &lt;span class="nt"&gt;--progress&lt;/span&gt; &lt;span class="nt"&gt;--delete&lt;/span&gt; /bu/sunucuda/esitlenecek/dizin/ server02:/karsi/sunucuda/esitlenecek/dizin/
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;betiği yerleştirilir. &lt;/p&gt;

&lt;p&gt;Bu betikte, &lt;strong&gt;/bu/sunucuda/esitlenecek/dizin/&lt;/strong&gt; yerine betiğin yerleştirileceği sunucuda eşitlenmesi istenen dizinin yolu verilmelidir.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;/karsi/sunucuda/esitlenecek/dizin/&lt;/strong&gt; yerine ise, karşı sunucuda eşitlenmesi istenen dizinin yolu verilmelidir.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bu uygulama her iki sunucuya da çapraz olarak yapılmalıdır.&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#5-servis-dosyalar%C4%B1n%C4%B1n-olu%C5%9Fturulmas%C4%B1"&gt;
  &lt;/a&gt;
  5- Servis Dosyalarının Oluşturulması
&lt;/h3&gt;

&lt;p&gt;Yukarıda hazırlanan betiklerin servis haline getirilip sürekli olarak senkronizasyonun sağlanması gerekmektedir. Bunun için,&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;nano /etc/systemd/system/rsync-client.service
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;komutu ile servis dosyası açılır ve içerisine,&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;&lt;span class="o"&gt;[&lt;/span&gt;Unit]
Description &lt;span class="o"&gt;=&lt;/span&gt; SyncService
After &lt;span class="o"&gt;=&lt;/span&gt; network.target
&lt;span class="o"&gt;[&lt;/span&gt;Service]
PIDFile &lt;span class="o"&gt;=&lt;/span&gt; /run/syncservice/syncservice.pid
User &lt;span class="o"&gt;=&lt;/span&gt; root
Group &lt;span class="o"&gt;=&lt;/span&gt; root
ExecStartPre &lt;span class="o"&gt;=&lt;/span&gt; /bin/mkdir /run/syncservice
ExecStart &lt;span class="o"&gt;=&lt;/span&gt; /bin/bash /usr/share/rsync/file-sync.sh
ExecReload &lt;span class="o"&gt;=&lt;/span&gt; /bin/kill &lt;span class="nt"&gt;-s&lt;/span&gt; HUP &lt;span class="nv"&gt;$MAINPID&lt;/span&gt;
ExecStop &lt;span class="o"&gt;=&lt;/span&gt; /bin/kill &lt;span class="nt"&gt;-s&lt;/span&gt; TERM &lt;span class="nv"&gt;$MAINPID&lt;/span&gt;
ExecStopPost &lt;span class="o"&gt;=&lt;/span&gt; /bin/rm &lt;span class="nt"&gt;-rf&lt;/span&gt; /run/syncservice
PrivateTmp &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;Install]
WantedBy &lt;span class="o"&gt;=&lt;/span&gt; multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;dosyası eklenir. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bu dosyanın değiştirilmesine gerek yoktur. Her iki sunucuya da aynı şekilde eklenmelidir.&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#6-servislerin-ba%C5%9Flat%C4%B1lmas%C4%B1"&gt;
  &lt;/a&gt;
  6- Servislerin Başlatılması
&lt;/h3&gt;

&lt;p&gt;Servisler hazırlandıktan sonra,&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;systemctl daemon-reload
systemctl start rsync-client.service
systemctl &lt;span class="nb"&gt;enable &lt;/span&gt;rsync-client.service
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;strong&gt;işlemleri her iki sunucuda da yapılmalıdır.&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;Servisin doğru çalıştığı, çıktının aşağıda yer alan çıktıya benzerliği ile kontrol edilebilir.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;&lt;span class="o"&gt;[&lt;/span&gt;root@localhost ~]# systemctl status rsync-client.service
● rsync-client.service - SyncService
   Loaded: loaded &lt;span class="o"&gt;(&lt;/span&gt;/etc/systemd/system/rsync-client.service&lt;span class="p"&gt;;&lt;/span&gt; disabled&lt;span class="p"&gt;;&lt;/span&gt; vendor preset: disabled&lt;span class="o"&gt;)&lt;/span&gt;
   Active: active &lt;span class="o"&gt;(&lt;/span&gt;running&lt;span class="o"&gt;)&lt;/span&gt; since Tue 2021-09-28 06:10:30 EDT&lt;span class="p"&gt;;&lt;/span&gt; 13s ago
  Process: 8286 &lt;span class="nv"&gt;ExecStopPost&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/bin/rm &lt;span class="nt"&gt;-rf&lt;/span&gt; /run/syncservice &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;code&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;exited, &lt;span class="nv"&gt;status&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0/SUCCESS&lt;span class="o"&gt;)&lt;/span&gt;
  Process: 8284 &lt;span class="nv"&gt;ExecStop&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/bin/kill &lt;span class="nt"&gt;-s&lt;/span&gt; TERM &lt;span class="nv"&gt;$MAINPID&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;code&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;exited, &lt;span class="nv"&gt;status&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0/SUCCESS&lt;span class="o"&gt;)&lt;/span&gt;
  Process: 8289 &lt;span class="nv"&gt;ExecStartPre&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/bin/mkdir /run/syncservice &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;code&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;exited, &lt;span class="nv"&gt;status&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0/SUCCESS&lt;span class="o"&gt;)&lt;/span&gt;
 Main PID: 8291 &lt;span class="o"&gt;(&lt;/span&gt;bash&lt;span class="o"&gt;)&lt;/span&gt;
    Tasks: 2 &lt;span class="o"&gt;(&lt;/span&gt;limit: 4755&lt;span class="o"&gt;)&lt;/span&gt;
   Memory: 592.0K
   CGroup: /system.slice/rsync-client.service
           ├─8291 /bin/bash /usr/share/rsync/file-sync.sh
           └─8294 inotifywait &lt;span class="nt"&gt;-r&lt;/span&gt; &lt;span class="nt"&gt;-e&lt;/span&gt; modify,attrib,close_write,move,create,delete /root/yedek/

Sep 28 06:10:30 localhost.localdomain systemd[1]: rsync-client.service: Succeeded.
Sep 28 06:10:30 localhost.localdomain systemd[1]: Stopped SyncService.
Sep 28 06:10:30 localhost.localdomain systemd[1]: Starting SyncService...
Sep 28 06:10:30 localhost.localdomain systemd[1]: Started SyncService.
Sep 28 06:10:30 localhost.localdomain bash[8291]: Setting up watches.  Beware: since &lt;span class="nt"&gt;-r&lt;/span&gt; was given, th&amp;gt;
Sep 28 06:10:30 localhost.localdomain bash[8291]: Watches established.

&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;strong&gt;NOT : Senkronizasyonda yaşanan gecikmeler veri kaybına sebep olabilir !&lt;/strong&gt;&lt;/p&gt;

</description>
      <category>rsync</category>
      <category>linux</category>
      <category>filesync</category>
    </item>
    <item>
      <title>📚Angular Interview Questions Part 3</title>
      <author>Stacksjar</author>
      <pubDate>Thu, 30 Sep 2021 10:31:03 +0000</pubDate>
      <link>https://dev.to/stacksjar/angular-interview-questions-part-3-4m2h</link>
      <guid>https://dev.to/stacksjar/angular-interview-questions-part-3-4m2h</guid>
      <description>&lt;p&gt;In this article we are going to see a well curated list of angular interview questions 2021 and answers for experienced as well as freshers.&lt;/p&gt;

&lt;p&gt;Part 1 of this series :- &lt;a href="https://stacksjar.com/post/angular-interview-questions-part-1"&gt;Angular Interview Questions Part 1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Part 2 of this series :- &lt;a href="https://stacksjar.com/post/angular-interview-questions-part-2"&gt;Angular Interview Questions Part 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Part 3 of this series :- &lt;a href="https://stacksjar.com/post/angular-interview-questions-part-3"&gt;Angular Interview Questions Part 3&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#why-prioritize-typescript-over-javascript-in-angular"&gt;
  &lt;/a&gt;
  Why prioritize TypeScript over JavaScript in Angular?
&lt;/h2&gt;

&lt;p&gt;TypeScript simplifies JavaScript code, making it easier to read and debug. TypeScript provides highly productive development tools for JavaScript IDEs and practices, like static checking. TypeScript makes code easier to read and understand. With TypeScript, we can make a huge improvement over plain JavaScript.&lt;/p&gt;

&lt;p&gt;There are many more benefits of TypeScript over Javascript:&lt;/p&gt;

&lt;p&gt;Consistency&lt;br&gt;
Productivity&lt;br&gt;
Maintainability&lt;br&gt;
Modularity&lt;br&gt;
Catch Errors Early&lt;/p&gt;
&lt;h2&gt;
  &lt;a href="#what-is-a-bootstrapping-module-in-angular"&gt;
  &lt;/a&gt;
  What is a Bootstrapping module in Angular?
&lt;/h2&gt;

&lt;p&gt;Bootstrapping in Angular is a function component in the core ng module that is used for starting up the Angular application. By default the Appcomponent is the default component that will be bootstraped.&lt;/p&gt;

&lt;p&gt;Below is the Default code for bootstrapping an angular application in app.module.ts&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;@NgModule({
    declarations: [
        AppComponent,

    ],
    imports: [
        BrowserModule,
        AppRoutingModule,
    ],
    providers: [],
    bootstrap: [AppComponent],
    schemas: []
})
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#what-is-the-difference-between-pure-and-impure-pipe-in-angular"&gt;
  &lt;/a&gt;
  What is the difference between Pure and Impure pipe in Angular?
&lt;/h2&gt;

&lt;p&gt;A Pure Pipe is only called when Angular detects a change in the value or the parameters passed to a pipe.&lt;/p&gt;

&lt;p&gt;An Impure Pipe is called for every change detection cycle no matter whether the value or parameter(s) changes.&lt;/p&gt;

&lt;p&gt;Below is an example of pipe and its decorator for setting pipe type&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;@Pipe({
 name: 'myCustomPipe', 
 pure: true    // true means this is a Pure Pipe and false means its and Impure Pipe (default is true)
})

export class MyCustomPipe {}
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#what-is-rxjs"&gt;
  &lt;/a&gt;
  What is RxJS?
&lt;/h2&gt;

&lt;p&gt;The full form of RxJS is Reactive Extension for Javascript. It is a javascript library that uses observables to work with reactive programming that deals with asynchronous data calls, callbacks and event-based programs.&lt;/p&gt;

&lt;p&gt;RxJS is a library for reactive programming using observables that makes it easier to compose asynchronous or callback-based code. RxJS can be used with any other Javascript libraries and frameworks.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#what-is-an-observable"&gt;
  &lt;/a&gt;
  What is an observable?
&lt;/h2&gt;

&lt;p&gt;Observables are simply a function that are able to give multiple values over time, either synchronously or asynchronously. You can also consider Observables as lazy Push collections of multiple values.&lt;/p&gt;

&lt;p&gt;Observables provide support for passing messages between parts of your application. They are used frequently in Angular and are a technique for event handling, asynchronous programming, and handling multiple values.&lt;/p&gt;

&lt;p&gt;We can subscribe to an observable and get values synchronously or asynchronously.&lt;/p&gt;

&lt;p&gt;Below is an example of how to create and Observable:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;var observable = Rx.Observable.create((observer: any) =&amp;gt;{

   observer.next("This is an Observable");

})

observable.subscribe((data)=&amp;gt;{
   console.log(data);    // output - "This is an Observable"
});
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#what-is-an-observer"&gt;
  &lt;/a&gt;
  What is an observer?
&lt;/h2&gt;

&lt;p&gt;Observers are just objects with three callbacks, one for each type of notification that an Observable may deliver.&lt;/p&gt;

&lt;p&gt;An Observer is a consumer of values delivered by an Observable. Observers are simply a set of callbacks, one for each type of notification delivered by the Observable: next , error , and complete.&lt;/p&gt;

&lt;p&gt;Below is an example of Observer and values retrieved after being Subscribed to it:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;const observer = {
 next: x =&amp;gt; console.log('This is next value: ' + x),
 error: err =&amp;gt; console.error('Observer got an error: ' + err),
};

observable.subscribe(observer);

//OR

observable.subscribe(observer =&amp;gt; {
  observer.next(10);
  observer.error("something went wrong");  
});
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#what-are-angular-elements"&gt;
  &lt;/a&gt;
  What are Angular Elements?
&lt;/h2&gt;

&lt;p&gt;Angular elements are Angular components packaged as custom elements (also called Web Components), a web standard for defining new HTML elements in a framework-agnostic way.&lt;/p&gt;

&lt;p&gt;A custom element extends HTML by allowing you to define a tag whose content is created and controlled by JavaScript code. The browser maintains a CustomElementRegistry of defined custom elements, which maps an instantiable JavaScript class to an HTML tag.&lt;/p&gt;

&lt;p&gt;Live Example of Angular Elements :- Angular Elements Working Example&lt;/p&gt;

&lt;p&gt;The custom elements standard is currently supported by browsers like Chrome, Opera, and Safari. To be able to use it Firefox and Edge polyfills are available. The Angular Elements functionality is available with the package @angular/elements. &lt;/p&gt;

&lt;p&gt;In order to keep track of all available custom elements the browser maintains a registry in which every elements needs to be registered first. In this registry the name of the tag is mapped to the JavaScript class which controls the behavior and the output of that element.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#what-is-angular-universal-or-angular-ssr"&gt;
  &lt;/a&gt;
  What is Angular Universal or Angular SSR?
&lt;/h2&gt;

&lt;p&gt;Angular Universal is mechanism provided by Angular team by which you can render your single page angular application on server instead of Browser. Typical Angular applications are Single-Page Applications (aka SPA's) where the rendering occurs on the Browser. This process can also be referred to as client-side rendering (CSR).&lt;/p&gt;

&lt;p&gt;Angular Universal is a very helpful and SEO friendly approach for modern web applications.&lt;/p&gt;

&lt;p&gt;The Angular Universal provides 2 options:&lt;/p&gt;

&lt;p&gt;Server Side Rendering : In this method the requested page will be completely rendered on server and send to the browser&lt;br&gt;
Pre-Rendering : In this method you have to provide a list of routes you want to pre-render then by using the pre rendering command and the routes mentioned it will complete the Build with fully rendered HTML pages&lt;br&gt;
To add Angular Universal to your project use below command:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ng add @nguniversal/express-engine&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;
  &lt;a href="#what-are-service-workers-in-angular"&gt;
  &lt;/a&gt;
  What are Service Workers in Angular?
&lt;/h2&gt;

&lt;p&gt;Service Worker in Angular is a script that runs in the web browser and manages caching for an application. Service workers function as a network proxy. They intercept all outgoing HTTP requests made by the application and can choose how to respond to them.&lt;/p&gt;

&lt;p&gt;Service Workers helps in improving your application's performance.&lt;/p&gt;

&lt;p&gt;To add Service Workers in your Angular application use below command:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ng add @angular/pwa&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Checkout this Article: It covers complete Steps to Add Service Worker in Angular Application&lt;/p&gt;
&lt;h2&gt;
  &lt;a href="#what-is-lazy-loading-in-angular"&gt;
  &lt;/a&gt;
  What is Lazy Loading in Angular?
&lt;/h2&gt;

&lt;p&gt;Lazy Loading is a technique in Angular that allows you to load JavaScript components asynchronously when a specific route is activated. It improves the speed of the application load time by splitting the application into several bundles. When the user navigates through the app, the bundles are loaded as required.&lt;/p&gt;

&lt;p&gt;Instead of loading the entire web page and rendering it to the user in one go as in bulk loading, the concept of lazy loading assists in loading only the required section and delays the remaining, until it is needed by the user.&lt;/p&gt;

&lt;p&gt;Below is an example route for a lazy loaded module:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;const routes: Routes = [
  {path: '', redirectTo: 'home', pathMatch: 'full'},
  {path: 'home', component: HomeComponent},
  {path: 'lazy', loadChildren: './lazy/lazy.module#LazyModule'}
];
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#what-is-a-shared-module-in-angular"&gt;
  &lt;/a&gt;
  What is a Shared Module in Angular?
&lt;/h2&gt;

&lt;p&gt;Shared modules in Angular helps you write more organized code in less time, helping you be more productive. Shared modules are an ideal spot to declare components in order to make them reusable. You won’t have to re-import the same components in every module—you’ll just import the shared module.&lt;/p&gt;

&lt;p&gt;Creating shared modules allows you to organize and streamline your code. You can put commonly used directives, pipes, and components into one module and then import just that module wherever you need it in other parts of your app.&lt;/p&gt;

&lt;p&gt;Below is an example of a Shared Module:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;﻿import { NgModule } from "@angular/core";
import { CommonModule } from "@angular/common";

import { SharedRoutingModule } from "./shared-routing.module";
import { SharedComponent } from "./components/shared/shared.component";

@NgModule({
 declarations: [SharedComponent],
 imports: [CommonModule, SharedRoutingModule],
 exports: [SharedComponent]
})

export class SharedModule {}
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#what-is-dom-sanitizer-in-angular"&gt;
  &lt;/a&gt;
  What is DOM Sanitizer in Angular?
&lt;/h2&gt;

&lt;p&gt;Dom Sanitizer in Angular helps preventing Cross Site Scripting Security bugs (XSS) by sanitizing values to be safe to use in the different DOM contexts.&lt;/p&gt;

&lt;p&gt;Below are the different methods Provided by Angular for Sanitization and make sure any user data is appropriately escaped for this security context.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;//default sanitize data
abstract sanitize(context: SecurityContext, value: string | SafeValue): string | null

//sanitize html
abstract bypassSecurityTrustHtml(value: string): SafeHtml

//sanitize css
abstract bypassSecurityTrustStyle(value: string): SafeStyle

//sanitize scripts
abstract bypassSecurityTrustScript(value: string): SafeScript

//sanitize url
abstract bypassSecurityTrustUrl(value: string): SafeUrl

//sanitize resource urls
abstract bypassSecurityTrustResourceUrl(value: string): SafeResourceUrl
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;strong&gt;Checkout other Articles in this series:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Part 1 of this series :- &lt;a href="https://stacksjar.com/post/angular-interview-questions-part-1"&gt;Angular Interview Questions Part 1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Part 2 of this series :- &lt;a href="https://stacksjar.com/post/angular-interview-questions-part-2"&gt;Angular Interview Questions Part 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Part 3 of this series :- &lt;a href="https://stacksjar.com/post/angular-interview-questions-part-3"&gt;Angular Interview Questions Part 3&lt;/a&gt;&lt;/p&gt;

</description>
      <category>angular</category>
      <category>typescript</category>
      <category>tutorial</category>
      <category>javascript</category>
    </item>
    <item>
      <title>5 reasons for AI implementation in healthcare</title>
      <author>Addepto</author>
      <pubDate>Thu, 30 Sep 2021 10:20:51 +0000</pubDate>
      <link>https://dev.to/addepto/5-reasons-for-ai-implementation-in-healthcare-38li</link>
      <guid>https://dev.to/addepto/5-reasons-for-ai-implementation-in-healthcare-38li</guid>
      <description>&lt;p&gt;Today, AI technology can be found in almost every industry and business, including healthcare. Artificial intelligence in healthcare has huge and wide potential, from solutions for mobile diagnostics to drug discovery that can be achieved using machine learning.&lt;/p&gt;

&lt;p&gt;💡The global artificial intelligence in healthcare market size is expected to reach USD 120.2 billion by 2028 and is expected to expand at a CAGR of 41.8% over the forecast period, according to a new report by Grand View Research, Inc. &lt;/p&gt;

&lt;p&gt;If you've been thinking about implementing AI to your business, keep reading to find out 5 reasons for adopting AI in the healthcare industry. In addition, you can find predictions for the future that will help you be convinced of the potential of &lt;a href="https://addepto.com/ai-consulting/"&gt;AI consulting&lt;/a&gt; in healthcare.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#5-reasons-for-ai-implementation-in-healthcare"&gt;
  &lt;/a&gt;
  5 reasons for AI implementation in healthcare
&lt;/h2&gt;

&lt;p&gt;In this article, you can find 5 reasons to implement AI in the healthcare industry. These solutions help doctors work more productively, improve processes and clinical outcomes. These are also 5 reasons that indicate that AI technology in healthcare is gaining popularity and is a potential source of major investments. &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#1robotassisted-surgery"&gt;
  &lt;/a&gt;
  1.Robot-assisted surgery
&lt;/h3&gt;

&lt;p&gt;Robotic or robot-assisted surgery allows doctors to perform many types of complex procedures with greater precision, flexibility, and control than is possible with traditional methods. Surgeons using an AI-based robotic system believe that, for many procedures, it improves accuracy, flexibility and control during surgery, and allows them to see the surgery site better than traditional methods. &lt;br&gt;
Robotic surgery offers patients many benefits to open surgery, including:&lt;br&gt;
👉Shorter hospitalization&lt;br&gt;
👉Reduced pain and discomfort&lt;br&gt;
👉Faster recovery time and return to normal activities&lt;br&gt;
👉Smaller incisions that reduce the risk of infection&lt;br&gt;
👉Reduced blood loss and transfusions&lt;br&gt;
👉Minimal scars&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#2chatbots-amp-mobile-apps-for-healthcare"&gt;
  &lt;/a&gt;
  2.Chatbots &amp;amp; Mobile Apps for healthcare
&lt;/h3&gt;

&lt;p&gt;The digital health and wellness market is valued at over $106 billion in 2019, according to a recent Global Market Insights report. And with the global pandemic in 2020 that increases the demand for digital health technologies, the industry is expected to continue to grow 28.5% by 2026. [1] &lt;br&gt;
💡The AI healthcare market is predicted to exceed $34 billion by 2025.[1] &lt;/p&gt;

&lt;p&gt;Artificial Intelligence is useful in diagnostics, therapy and customer service through intelligent applications. Chatbots are by far the most popular AI application. What's more, the adoption of chatbots in organizations is growing year by year, and the chatbot market is expected to reach over USD 300 million by 2023. [1]&lt;/p&gt;

&lt;p&gt;The development of medical applications has played an important role in providing patients with quick information. Using mobile apps for healthcare, you can find a doctor, make an appointment and view all test reports without leaving your home. Mobile applications not only help patients, but also allows doctors to seamlessly collect and store personal health data. &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#3aipowered-alerting-systems-amp-virtual-assistants"&gt;
  &lt;/a&gt;
  3.AI-powered alerting systems &amp;amp; Virtual assistants
&lt;/h3&gt;

&lt;p&gt;Today, more and more AI solutions are implemented to support the transition from inpatient to home care, such as remote monitoring, AI alarm systems or virtual assistants. These technologies include the increased use of NLP solutions in hospitals and at home.&lt;/p&gt;

&lt;p&gt;Artificial intelligence systems make it easier for virtual nursing assistants to perform a wide range of tasks, from communicating with patients to sending them to a better and more efficient healthcare facility. Such virtual nurses are available 24/7 and can answer questions as well as monitor patients and provide immediate solutions. Nowadays, AI technology is also used in a wider range of specialties, such as oncology, cardiology and neurology.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#4improved-data-processing-for-diagnosis"&gt;
  &lt;/a&gt;
  4.Improved data processing for diagnosis
&lt;/h3&gt;

&lt;p&gt;One of the benefits of AI is the processing of large amounts of data, which allows AI to change various medical devices. For example, one NIH study that used artificial intelligence to diagnose metastatic breast cancer found the technology to be 99% accurate. [2] It has also been shown that artificial intelligence is capable of detecting the spread of cancer cells - micrometastases - which are usually very difficult to detect by specialist human pathologists.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#5enhanced-drug-development-process"&gt;
  &lt;/a&gt;
  5.Enhanced drug development process
&lt;/h3&gt;

&lt;p&gt;The development of new drugs also requires processing of large amounts of data due to the enormous number of possibilities for chemical combinations. The solution is AI technology, which works amazingly well with large amounts of data. This ability enables more approved drugs to be produced in a faster, more cost-effective process with higher R&amp;amp;D efficiency. &lt;/p&gt;

&lt;p&gt;By implementing &lt;a href="https://addepto.com/artificial-intelligence-in-drug-discovery-with-machine-learning/"&gt;AI into the drug development process&lt;/a&gt;, this technology has great potential to identify the most promising developments in the research process, helping to save time and resources at an early stage.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#future-predictions"&gt;
  &lt;/a&gt;
  Future Predictions
&lt;/h2&gt;

&lt;p&gt;AI technology is developing at an incredible pace, so below you can find 3 future predictions about AI in healthcare industry: &lt;/p&gt;

&lt;p&gt;👩‍🔬 VR and AR promise to be very useful in the healthcare industry. It is believed that using VR to train healthcare workers improves skill retention by 75% and reduces skill loss by up to 52%. [3]&lt;/p&gt;

&lt;p&gt;🧬 AI-powered predictive care: In 2030 health systems can predict when a person is at risk of a chronic disease, for example, and propose preventive measures before they get worse. [4] &lt;/p&gt;

&lt;p&gt;👨‍🔬 The future of AI will be based in predicting future diseases, their origins and the spread of the pandemic.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#key-takeaways"&gt;
  &lt;/a&gt;
  Key Takeaways
&lt;/h2&gt;

&lt;p&gt;▶️Artificial Intelligence truly transforms the modern healthcare industry in many ways.&lt;/p&gt;

&lt;p&gt;▶️5 reasons why you should implement AI in your company: robot-assisted surgery, chatbots, AI-powered alerting systems, enhanced drug development and improved data processing for diagnosis. &lt;/p&gt;

&lt;p&gt;▶️AI is a huge investment potential for investors and entrepreneurs. &lt;/p&gt;




&lt;p&gt;[1]Eleks.com. The Rise of Digital Health and Wellness Software. Accessed on Aug 2, 2021. &lt;br&gt;
[2]Pubmed.gov.  Artificial Intelligence-Based Breast Cancer Nodal Metastasis Detection: Insights Into the Black Box for Pathologists. Accessed on Aug 2, 2021. &lt;br&gt;
[3] Bisresearch.com. Global Augmented Reality and Virtual Reality Market in Healthcare.Accessed on Aug 2, 2021. &lt;br&gt;
[4] Weforum.org. Here are 3 ways AI will change healthcare by 2030. &lt;/p&gt;

</description>
      <category>technology</category>
      <category>healthcare</category>
      <category>artificialintelligence</category>
    </item>
    <item>
      <title>Lightweight Markdown dialect for Python desktop apps</title>
      <author>pyrustic</author>
      <pubDate>Thu, 30 Sep 2021 09:43:26 +0000</pubDate>
      <link>https://dev.to/pyrustic/lightweight-markdown-dialect-for-python-desktop-apps-19po</link>
      <guid>https://dev.to/pyrustic/lightweight-markdown-dialect-for-python-desktop-apps-19po</guid>
      <description>&lt;p&gt;Project: &lt;a href="https://github.com/pyrustic/litemark"&gt;https://github.com/pyrustic/litemark&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Plain text written in &lt;code&gt;Litemark&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--WiN4AGVB--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/k20rmol5v667twcr7caf.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--WiN4AGVB--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/k20rmol5v667twcr7caf.png" alt="Plain text in Litemark"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is the rendered same text in a &lt;code&gt;Python&lt;/code&gt; desktop app:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--pOjZ7cSt--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5tt6ccgaywdovnjs1hdl.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--pOjZ7cSt--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5tt6ccgaywdovnjs1hdl.png" alt="Rendered Litemark"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Project: &lt;a href="https://github.com/pyrustic/litemark"&gt;https://github.com/pyrustic/litemark&lt;/a&gt;&lt;/p&gt;

</description>
      <category>showdev</category>
      <category>python</category>
      <category>markdown</category>
      <category>tkinter</category>
    </item>
    <item>
      <title>Hanlon's razor, a great tool to be more positive </title>
      <author>Jelle Smeets</author>
      <pubDate>Thu, 30 Sep 2021 09:33:27 +0000</pubDate>
      <link>https://dev.to/smeetsmeister/hanlon-s-razor-a-great-tool-to-be-more-positive-368l</link>
      <guid>https://dev.to/smeetsmeister/hanlon-s-razor-a-great-tool-to-be-more-positive-368l</guid>
      <description>&lt;p&gt;In my professional career, I have come across several tools to help you get the correct mindset. Hanlon's razor is one of those. In a difficult negative situation in one of my teams, I discovered Hanlon's razor and I have used it ever since!&lt;/p&gt;

&lt;p&gt;In this blog post, we will take a look at what exactly a razor is, and how you can use Hanlon's razor to your benefit!&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#what-is-a-razor"&gt;
  &lt;/a&gt;
  What is a razor?
&lt;/h2&gt;

&lt;p&gt;In the school of philosophy, a razor is used as a principle or rule of thumb to eliminate or shave off explanations for a situation. Philosophers, scientists, and other great people have used Razors in their mental models.&lt;/p&gt;

&lt;p&gt;One of the most know razors is Occam's razor. Which states that "&lt;em&gt;the simplest explanation is usually the best one.&lt;/em&gt;". There are more Razors: Hitchen's, Alder's but let's take a look at the one that had the biggest impact on my life: Hanlon's razor.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#hanlons-razor"&gt;
  &lt;/a&gt;
  Hanlon's Razor
&lt;/h2&gt;

&lt;p&gt;Hanlon's razor is likely named after John Hanlon who submitted the statement to a joke book. It has been used by several important people in history. Napoleon, Winston Churchill, Jane West, and many more.&lt;/p&gt;

&lt;p&gt;There are several small variations of Hanlon's razor, my favorite one was spoken by Napoleon Bonaparte and goes as follows:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--cYTXDmSA--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zjltm00m6r29vg83j2hl.jpg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--cYTXDmSA--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zjltm00m6r29vg83j2hl.jpg" alt="Napoleon Bonaparte"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Napoleon said it first&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Never ascribe to malice that which is adequately explained by incompetence.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Napoleon Bonaparte in 1774&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You can use Hanlon's razor by not assuming there is malice, but trying to find an explanation that is more plausible. There are usually better explanations than "this person is out here to hurt me".&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#day-to-day-use"&gt;
  &lt;/a&gt;
  Day to day use
&lt;/h2&gt;

&lt;p&gt;As a Scrum master, I have interactions with all layers of the business. Other development teams, the management team, stakeholders. They all make decisions based on their own knowledge and opinions. In your career, there will be decisions that will have a negative impact on you or your team.&lt;/p&gt;

&lt;p&gt;In some cases, it can be easy to think someone's intent has malice. For example, another development team has made a decision in their architecture that really negatively impacts your team. It might be easy to reach the conclusion they have just done it on purpose because they just don't care about other teams.&lt;/p&gt;

&lt;p&gt;With Hanlon's razor, you can take a look at the different sides of the same coin. Try to take a look at the decision from their side. Are they really there to hurt your team? Or can this be explained by "incompetence". The other team might not know this impacts you, they might have thought of it and there are other things that are more important, etc. I have learned over the years this kind of situation almost never comes down to malice.&lt;/p&gt;

&lt;p&gt;Realizing that there usually is no ill intent in this kind of situation really helps me to stay positive. Other people or teams are not out there to hurt you on purpose. Instead of staying in this negative mindset "I can't believe they did this", you can focus your energy on the things that matter!&lt;/p&gt;

&lt;p&gt;What do you think of Hanlon's razor? Can you use it in your day-to-day work? Let me know in the comments down below!&lt;/p&gt;

&lt;p&gt;If you are interested in more things you can try to make your life more positive? I recommend checking &lt;a href="https://blog.jellesmeets.nl/articles/the-importance-of-off-screen-hobbies/"&gt;the Importance of off-screen hobbies&lt;/a&gt; blogpost.&lt;/p&gt;

</description>
      <category>career</category>
      <category>beginners</category>
      <category>mentalhealth</category>
      <category>discuss</category>
    </item>
    <item>
      <title>How I Increase my Productivity</title>
      <author>Harshit paliwal</author>
      <pubDate>Thu, 30 Sep 2021 09:32:05 +0000</pubDate>
      <link>https://dev.to/harshitpaliwal95/how-i-increase-my-productivity-ga6</link>
      <guid>https://dev.to/harshitpaliwal95/how-i-increase-my-productivity-ga6</guid>
      <description>&lt;h4&gt;You are here means you want you can improve your productivity too I can help you but first thing first. &lt;/h4&gt;

&lt;p&gt;Here are some quick tips.&lt;/p&gt;

&lt;p&gt;let us talk about productivity first.&lt;br&gt;
In simple words, productivity is a way to measure efficiency. you might know that already.&lt;/p&gt;

&lt;p&gt;now to improve productivity.&lt;br&gt;
I don’t want to lecture you about do this that but since I followed these things in my life I feel like how shorted life is the thing only we need is to go with the flow.&lt;/p&gt;

&lt;h2&gt;Prepare your morning&lt;/h2&gt;

&lt;p&gt;Yes, prepare your morning it doesn't mean wake up early at 4 am do work out take bath, and get back to work at 5:30 am 😂if you follow this routine then awesome it is cool you are a superhuman.&lt;/p&gt;

&lt;p&gt;But for a normal human like me I have an advice for you I know we sleep late at night because of study learning coding and some personal stuff it is okay &lt;b&gt;👊🏻wake up early &amp;lt; complete sleep&lt;/b&gt; so chill out no need to wake up at 4 or 5 &lt;br&gt;
just need to make a fixed time if you like 6 it is okay if you like 7 it is okay, just make a routine.&lt;br&gt;
Discipline is the only thing that can make you a real person tho,&lt;/p&gt;

&lt;p&gt;wake up in morning time as you like but fix the time you can wake up at that time totally after you decide &lt;br&gt;
do workout take a bath and done,&lt;br&gt;
your 1st hr of the day is only for yourself not for your boss/teacher/parents.&lt;br&gt;
Here is complete step one. &lt;/p&gt;

&lt;h2&gt;Prepare your surroundings&lt;/h2&gt;

&lt;p&gt;Hey!! 😂 surroundings I don't mean you wake up your sibling's parents early in the morning, &lt;br&gt;
What I mean is fix the thing you messed up it can be your bed your room check your desktop there might be useless folders everywhere 😂 ya I m serious fix everything that looks messed up the reason behind this is the atmosphere of your workstation room everything you see it can reflect in your mind too you don’t want to mess up your mind right then fix it now and see the change how you feeling after that.&lt;/p&gt;

&lt;h2&gt;Go with flow &lt;/h2&gt;

&lt;p&gt;Go with flow my friend don’t try every new thing you heard in a single day it might work or might not but after trying different things can decrease your will. no need to push your self no need to stop either be on the skateboard you can control and keep moving. &lt;/p&gt;

&lt;p&gt;The last thing is discipline is the only thing that can make you a real person I don’t want to sound like some old principle but it is the true start to be in discipline it can help you in many other ways.&lt;/p&gt;

&lt;p&gt;I am glad if this post helps you even a little bit.🥳&lt;/p&gt;

</description>
      <category>productivity</category>
    </item>
    <item>
      <title>Low-code—the future of app development</title>
      <author>Technocrat</author>
      <pubDate>Thu, 30 Sep 2021 09:21:09 +0000</pubDate>
      <link>https://dev.to/technocrat/low-code-the-future-of-app-development-44ij</link>
      <guid>https://dev.to/technocrat/low-code-the-future-of-app-development-44ij</guid>
      <description>&lt;p&gt;&lt;strong&gt;Introduction to low-code platforms&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;Low-code can be traced back to the 1980s, when technology thinkers conceptualized ways to make application development easier and more accessible to people. This was done using either 4th generation programming languages or rapid application development tools. But the lack of industry standards, looming security threats, and limited capabilities, made low-code platforms stay dormant until the 21st century. &lt;/p&gt;

&lt;p&gt;With the rise in cloud computing, form-based applications entered the market with a whole new visual app-building experience. This was the advent of low-code application development platforms (LCAP) in their current form, with Forrester Research defining the term "low-code" for the first time in 2014.&lt;/p&gt;

&lt;p&gt;Today, low-code application development is a methodology that tries to incorporate both UI-based app building and backend development using pro-code languages to make applications more powerful. These platforms are powered by cloud technology and hence follows the "write once, use anywhere" approach.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What's pro-code, no-code, and low-code?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pro-code&lt;/strong&gt; development has been and will always be, a part of software development. The most powerful software is hand coded by developers using the latest practices. But the question is, &lt;strong&gt;do we really need to put in the same effort for building applications that could run a business by configuring the backend, checking security threats, and manually updating each component?&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;No-code&lt;/strong&gt; development is predominantly form-based. These platforms promote visual app building using drag-and-drop techniques, wherein the backend of each component is already configured by the platform provider. Although no-code boasts a very short learning curve and helps citizen developers and non-programmers build high quality applications, &lt;strong&gt;the applications are not feature-heavy and powerful&lt;/strong&gt;. This leads to a rise in demand for other legacy software that can supplement the use case. This is where low-code comes into play, with the best of both worlds. &lt;/p&gt;

&lt;p&gt;Low-code platforms not only offer a visual app-building interface, like no-code platforms, but also accommodate compilers and interpreters, which gives these platforms pro-code capabilities. They're compliant with the latest data privacy and protection laws and provide unmatched security through audit logs and automated threat assessments. Applications built on low-code platforms are also mobile-ready; meaning you can build an application on the web and deploy it on your mobile device in a few clicks. Ready-to-use code snippets, built-in connectors, and automatic upgrades make low-code platforms an infinitely scalable solution. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Integrating low-code applications&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Low-code application development platforms can seamlessly integrate with almost any third-party application. These platforms have a "one-click configure" rule for APIs and setting up connectors to different services, which bypasses the need for excessive scripting. And if such a platform is part of a broader business suite, then the benefits can be tremendous. Integrations, once set up, can be used for all applications on an account. This saves considerable time, money, and effort. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why developers are moving to low-code&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;Pro-code development is here to stay, and there's no 100% replacement for it. But most problem statements can be tackled more efficiently using low-code methodologies. This logic applies not only to pro-code development but also to spreadsheets. For example, there are processes on spreadsheets that cannot be substituted—but that doesn't mean spreadsheets are the only tool!&lt;/p&gt;

&lt;p&gt;This void between pro-code and no-code development is seeing a flow of developers heading to low-code platforms. &lt;/p&gt;

&lt;p&gt;Some of the reasons why developers prefer to have a low-code platform on hand: &lt;/p&gt;

&lt;p&gt;-&amp;gt; &lt;strong&gt;Zero maintenance&lt;/strong&gt;&lt;br&gt;
From automatic updates to one-click server and API configurations, low-code platforms take care of all the middleware.&lt;/p&gt;

&lt;p&gt;-&amp;gt; &lt;strong&gt;Security and reliability&lt;/strong&gt; &lt;br&gt;
Security is handled by the service provider, which gives the developer ample time to focus on getting the apps market-ready and flawless.&lt;/p&gt;

&lt;p&gt;-&amp;gt; &lt;strong&gt;Easy collaboration&lt;/strong&gt; &lt;br&gt;
Data is stored on the cloud, and this allows developers from all over the world to collaborate in real time. &lt;/p&gt;

&lt;p&gt;-&amp;gt; &lt;strong&gt;Platform independence&lt;/strong&gt; &lt;br&gt;
All apps built on low-code platforms are not only mobile and tablet ready but also can be white labelled and made available on iOS and Android marketplaces. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Two cents&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Long story short, low-code development is on the rise and is showing no signs of slowing. Even the giants of the tech world are trying to make inroads into low-code application development. &lt;a href="https://www.researchandmarkets.com/reports/5184624/low-code-development-platform-market-research"&gt;Analysts&lt;/a&gt; predict this market to reach $187B by 2030 from a mere $10B in 2019. &lt;/p&gt;

&lt;p&gt;Cloud companies like &lt;a href="http://www.microsoft.com"&gt;Microsoft&lt;/a&gt; and &lt;a href="http://www.zoho.com/creator"&gt;Zoho&lt;/a&gt; offer platforms that can integrate with their multi-app business suite, as well as with an array of third-party services, in order make it a one-stop solution for businesses. This is in addition to standalone low-code companies like OutSystems, Mendix, and Quickbase, that aim to target specific markets, like business-IT and citizen development.&lt;/p&gt;

</description>
      <category>lowcode</category>
      <category>agile</category>
      <category>webdev</category>
      <category>programming</category>
    </item>
    <item>
      <title>Nebula Operator Kind, oneliner installer for Nebula K8s Operator Playground</title>
      <author>lisahui</author>
      <pubDate>Thu, 30 Sep 2021 08:54:38 +0000</pubDate>
      <link>https://dev.to/lisahui/nebula-operator-kind-oneliner-installer-for-nebula-k8s-operator-playground-27k8</link>
      <guid>https://dev.to/lisahui/nebula-operator-kind-oneliner-installer-for-nebula-k8s-operator-playground-27k8</guid>
      <description>&lt;p&gt;Nebula-Kind, an one-liner command to try K8s Operator based Nebula Graph Cluster on your machine, with the help of KIND (K8s in Docker)&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#nebulaoperatorkind"&gt;
  &lt;/a&gt;
  Nebula-Operator-Kind
&lt;/h2&gt;

&lt;p&gt;As a Cloud Native Distributed Database, Nebula Graph comes with an open-source K8s Operator to enable boostrap and maintain Nebula Graph Cluster from a K8s CRD.&lt;/p&gt;

&lt;p&gt;Normally it takes you some time to setup all the dependencies and control plane resources of the Nebula Operator. If you are as lazy as I am, this Nebula-Operator-Kind is made for you to quick start and play with Nebula Graph in KIND.&lt;/p&gt;

&lt;p&gt;Nebula-Operator-Kind is the one-liner for setup everything for you including:&lt;/p&gt;

&lt;p&gt;Docker&lt;br&gt;
K8s(KIND)&lt;br&gt;
PV Provider&lt;br&gt;
Nebula-Operator&lt;br&gt;
Nebula-Console&lt;br&gt;
nodePort for accessing the Cluster&lt;br&gt;
Kubectl for playing with KIND and Nebula Operator&lt;/p&gt;
&lt;h2&gt;
  &lt;a href="#how-to-use"&gt;
  &lt;/a&gt;
  How To Use
&lt;/h2&gt;

&lt;p&gt;Install Nebula-Operator-Kind:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;curl -sL nebula-kind.siwei.io/install.sh | bash
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;You will see this after it’s done&lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--bCI17jSy--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://siwei.io/en/nebula-operator-kind/install_success.webp" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--bCI17jSy--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://siwei.io/en/nebula-operator-kind/install_success.webp" alt="alt text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can connect to the cluster via ~/.nebula-kind/bin/console as below:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;~/.nebula-kind/bin/console -u user -p password --address=127.0.0.1 --port=30000
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#more"&gt;
  &lt;/a&gt;
  More
&lt;/h2&gt;

&lt;p&gt;It’s in GitHub with more information you may be intrested in ;-), please try and feedback there~ &lt;a href="https://github.com/wey-gu/nebula-operator-kind"&gt;https://github.com/wey-gu/nebula-operator-kind&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Install on KubeSphere all-in-on cluster：&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;curl -sL nebula-kind.siwei.io/install-ks-1.sh | bash
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Install on existing K8s cluster:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;curl -sL nebula-kind.siwei.io/install-on-k8s.sh | bash
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



</description>
      <category>database</category>
      <category>opensource</category>
      <category>devops</category>
      <category>sql</category>
    </item>
    <item>
      <title>Coffee &amp; Code: Integrating Live Chat Feature to Your React Application with Chatwoot by Olubisi Idris Ayinde</title>
      <author>AnkithaTech11</author>
      <pubDate>Thu, 30 Sep 2021 08:51:01 +0000</pubDate>
      <link>https://dev.to/aviyel/coffee-code-integrating-live-chat-feature-to-your-react-application-with-chatwoot-by-olubisi-idris-ayinde-1bhc</link>
      <guid>https://dev.to/aviyel/coffee-code-integrating-live-chat-feature-to-your-react-application-with-chatwoot-by-olubisi-idris-ayinde-1bhc</guid>
      <description>&lt;p&gt;&lt;strong&gt;Join here 📌&lt;/strong&gt;&lt;br&gt;
&lt;a href="https://bit.ly/2Y0k7pE"&gt;https://bit.ly/2Y0k7pE&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Coding &amp;amp; Coffee&lt;/strong&gt; ☕️ is Aviyel’s flagship upskilling series for programmers who are passionate about codes. We feature tutorials, live coding sessions, Q&amp;amp;As, with tabs on some of the fastest-growing open-source projects out there.&lt;/p&gt;

&lt;p&gt;This time, we are bringing on board Olubisi Idris Ayinde, an expert blogger, and a popular name in the developer circle.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#whats-in-it-for-you"&gt;
  &lt;/a&gt;
  What’s in it for you?
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;
Hands-on experience with Chatwoot, a dynamic, open-source customer engagement software for businesses and developers.&lt;/li&gt;
&lt;li&gt;
Chatwoot’s configuration guide, quick start with React for both beginners and advanced users.&lt;/li&gt;
&lt;li&gt;
Bombard speakers with your questions to dive deep into open source technology (and maybe start a full-time career).&lt;/li&gt;
&lt;li&gt;
Insights from Ayinde’s very own developer journey (inspiring heh?).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Get started with Chatwoot through Aviyel today!&lt;/p&gt;

&lt;p&gt;Mark your Calendar: &lt;strong&gt;📆 1st October 2021&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Time: &lt;strong&gt;10 AM EST | 7 AM PST&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Join here 📌&lt;/strong&gt;&lt;br&gt;
&lt;a href="https://bit.ly/2Y0k7pE"&gt;https://bit.ly/2Y0k7pE&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Impressed? Check out the rest of our  &lt;a href="https://bit.ly/3FdPfDh"&gt;events&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;PS: The session is free, but your pre-event confirmation is required. Confirm your presence by registering today.&lt;/p&gt;

&lt;p&gt;Let’s build open source together. 🚩&lt;br&gt;
Team Aviyel&lt;/p&gt;

</description>
      <category>chatwoot</category>
      <category>chatbot</category>
      <category>opensource</category>
      <category>developers</category>
    </item>
    <item>
      <title>Val - Powerful Static and Dynamic programming language</title>
      <author>alkaemy</author>
      <pubDate>Thu, 30 Sep 2021 08:46:57 +0000</pubDate>
      <link>https://dev.to/alkaemy/val-powerful-static-and-dynamic-programming-language-3go2</link>
      <guid>https://dev.to/alkaemy/val-powerful-static-and-dynamic-programming-language-3go2</guid>
      <description>&lt;p&gt;Hi All!,&lt;/p&gt;

&lt;p&gt;We have been working on a radically new programming language called &lt;strong&gt;Val&lt;/strong&gt; for a couple of months now.&lt;br&gt;
Val language is a static language with a compile-time dynamic language called &lt;strong&gt;Valet&lt;/strong&gt;. &lt;br&gt;
Think writing Python/Ruby/JS to generate C/Rust for quickly building Web, AI, Data-Intensive, and Low-Level applications.&lt;/p&gt;

&lt;p&gt;The documentation is here to give you a glimpse of Val. &lt;br&gt;
We are looking to raise funds for our funding platform, and we need to clarify some points before publishing the source code.&lt;/p&gt;

&lt;p&gt;We are at the beginning of our journey. Feedbacks, ideas, suggestions are highly appreciated.&lt;/p&gt;

&lt;p&gt;Check it out at &lt;a href="//www.val-lang.org"&gt;Val&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Stay tuned at:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="//www.reddit.com/r/vallang"&gt;Reddit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="//www.github.com/alkaemy/val-lang"&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/vallanguage"&gt;Twitter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thanks all!&lt;/p&gt;

</description>
      <category>javascript</category>
      <category>rust</category>
      <category>ruby</category>
      <category>python</category>
    </item>
    <item>
      <title>Web Animation with CSS - Animation Property</title>
      <author>Suraj Vishwakarma</author>
      <pubDate>Thu, 30 Sep 2021 08:45:29 +0000</pubDate>
      <link>https://dev.to/basecampxd/web-animation-with-css-animation-property-bdb</link>
      <guid>https://dev.to/basecampxd/web-animation-with-css-animation-property-bdb</guid>
      <description>&lt;h1&gt;
  &lt;a href="#introduction"&gt;
  &lt;/a&gt;
  Introduction
&lt;/h1&gt;

&lt;p&gt;Continuing with the series of &lt;strong&gt;Web Animation with CSS&lt;/strong&gt;. Today we are going to learn more about animation using CSS. &lt;/p&gt;

&lt;p&gt;The last part of the series paved the way to start animating in web pages for beginners. You can read &lt;a href="https://dev.to/basecampxd/web-animation-with-css-2c5d"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So let's get started with today's topic.&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#animation-direction"&gt;
  &lt;/a&gt;
  Animation Direction
&lt;/h2&gt;

&lt;p&gt;As the name suggests this property is used to alter the direction of animation. It has four values &lt;code&gt;normal&lt;/code&gt;, &lt;code&gt;reverse&lt;/code&gt;, &lt;code&gt;alternate&lt;/code&gt; and &lt;code&gt;alternate-reverse&lt;/code&gt;. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#code"&gt;
  &lt;/a&gt;
  Code
&lt;/h2&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight css"&gt;&lt;code&gt;&lt;span class="p"&gt;{&lt;/span&gt;
 &lt;span class="nl"&gt;animation-direction&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;reverse&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#codepen-example"&gt;
  &lt;/a&gt;
  Codepen Example
&lt;/h2&gt;

&lt;p&gt;&lt;iframe height="600" src="https://codepen.io/surajsrv11/embed/BaZGYGj?height=600&amp;amp;default-tab=result&amp;amp;embed-version=2"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;In the above example, I have used the &lt;code&gt;animation-direction&lt;/code&gt; with the value &lt;code&gt;alternate&lt;/code&gt;. The animation first started as normal and for 2nd time it reversed. This happens alternately once &lt;code&gt;normal&lt;/code&gt; and other times &lt;code&gt;reverse&lt;/code&gt;. It becomes a sequence of animation looping again and again.&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#animation-timing-function"&gt;
  &lt;/a&gt;
  Animation Timing Function
&lt;/h2&gt;

&lt;p&gt;The animation timing function helps you to control the animation speed curve. This curve defines the speed of animation transition at the different time frames. This curve helps you to make the transition smoothly.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#code"&gt;
  &lt;/a&gt;
  Code
&lt;/h2&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight css"&gt;&lt;code&gt;&lt;span class="p"&gt;{&lt;/span&gt;
 &lt;span class="nl"&gt;animation-timing-function&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;linear&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#codepen-example"&gt;
  &lt;/a&gt;
  CodePen Example
&lt;/h2&gt;

&lt;p&gt;&lt;iframe height="600" src="https://codepen.io/surajsrv11/embed/MWoxKpq?height=600&amp;amp;default-tab=result&amp;amp;embed-version=2"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;In the above Codepen, the ball is moving from top to bottom. The animation is not linear as the speed of transition changes. We have used &lt;code&gt;animation-timing-function&lt;/code&gt; with the value &lt;code&gt;ease&lt;/code&gt;. Using &lt;code&gt;ease&lt;/code&gt; value the animation has a slow start and end but fast in between slow and end. There is various value to animation timing function, you can learn more about it &lt;a href="https://www.w3schools.com/cssref/css3_pr_animation-timing-function.asp"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;One such value of the animation-timing function is &lt;code&gt;cubic-bezier&lt;/code&gt;. It let you create your timing function to control your animation. It takes 4 numeric values between 0 and 1. You can visit &lt;a href="https://cubic-bezier.com/"&gt;&lt;strong&gt;Cubic Bezier&lt;/strong&gt;&lt;/a&gt; to create your animation timing function.&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#animation-shorthand"&gt;
  &lt;/a&gt;
  Animation Shorthand
&lt;/h2&gt;

&lt;p&gt;Till now we have used lot of animation property to define animation such as &lt;code&gt;animation-direction&lt;/code&gt;, &lt;code&gt;animation-delay&lt;/code&gt;, &lt;code&gt;animation-timing-function&lt;/code&gt;, etc. You can use shorthand technique to define all animation related property into single property that is &lt;code&gt;animation&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#code"&gt;
  &lt;/a&gt;
  Code
&lt;/h2&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight css"&gt;&lt;code&gt;&lt;span class="p"&gt;{&lt;/span&gt;
 &lt;span class="nl"&gt;animation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;bounce&lt;/span&gt; &lt;span class="m"&gt;1s&lt;/span&gt; &lt;span class="n"&gt;infinite&lt;/span&gt; &lt;span class="n"&gt;ease&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#codepen-example"&gt;
  &lt;/a&gt;
  CodePen Example
&lt;/h2&gt;

&lt;p&gt;&lt;iframe height="600" src="https://codepen.io/surajsrv11/embed/YzQgwJp?height=600&amp;amp;default-tab=result&amp;amp;embed-version=2"&gt;
&lt;/iframe&gt;
&lt;/p&gt;




&lt;h1&gt;
  &lt;a href="#weekly-newsletter-of-surajondev"&gt;
  &lt;/a&gt;
  &lt;a href="https://www.getrevue.co/profile/surajondev"&gt;Weekly Newsletter of SurajOnDev&lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--cL1qWSEE--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/boivc5mdfzzs2kt0s5wg.PNG" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--cL1qWSEE--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/boivc5mdfzzs2kt0s5wg.PNG" alt="Weekly Newsletter of SurajOnDev"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#what-you-will-get"&gt;
  &lt;/a&gt;
  What you will get?
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Read of the Week&lt;/strong&gt;: 5 best articles hand-picked by myself from different platforms. This article will be developer, self-growth, and productivity-oriented.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Tool of the Week&lt;/strong&gt;: A resource or tool link that will help in easing your work.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Our latest blog post&lt;/strong&gt;: Latest 3 blog post from SurajOnDev that is me.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Free eBook and Resources&lt;/strong&gt;: Occasionally you will get free eBook that are by developers and for developers.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Frequency&lt;/strong&gt;: Weekly&lt;br&gt;
&lt;strong&gt;Subscribe Here&lt;/strong&gt;:  &lt;a href="https://tripetto.app/run/4OQIUAO89L"&gt;Weekly Newsletter of SurajOnDev&lt;/a&gt;&lt;/p&gt;




&lt;h1&gt;
  &lt;a href="#last-note"&gt;
  &lt;/a&gt;
  Last Note
&lt;/h1&gt;

&lt;p&gt;Now with these properties of CSS, you can have more control over your animation.&lt;/p&gt;

&lt;p&gt;I highly recommend you to read our previous blog post of this series &lt;a href="https://dev.to/basecampxd/web-animation-with-css-2c5d"&gt;Web Animation with CSS - Learn the Basics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thank You for reading the blog post.&lt;/p&gt;

</description>
      <category>css</category>
      <category>webdev</category>
      <category>javascript</category>
      <category>beginners</category>
    </item>
    <item>
      <title>Modernizing Amazon database infrastructure - migrating from Oracle to AWS | AWS White Paper Summary </title>
      <author>Haytham Mostafa</author>
      <pubDate>Thu, 30 Sep 2021 08:40:20 +0000</pubDate>
      <link>https://dev.to/awsmenacommunity/modernizing-amazon-database-infrastructure-migrating-from-oracle-to-aws-aws-white-paper-summary-ej7</link>
      <guid>https://dev.to/awsmenacommunity/modernizing-amazon-database-infrastructure-migrating-from-oracle-to-aws-aws-white-paper-summary-ej7</guid>
      <description>&lt;h1&gt;
  &lt;a href="#1-challenges-with-using-oracle-databases"&gt;
  &lt;/a&gt;
  1. Challenges with using Oracle databases
&lt;/h1&gt;

&lt;p&gt;Amazon started facing a number of challenges with using Oracle databases to scale its services. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#11-complex-database-engineering-required-to-scale"&gt;
  &lt;/a&gt;
  1.1   Complex database engineering required to scale
&lt;/h2&gt;

&lt;p&gt;• Hundreds of hours spent each year trying to scale the Oracle databases horizontally. &lt;br&gt;
• Database shards was used to handle the additional service throughputs and manage the growing data volumes but in doing so increased the database administration workloads.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#12-complex-expensive-and-errorprone-database-administration"&gt;
  &lt;/a&gt;
  1.2   Complex, expensive, and error-prone database administration
&lt;/h2&gt;

&lt;p&gt;Hundreds of hours spent each month monitoring database performance, upgrading, database backups and patching the operating system for each instance and shard. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#13-inefficient-and-complex-hardware-provisioning"&gt;
  &lt;/a&gt;
  1.3   Inefficient and complex hardware provisioning
&lt;/h2&gt;

&lt;p&gt;• Database and the infrastructure teams expended substantial time forecasting demand and planning hardware capacity to meet it. &lt;br&gt;
• After forecasting, hundreds of hours spent in purchasing, installing, and testing the hardware.&lt;br&gt;
•  Additionally, teams had to maintain a sufficiently large pool of spare infrastructure to fix any hardware issues and perform preventive maintenance. &lt;br&gt;
• The high licensing costs were just some of the compelling reasons for the Amazon consumer and digital business to migrate the persistence layer of all its services to AWS.&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#2-aws-services"&gt;
  &lt;/a&gt;
  2. AWS Services
&lt;/h1&gt;

&lt;p&gt;Overview about the key AWS database Services. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#21-purposebuilt-databases"&gt;
  &lt;/a&gt;
  2.1   Purpose-built databases
&lt;/h2&gt;

&lt;p&gt;• Amazon expects all its services be globally available, operate with microsecond to millisecond latency, handle millions of requests per second, operate with near zero downtime, cost only what is needed, and be managed efficiently by offering a range of purpose-built databases.&lt;br&gt;
The three key database services to host the persistence layer of their services: &lt;br&gt;
&lt;a href="https://aws.amazon.com/dynamodb"&gt;Amazon DynamoDB&lt;/a&gt; &lt;br&gt;
&lt;a href="https://aws.amazon.com/rds/aurora/?aurora-whats-new.sort-by=item.additionalFields.postDateTime&amp;amp;aurora-whats-new.sort-order=desc"&gt;Amazon Aurora&lt;/a&gt; &lt;br&gt;
&lt;a href="https://aws.amazon.com/rds/"&gt;Amazon Relational Database Service (Amazon RDS) for MySQL or PostgreSQL&lt;/a&gt; &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#22-other-aws-services-used-in-implementation"&gt;
  &lt;/a&gt;
  2.2   Other AWS Services used in implementation
&lt;/h2&gt;

&lt;p&gt;• &lt;a href="https://aws.amazon.com/s3"&gt;Amazon Simple Storage Service (Amazon S3)&lt;/a&gt; &lt;br&gt;
• &lt;a href="https://aws.amazon.com/dms"&gt;AWS Database Migration Service&lt;/a&gt;&lt;br&gt;
• &lt;a href="https://aws.amazon.com/ec2"&gt;Amazon Elastic Compute Cloud (Amazon EC2)&lt;/a&gt; &lt;br&gt;
• &lt;a href="https://aws.amazon.com/emr"&gt;Amazon EMR&lt;/a&gt;&lt;br&gt;
• &lt;a href="https://aws.amazon.com/glue"&gt;AWS Glue&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#23-picking-the-right-database"&gt;
  &lt;/a&gt;
  2.3   Picking the right database
&lt;/h2&gt;

&lt;p&gt;• Pick the most appropriate database based on scale, complexity, and features of its service.&lt;br&gt;
• Business units running services that use relatively static schemas, perform complex table lookups, and experience high service throughputs picked Amazon Aurora. &lt;br&gt;
•  Business units using operational data stores that had moderate read and write traffic, and relied on the features of relational databases selected Amazon RDS for PostgreSQL or MySQL.&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#3-challenges-during-migration"&gt;
  &lt;/a&gt;
  3. Challenges during migration
&lt;/h1&gt;

&lt;p&gt;The key challenges faced by Amazon during the transformation journey.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#31-diverse-application-architectures-inherited"&gt;
  &lt;/a&gt;
  3.1   Diverse application architectures inherited
&lt;/h2&gt;

&lt;p&gt;• Amazon has been defined by a culture of decentralized ownership that offered engineers the freedom to make design decisions that would deliver value to their customers. This freedom proliferated a wide range of design patterns and frameworks across teams. Another source of diversity was infrastructure management and its impact on service architectures.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#32-distributed-and-geographically-dispersed-teams"&gt;
  &lt;/a&gt;
  3.2   Distributed and geographically dispersed teams
&lt;/h2&gt;

&lt;p&gt;• Amazon operates in a range of customer business segments in multiple geographies which operate independently. &lt;br&gt;
• Managing the migration program across this distributed workforce posed challenges including effectively communicating the program vision and mission, driving goal alignment with business and technical leaders across these businesses, defining and setting acceptable yet ambitious goals for each business units, and dealing with conflicts. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#33-interconnected-and-highly-interdependent-services"&gt;
  &lt;/a&gt;
  3.3   Interconnected and highly interdependent services
&lt;/h2&gt;

&lt;p&gt;Amazon operates a vast set of microservices that are interconnected and use common databases. Migrating interdependent and interconnected services and their underlying databases required finely coordinated movement between teams. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#34-gap-in-skills"&gt;
  &lt;/a&gt;
  3.4   Gap in skills
&lt;/h2&gt;

&lt;p&gt;As Amazon engineers used Oracle databases, they developed expertise over the years in operating, maintaining, and optimizing them. Most service teams shared databases that were managed by a shared pool of database engineers and the migration to AWS was a paradigm shift for them. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#35-competing-initiatives"&gt;
  &lt;/a&gt;
  3.5   Competing initiatives
&lt;/h2&gt;

&lt;p&gt;Lastly, each business unit was grappling with competing initiatives. In certain situations, competing priorities created resource conflicts that required intervention from the senior leadership. &lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#4-people-processes-and-tools"&gt;
  &lt;/a&gt;
  4. People, processes, and tools
&lt;/h1&gt;

&lt;p&gt;The following three sections discuss how three levers were engaged to drive the project forward. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#41-people"&gt;
  &lt;/a&gt;
  4.1   People
&lt;/h2&gt;

&lt;p&gt;One of the pillars of success was founding the Center of Excellence (CoE). &lt;br&gt;
The CoE was staffed with experienced enterprise program managers.&lt;br&gt;
The leadership team ensured that these program managers had a combination of technical knowledge and program management capabilities. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#42-processes-and-mechanisms"&gt;
  &lt;/a&gt;
  4.2   Processes and mechanisms
&lt;/h2&gt;

&lt;p&gt;This section elaborates on the processes and mechanisms established by the CoE and their impact on the outcome of the project. &lt;br&gt;
&lt;strong&gt;Goal setting and leadership review&lt;/strong&gt; &lt;br&gt;
It was realized early in the project that the migration would require attention from senior leaders. They used the review meeting to highlight systemic risks, recurrent issues, and progress.&lt;br&gt;
&lt;strong&gt;Establishing a hub-and-spoke model&lt;/strong&gt; &lt;br&gt;
It would be arduous to individually track the status of each migration. Therefore, they established a hub-and-spoke model where service teams nominated a team member, typically a technical program manager, who acted as the spoke and the CoE program managers were the hub.&lt;br&gt;
&lt;strong&gt;Training and guidance&lt;/strong&gt; &lt;br&gt;
A key objective for the CoE was to ensure that Amazon engineers were comfortable moving their services to AWS. To achieve this, it was essential to train these teams on open source and AWS native databases, and cloud-based design patterns.&lt;br&gt;
&lt;strong&gt;Establishing product feedback cycles with AWS&lt;/strong&gt; &lt;br&gt;
This feedback mechanism was instrumental in helping AWS rapidly test and release features to support internet scale workloads. This feedback mechanism also enabled AWS to launch product features essential for its other customers operating similar sized workloads. &lt;br&gt;
&lt;strong&gt;Establishing positive reinforcement&lt;/strong&gt; &lt;br&gt;
To ensure that teams make regular progress towards goals, it is important to promote and reinforce positive behaviors, recognize teams, and celebrate their progress. The CoE established multiple mechanisms to achieve this.&lt;br&gt;
&lt;strong&gt;Risk management and issue tracking&lt;/strong&gt; &lt;br&gt;
Enterprise scale projects involving large numbers of teams across geographies are bound to face issues and setbacks.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#43-tools"&gt;
  &lt;/a&gt;
  4.3   Tools
&lt;/h2&gt;

&lt;p&gt;Due to the complexity of the project management process, the CoE decided to invest in tools that would automate the project management and tracking.&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#5-common-migration-patterns-and-strategies"&gt;
  &lt;/a&gt;
  5. Common migration patterns and strategies
&lt;/h1&gt;

&lt;p&gt;The following section describes the migration of four systems used in Amazon from Oracle to AWS. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#51-migrating-to-amazon-dynamodb-flash"&gt;
  &lt;/a&gt;
  5.1   Migrating to Amazon DynamoDB – FLASH
&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Overview of FLASH&lt;/strong&gt; &lt;br&gt;
• Set of critical services called the Financial Ledger and Accounting Systems Hub (FLASH).&lt;br&gt;
• Enable various business entities to post financial transactions to Amazon’s sub-ledger.&lt;br&gt;
• It supports four categories of transactions compliant with Generally Accepted Accounting Principles (GAAP)—account receivables, account payables, remittances, and payments. &lt;br&gt;
• FLASH aggregates these sub-ledger transactions and populates them to Amazon’s general ledger for financial reporting, auditing, and analytics. &lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--yBDe5UTR--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/off6apeqwwplwnv49l56.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--yBDe5UTR--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/off6apeqwwplwnv49l56.png" alt="FLASH"&gt;&lt;/a&gt; &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; &lt;em&gt;Data flow diagram of FLASH&lt;/em&gt; &amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&lt;br&gt;
&lt;strong&gt;Challenges with operating FLASH services on Oracle&lt;/strong&gt; &lt;br&gt;
FLASH is a high-throughput, complex, and critical system at Amazon. It experienced many challenges while operating on Oracle databases. &lt;br&gt;
&lt;strong&gt;a.    Poor latency&lt;/strong&gt; &lt;br&gt;
The poor service latency despite having performed extensive database optimization.&lt;br&gt;
&lt;strong&gt;b.    Escalating database costs&lt;/strong&gt; &lt;br&gt;
Each year, the database hosting costs were growing by at least 10%, and the FLASH team was unable to circumvent the excessive database administration overhead associated with this growth. &lt;br&gt;
&lt;strong&gt;c.    Difficult to achieve scale&lt;/strong&gt; &lt;br&gt;
As FLASH used a monolithic Oracle database service, the interdependencies between the various components of the FLASH system were preventing efficient scaling of the system. &lt;br&gt;
&lt;strong&gt;Reasons to choose Amazon DynamoDB as the persistence layer&lt;/strong&gt; &lt;br&gt;
a.  Easier to scale &lt;br&gt;
b.  Easier change management &lt;br&gt;
c.  Speed of transactions &lt;br&gt;
d.  Easier database management &lt;br&gt;
&lt;strong&gt;Challenges and design considerations during refactoring&lt;/strong&gt; &lt;br&gt;
The FLASH team faced the following challenges during the re-design of its services on DynamoDB: &lt;br&gt;
&lt;strong&gt;a.    Time stamping transactions and indexed ordering&lt;/strong&gt; &lt;br&gt;
After a timestamp was assigned, these transactions were logged in a S3 bucket for durable backup. DynamoDB Streams along with Amazon Kinesis Client Libraries were used to ensure exactly-once, ordered indexing of records. When enabled, DynamoDB Streams captures a time-ordered sequence of item-level modifications in a DynamoDB table and durably stores the information for up to 24 hours. Applications can access a series of stream records, which contain an item change, from a DynamoDB stream in near real time. &lt;br&gt;
After a transaction appears on the DynamoDB stream, it is routed to a Kinesis stream and indexed.&lt;br&gt;
&lt;strong&gt;b.    Providing data to downstream services&lt;/strong&gt; &lt;br&gt;
• Enable financial analytics. &lt;br&gt;
• FLASH switched the model to an event-sourcing model where an S3 backup of commit logs was created continuously. &lt;br&gt;
• The use of unstructured and disparate tables was eliminated for analytics and data processing. &lt;br&gt;
• The team created a single source of truth and converged all the data models to the core event log/journal, to ensure deterministic data processing.&lt;br&gt;
•  Amazon S3 was used as an audit trail of all changes to the DynamoDB journal table. &lt;br&gt;
• &lt;a href="https://aws.amazon.com/sns/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;amp;whats-new-cards.sort-order=desc"&gt;Amazon SNS&lt;/a&gt; was used to publish these commit logs in batches for downstream consumption. &lt;br&gt;
• The artifact creation was coordinated using &lt;a href="https://aws.amazon.com/sqs/"&gt;Amazon SQS&lt;/a&gt;. The entire system is SOX compliant. &lt;br&gt;
• These data batches were delivered to the general ledger for financial reporting and analysis. &lt;br&gt;
&lt;strong&gt;c.    Archiving historical data&lt;/strong&gt; &lt;br&gt;
FLASH used a common data model and columnar format for ease of access and migrated historical data to Amazon S3 buckets that are accessible by Amazon Athena. Amazon Athena was ideal as it allows for a query-as-you-go model which works well as this data is queried on average once every two years. Also, because Amazon Athena is serverless.&lt;br&gt;
&lt;strong&gt;Performing data backfill&lt;/strong&gt; &lt;br&gt;
&lt;a href="https://aws.amazon.com/dms/"&gt;AWS DMS&lt;/a&gt; was used to ensure reliable and secure data transfer. It is also SOX compliant from source to target, provided the team granular insights during the process. &lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--pR2brmZh--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9n6t7hhi5y5b29rf5jjl.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--pR2brmZh--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9n6t7hhi5y5b29rf5jjl.png" alt="DMS"&gt;&lt;/a&gt; &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; &lt;em&gt;Lift and shift using AWS DMS and RDS&lt;/em&gt; &amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&lt;br&gt;
&lt;strong&gt;Benefits&lt;/strong&gt; &lt;br&gt;
• Rearchitecting the FLASH system to work on AWS database services improved its performance. &lt;br&gt;
• Although FLASH provisioned more compute and larger storage, the database operating costs have remained flat or reduced despite processing higher throughputs. &lt;br&gt;
• The migration reduced administrative overhead enabling focus on optimizing the application. &lt;br&gt;
• Automatic scaling has also allowed the FLASH team to reduce costs.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#52-migration-to-amazon-dynamodb-items-and-offers"&gt;
  &lt;/a&gt;
  5.2   Migration to Amazon DynamoDB – Items and Offers
&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Overview of Items and Offers&lt;/strong&gt; &lt;br&gt;
• A system manages three components associated with an item – item information, offer information, and relationship information.&lt;br&gt;
• A key service within the Items and Offers system is the Item Service which updates the item information.&lt;br&gt;
&lt;strong&gt;Challenges faced when operating Item Service on Oracle databases&lt;/strong&gt; &lt;br&gt;
The Item Service team was facing many challenges when operating on Oracle databases.&lt;br&gt;
&lt;strong&gt;Challenging to administer partitions&lt;/strong&gt; &lt;br&gt;
The Item data was partitioned using hashing, and partition maps were used to route requests to the correct partition. These partitioned databases were becoming difficult to scale and manage.&lt;br&gt;
&lt;strong&gt;Difficult to achieve high availability&lt;/strong&gt; &lt;br&gt;
To optimize space utilization by the databases, all tables were partitioned and stored across 24 databases. &lt;br&gt;
&lt;strong&gt;Reaching scaling limits&lt;/strong&gt; &lt;br&gt;
Due to the preceding challenges of operating the Items and Offers system on Oracle databases, the team was not able to support the growing service throughputs.&lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--_fr_oy3p--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/0dj7frnv3w0t2u09min5.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--_fr_oy3p--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/0dj7frnv3w0t2u09min5.png" alt="Item"&gt;&lt;/a&gt; &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; &lt;em&gt;Scale of the Item Service&lt;/em&gt; &amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&lt;br&gt;
&lt;strong&gt;Reasons for choosing Amazon DynamoDB&lt;/strong&gt; &lt;br&gt;
Amazon DynamoDB was the best suited persistence layer for IMS. It offered an ideal combination of features suited for easily operating a highly available and large-scale distributed system like IMS.&lt;br&gt;
&lt;strong&gt;a.    Automated database management&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;b.    Automatic scaling&lt;/strong&gt; &lt;br&gt;
&lt;strong&gt;c.    Cost effective and secure&lt;/strong&gt;&lt;br&gt;
The following figure displays one of the index tables on Oracle that stored SKU to ASIN mappings.&lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--_kr8SYh8--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rz34ythkr39nfmsui1bs.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--_kr8SYh8--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rz34ythkr39nfmsui1bs.png" alt="Oracle"&gt;&lt;/a&gt; &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; &lt;em&gt;Table structure of Item Service on Oracle&lt;/em&gt; &amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&lt;br&gt;
The following figure shows the equivalent table represented in DynamoDB. All other Item Service schemas were redesigned using similar principles.&lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--WsqJq_PH--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/szsy8ezndrsermo2ee16.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--WsqJq_PH--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/szsy8ezndrsermo2ee16.png" alt="DynamoDB"&gt;&lt;/a&gt; &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; &lt;em&gt;Table structure of Item Service on DynamoDB&lt;/em&gt; &amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&lt;br&gt;
&lt;strong&gt;Execution&lt;/strong&gt; &lt;br&gt;
After building the new data model, the next challenge was performing the migration. The Item Service team devised a two-phased approach to achieve the migration — live migration and &lt;br&gt;
backfill.&lt;br&gt;
&lt;strong&gt;i.    Live migration&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Transition the main store from Oracle to DynamoDB without any failures and actively migrate all the data being processed by the application.&lt;/li&gt;
&lt;li&gt;The item Service team used three stages to achieve the goal:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;a. The copy mode: Validate the correctness, scale, and performance of DynamoDB.&lt;br&gt;
b. The compatibility mode: Allowed the Item Service team to pause the migration should issues arise.&lt;br&gt;
c. The move mode: After the move mode, the Item Service team began the backfill phase of migration that would make DynamoDB the single main database and deprecate Oracle.&lt;br&gt;
&lt;strong&gt;ii.   Backfill&lt;/strong&gt; &lt;br&gt;
• AWS DMS was used to backfill records that were not migrated by the application write logic. &lt;br&gt;
• Oracle source tables were partitioned across 24 databases and the destination store on DynamoDB was elastically scalable. &lt;br&gt;
• The migration has scaled by running multiple AWS DMS replication instances per table and each instance had parallel loads configured. &lt;br&gt;
• To handle AWS DMS replication errors, the process automated by creating a library using the AWS DMS SDK. &lt;br&gt;
• Finally, fine tune configurations on AWS DMS and Amazon DynamoDB to maximize the throughput and minimize cost. &lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--VaIOle-e--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/76oiw5je6m5ze2roznis.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--VaIOle-e--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/76oiw5je6m5ze2roznis.png" alt="IMS"&gt;&lt;/a&gt; &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; &lt;em&gt;Backfill process of IMS&lt;/em&gt; &amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&lt;br&gt;
&lt;strong&gt;Benefits&lt;/strong&gt; &lt;br&gt;
After the migration, the availability of Item Service has improved, ensuring consistent performance and significantly reduced the operational workload for the team. Also, the team used the point-in-time recovery feature to simplify backup and restore operations. The team received these benefits at a lower overall cost than previously, due to dynamic automatic scaling capacity feature. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#53-migrating-to-aurora-for-postgresql-amazon-fulfillment-technologies-aft"&gt;
  &lt;/a&gt;
  5.3   Migrating to Aurora for PostgreSQL – Amazon Fulfillment Technologies (AFT)
&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Overview of AFT&lt;/strong&gt; &lt;br&gt;
The Amazon Fulfillment Technologies (AFT) business unit builds and maintains the dozens of services that facilitate all fulfillment activities. A set of services called the Inventory Management Services facilitate inventory movement and are used by all other major services to perform critical functions within the FC.&lt;br&gt;
&lt;strong&gt;Challenges faced operating AFT on Oracle databases&lt;/strong&gt; &lt;br&gt;
The AFT team faced many challenges operating its services on Oracle databases in the past. &lt;br&gt;
&lt;strong&gt;a.    Difficult to scale&lt;/strong&gt; &lt;br&gt;
All the services were becoming difficult to scale and were facing availability issues during peak throughputs due to both hardware and software limitations. &lt;br&gt;
&lt;strong&gt;b.    Complex hardware management&lt;/strong&gt; &lt;br&gt;
Hardware management was also becoming a growing concern due to the custom hardware requirements required from these Oracle clusters.&lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--JuLtYTGd--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/h0v2xqkjlvbcr8yv02tk.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--JuLtYTGd--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/h0v2xqkjlvbcr8yv02tk.png" alt="AFT"&gt;&lt;/a&gt; &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; &lt;em&gt;Databases services used by AFT&lt;/em&gt; &amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&lt;br&gt;
&lt;strong&gt;Reasons for choosing Amazon Aurora for PostgreSQL&lt;/strong&gt; &lt;br&gt;
Picking Amazon Aurora for three primary reasons. &lt;br&gt;
a.  Static schemas and relational lookups. &lt;br&gt;
b.  Ease of scaling and feature parity. &lt;br&gt;
c.  Automated administration. &lt;/p&gt;

&lt;p&gt;Before the migration, the team decided to re-platform the services rather than rearchitect them. Re-platforming accelerated the migration by preserving the existing architecture while minimizing service disruptions.&lt;br&gt;
&lt;strong&gt;Migration strategy and challenges:&lt;/strong&gt; &lt;br&gt;
The migration to Aurora was performed in three phases:&lt;br&gt;
&lt;strong&gt;a.    Preparation phase&lt;/strong&gt; &lt;br&gt;
• Separate production and non-production accounts to ensure secure and reliable deployment. &lt;br&gt;
• Aurora offers fifteen near real-time read replicas while a central node manages all writes.&lt;br&gt;
• Aurora uses SSL (AES-256) to secure connections between the database and the application.&lt;br&gt;
Important differences to note are:&lt;br&gt;
i.  How Oracle and PostgreSQL treat time zones differently.&lt;br&gt;
ii. Oracle and PostgreSQL 9.6 is different partitioning strategies and their implementations.&lt;br&gt;
&lt;strong&gt;b.    Migration phase&lt;/strong&gt; &lt;br&gt;
&lt;a href="https://aws.amazon.com/dms/schema-conversion-tool/"&gt;AWS SCT&lt;/a&gt; was used to convert the schemas from Oracle to PostgreSQL. Subsequently DMS performed a full load and ongoing Change Data Capture (CDC) replication to move real-time transactional data.&lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--KiFfzUfR--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/36s9pwoqr612zyf4mvmm.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--KiFfzUfR--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/36s9pwoqr612zyf4mvmm.png" alt="SCT"&gt;&lt;/a&gt; &amp;gt;&amp;gt;&amp;gt; &lt;em&gt;Steps in the migration of schemas using AWS SCT and AWS DMS&lt;/em&gt; &amp;lt;&amp;lt;&amp;lt;&lt;br&gt;
The maxFileSize parameter specifies the maximum size (in KB) of any CSV file used to transfer data to PostgreSQL. It was observed that setting maxFileSize to 1.1 GB significantly improved migration speed. Since version 2.x AWS DMS has been to increase this parameter to 30 GB.&lt;br&gt;
&lt;strong&gt;c.    Post-migration phase&lt;/strong&gt; &lt;br&gt;
Monitoring the health of the database becomes paramount in this phase.One important activity that must occur in PostgreSQL is vacuuming. Aurora PostgreSQL sets auto-vacuum settings according to instance size by default, but one size does not always fit all different workloads, so it is important to ensure auto-vacuum is working properly as expected. &lt;br&gt;
&lt;strong&gt;Benefits&lt;/strong&gt; &lt;br&gt;
• After migrating to Amazon Aurora, provisioning additional capacity is achieved through a few simple mouse clicks or API calls reducing the scaling effort by as much as 95%.&lt;br&gt;
• High availability is another key benefit of Amazon Aurora.&lt;br&gt;
• The business unit is no longer limited by the input/output operations.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#54-migrating-to-amazon-aurora-buyer-fraud-detection"&gt;
  &lt;/a&gt;
  5.4   Migrating to Amazon Aurora – buyer fraud detection
&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt; &lt;br&gt;
Amazon retail websites operate a set of services called Transaction Risk Management Services (TRMS) to protect brands, sellers, and consumers from transaction fraud by actively detecting and preventing it. The Buyer Fraud Service applies machine learning algorithms over real-time and historical data to detect and prevent fraudulent activity.&lt;br&gt;
Challenges of operating on oracle &lt;br&gt;
The Buyer Fraud Service team faced three challenges operating its services using on-premises Oracle databases. &lt;br&gt;
&lt;strong&gt;a.    Complex error-prone database administration&lt;/strong&gt; &lt;br&gt;
The Buyer Fraud Service business unit shared an Oracle cluster of more than one hundred databases with other fraud detection services at Amazon.&lt;br&gt;
&lt;strong&gt;b.    Poor latency&lt;/strong&gt; &lt;br&gt;
To maintain performance at scale, Oracle databases were horizontally partitioned. As application code required new database shards to handle the additional throughput, each shard added incremental workload on the infrastructure in terms of backups, patching, and performance. &lt;br&gt;
&lt;strong&gt;c.    Complication hardware provisioning&lt;/strong&gt; &lt;br&gt;
After capacity planning, the hardware business unit coordinated suppliers, vendors, and Amazon finance business units to purchase the hardware and prepare for installation and testing.&lt;br&gt;
Application design and migration strategy &lt;br&gt;
The Buyer Fraud Service business unit decided to migrate its databases from Oracle to Amazon Aurora. The team chose to re-factor the service to accelerate the migration and minimize service disruption. The migration was accomplished in two phases: &lt;br&gt;
&lt;strong&gt;i.    Preparation phase&lt;/strong&gt; &lt;br&gt;
• Amazon Aurora clusters were launched to replicate the existing Oracle databases. &lt;br&gt;
• A shim layer has built to perform simultaneous r/w operations to both database engines. &lt;br&gt;
• The business unit migrated the initial data, and used AWS DMS to establish active replication from Oracle to Aurora. &lt;br&gt;
• Once the migration was complete, AWS DMS was used to perform a row-by-row validation and a sum count to ensure that the replication was accurate.&lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--sJ_fUwX0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5obu46rwwilvsipz7erj.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--sJ_fUwX0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/5obu46rwwilvsipz7erj.png" alt="SHIM"&gt;&lt;/a&gt; &amp;gt;&amp;gt;&amp;gt;&amp;gt; &lt;em&gt;Dual write mode of the Buyer Fraud Service using SHIM layer&lt;/em&gt; &amp;lt;&amp;lt;&amp;lt;&amp;lt;&lt;br&gt;
&lt;strong&gt;ii.   Execution phase&lt;/strong&gt; &lt;br&gt;
Buyer Fraud Service began load testing the Amazon Aurora databases to evaluate read/write latencies and simulate peak throughput events such as Prime Day. Results from these load tests indicated that Amazon Aurora could handle twice the throughput of the legacy infrastructure. &lt;br&gt;
&lt;strong&gt;Benefits&lt;/strong&gt; &lt;br&gt;
• Performance, scalability, availability, hardware management, cloud-based automation, and cost. &lt;br&gt;
• AWS manages patching, maintenance, backups, and upgrades improved the application performance.&lt;br&gt;
• The migration has also lowered the cost of delivering the same performance as before.&lt;br&gt;
• The improved performance of Amazon Aurora has allowed to handle high throughput.&lt;br&gt;
• Buyer Fraud service was able to scale its largest workloads, support strict latency requirements with no impact to snapshot backups.&lt;br&gt;
• Hardware management has gotten exponentially easier with new hardware being commissioned in minutes instead of months.&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#6-organizationwide-benefits"&gt;
  &lt;/a&gt;
  6. Organization-wide benefits
&lt;/h1&gt;

&lt;p&gt;• Services that migrated to DynamoDB, saw significant performance improvements such as a 40% drop in 99th percentile latency, OS patching, database maintenance and software upgrades.&lt;br&gt;
• Additionally, the elastic capacity of preconfigured database hosts on AWS has eliminated administrative overhead to scale by allowing for capacity provisioning. &lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#7-postmigration-operating-model"&gt;
  &lt;/a&gt;
  7. Post-migration operating model
&lt;/h1&gt;

&lt;p&gt;This section discusses key changes in the operating model for service teams and its benefits. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#71-distributed-ownership-of-databases"&gt;
  &lt;/a&gt;
  7.1   Distributed ownership of databases
&lt;/h2&gt;

&lt;p&gt;• The migration transformed the operating model to one focused on distributed ownership. &lt;br&gt;
• Individual teams now control every aspect of their infrastructure including capacity provisioning, forecasting and cost allocation. &lt;br&gt;
• Each team also had the option to launch Reserved or On-Demand Instances to optimize costs based on the nature of demand. &lt;br&gt;
• The CoE developed heuristics to identify the optimal ratio of On-Demand to Reserved Instances based on service growth, cyclicality, and price discounts. &lt;br&gt;
• Focusing on innovation on behalf of customers. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#72-career-growth"&gt;
  &lt;/a&gt;
  7.2   Career growth
&lt;/h2&gt;

&lt;p&gt;The migration presented an excellent opportunity to advance the career paths of scores of database engineers. These engineers who exclusively managed Oracle databases in data centers were offered new avenues of growth and development in the rapidly growing field of cloud services, NoSQL databases, and open-source databases.&lt;/p&gt;

</description>
      <category>aws</category>
      <category>database</category>
    </item>
  </channel>
</rss>
