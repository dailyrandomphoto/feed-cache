<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>DEV Community</title>
    <author>DEV Community</author>
    <description>A constructive and inclusive social network for software developers. With you every step of your journey.</description>
    <link>https://dev.to</link>
    <language>en</language>
    <item>
      <title>Cryptocurrency exchange software and their types</title>
      <author>Mathew Broman</author>
      <pubDate>Fri, 17 Dec 2021 10:46:06 +0000</pubDate>
      <link>https://dev.to/mathew_b/cryptocurrency-exchange-software-and-their-types-16lc</link>
      <guid>https://dev.to/mathew_b/cryptocurrency-exchange-software-and-their-types-16lc</guid>
      <description>&lt;p&gt;&lt;strong&gt;Introduction:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A cryptocurrency exchange software is a ready-to-deploy, ready-made software using which you can launch an entirely operable cryptocurrency exchange. By using a crypto exchange software, you can deploy a feature-rich, technically advanced cryptocurrency exchange platform where anyone is entitled to buy, sell and trade cryptocurrencies.&lt;/p&gt;

&lt;p&gt;Just like conventional trading exchanges, cryptocurrency exchange facilitates trade activities involving digital currencies. You can carry out buy, sell, trade or hold cryptocurrency and gain considerable profits from it.The recent years have witnessed a mass adoption of cryptocurrencies. On seeing the potential profit margin combined with the user count, several new startups have emerged. By taking advantage of the increase in demand for a new business, entrepreneurs started to inherit cryptocurrency exchange for business.&lt;/p&gt;

&lt;p&gt;The level of revenue that comes with integrating a cryptocurrency exchange is the fuel beneath the launch of multiple crypto exchanges all over the globe. Having said that, there are several branches and classifications based on which a cryptocurrency exchange is categorized. &lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--s2ACc9Wh--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rpb6m10jre083e42bomh.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--s2ACc9Wh--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rpb6m10jre083e42bomh.png" alt="Image description" width="880" height="495"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#types-of-trading-platforms-in-cryptocurrency-exchange-software"&gt;
  &lt;/a&gt;
  &lt;strong&gt;Types of trading platforms in cryptocurrency exchange software:&lt;/strong&gt;
&lt;/h2&gt;

&lt;p&gt;Any freshly emerged business module has to have varied options in order to reel in more users as well as businessmen. Inheriting a cryptocurrency exchange software is no exception to that. Basically, there are three basic type of cryptocurrency exchanges:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Orders Book:&lt;/strong&gt; An order book is described as an electronic list of buy &amp;amp; sell orders that are executed in an organized manner in terms of price level. Order books are used by most of the cryptocurrency exchanges. The common buy and sell data will appear on the top &amp;amp; bottom, or in some cases, it will be displayed on the left and right side of the screen.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ads-based exchange:&lt;/strong&gt; Ads-based exchange acts as an extended branch of P2P(peer-to-peer) cryptocurrency exchange. Here, the buyer and seller are matched by the integrated software based on the requirements. Any buyer/seller wishing to carry out trading using ads-based crypto exchange is required to select the desired price range and proceed with buying/selling. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;OTC:&lt;/strong&gt; Crypto OTC(over-the-counter) is the process of trading crypto assets directly between two involved parties. This type of trading came into effect in order to avoid any sort of slippage. OTC is a conventional trade happening between two parties where one of them usually being a trading desk. OTC trading deals in huge transaction volume when compared with other exchanges. Any potential exchange related risks can be minimized while executing through an experienced trading desk. Through dealing in large volumes, liquidity can be highly fragmented over the crypto exchanges. &lt;/p&gt;

&lt;p&gt;Considering the above-mentioned cryptocurrency exchange types, the order book is the most common. This is because this type of exchange already exists in the conventional stock exchanges. However, over-the-counter and ads based exchanges, primarily are introduced after the advent of cryptocurrencies. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#developing-types-of-cryptocurrency-exchange-softwares"&gt;
  &lt;/a&gt;
  &lt;strong&gt;Developing types of cryptocurrency exchange softwares:&lt;/strong&gt;
&lt;/h2&gt;

&lt;p&gt;Considering the business requirements, you can build a cryptocurrency exchange through two basic ways viz., building cryptocurrency exchange from scratch and using white label cryptocurrency exchange software. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Crypto exchange from scratch:&lt;/strong&gt; Creating a cryptocurrency exchange from scratch will aid the entrepreneur in implementing the fundamental idea. Developing cryptocurrency exchange from scratch will enable you to fix the functionalities as per you desire, but there is a catch. The main hardship in starting a cryptocurrency exchange from scratch is that you should have a clear cut route for developing since every single feature is entirely based on your choice. Even though developing from scratch will consume a considerable amount of time and money, you can get your desired output.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;White label cryptocurrency exchange development:&lt;/strong&gt; A white label cryptocurrency exchange software or clone script is a ready-made, prefabricated crypto exchange script that consists of primary trading features and security attributes that is needed for a crypto exchange. You do not need to be technically well versed to start a cryptocurrency exchange using white label cryptocurrency exchange software. The mere adoption of the script for getting your desired output is more than enough to start a cryptocurrency exchange using white label cryptocurrency exchange software.&lt;/p&gt;

&lt;p&gt;Depending on the business requirements of the entrepreneurs or startups, they can build a cryptocurrency exchange from scratch or use white label cryptocurrency exchange software. Taking the above mentioned points in account, the common type preferred by most entrepreneurs is the development of white label cryptocurrency exchange software. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#why-white-label-cryptocurrency-exchange-software-is-preferred-to-scratch"&gt;
  &lt;/a&gt;
  &lt;strong&gt;Why white label cryptocurrency exchange software is preferred to scratch?&lt;/strong&gt;
&lt;/h2&gt;

&lt;p&gt;Developing cryptocurrency exchange using white label cryptocurrency exchange software, compared with other methods is regarded as highly beneficial for several reasons. The speciality of the white label cryptocurrency exchange software is that it has a special language script and design that is void of any type of plagiarism. The white label cryptocurrency exchange software is more often than not compared with clone scripts due the availability of customization techniques. Some of the mentionable features of using a white label cryptocurrency exchange software is:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Time:&lt;/strong&gt; We are living in a space where time is considered precious and no one could dare to lose it lethargically. In that regard, the time required to start any fresh business is very crucial to carry on with the momentum. Starting a cryptocurrency exchange is no exception. Integrating a white label cryptocurrency exchange software for commencing your cryptocurrency exchange saves a considerable amount of time.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cost:&lt;/strong&gt; Money is a major factor that plays a telling role in starting any new business. The cost for starting a cryptocurrency exchange from scratch will require a substantial amount of financial support, whereas using a white label crypto exchange software does not need a sound financial backing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Built-in features:&lt;/strong&gt; Incorporating a white label crypto exchange software comes with a lot of built-in features. The same might be available with developing from scratch, but it will cost a fortune to bring out the desired features. Having all the exclusive features in a cost-effective manner will serve as a huge boost for any new business.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Test run:&lt;/strong&gt; Any software that is built for a purpose requires extensive testing before it hits the ground running. Creating a cryptocurrency exchange from scratch won‚Äôt have sufficient space for conducting test runs. On the other hand, starting a crypto exchange using white label cryptocurrency exchange software is an exception to that fact. This is mainly because a white label software would undergo several testing phases before it can be cleared for complete implementation.&lt;/p&gt;

&lt;p&gt;In addition to these mentioned features, the quality that involves creation as well as adoption is a crucial factor to consider. Any business module requires a degree of quality while implementing those in real time and being a volatile aspect, crypto related business requires a high degree of quality to gain user prominence. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#how-to-choose-a-reliable-cryptocurrency-exchange-software-development-company"&gt;
  &lt;/a&gt;
  &lt;strong&gt;How to choose a reliable cryptocurrency exchange software development company?&lt;/strong&gt;
&lt;/h2&gt;

&lt;p&gt;More than developing a cryptocurrency exchange software, getting a quality white label cryptocurrency exchange software is very important. This is because, once a crypto exchange is created, any kind of alteration is considerably difficult to implement. This situation applies to more or less every business sector. Due to the presence of numerous features, it is vital to have a top-notch script in order to taste success.&lt;/p&gt;

&lt;p&gt;On the flip side, the current cryptocurrency market is filled with countless developers across the cryptocurrency ecosystem. Therefore, finding a completely reliable white label cryptocurrency exchange software development company is a tedious process. In order to get your desired output, it is highly advisable to approach the best &lt;strong&gt;&lt;a href="https://www.zabtechnologies.net/cryptocurrency-exchange-software-development-company?utm_campaign=sanblog5&amp;amp;utm_medium=mathew&amp;amp;utm_source=dev"&gt;white label cryptocurrency exchange software development company&lt;/a&gt;&lt;/strong&gt; in the crypto ecosystem. You can pick out the best through conducting extensive market research and thorough analysis of the current market. &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#wrapping-up"&gt;
  &lt;/a&gt;
  &lt;strong&gt;Wrapping up:&lt;/strong&gt;
&lt;/h3&gt;

&lt;p&gt;The financial sector took a grand detour upon the arrival of cryptocurrencies. This is mainly due to the ease with which cryptocurrencies can be facilitated across the globe. This is the same factor behind the involvement of cryptocurrencies into business sectors. Talking about the businesses powered by cryptocurrencies, starting a cryptocurrency exchange is one of the most profitable business modules in the cryptocurrency ecosystem. &lt;/p&gt;

&lt;p&gt;Just like any other businesses, there are numerous ways to commence your cryptocurrency exchange. Among the already established methods, starting a cryptocurrency exchange using white label crypto exchange software is more ideal for startups and budding entrepreneurs. Since, this is a booming business, considering the well-being of startups is more crucial for any future enhancement of the cryptocurrency ecosystem.&lt;/p&gt;

</description>
      <category>blockchain</category>
      <category>discuss</category>
      <category>beginners</category>
      <category>career</category>
    </item>
    <item>
      <title>Number one question on every PHP interview </title>
      <author>Damian Brdej</author>
      <pubDate>Fri, 17 Dec 2021 10:40:27 +0000</pubDate>
      <link>https://dev.to/quentindamianino/number-one-question-on-every-php-interview-2na2</link>
      <guid>https://dev.to/quentindamianino/number-one-question-on-every-php-interview-2na2</guid>
      <description>&lt;p&gt;I am a PHP programmer and have been on many job interviews. I noticed that on each of them one question always came up.&lt;/p&gt;

&lt;p&gt;That question is:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What's the difference between abstract class and interface?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The answer to this question is simple and proves the candidate's familiarity with object-oriented programming.&lt;/p&gt;

&lt;p&gt;So let's compare these two:&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#abstract-class"&gt;
  &lt;/a&gt;
  Abstract class
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;It can provide some functionality and leave the rest for the derived class.&lt;/li&gt;
&lt;li&gt;The derived class may or may not override the concrete functions defined in the base class.&lt;/li&gt;
&lt;li&gt;A child class extended from an abstract class should logically be related.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To declare class to be abstract just simply &lt;code&gt;abstract&lt;/code&gt; before &lt;code&gt;class&lt;/code&gt; keyword&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#interface"&gt;
  &lt;/a&gt;
  Interface
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;It cannot contain any functionality. It only contains definitions of the methods.&lt;/li&gt;
&lt;li&gt;The derived class MUST provide code for all the methods defined in the interface.&lt;/li&gt;
&lt;li&gt;Completely different and non-related classes can be logically grouped together using an interface.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To declare interface use &lt;code&gt;interface&lt;/code&gt; keyword&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;interface Template
{
    public function setVariable($name, $var);
    public function getHtml($template);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



</description>
      <category>programming</category>
      <category>webdev</category>
      <category>php</category>
      <category>beginners</category>
    </item>
    <item>
      <title>Test Orchestration: What, Why, and How</title>
      <author>Cody Simons</author>
      <pubDate>Fri, 17 Dec 2021 10:20:18 +0000</pubDate>
      <link>https://dev.to/codysimons20/test-orchestration-what-why-and-how-3cbc</link>
      <guid>https://dev.to/codysimons20/test-orchestration-what-why-and-how-3cbc</guid>
      <description>&lt;p&gt;In the agile development methodology, the velocity of testing holds the key to delivering the best quality software within the stipulated budget and time constraints. As such, this drives the need to automate the testing process. Most software development teams visualize test automation as a discreet step in the delivery lifecycle instead of viewing it as a designed sequence of steps. There are various aspects of testing through which software needs to pass, such as unit testing, functional testing, integration testing, smoke testing, performance testing, and more. Each of these needs to be automated in order to achieve certainty of software behavior and reduce the time to test. However, simply automating the tests is not enough to achieve the desired levels of quality and reduction in the time and cost to get there. This is where test orchestration plays an essential role.&lt;br&gt;
&lt;a href="https://www.katalon.com/resources-center/blog/test-orchestration/"&gt;Continue reading&lt;/a&gt;&lt;/p&gt;

</description>
      <category>testorchestration</category>
      <category>testautomation</category>
      <category>tests</category>
    </item>
    <item>
      <title>The Arrow Function in JS!</title>
      <author>Ustariz Enzo</author>
      <pubDate>Fri, 17 Dec 2021 10:14:06 +0000</pubDate>
      <link>https://dev.to/ziratsu/the-arrow-function-in-js-528f</link>
      <guid>https://dev.to/ziratsu/the-arrow-function-in-js-528f</guid>
      <description>&lt;p&gt;Hey &lt;strong&gt;fellow creators&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;arrow function&lt;/em&gt; exists since 2015 and is quite different from the classic functions. Let‚Äôs see how !&lt;/p&gt;

&lt;p&gt;If you prefer to watch the video &lt;strong&gt;version&lt;/strong&gt;, it's right here :&lt;/p&gt;

&lt;p&gt;&lt;iframe width="710" height="399" src="https://www.youtube.com/embed/O6lMYOT6XLs"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#1-how-to-use-an-arrow-function"&gt;
  &lt;/a&gt;
  1. How to use an arrow function.
&lt;/h2&gt;

&lt;p&gt;Here is the basic syntax, we don't need the "function" keyword and we put it by default in a constant, that way we won't have hoisting issues.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;add&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;b&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nx"&gt;console&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;If you have just a return, you can use the short version.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;add&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;b&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;If you have one parameter (but only one), you can remove the parenthesis which would make it even more concise:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;add&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;It‚Äôs very useful when you use it with some higher order function like the &lt;em&gt;map.()&lt;/em&gt; method:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;multiplied&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;array&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;num&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;num&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#2-the-difference-between-a-classic-function-and-an-arrow-function"&gt;
  &lt;/a&gt;
  2. The difference between a classic function and an arrow function.
&lt;/h2&gt;

&lt;p&gt;The main difference between the classic and arrow function is the value of "this".&lt;/p&gt;

&lt;p&gt;If you use a classic function as the value of a property in an object, "this" will refer to the calling context, i.e. the obj where the function is defined :&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;obj&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="na"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;foo&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nx"&gt;console&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nx"&gt;obj&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;foo&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;// {a: 5, foo: ∆í}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Otherwise, if you use an arrow function, "this" will return the global object.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;obj&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="na"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;foo&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nx"&gt;console&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nx"&gt;obj&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;foo&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;// Window Object&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;In that case, this will refer to the parent of the calling context, thus the global object.&lt;/p&gt;

&lt;p&gt;Instead of refering the direct context, it will refer to the parent of that context.&lt;/p&gt;

&lt;p&gt;You need to keep that difference in mind when you are dealing with functions and the "this" keyword.&lt;/p&gt;

&lt;p&gt;Come and take a look at my &lt;strong&gt;Youtube channel&lt;/strong&gt;: &lt;a href="https://www.youtube.com/c/Learntocreate/videos"&gt;https://www.youtube.com/c/Learntocreate/videos&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Enzo. &lt;/p&gt;

</description>
      <category>javascript</category>
      <category>tutorial</category>
    </item>
    <item>
      <title>What Is Event-Driven Machine Learning?</title>
      <author>RudderStack</author>
      <pubDate>Fri, 17 Dec 2021 09:34:43 +0000</pubDate>
      <link>https://dev.to/rudderstack/what-is-event-driven-machine-learning-5ed0</link>
      <guid>https://dev.to/rudderstack/what-is-event-driven-machine-learning-5ed0</guid>
      <description>&lt;p&gt;Machine learning algorithms will build a model based on sample data to decide or predict without being explicitly programmed. This sample is also known as¬†training data, and it's the key to any precise analysis.&lt;/p&gt;

&lt;p&gt;The highly modern technology is mostly used in healthcare, where diagnostics are predicted for doctors' reviews. The sentiment analysis that tech giants are doing on social media is another interesting way to apply machine learning, fraud detection in the finance sector, and predicting customer churn in eCommerce.&lt;/p&gt;

&lt;p&gt;Buckle up, and let's dive deeper into the essence of machine learning, its overall impact on technology and businesses, and its core industry applications.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#better-software-with-machine-learning"&gt;
  &lt;/a&gt;
  Better Software With Machine Learning
&lt;/h2&gt;

&lt;p&gt;The possibility of machine learning has introduced new encouraging abilities in the software engineering world. All types of software applications are personal assistants trained to answer predetermined questions such as¬†&lt;em&gt;Will these types of customers purchase that kind of products?¬†*Earlier, the only way to build software was to code human-defined rules for a specific application whose purpose was to answer those particular questions. Back in those¬†*ancient&lt;/em&gt;¬†times, the human operator had to tell the software how to calculate the probability of a specific customer purchasing a certain product!&lt;/p&gt;

&lt;p&gt;However, instead of giving the software a specific command on doing a certain calculation, we can now provide precedents from the past using machine learning. Now, the software can make thorough data analysis and come up with its own rules. This entire process is what we now call "learning."&lt;/p&gt;

&lt;p&gt;Data is the most crucial element in the process of machine learning. The overall data quality will decide the software-defined rules and their quality. If the required information is not met, the software will not find the correct answers. Answering the question about getting the most data is the key to leveraging the power of machine learning.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#eventdriven-data-collection"&gt;
  &lt;/a&gt;
  Event-Driven Data Collection
&lt;/h2&gt;

&lt;p&gt;The initial and essential step in any data science project is data collection. There are numerous ways when it comes to collecting data - from one or many sources. Sometimes it is hard to have direct entry to essential but sensitive information when working with enterprises. Many times, data scientists need to request data dumps. This implementation may prevent accidents, but it separates ongoing projects and enterprise reality. It averts teams from having real-time data access. It also requires activating various bureaucratic-based processes that will further cause additional work. This can be fixed with event-driven data collection.&lt;/p&gt;

&lt;p&gt;Data scientists can recreate brand new and real-time data by just listening to events emitted by the enterprise systems. Moreover, a data scientist can process the data with no risks of altering the enterprise data whatsoever. The enterprise system can also anonymize any kind of sensitive information or filter it. However, there is one requirement for the enterprise system: to adopt an event sourcing or event-driven.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#eventdriven-data-exploration"&gt;
  &lt;/a&gt;
  Event-Driven Data Exploration
&lt;/h2&gt;

&lt;p&gt;Data scientists need to pre-process and filter the correct information to produce the most accurate answers. Data exploration is crucial to finding the correct combination of transformations to deliver the optimal data for the machine learning model. Thanks to a huge variety of data visualization tools, understanding data through visual representation helps us grasp its underlying connections.&lt;/p&gt;

&lt;p&gt;Remember, events are essentials to master data!&lt;/p&gt;

&lt;p&gt;The use of events will assist data scientists in putting all data into context. The reason for any data modification cannot be discovered if there are limits to the exploration process, which should be applied to the mutable data. The updated data cannot be explored without calculating the previous value. All supplementary information events will provide patterns, habits, and highlight behavior, among other things. More qualitative data can be achieved by increasing machine learning capabilities. The enhancement of these capabilities can be further achieved with the overall process of event-driven data exploration.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#eventdriven-data-preparation"&gt;
  &lt;/a&gt;
  Event-Driven Data Preparation
&lt;/h2&gt;

&lt;p&gt;When a product is developed, data scientists don't design it for data machines but for humans. A machine cannot understand something like literature, but the human brain can process it as easily as focused reading.&lt;/p&gt;

&lt;p&gt;However, a human brain will have difficulty calculating massive matrix operations, while the machine will do it with ease. This tells us that machines cannot compute abstract data, but this field is humans' expertise. To optimize machine learning, scientists need to convert this human-centric data.&lt;/p&gt;

&lt;p&gt;Data preparation will pre-process, specialize, and clean data for each ML model. Since each model will be set to answer questions different from one another, the data coming from a single model won't fit another's requirements. The event-driven data technique can dramatically shorten the otherwise arduous task by incrementally producing specific data in a way that will process incoming events in continuity, making it a very valuable tool.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#summary"&gt;
  &lt;/a&gt;
  Summary
&lt;/h2&gt;

&lt;p&gt;While machine learning is nothing new, not all current companies have the potential to store exponential data volumes and use it adequately in equipping soon-to-be learning machines. In other words, not being able to enforce machine learning properly will prevent many, and especially smaller enterprises, from acquiring a favorable business outcome.&lt;/p&gt;

&lt;p&gt;In due time, and as technology continues to evolve, the models based on which machine learning now operates are expected to be facilitated and made available for all-size enterprises.&lt;/p&gt;

&lt;p&gt;The biggest hope researchers have in terms of sophistication of machine learning in the foreseeable future lies in making current models far more flexible and applicable. This will ultimately train respective machines to handle more than one task at a time - all by learning faster, smarter, and better.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#sign-up-for-free-and-start-sending-data"&gt;
  &lt;/a&gt;
  Sign Up For Free And Start Sending Data
&lt;/h3&gt;

&lt;p&gt;Test out our event stream, ELT, and reverse-ETL pipelines. Use our HTTP source to send data in less than 5 minutes, or install one of our 12 SDKs in your website or app.&lt;a href="https://app.rudderstack.com/signup?type=freetrial"&gt;Get Started&lt;/a&gt;&lt;/p&gt;

</description>
      <category>eventdriven</category>
      <category>machinelearning</category>
      <category>cdp</category>
      <category>dataengineering</category>
    </item>
    <item>
      <title>How to use spot instances during the Christmas frenzy?</title>
      <author>CAST AI</author>
      <pubDate>Fri, 17 Dec 2021 09:26:32 +0000</pubDate>
      <link>https://dev.to/castai/how-to-use-spot-instances-during-the-christmas-frenzy-5c0f</link>
      <guid>https://dev.to/castai/how-to-use-spot-instances-during-the-christmas-frenzy-5c0f</guid>
      <description>&lt;p&gt;You‚Äôve been doing a great job using spot instances for the entire year. But as 2021 is coming to a close, you might have noticed that there‚Äôs less inventory of your favorite instance types.&lt;/p&gt;

&lt;p&gt;And even when you manage to get your hands on some, &lt;strong&gt;they get reclaimed rather quickly&lt;/strong&gt;. Everyone else needs extra capacity during the holiday season.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Can you even trust spot instances to reduce your costs at such a critical time?&lt;/strong&gt; Should you run everything on on-demand instances and pay the premium just to keep the application running?¬†&lt;/p&gt;

&lt;p&gt;Those are the questions we will answer in this article.&lt;/p&gt;

&lt;p&gt;Jump to the section that‚Äôs most relevant to you:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#dealingwithspotinstanceinterruptions"&gt;Dealing with spot instance interruptions manually is hard&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#andwhatifyouhandledspotinstancesautomatically"&gt;And what if you handled the spot interruptions automatically?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#automationtakesmanytasksoffyourplate"&gt;Automation takes many tasks off your plate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hereshowspotinstanceautomationworks"&gt;Here‚Äôs how spot instance automation works in practice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#theresultsmaysurpriseyou"&gt;The results may surprise you&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="dealingwithspotinstanceinterruptions"&gt;Dealing with spot instance interruptions manually is hard&lt;/h2&gt;

&lt;p&gt;Hyperscalers like AWS, Google Cloud, and Azure have massive data centers scattered around the globe. At some point, they find themselves with spare capacity and they offer it at a discounted price.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The only catch is that the vendor can pull the plug at any time. This makes spot instances generally difficult to manage for production workloads. When getting a spot instance, you have no guarantee on how long it will stay available.¬†¬†&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Vendors only share some historical data about past interruptions to help you make a more informed decision. Or notify you when the risk of interruption increases for the instance you‚Äôre using to give you some extra time.¬†&lt;/p&gt;

&lt;p&gt;A spot instance is reclaimed with short notice - from 2 minutes to as little as 30 seconds. &lt;strong&gt;Is that enough time to drop everything and find a replacement for your instance?¬†&lt;/strong&gt;You may know a superhuman engineer who‚Äôs able to do that, but they surely have more productive tasks to do.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spinning up a new instance takes time¬†&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;And even if you identify the virtual machine where to move your workload, creating a new one takes time, so you‚Äôre still facing the danger of potential downtime.¬†&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Running paused machines means extra costs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Alternatively, you could have some paused machines at your disposal that could step in if you happen to lose a spot instance. But this comes at an additional price, and aren‚Äôt you in the spot business to save on your cloud costs?¬†&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spot instance availability changes rapidly&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Another issue is that the available capacity sold as spot instances can differ a lot based on size, region, time of day, and many other factors - all subject to frequent changes.¬†&lt;/p&gt;

&lt;p&gt;The availability of an instance is based on supply and demand. So if you pick the most popular instance types and a market surge like Black Friday occurs, prepare for a nasty surprise.&lt;/p&gt;

&lt;h2 id="andwhatifyouhandledspotinstancesautomatically"&gt;And what if you handled the spot interruptions automatically?&lt;/h2&gt;

&lt;p&gt;Using an automation platform addresses all the problems listed above and makes all the difference when it comes to your cloud bill. You can rely on spot instances even during the most intense times of the year.&lt;/p&gt;

&lt;p&gt;At CAST AI, we have the &lt;strong&gt;Spot Fallback feature that keeps your workloads running even if the cloud provider sells all the capacity and there are no spot instances left for you to use&lt;/strong&gt;. If no spot instances matching your requirements are available, the autoscaler temporarily adds an on-demand node for your spot-only workloads so they have a place to run on.¬†&lt;/p&gt;

&lt;p&gt;Once the inventory of Spot/Preemptible instances is available again, the autoscaler replaces the on-demand nodes used during the fallback with actual spot instances.&lt;/p&gt;

&lt;p&gt;That way you can still get the benefits of spot instances without having to be anxious about the potential downtime.&lt;/p&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--5IH2fgwS--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://cast.ai/wp-content/uploads/2021/12/Spot-Fallback-GIF.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--5IH2fgwS--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://cast.ai/wp-content/uploads/2021/12/Spot-Fallback-GIF.gif" alt="spot instances" width="600" height="314"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id="automationtakesmanytasksoffyourplate"&gt;Automation takes many tasks off your plate&lt;/h2&gt;

&lt;p&gt;But arguably the best part is that you don‚Äôt need to care about provisioning instances. Moving a workload from one spot instance to another is challenging if you do it manually because it‚Äôs a multi-step process:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Examine the cloud provider offer&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You can check the less popular instances - they're less likely to be interrupted and can run for longer periods of time. Before deciding to buy an instance, look at how often it is interrupted (its interruption or eviction rate).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Make your bid&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Set the maximum price you want to spend for your preferred spot instance. The rule of thumb is to follow the level of on-demand pricing and avoid getting interrupted when the price of that instance spikes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Manage spot instances in groups&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This opens the doors to request a variety of instance types at the same time and increases your chances of securing a spot instance. You can get more tips about managing spot instances here: &lt;a href="https://cast.ai/blog/how-to-reduce-cloud-costs-by-90-spot-instances-and-how-to-use-them/" rel="noreferrer noopener"&gt;Spot instances: How to reduce AWS, Azure, and GCP costs by 90%&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;To make spot instances work and secure the right mix of instances, you‚Äôll have to dedicate a lot of time and effort to configuration, setup, and maintenance.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;A good automation platform will do them all for you. All you need to do is set up the right policies and see your workloads gracefully moved from one instance to another when interruption happens. Here‚Äôs how.&lt;/p&gt;

&lt;h2 id="hereshowspotinstanceautomationworks"&gt;Here‚Äôs how spot instance automation works in practice¬†&lt;/h2&gt;



&lt;p&gt;CAST AI is an automation platform that uses a mix of tactics to optimize cloud costs and help teams get the performance they need. This approach extends to how spot instances are automated in CAST AI.¬†&lt;/p&gt;

&lt;p&gt;The platform doesn‚Äôt stick to a predefined list of instance types, but instead scans workloads for spot suitability and matches their requirements automatically. You can edit the list of spot-suitable workloads afterwards.¬†&lt;/p&gt;

&lt;p&gt;This flow is a lot faster than selecting the instance types you prefer manually and picking the pods which shouldn‚Äôt run on spot instances at all. The AI checks it all for you in minutes.¬†&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;During the times when everyone is competing for spot instances they know and use, you will be playing in a mostly empty playground of ALL instance types suitable for your application.&lt;/p&gt;&lt;/blockquote&gt;

&lt;h3&gt;Configuring the automation&lt;/h3&gt;

&lt;p&gt;You can automate spot instances on Kubernetes workloads running on EKS, Kops, AKS, and GKE. Here‚Äôs a short overview of how you can configure CAST AI to cover all the scenarios your application might face.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tolerations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This configuration comes in handy when spot instances are an optional choice for your workload.&lt;/p&gt;

&lt;p&gt;When your pod is marked only with tolerations, the Kubernetes scheduler can place the pod on both spot and regular nodes, always picking the most cost-effective resources.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Node Selectors&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In this configuration, workloads will only use spot instances. The autoscaling mechanism will pick a spot instance whenever your pod requires an additional workload in the cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Node Affinity&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here we use spot instances if they‚Äôre available - if not, the application falls back to on-demand instances.&lt;/p&gt;

&lt;p&gt;For example, if a spot instance gets interrupted and the on-demand instances in the cluster have some available capacity, pods that previously ran on that spot instance will be scheduled on the available on-demand resources. Moving your workloads back to spot instances is possible via the Rebalancer feature.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spot Reliability&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This configuration focuses on minimizing the chance of workload interruptions. The autoscaler identifies which instance types are less likely to be interrupted and you can set a default reliability value to be applied across the entire cluster.&lt;/p&gt;

&lt;p&gt;The reliability value is measured by the percentage of reclaimed instances during the trailing month for this instance type. Click &lt;a href="https://docs.cast.ai/guides/spot/#tolerations"&gt;here&lt;/a&gt; for more info.&lt;/p&gt;

&lt;p&gt;You can control the reliability value at a more granular level, per workload. For example, you can leave the most cost-efficient value globally and choose more stable instances for specific workloads.¬†&lt;/p&gt;

&lt;h2 id="theresultsmaysurpriseyou"&gt;The results may surprise you&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--6yeviT4A--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cast.ai/wp-content/uploads/2021/12/Optimize-CAST-AI.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--6yeviT4A--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cast.ai/wp-content/uploads/2021/12/Optimize-CAST-AI.png" alt="" width="880" height="461"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A few months ago, we decided to check whether we were really using the best instances available and were shocked to see this spot instance recommendation: the INF1 instance. Who would CAST AI pick that for us? It's essentially a supercomputer for high-performance ML inference that costs a lot if you buy it for on-demand pricing.&lt;/p&gt;

&lt;p&gt;Funnily enough, the CAST AI platform didn‚Äôt go off rails. The INF1 spot instance was actually cheaper at that point in time than the typical instance we were getting.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So while everyone was manually competing for what they knew, we rented a Ferrari for the price of a Fiat just because automation looked at every single instance type that can do the job.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If we were choosing instances manually, we‚Äôd never look into this category. Automation expanded our reach and got us this gem of an instance.&lt;/p&gt;

&lt;p&gt;If you are ready to quit competing on the same level everyone‚Äôs on - let us know and we‚Äôll happily show you around the CAST AI platform.&lt;/p&gt;



</description>
      <category>cloud</category>
      <category>kubernetes</category>
      <category>tutorial</category>
      <category>devops</category>
    </item>
    <item>
      <title>Fun times with MySQL upgrade</title>
      <author>Eva Marija Banaj</author>
      <pubDate>Fri, 17 Dec 2021 09:25:00 +0000</pubDate>
      <link>https://dev.to/trikoder/fun-times-with-mysql-upgrade-1ei4</link>
      <guid>https://dev.to/trikoder/fun-times-with-mysql-upgrade-1ei4</guid>
      <description>&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--EGnMpsxJ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn-images-1.medium.com/max/1024/1%2AhjC30NH5V2DHVq-POKlMpg.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--EGnMpsxJ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn-images-1.medium.com/max/1024/1%2AhjC30NH5V2DHVq-POKlMpg.png" alt="" width="880" height="440"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Like all service upgrades, MySQL is no different. Bump the service version, build the docker image, try to ‚Äúmake up‚Äú the project and hope for the best. I‚Äôve decided to dedicate this blog to four things that turned this ‚Äúit is going to be an easy project‚Äù to ‚Äú8 months in hell while upgrading MySQL from v5.6 to v8.0‚Äù.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#1-fantastic-sql-modes-and-where-to-find-them"&gt;
  &lt;/a&gt;
  1. Fantastic SQL modes and where to find them
&lt;/h3&gt;

&lt;p&gt;While looking at all changes made to MySQL between v5.6 and v8.0, I came across something rather interesting. They announced they had enabled a bunch of other previously optional SQL modes as a part of strict mode.&lt;/p&gt;

&lt;p&gt;To my surprise, only &lt;em&gt;NO_ENGINE_SUBSTITUTION&lt;/em&gt; mode was enabled in our database. üòï?!?! What could possibly go wrong after years of using the database with strict mode off?&lt;/p&gt;

&lt;p&gt;A lot of things apparently, so I made a list of modes I need to enable/check out before finally enabling &lt;em&gt;STRICT_TRANS_TABLES&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;em&gt;ERROR_FOR_DIVISION_BY_ZERO&lt;/em&gt; ‚Üí &lt;a href="https://dev.mysql.com/doc/refman/5.7/en/sql-mode.html#sqlmode_error_for_division_by_zero"&gt;in later versions, no longer an option but part of strict mode&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;em&gt;NO_ZERO_IN_DATE&lt;/em&gt; ‚Üí &lt;a href="https://dev.mysql.com/doc/refman/5.7/en/sql-mode.html#sqlmode_no_zero_in_date"&gt;in later versions, no longer an option but part of strict mode&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;em&gt;NO_AUTO_CREATE_USER&lt;/em&gt; ‚Üí &lt;a href="https://dev.mysql.com/doc/refman/5.7/en/sql-mode.html#sqlmode_no_auto_create_user"&gt;in version 8.0, no longer an option but part of strict mode&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;em&gt;NO_ZERO_DATE&lt;/em&gt; ‚Üí &lt;a href="https://dev.mysql.com/doc/refman/5.7/en/sql-mode.html#sqlmode_no_zero_in_date"&gt;in later versions, no longer an option but part of strict mode&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;em&gt;ONLY_FULL_GROUP_BY&lt;/em&gt; ‚Üí decided to leave it disabled&lt;/li&gt;
&lt;li&gt;
&lt;em&gt;NO_ENGINE_SUBSTITUTION&lt;/em&gt; ‚Üí was already enabled&lt;/li&gt;
&lt;li&gt;
&lt;em&gt;STRICT_TRANS_TABLES&lt;/em&gt; ‚Üí needs to be enabled&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This little side quest of cleaning up migrations, Doctrine entities which were not in sync with corresponding database tables, fixtures, using null for everything‚Ää‚Äî‚Äädoes not matter is the field nullable or not, missing primary keys on tables etc. cost us 193,4 hours (around 33 days) just to make the application run with some additional SQL modes enabled.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#2-sphinx-client-issues"&gt;
  &lt;/a&gt;
  2. Sphinx client issues
&lt;/h3&gt;

&lt;p&gt;Some legacy parts of our application still used Sphinx instead of ElasticSearch. We faced a tough choice then and there because MySQL v8.0 had a new default authentication plugin &lt;em&gt;caching_sha2_password&lt;/em&gt;, and Sphinx v2.2.4, that we were still using, uses the old authentication plugin &lt;em&gt;mysql_native_password&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Connecting to Sphinx with a user that was using &lt;em&gt;caching_sha2_password&lt;/em&gt; authentication plugin resulted in:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;bash-4.4$ mysql -hsphinx -usphinx 
ERROR 2003 (HY000): Can't connect to MySQL server on 'sphinx' (111)
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;So, we tried to create a user that used the old authentication plugin. That still resulted with an error:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;bash-4.4$ mysql -hsphinx -P9306 
ERROR 2000 (HY000): Unknown MySQL error
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="http://sphinxsearch.com/docs/sphinx3.html#version-3.1.1-17-oct-2018"&gt;Issues with connecting to MySQL client 8.0+ were fixed in Sphinx v3.1.1&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This left us with two possible ways to go:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Upgrade legacy parts of the application, that we all want removed, to v3.1.1‚Ää‚Äî‚Äänot too much effort&lt;/li&gt;
&lt;li&gt;Remove Sphinx from the project‚Ää‚Äî‚Äälittle sub-project&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We went with a 751 hour endeavor for 5 people and removed Sphinx from the project.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#3-the-magnificent-world-of-charsets-collates-and-row-formats"&gt;
  &lt;/a&gt;
  3. The magnificent world of charsets, collates and row formats
&lt;/h3&gt;

&lt;h4&gt;
  &lt;a href="#about-charsets-and-collates"&gt;
  &lt;/a&gt;
  About charsets and collates
&lt;/h4&gt;

&lt;p&gt;MySQL uses &lt;em&gt;UTF8&lt;/em&gt; as an alias for the now deprecated &lt;em&gt;UTF8MB3&lt;/em&gt;. It is expected, at some point in the future, that &lt;em&gt;UTF8&lt;/em&gt; will become an alias for the &lt;em&gt;UTF8MB4&lt;/em&gt; charset. In a future MySQL release, &lt;em&gt;UTF8MB3&lt;/em&gt; should be removed. You can read more about it &lt;a href="https://dev.mysql.com/doc/refman/8.0/en/charset-unicode-utf8mb3.html"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To follow the recommendation, we decided to change our charset to &lt;em&gt;UTF8MB4&lt;/em&gt;. To match our brand new charset, we had to change the collate to any one compatible with &lt;em&gt;UTF8MB4&lt;/em&gt; charset.&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#about-row-formats"&gt;
  &lt;/a&gt;
  About row formats
&lt;/h4&gt;

&lt;p&gt;There are four row formats: &lt;em&gt;REDUNDANT, COMPACT, DYNAMIC&lt;/em&gt; and &lt;em&gt;COMPRESSED&lt;/em&gt;. MySQL v5.6 uses &lt;em&gt;COMPACT&lt;/em&gt; by default, and v5.7 and later use DYNAMIC.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-row-format.html"&gt;&lt;em&gt;The DYNAMIC row format offers the same storage characteristics as the COMPACT row format but adds enhanced storage capabilities for long variable-length columns and supports large index key prefixes.&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In our code, we like to use these ‚Äúcutting edge‚Äù things from the ‚Äúera gone by‚Äù, meaning, we used row format &lt;em&gt;FIXED&lt;/em&gt;. It is so deprecated, that if &lt;em&gt;innodb_strict_mode&lt;/em&gt; is disabled, InnoDB issues a warning and assumes row format &lt;em&gt;DYNAMIC&lt;/em&gt;, and if innodb_strict_mode is enabled, InnoDB returns an error. We replaced &lt;em&gt;FIXED&lt;/em&gt; and &lt;em&gt;COMPACT&lt;/em&gt; row formats with &lt;em&gt;DYNAMIC&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;There was a bug. üêõ&lt;/p&gt;

&lt;p&gt;If you try to create an index on a field that exceeds 767 bytes you will get an error that looks like this:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;ERROR 1709 (HY000): Index column size too large. The maximum column size is 767 bytes
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;But it was still possible to create an index like that if you were using row format &lt;em&gt;COMPRESSED&lt;/em&gt;/&lt;em&gt;REDUNDANT&lt;/em&gt;, or you didn‚Äôt explicitly define row format &lt;em&gt;DYNAMIC&lt;/em&gt;. The result, after a server reboot, the table was inaccessible and could not be recovered. But luckily, this &lt;a href="https://bugs.mysql.com/bug.php?id=99791"&gt;issue&lt;/a&gt; was fixed in MySQL v8.0.22.&lt;/p&gt;

&lt;p&gt;If you still want to create an index on VARCHAR field, make sure the length is less or equal to 190. This is because &lt;em&gt;UTF8&lt;/em&gt; takes up to (3*255) 765 bytes, and &lt;em&gt;UTF8MB4&lt;/em&gt; takes up to (4*255) 1020 bytes.&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#the-real-issue"&gt;
  &lt;/a&gt;
  The real issue
&lt;/h4&gt;

&lt;p&gt;Ok, not to hard. So we change the charset, collate and row format. Big woop, right?&lt;/p&gt;

&lt;p&gt;The real hard part was altering every single table in the production database. This will not be a problem if you do not have any huge tables, but if you do, these alters can and will take hours.&lt;/p&gt;

&lt;p&gt;The trickiest thing of all is altering all these tables with some reasonable downtime. Because, if you try to join two tables with a different collate and charset‚Ää‚Äî‚Ääit will fail. If you try to alter everything on one slave, and then replicate it‚Ää‚Äî‚Ääit will fail.&lt;/p&gt;

&lt;p&gt;Your safest bet is to backup or delete some data you can spare to reduce the table size. Or create new empty tables that will be used while the real ones are being altered and sync the deltas once it is over.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#4-mysql-query-cache-is-no-more"&gt;
  &lt;/a&gt;
  4. MySQL query cache is no more
&lt;/h3&gt;

&lt;p&gt;If you rely on MySQL query cache, you will have to replace it with something else. It was deprecated in MySQL v5.7 and completely removed in MySQL v8.0.&lt;/p&gt;

&lt;p&gt;There are some alternatives, like ProxySQL query cache. But there are definitely some cutbacks.&lt;/p&gt;

&lt;p&gt;It is the simplest alternative, really easy to setup, benchmarks show better throughput‚Ää‚Äî‚Äämeaning performance boost. But‚Ä¶&lt;/p&gt;

&lt;p&gt;Unlike MySQL query cache that would invalidate the cache every time there was a write, in ProxySQL there is no way to define a way to invalidate the cache other then &lt;em&gt;cache_ttl&lt;/em&gt;. This can definitely be a limitation because there is a chance you will serve some stale data.&lt;/p&gt;

&lt;p&gt;Other then that, it does not support caching prepared statements and there is no way to manually purge the query cache. There is a parameter &lt;em&gt;mysql-query_cache_size_MB&lt;/em&gt; that defines how big your cache can get. But this is not strict, it is only used to automatically trigger the query cache purge.&lt;/p&gt;

&lt;p&gt;In any case, it just depends on whether or not this is acceptable to you. You can find more about it &lt;a href="https://www.percona.com/blog/2018/02/07/proxysql-query-cache/"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you are planning on upgrading MySQL, I hope you will find this helpful. The biggest problem for me was underestimating the time needed for delivering the project. I wrote this post, if for nothing else, to help you know what to look out for. :)&lt;/p&gt;




</description>
      <category>sql</category>
      <category>tips</category>
      <category>tipsandtricks</category>
      <category>mysql</category>
    </item>
    <item>
      <title>Deploying Next.js apps to a VPS using Github actions and  Docker</title>
      <author>Lewis kori</author>
      <pubDate>Fri, 17 Dec 2021 09:18:54 +0000</pubDate>
      <link>https://dev.to/lewiskori/deploying-nextjs-apps-to-a-vps-using-github-actions-and-docker-564n</link>
      <guid>https://dev.to/lewiskori/deploying-nextjs-apps-to-a-vps-using-github-actions-and-docker-564n</guid>
      <description>&lt;p&gt;Recently, I had to deploy a project to a DigitalOcean droplet. One of the features I really wanted for this particular project was a Continuous Delivery pipeline.&lt;/p&gt;

&lt;p&gt;The continuous delivery website defines this as&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;the ability to get changes of all types‚Äîincluding new features, configuration changes, bug fixes and experiments‚Äîinto production, or into the hands of users, safely and quickly in a sustainable way.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The goal is to make deployments‚Äîwhether of a large-scale distributed system, a complex production environment, an embedded system, or an app‚Äîpredictable, routine affairs that can be performed on demand.&lt;/p&gt;

&lt;p&gt;For my case I wanted the web app to auto-deploy to the VPS whenever I pushed changes to the main Github branch. This would consequently save a lot of development time in the process.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#alternative-solutions"&gt;
  &lt;/a&gt;
  Alternative solutions
&lt;/h2&gt;

&lt;p&gt;There are alternative and hustle-free solutions to this such as &lt;a href="https://vercel.com/"&gt;Vercel&lt;/a&gt; and &lt;a href="https://www.digitalocean.com/products/app-platform/"&gt;DigitalOcean app platform&lt;/a&gt;. However one may take my route if:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;You want to better understand Github actions&lt;/li&gt;
&lt;li&gt;Learn more about docker&lt;/li&gt;
&lt;li&gt;For Vercel's case, your client or organization may want to keep their apps in a central platform for easier management.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;
  &lt;a href="#prerequisites"&gt;
  &lt;/a&gt;
  Prerequisites
&lt;/h2&gt;

&lt;p&gt;Please note that some of the links below are affiliate links and at no additional cost to you. Know that I only recommend products, tools and learning services I've personally used and believe are genuinely helpful. Most of all, I would never advocate for buying something you can't afford or that you aren't ready to implement.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A &lt;a href="https://github.com"&gt;Github account&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;A virtual private server. I used a DigitalOcean droplet running Ubuntu 20.04 LTS. Sign up with &lt;a href="https://www.digitalocean.com/?refcode=2282403be01f&amp;amp;utm_campaign=Referral_Invite&amp;amp;utm_medium=Referral_Program"&gt;my referral link&lt;/a&gt; and get $100 in credit valid for 60 days.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;
  &lt;a href="#create-nextjs-app"&gt;
  &lt;/a&gt;
  Create next.js app
&lt;/h2&gt;

&lt;p&gt;We'll use npx to create a standard next.js app&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;npx create-next-app meta-news &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;cd &lt;/span&gt;meta-news
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Once we're inside the project directory, we'll install a few dependencies for demonstration purposes&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;yarn add @chakra-ui/react @emotion/react@^11 @emotion/styled@^11 framer-motion@^4 axios
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;We'll also declare environment variables inside the &lt;code&gt;.env.local&lt;/code&gt; file. We can then reference these variables from our app like so &lt;code&gt;process.env.NEXT_PUBLIC_VARIABLE_NAME&lt;/code&gt;&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;NEXT_PUBLIC_BACKEND_URL=http://localhost:8000/api
NEXT_PUBLIC_META_API_KEY=your_api_key
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;These variables are for demonstration purposes only. So we won't really be referencing them within our app. An example of a place you'd call them is when instantiating an axios instance or setting a google analytics id and you don't want to commit that to the version control system.&lt;/p&gt;

&lt;p&gt;Let's do a quick test run. The app should be running on &lt;code&gt;localhost:3000&lt;/code&gt; if everything is setup properly.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;yarn start
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#dockerizing-the-app"&gt;
  &lt;/a&gt;
  Dockerizing the app
&lt;/h2&gt;

&lt;p&gt;Docker is an open-source tool that automates the deployment of an application inside a software container. which are like virtual machines, only more portable, more resource-friendly, and more dependent on the host operating system. for detailed information on the workings of docker, I'd recommend reading &lt;a href="https://dev.to/django_stars/what-is-docker-and-how-to-use-it-with-python-tutorial-87a"&gt;this article&lt;/a&gt; and for those not comfortable reading long posts, &lt;a href="https://www.youtube.com/playlist?list=PLhW3qG5bs-L99pQsZ74f-LC-tOEsBp2rK"&gt;this tutorial series on youtube&lt;/a&gt; was especially useful in introducing me to the concepts of docker.&lt;/p&gt;

&lt;p&gt;We'll add a Dockerfile to the project root by running&lt;br&gt;
&lt;code&gt;touch Dockerfile&lt;/code&gt; within the CLI.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight docker"&gt;&lt;code&gt;&lt;span class="c"&gt;# Install dependencies only when needed&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;&lt;span class="s"&gt; node:alpine AS deps&lt;/span&gt;
&lt;span class="c"&gt;# Check https://github.com/nodejs/docker-node/tree/b4117f9333da4138b03a546ec926ef50a31506c3#nodealpine to understand why libc6-compat might be needed.&lt;/span&gt;
&lt;span class="k"&gt;RUN &lt;/span&gt;apk update &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; apk add &lt;span class="nt"&gt;--no-cache&lt;/span&gt; libc6-compat &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; apk add git
&lt;span class="k"&gt;WORKDIR&lt;/span&gt;&lt;span class="s"&gt; /app&lt;/span&gt;
&lt;span class="k"&gt;COPY&lt;/span&gt;&lt;span class="s"&gt; package.json yarn.lock ./&lt;/span&gt;
&lt;span class="k"&gt;RUN &lt;/span&gt;yarn &lt;span class="nb"&gt;install&lt;/span&gt; &lt;span class="nt"&gt;--immutable&lt;/span&gt;


&lt;span class="c"&gt;# Rebuild the source code only when needed&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;&lt;span class="s"&gt; node:alpine AS builder&lt;/span&gt;
&lt;span class="c"&gt;# add environment variables to client code&lt;/span&gt;
&lt;span class="k"&gt;ARG&lt;/span&gt;&lt;span class="s"&gt; NEXT_PUBLIC_BACKEND_URL&lt;/span&gt;
&lt;span class="k"&gt;ARG&lt;/span&gt;&lt;span class="s"&gt; NEXT_PUBLIC_META_API_KEY&lt;/span&gt;


&lt;span class="k"&gt;ENV&lt;/span&gt;&lt;span class="s"&gt; NEXT_PUBLIC_BACKEND_URL=$NEXT_PUBLIC_BACKEND_URL&lt;/span&gt;
&lt;span class="k"&gt;ENV&lt;/span&gt;&lt;span class="s"&gt; NEXT_PUBLIC_META_API_KEY=$NEXT_PUBLIC_META_API_KEY&lt;/span&gt;

&lt;span class="k"&gt;WORKDIR&lt;/span&gt;&lt;span class="s"&gt; /app&lt;/span&gt;
&lt;span class="k"&gt;COPY&lt;/span&gt;&lt;span class="s"&gt; . .&lt;/span&gt;
&lt;span class="k"&gt;COPY&lt;/span&gt;&lt;span class="s"&gt; --from=deps /app/node_modules ./node_modules&lt;/span&gt;
&lt;span class="k"&gt;ARG&lt;/span&gt;&lt;span class="s"&gt; NODE_ENV=production&lt;/span&gt;
&lt;span class="k"&gt;RUN &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;NODE_ENV&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;RUN &lt;/span&gt;&lt;span class="nv"&gt;NODE_ENV&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;NODE_ENV&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt; yarn build

&lt;span class="c"&gt;# Production image, copy all the files and run next&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;&lt;span class="s"&gt; node:alpine AS runner&lt;/span&gt;
&lt;span class="k"&gt;WORKDIR&lt;/span&gt;&lt;span class="s"&gt; /app&lt;/span&gt;
&lt;span class="k"&gt;RUN &lt;/span&gt;addgroup &lt;span class="nt"&gt;-g&lt;/span&gt; 1001 &lt;span class="nt"&gt;-S&lt;/span&gt; nodejs
&lt;span class="k"&gt;RUN &lt;/span&gt;adduser &lt;span class="nt"&gt;-S&lt;/span&gt; nextjs &lt;span class="nt"&gt;-u&lt;/span&gt; 1001

&lt;span class="c"&gt;# You only need to copy next.config.js if you are NOT using the default configuration. &lt;/span&gt;
&lt;span class="c"&gt;# Copy all necessary files used by nex.config as well otherwise the build will fail&lt;/span&gt;

&lt;span class="k"&gt;COPY&lt;/span&gt;&lt;span class="s"&gt; --from=builder /app/next.config.js ./next.config.js&lt;/span&gt;
&lt;span class="k"&gt;COPY&lt;/span&gt;&lt;span class="s"&gt; --from=builder /app/public ./public&lt;/span&gt;
&lt;span class="k"&gt;COPY&lt;/span&gt;&lt;span class="s"&gt; --from=builder --chown=nextjs:nodejs /app/.next ./.next&lt;/span&gt;
&lt;span class="k"&gt;COPY&lt;/span&gt;&lt;span class="s"&gt; --from=builder /app/node_modules ./node_modules&lt;/span&gt;
&lt;span class="k"&gt;COPY&lt;/span&gt;&lt;span class="s"&gt; --from=builder /app/package.json ./package.json&lt;/span&gt;
&lt;span class="k"&gt;COPY&lt;/span&gt;&lt;span class="s"&gt; --from=builder /app/pages ./pages&lt;/span&gt;

&lt;span class="k"&gt;USER&lt;/span&gt;&lt;span class="s"&gt; nextjs&lt;/span&gt;

&lt;span class="c"&gt;# Expose&lt;/span&gt;
&lt;span class="k"&gt;EXPOSE&lt;/span&gt;&lt;span class="s"&gt; 3000&lt;/span&gt;

&lt;span class="c"&gt;# Next.js collects completely anonymous telemetry data about general usage.&lt;/span&gt;
&lt;span class="c"&gt;# Learn more here: https://nextjs.org/telemetry&lt;/span&gt;
&lt;span class="c"&gt;# Uncomment the following line in case you want to disable telemetry.&lt;/span&gt;
&lt;span class="k"&gt;ENV&lt;/span&gt;&lt;span class="s"&gt; NEXT_TELEMETRY_DISABLED 1&lt;/span&gt;
&lt;span class="k"&gt;CMD&lt;/span&gt;&lt;span class="s"&gt; ["yarn", "start"]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;We're running a &lt;a href="https://docs.docker.com/develop/develop-images/multistage-build/"&gt;multi-stage build&lt;/a&gt; for this deployment.&lt;br&gt;
Notice the ARG and ENV keywords? That's how we pass our environment variables to the client code since we won't have access to any &lt;code&gt;.env&lt;/code&gt; files within the container. More on this later.&lt;/p&gt;

&lt;p&gt;We'll then build and tag our image&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;docker build &lt;span class="nt"&gt;--build-arg&lt;/span&gt; &lt;span class="nv"&gt;NEXT_PUBLIC_BACKEND_URL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;http://localhost:8000/api &lt;span class="nt"&gt;--build-arg&lt;/span&gt; &lt;span class="nv"&gt;NEXT_PUBLIC_META_API_KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;your_api_key &lt;span class="nt"&gt;-t&lt;/span&gt; meta-news &lt;span class="nb"&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;This may take a while depending on your internet connection and hardware specs.&lt;br&gt;
Once everything checks out run the container&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;docker run &lt;span class="nt"&gt;-p&lt;/span&gt; 3000:3000 meta-news
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Launch your browser and your app should be accessible at '&lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;' üéâ&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#set-up-github-actions"&gt;
  &lt;/a&gt;
  Set up Github actions
&lt;/h2&gt;

&lt;p&gt;GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline. You can create workflows that build and test every pull request to your repository, or deploy merged pull requests to production.&lt;/p&gt;

&lt;p&gt;For more about this wonderful platform, head over to their &lt;a href="https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions"&gt;official tutorial page&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We'll create our first workflow by running the following commands in the CLI. You can use the GUI if you aren't comfortable with the command line ü§ó.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;&lt;span class="nb"&gt;mkdir&lt;/span&gt; .github &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;mkdir&lt;/span&gt; ./github/workflow &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;touch&lt;/span&gt; ./github/workflows/deploy.yml &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; nano ./github/workflows/deploy.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Populate the deploy.yml file with the following values.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight yaml"&gt;&lt;code&gt;&lt;span class="na"&gt;name&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;Build and Deploy&lt;/span&gt;

&lt;span class="c1"&gt;# Controls when the action will run. Triggers the workflow on push or pull request&lt;/span&gt;
&lt;span class="c1"&gt;# events but only for the master branch&lt;/span&gt;
&lt;span class="na"&gt;on&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
  &lt;span class="na"&gt;push&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
    &lt;span class="na"&gt;branches&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="pi"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;main&lt;/span&gt;&lt;span class="pi"&gt;]&lt;/span&gt;
  &lt;span class="na"&gt;workflow_dispatch&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
    &lt;span class="na"&gt;inputs&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
      &lt;span class="na"&gt;logLevel&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
        &lt;span class="na"&gt;description&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="s"&gt;Log&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;level'&lt;/span&gt;
        &lt;span class="na"&gt;required&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="no"&gt;true&lt;/span&gt;
        &lt;span class="na"&gt;default&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="s"&gt;warning'&lt;/span&gt;

&lt;span class="c1"&gt;# A workflow run is made up of one or more jobs that can run sequentially or in parallel&lt;/span&gt;
&lt;span class="na"&gt;jobs&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
  &lt;span class="c1"&gt;# This workflow contains a single job called "build"&lt;/span&gt;
  &lt;span class="na"&gt;build&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# The type of runner that the job will run on&lt;/span&gt;
    &lt;span class="na"&gt;runs-on&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;ubuntu-latest&lt;/span&gt;
    &lt;span class="na"&gt;container&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;node:14&lt;/span&gt;

    &lt;span class="c1"&gt;# Steps represent a sequence of tasks that will be executed as part of the job&lt;/span&gt;
    &lt;span class="na"&gt;steps&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
      &lt;span class="c1"&gt;# Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it&lt;/span&gt;
      &lt;span class="pi"&gt;-&lt;/span&gt; &lt;span class="na"&gt;uses&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;actions/checkout@v2&lt;/span&gt;

      &lt;span class="pi"&gt;-&lt;/span&gt; &lt;span class="na"&gt;name&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;Build and Publish to Github Packages Registry&lt;/span&gt;
        &lt;span class="na"&gt;uses&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;elgohr/Publish-Docker-Github-Action@master&lt;/span&gt;
        &lt;span class="na"&gt;env&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
          &lt;span class="na"&gt;NEXT_PUBLIC_BACKEND_URL&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;${{ secrets.APP_NEXT_PUBLIC_BACKEND_URL }}&lt;/span&gt;
          &lt;span class="na"&gt;NEXT_PUBLIC_META_API_KEY&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;${{ secrets.APP_NEXT_PUBLIC_META_API_KEY }}&lt;/span&gt;
        &lt;span class="na"&gt;with&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
          &lt;span class="na"&gt;name&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;my_github_username/my_repository_name/my_image_name&lt;/span&gt;
          &lt;span class="na"&gt;registry&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;ghcr.io&lt;/span&gt;
          &lt;span class="na"&gt;username&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;${{ secrets.USERNAME }}&lt;/span&gt;
          &lt;span class="na"&gt;password&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;${{ secrets. GITHUB_TOKEN }}&lt;/span&gt;
          &lt;span class="na"&gt;dockerfile&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;Dockerfile&lt;/span&gt;
          &lt;span class="na"&gt;buildargs&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;NEXT_PUBLIC_BACKEND_URL,NEXT_PUBLIC_META_API_KEY&lt;/span&gt;
          &lt;span class="na"&gt;tags&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;latest&lt;/span&gt;

      &lt;span class="pi"&gt;-&lt;/span&gt; &lt;span class="na"&gt;name&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;Deploy package to digitalocean&lt;/span&gt;
        &lt;span class="na"&gt;uses&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;appleboy/ssh-action@master&lt;/span&gt;
        &lt;span class="na"&gt;env&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
          &lt;span class="na"&gt;GITHUB_USERNAME&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;${{ secrets.USERNAME }}&lt;/span&gt;
          &lt;span class="na"&gt;GITHUB_TOKEN&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;${{ secrets. GITHUB_TOKEN }}&lt;/span&gt;
        &lt;span class="na"&gt;with&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
          &lt;span class="na"&gt;host&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;${{ secrets.DEPLOY_HOST }}&lt;/span&gt;
          &lt;span class="na"&gt;port&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;${{ secrets.DEPLOY_PORT }}&lt;/span&gt;
          &lt;span class="na"&gt;username&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;${{ secrets.DEPLOY_USER }}&lt;/span&gt;
          &lt;span class="na"&gt;key&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;${{ secrets.DEPLOY_KEY }}&lt;/span&gt;
          &lt;span class="na"&gt;envs&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;GITHUB_USERNAME, GITHUB_TOKEN&lt;/span&gt;
          &lt;span class="na"&gt;script&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="pi"&gt;|&lt;/span&gt;
            &lt;span class="s"&gt;docker login ghcr.io -u $GITHUB_USERNAME -p $GITHUB_TOKEN&lt;/span&gt;
            &lt;span class="s"&gt;docker pull ghcr.io/my_github_username/my_repository_name/my_image_name:latest&lt;/span&gt;
            &lt;span class="s"&gt;docker stop containername&lt;/span&gt;
            &lt;span class="s"&gt;docker system prune -f&lt;/span&gt;
            &lt;span class="s"&gt;docker run --name containername -dit -p 3000:3000 ghcr.io/my_github_username/my_repository_name/my_image_name:latest&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;You may have noticed our actions are very secretive üòÇ. Worry not, this is deliberately done to protect your sensitive information from prying eyes. They're encrypted environment variables that you(repo owner) creates for a repo that uses Github actions.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;One thing to note is that the &lt;code&gt;GITHUB_TOKEN&lt;/code&gt; secret is &lt;a href="https://docs.github.com/en/actions/security-guides/automatic-token-authentication"&gt;automatically created&lt;/a&gt; for us when running the action.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To create secrets go to your repository &amp;gt; settings &amp;gt; left-sidebar &amp;gt; secrets&lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--pIS7-oC5--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://docs.github.com/assets/cb-21851/images/help/repository/repo-actions-settings.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--pIS7-oC5--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://docs.github.com/assets/cb-21851/images/help/repository/repo-actions-settings.png" alt="secrets_creation" width="880" height="156"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For an in-depth walkthrough, see &lt;a href="https://docs.github.com/en/actions/security-guides/encrypted-secrets"&gt;this guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The expected Github secrets are&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;APP_NEXT_PUBLIC_BACKEND_URL - live backend server url
APP_NEXT_PUBLIC_META_API_KEY - prod api key to thirdparty integration
DEPLOY_HOST - IP to Digital Ocean (DO) droplet
DEPLOY_KEY - SSH secret (pbcopy &amp;lt; ~/.ssh/id_rsa) and the public key should be added to `.ssh/authorized_keys` in server
DEPLOY_PORT - SSH port (22)
DEPLOY_USER  - User on droplet
USERNAME - Your Github username
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h3&gt;
  &lt;a href="#lift-off"&gt;
  &lt;/a&gt;
  Lift Off üöÄ
&lt;/h3&gt;

&lt;p&gt;Push to the main branch&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;git add &lt;span class="nt"&gt;-A&lt;/span&gt;
git commit &lt;span class="nt"&gt;-m&lt;/span&gt; &lt;span class="s2"&gt;"Initial commit"&lt;/span&gt;
git push origin main
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;If everything runs as expected, you should see a green checkmark in your repository with the build steps complete.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--yHRm7dx1--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ts6y2zrz339m3vlenqsj.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--yHRm7dx1--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ts6y2zrz339m3vlenqsj.png" alt="Github_actions_deploy" width="880" height="372"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;From there, you can setup a reverse proxy such as nginx within your server and point the host to "&lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;".&lt;/p&gt;

&lt;p&gt;Yay!ü•≥ we have successfully created a continuous delivery pipeline and hopefully, now you'll concentrate on code instead of infrastructure.&lt;/p&gt;

&lt;p&gt;Should you have any questions, please do not hesitate to reach out to me on &lt;a href="https://twitter.com/lewis_kihiu"&gt;Twitter&lt;/a&gt;.&lt;br&gt;
Comment below if you have feedback or additional input.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#shameless-plug"&gt;
  &lt;/a&gt;
  Shameless plug
&lt;/h2&gt;

&lt;p&gt;Do you need to do a lot of data mining?&lt;/p&gt;

&lt;p&gt;Scraper API is a startup specializing in strategies that'll ease the worry of your IP address from being blocked while web scraping.They utilize IP rotation so you can avoid detection. Boasting over 20 million IP addresses and unlimited bandwidth.&lt;/p&gt;

&lt;p&gt;In addition to this, they provide CAPTCHA handling for you as well as enabling a headless browser so that you'll appear to be a real user and not get detected as a web scraper. It has integration for popular platforms such as python ,node.js, bash, PHP and ruby. All you have to do is concatenate your target URL with their API endpoint on the HTTP get request then proceed as you normally would on any web scraper. Don't know how to webscrape? &lt;br&gt;
Don't worry, I've covered that topic extensively on the &lt;a href="https://lewiskori.com/series/web-scraping-techniques-with-python/"&gt;webscraping series&lt;/a&gt;. All entirely free!&lt;/p&gt;

&lt;p&gt;&lt;a href="https://www.scraperapi.com?fpr=lewiskori"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Tbk7e1mm--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://d2gdx5nv84sdx2.cloudfront.net/uploads/ssvxh57a/marketing_asset/banner/2670/069-ScraperAPI-GIF-320x50-v1.gif" alt="scraperapi" width="320" height="50"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Using &lt;a href="https://www.scraperapi.com?via=lewis93"&gt;my scraperapi referrall link&lt;/a&gt; and the promo code lewis10, you'll get a 10% discount on your first purchase!! You can always start on their generous free plan and upgrade when the need arises.&lt;/p&gt;

</description>
      <category>github</category>
      <category>javascript</category>
      <category>nextjs</category>
      <category>todayilearned</category>
    </item>
    <item>
      <title>Open Source win: IBM shifts legacy commerce systems to my one-man OSS project</title>
      <author>Michael Bromley</author>
      <pubDate>Fri, 17 Dec 2021 08:58:10 +0000</pubDate>
      <link>https://dev.to/michlbrmly/open-source-win-ibm-shifts-legacy-commerce-systems-to-my-one-man-oss-project-4n9b</link>
      <guid>https://dev.to/michlbrmly/open-source-win-ibm-shifts-legacy-commerce-systems-to-my-one-man-oss-project-4n9b</guid>
      <description>&lt;p&gt;For the past 3.5 years I've been working on an open-source e-commerce platform, &lt;a href="https://www.vendure.io/"&gt;Vendure&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It's in a niche known as "headless commerce" which is really hot right now, so I've had lots of interest from investors over the past year. However, I'm determined to see how far I can take things as a bootstrapped indie-hacker.&lt;/p&gt;

&lt;p&gt;Yesterday was a major milestone as I was able to announce that &lt;a href="https://www.vendure.io/case-study/ibm/"&gt;IBM have adopted Vendure&lt;/a&gt; to replace an outdated, proprietary system that had been slowing them down and causing huge expense in terms of license fees as well as lost developer productivity.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--z2PaT0C6--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://www.vendure.io/case-study/ibm/ibm-header.jpg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--z2PaT0C6--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://www.vendure.io/case-study/ibm/ibm-header.jpg" alt="case study image" width="877" height="438"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is really exciting because it shows just how much progress the open-source movement has made, that even the largest enterprises are willing to bet on lean, relatively new projects like Vendure - a single-person open-source project!&lt;/p&gt;

&lt;p&gt;It's a sign that the technology stack &lt;em&gt;is&lt;/em&gt; important. Developer experience, maintainability, documentation, openness - all these things actually contribute to the cost of running a system. So from both a technical &lt;em&gt;and&lt;/em&gt; financial viewpoint, OSS solutions increasingly make more sense.&lt;/p&gt;

&lt;p&gt;There's that old phrase, "nobody got fired for buying IBM". Well, I'm hoping that this will make it clear that my project is a safe choice, no matter the company size.&lt;/p&gt;

&lt;p&gt;Just thought I'd share the success!&lt;/p&gt;

&lt;p&gt;If you want to know more about Vendure, I recently wrote a &lt;a href="https://dev.to/michlbrmly/set-up-a-nodejs-e-commerce-app-in-10-minutes-with-vendure-287b"&gt;short intro tutorial&lt;/a&gt;, and I plan a more in-depth series of tutorials in the coming weeks.&lt;/p&gt;

</description>
      <category>opensource</category>
      <category>startup</category>
    </item>
    <item>
      <title>Getting Started with Comet ML</title>
      <author>Angelica Lo Duca</author>
      <pubDate>Fri, 17 Dec 2021 08:24:46 +0000</pubDate>
      <link>https://dev.to/alod83/getting-started-with-comet-ml-169k</link>
      <guid>https://dev.to/alod83/getting-started-with-comet-ml-169k</guid>
      <description>&lt;p&gt;An overview of the popular experimentation platform for Machine Learning, with a practical example.&lt;/p&gt;

&lt;p&gt;Comet ML is an experimentation platform, which permits testing Machine Learning projects, from the beginning up to the final monitoring. Many other similar platforms exist on the Web, including Neptune.ai, Guild.ai, Sacred, and so on.&lt;br&gt;
Comet ML can be easily integrated with the most popular Machine Learning libraries, including scikit-learn, Pytorch, Tensorflow, Keras, and so on. Experiments can be written in Python, Javascript, Java, R, and REST APIs.&lt;br&gt;
In this article, I focus on Python.&lt;/p&gt;

&lt;p&gt;The article is organized as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Overview of Comet ML&lt;/li&gt;
&lt;li&gt;Working with Comet ML&lt;/li&gt;
&lt;li&gt;Example of usage&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;
  &lt;a href="#1-overview-of-comet-ml"&gt;
  &lt;/a&gt;
  1 Overview of Comet ML
&lt;/h2&gt;

&lt;p&gt;Comet ML is an online platform, which permits tracking experiments. The main advantage of Comet ML is that I can easily build a reporting dashboard and a monitoring system.&lt;br&gt;
Comet ML provides the following features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;compare experiments: I can easily build different experiments for the same project, and compare the results, in terms of metrics, system metrics, parameters, and so on;
monitor the model: I can monitor the model from the early stages up to production. This can be done through alerts and debugging strategies;&lt;/li&gt;
&lt;li&gt;collaborate with other people: I can share my workspace project with other people;&lt;/li&gt;
&lt;li&gt;build reports and panels: starting from the results of my experiment, I can easily build reports and dashboards;
keep my project public or make it public.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;
  &lt;a href="#2-working-with-comet-ml"&gt;
  &lt;/a&gt;
  2 Working with Comet ML
&lt;/h2&gt;

&lt;p&gt;Once I enter the Comet ML Web site, I can create a free account. Then, I log in to the platform and I create a new project, by clicking the relative top right button. I fill the form with needed information.&lt;/p&gt;

&lt;p&gt;When I click on the Create button, an empty dashboard appears.&lt;/p&gt;

&lt;p&gt;I can add a new experiment, by clicking the button on the top right of the page (+Add). I select Experiment from the dropdown menu.&lt;br&gt;
The platform generates a new API for the experiment, that can be used in my Python code:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# import comet_ml at the top of your file
from comet_ml import Experiment

# Create an experiment with your api key
experiment = Experiment(
    api_key="PUT HERE MY API KEY",
    project_name="decisiontree",
    workspace="PUT HERE MY ACCOUNT NAME",
)
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Now, I need to install the comet_ml Python package on my local computer:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;pip3 install comet_ml 
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;And then I can start coding, as usual.&lt;/p&gt;

&lt;p&gt;Continue Reading on &lt;a href="https://towardsdatascience.com/getting-started-with-comet-ml-549d44aff0c9"&gt;Towards Data Science&lt;/a&gt;&lt;/p&gt;

</description>
      <category>machinelearning</category>
      <category>python</category>
      <category>datascience</category>
      <category>artificialintelligence</category>
    </item>
    <item>
      <title>Building Docker Images Faster for Webpack</title>
      <author>Pablo Chico de Guzman</author>
      <pubDate>Fri, 17 Dec 2021 07:58:33 +0000</pubDate>
      <link>https://dev.to/okteto/building-docker-images-faster-for-webpack-44jc</link>
      <guid>https://dev.to/okteto/building-docker-images-faster-for-webpack-44jc</guid>
      <description>&lt;p&gt;At Okteto we are very passionate about increasing developer productivity. We were looking for ways to improve our container build times when we came across an amazing project, &lt;a href="https://github.com/moby/buildkit"&gt;BuildKit&lt;/a&gt;. BuildKit helps us build Docker images faster than ever. &lt;/p&gt;

&lt;p&gt;But when getting started we found that there were very few practical examples on how one could optimize their Dockerfiles with Buildkit - so we thought why not change that? This article is going to be about how you too can optimize your Docker builds using &lt;a href="https://github.com/moby/buildkit/blob/master/frontend/dockerfile/docs/syntax.md#run---mounttypecache"&gt;Buildkit caches&lt;/a&gt;!&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#the-why"&gt;
  &lt;/a&gt;
  The Why
&lt;/h2&gt;

&lt;p&gt;The first question which arises is, why even bother with optimizing your Dockerfiles? Well, the thing is that containers have revolutionized how we build, ship, and run our applications these days. So much so that containers are now heavily integrated into the development cycle and are built over and over - manually or in CI pipelines for testing and live previews.&lt;/p&gt;

&lt;p&gt;The biggest quality of life improvement that comes with better image build times is faster testing and review cycles. The quicker everyone can see and test the changes they make, the more the productivity of the team would increase! We have first-hand witnessed this for our website, but more on that &lt;a href="#performance-evaluation"&gt;later&lt;/a&gt;. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#the-standard-dockerfile-for-webpack"&gt;
  &lt;/a&gt;
  The Standard Dockerfile for Webpack
&lt;/h2&gt;

&lt;p&gt;Before we look at how we can speed up build times using BuildKit, let's first see how a standard Dockerfile for Webpack should look like.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight yaml"&gt;&lt;code&gt;&lt;span class="c1"&gt;# Dockerfile&lt;/span&gt;

&lt;span class="s"&gt;FROM node:16 as build&lt;/span&gt;

&lt;span class="s"&gt;WORKDIR /usr/src/app&lt;/span&gt;

&lt;span class="s"&gt;COPY package.json yarn.lock ./&lt;/span&gt;
&lt;span class="s"&gt;RUN yarn install&lt;/span&gt;

&lt;span class="s"&gt;COPY . .&lt;/span&gt;
&lt;span class="s"&gt;RUN yarn build&lt;/span&gt;

&lt;span class="s"&gt;FROM nginx:alpine&lt;/span&gt;
&lt;span class="s"&gt;COPY --from=build /usr/src/app/dist /usr/share/nginx/html&lt;/span&gt;
&lt;span class="s"&gt;EXPOSE &lt;/span&gt;&lt;span class="m"&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;This Dockerfile uses a &lt;code&gt;build&lt;/code&gt; step to build the final artifacts, and then, it copies the artifacts to an &lt;code&gt;nginx&lt;/code&gt; image.&lt;/p&gt;

&lt;p&gt;In order to optimize the Dockerfile instructions cache, we are purposefully &lt;strong&gt;first&lt;/strong&gt; copying &lt;code&gt;package.json&lt;/code&gt; and &lt;code&gt;yarn.lock&lt;/code&gt; to install the runtime dependencies. We don't copy over everything else in this same step since most of the commits don't change the yarn dependencies, allowing this step to be cached in those cases leading to better build times! But this is already a commonly known best practice and BuildKit would help us optimize the process even more.&lt;/p&gt;

&lt;p&gt;The next thing we do is to &lt;code&gt;COPY&lt;/code&gt; all our remaining files from our local folder to our &lt;code&gt;WORKDIR&lt;/code&gt;. Our changes would be made to one of these files so this means that &lt;code&gt;yarn build&lt;/code&gt; is always executed unlike the &lt;code&gt;yarn install&lt;/code&gt; instruction above.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Remember to configure your &lt;code&gt;.dockerignore&lt;/code&gt; file. In this case, adding &lt;code&gt;.git&lt;/code&gt; or &lt;code&gt;node_modules&lt;/code&gt; to your &lt;code&gt;.dockerignore&lt;/code&gt; file will dramatically reduce the time to send your build context to the docker daemon. This is even more relevant if your build is running on a remote server.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2&gt;
  &lt;a href="#optimizing-the-standard-dockerfile"&gt;
  &lt;/a&gt;
  Optimizing the Standard Dockerfile
&lt;/h2&gt;

&lt;p&gt;Now that we've looked at how a standard Dockerfile for Webpack looks like, let's see how we can leverage BuildKit caches to further reduce our build times. BuildKit enables us to configure cache folders for the &lt;code&gt;RUN&lt;/code&gt; instructions in our Dockerfile.&lt;/p&gt;

&lt;p&gt;Contents of the BuildKit cache directories persist between builder invocations without invalidating the instruction cache. BuildKit also takes care of compatibility - our builds would work with any contents of the cache directory. This is necessary since another build may overwrite the cache files or GC may clean it if more storage space is needed.&lt;/p&gt;

&lt;p&gt;In our case, we will use BuildKit caches to persist the &lt;a href="https://classic.yarnpkg.com/en/docs/cli/cache"&gt;yarn cache&lt;/a&gt; and the &lt;a href="https://webpack.js.org/configuration/cache/#cachecachedirectory"&gt;webpack cache&lt;/a&gt;.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight yaml"&gt;&lt;code&gt;&lt;span class="c1"&gt;# Dockerfile.buildkit&lt;/span&gt;

&lt;span class="s"&gt;FROM node:16 as build&lt;/span&gt;

&lt;span class="s"&gt;WORKDIR /usr/src/app&lt;/span&gt;

&lt;span class="s"&gt;COPY package.json yarn.lock ./&lt;/span&gt;
&lt;span class="s"&gt;RUN --mount=type=cache,target=/root/.yarn YARN_CACHE_FOLDER=/root/.yarn yarn install&lt;/span&gt;

&lt;span class="s"&gt;COPY . .&lt;/span&gt;
&lt;span class="s"&gt;RUN --mount=type=cache,target=./node_modules/.cache/webpack yarn build&lt;/span&gt;

&lt;span class="s"&gt;FROM nginx:alpine&lt;/span&gt;
&lt;span class="s"&gt;COPY --from=build /usr/src/app/dist /usr/share/nginx/html&lt;/span&gt;
&lt;span class="s"&gt;EXPOSE &lt;/span&gt;&lt;span class="m"&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;What this means is that &lt;code&gt;yarn install&lt;/code&gt; and &lt;code&gt;yarn build&lt;/code&gt; will be much faster. This is because their contents will be cached now onwards instead of them being run from scratch every time like earlier.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Please note that we've not covered the installation process for BuildKit in this article since it's fairly simple and is documented well &lt;a href="https://github.com/moby/buildkit#quick-start"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2&gt;
  &lt;a href="#performance-evaluation"&gt;
  &lt;/a&gt;
  Performance Evaluation
&lt;/h2&gt;

&lt;p&gt;The improvement in build times we saw for our website after using BuildKit was phenomenal! Have a look yourself.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--IRdhgSSP--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/pem7j1ujloqqxpc9dfbq.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--IRdhgSSP--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/pem7j1ujloqqxpc9dfbq.png" alt="Image description" width="880" height="698"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The first graph shows the performance improvement we saw whenever we changed a dependency of our website and the second one shows the boost we got from using BuildKit when making code changes.&lt;/p&gt;

&lt;p&gt;This means if you're building your images 100 times a day, you save more than 1 hr 23 mins of build time every day!&lt;/p&gt;

&lt;p&gt;We hope you too can now appreciate how powerful BuildKit is. A change of only two lines of code in your Dockerfile can dramatically decrease your image build times! We hope this article was useful and you too are now able to optimize your Docker builds. &lt;/p&gt;

&lt;p&gt;Happy coding!&lt;/p&gt;

</description>
      <category>docker</category>
      <category>webpack</category>
      <category>buildkit</category>
      <category>dockerfile</category>
    </item>
    <item>
      <title>Data Recovery Algorithm For HFS+ File System</title>
      <author>Michael Mirosnichenko</author>
      <pubDate>Fri, 17 Dec 2021 07:57:50 +0000</pubDate>
      <link>https://dev.to/hetmansoftware/data-recovery-algorithm-for-hfs-file-system-428k</link>
      <guid>https://dev.to/hetmansoftware/data-recovery-algorithm-for-hfs-file-system-428k</guid>
      <description>&lt;p&gt;In this article, we will explore the structure of HFS+ file system, and the changes that distinguish it from the previous version, HFS. We will also analyze algorithms for data recovery applied to HFS+ drives.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#advantages-and-peculiarities-of-hfs"&gt;
  &lt;/a&gt;
  Advantages and peculiarities of HFS+
&lt;/h2&gt;

&lt;p&gt;The main distinction of HFS+ is using the 32-bit architecture instead of the 16-bit in HFS. The older addressing system was a serious limitation by itself as it didn‚Äôt support work with volumes containing more than 65 536 blocks. For example, on a 1 GB disk, the allocation block size (cluster) under HFS is 16 KB, so even a 1 byte file would take up the entire 16 KB of disk space.&lt;/p&gt;

&lt;p&gt;Just as its predecessor, HFS+ uses the so-called B-tree to store a major part of metadata.&lt;/p&gt;

&lt;p&gt;HFS+ volumes are divided into sectors (called logical blocks in HFS), usually 512 bytes in size. One or more sectors make up an allocation block (cluster); the number of allocation blocks depends on the total size of the volume. The 32-bit addressing allows to use over 4 294 967 296 clusters compared to the 65 536 available for 16 bit. In comparison, the two file systems differ by:&lt;/p&gt;

&lt;p&gt;file name length: in HFS: 31, in HFS+: 255;&lt;br&gt;
file name encoding: HFS: Mac Roman, HFS+: Unicode;&lt;br&gt;
record size: HFS: 512 bytes, HFS+: 4 Kbyte;&lt;br&gt;
and the maximal number of files has increased: HFS: 2^31, HFS+: 2^63.&lt;/p&gt;

&lt;p&gt;YouTube: &lt;iframe width="710" height="399" src="https://www.youtube.com/embed/WUoCNzx1VwI"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#hfs-plus-architecture"&gt;
  &lt;/a&gt;
  HFS Plus architecture
&lt;/h2&gt;

&lt;p&gt;HFS Plus volumes are divided into sectors (called logical blocks in HFS), usually 512 bytes in size. These sectors are then grouped together into allocation blocks (similar to clusters in Windows) which can contain one or more sectors. The number of allocation blocks depends on the total size of the volume. HFS Plus uses a larger value to address allocation blocks than HFS, 32 bits rather than 16 bits. The file system uses Big Endian encoding.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--3HtpvFQe--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vnkj2f70pnqsxwg7p8iu.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--3HtpvFQe--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vnkj2f70pnqsxwg7p8iu.png" alt="Image description" width="500" height="628"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To manage the process of data allocation on the disk, HFS+ stores special service information known as metadata. The following elements are most important for the proper operation of the file system and are of special interest when searching for missing data:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Volume Header. Has a table-type structure, and makes use of extent records (Extents);&lt;/li&gt;
&lt;li&gt;Allocation File. Structured as a bitmap, it makes use of extent records (Extents);&lt;/li&gt;
&lt;li&gt;Catalog File. Has B-tree structure, and makes use of extent records (Extents);&lt;/li&gt;
&lt;li&gt;Extents Overflow File. Has a B-tree structure;&lt;/li&gt;
&lt;li&gt;Bad block file. Has a B-tree structure;&lt;/li&gt;
&lt;li&gt;StartUp file. Fixed size;&lt;/li&gt;
&lt;li&gt;Journal. The disk area with a fixed size and location.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The operating system includes more other structures, but the ones we have listed are of primary importance for data recovery. In order to continue examining the structure, we need to study the basic notions, B-tree and extents.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#what-is-a-btree"&gt;
  &lt;/a&gt;
  What is a B-tree?
&lt;/h2&gt;

&lt;p&gt;To store a part of information, HFS+ uses B-trees. They are required to enable writing certain amounts of data (for example, 100 MB) into fixed-size blocks (for example, 4 KB). In this case, the first block doesn‚Äôt contain the data itself, but the links to the following blocks which may either contain links to another level of blocks, or contain the actual data. The tree elements containing links are known as nodes, and zero level elements containing data are referred to as leaves.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--PHt620BO--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8zi6gxpsprz8h1jxkbzt.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--PHt620BO--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8zi6gxpsprz8h1jxkbzt.png" alt="Image description" width="788" height="338"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#what-are-extent-records-and-extent-overflow-file"&gt;
  &lt;/a&gt;
  What are extent records and Extent Overflow File?
&lt;/h2&gt;

&lt;p&gt;To store information on the number of blocks occupied by a file, HFS+ may use from 0 to 8 extent records. Every record contains a link to the first block containing data, and the number of sequential clusters used to store data. If a file is too fragmented and 8 records are not enough to describe all of its parts, the remaining fragments (elements) are written to the file with additional extents ‚Äì Extent Overflow File.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#volume-header"&gt;
  &lt;/a&gt;
  Volume header
&lt;/h2&gt;

&lt;p&gt;Volume header is always located in the second sector of the disk (counting from the beginning) and contains general information on the volume. For example, the size of allocation blocks, a timestamp that indicates when the volume was created or the location of other volume structures: Catalog File, Extent Overflow File, Allocation File, Journal etc. The second to last sector of the disk always contains Volume Header Copy, that is, the copy of the Volume Header.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--V5Y_o9VN--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/0wc7yqu7ant353hyo06p.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--V5Y_o9VN--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/0wc7yqu7ant353hyo06p.png" alt="Image description" width="788" height="639"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--_2ggTxWH--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hn89wft0ngt4byo2xypd.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--_2ggTxWH--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hn89wft0ngt4byo2xypd.png" alt="Image description" width="788" height="618"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--Q-4AzQVG--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/iyck76rlbxe3n8dilsyf.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Q-4AzQVG--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/iyck76rlbxe3n8dilsyf.png" alt="Image description" width="533" height="943"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#allocation-file"&gt;
  &lt;/a&gt;
  Allocation file
&lt;/h2&gt;

&lt;p&gt;It contains data on occupied and vacant allocation blocks. Each allocation block is represented by one bit. A zero value means the block is free and a one value means the block is in use. Sometimes, this structure is referred to as bitmap. The allocation file can change size and does not have to be stored contiguously within a volume. All information on the elements of the file is stored in the Volume Header.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--Nr9upw7l--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/upz93frnk1t2lf4c8ec6.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Nr9upw7l--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/upz93frnk1t2lf4c8ec6.png" alt="Image description" width="788" height="494"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#catalog-file"&gt;
  &lt;/a&gt;
  Catalog file
&lt;/h2&gt;

&lt;p&gt;It stores a major part of data on allocation of files and folders on the disk, and it‚Äôs actually a large B-tree structure.&lt;/p&gt;

&lt;p&gt;In HFS Plus, this file is very similar to the catalog file in HFS, the main difference being the field size. Now it is bigger and contains more data. For example, it allows to use longer 255-character Unicode names for files. In HFS, the record size is 512 bytes, compared to 4KB in HFS Plus for Mac OS and 8 KB for OS X. The fields in HFS have fixed size, while HFS Plus lets them have varied size depending on the actual amount of data.&lt;/p&gt;

&lt;p&gt;Most fields store small attributes which can be fitted into 4 KB of space. For larger attributes, additional extents are used (the maximum number available is 8 extents, and if any more are needed, they are saved to Extent Overflow File). The extents contain links to the other fields where data of the larger attribute is stored.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#startup-file"&gt;
  &lt;/a&gt;
  StartUp file
&lt;/h2&gt;

&lt;p&gt;This file is intended for operating systems which don‚Äôt support HFS or HFS Plus, And is similar to boot blocks in HFS volumes.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#bad-blocks"&gt;
  &lt;/a&gt;
  Bad blocks
&lt;/h2&gt;

&lt;p&gt;This file contains data on all relocated (faulty) blocks.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#journal"&gt;
  &lt;/a&gt;
  Journal
&lt;/h2&gt;

&lt;p&gt;Journal is not a file but rather a fixed-size area (set of blocks) on the disk. Links to the first blocks and the size of the area are stored in the Volume Header. Before any changes are written to the disk, HFS+ writes them to the journal, and only then to the system files. In case of a power cut happening during the writing operation, the file system can be restored.&lt;/p&gt;

&lt;p&gt;It should be noted, though, that the journal size in HFS+ is limited and its contents get overwritten regularly. The boot volume journal in Mac Mini is usually overwritten within 5-10 minutes, and the time before it gets overwritten in MacBook is about 30 minutes. If Time Machine is enabled, this time is reduced to 20 seconds.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--knHVrwaQ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/27m1ipwe0vw2ylnegth3.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--knHVrwaQ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/27m1ipwe0vw2ylnegth3.png" alt="Image description" width="788" height="494"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#data-recovery-in-time-machine"&gt;
  &lt;/a&gt;
  Data recovery in Time Machine
&lt;/h2&gt;

&lt;p&gt;Beginning with Mac OS X Leopard, the operating system includes Time Machine. This utility creates backup copies of files to record all changes that happen to the file system. All these things help users restore the entire operating system, several files or an individual file in exactly the same form it was in at a certain moment of time.&lt;/p&gt;

&lt;p&gt;Allocate a separate disk for Time Machine to use. Apple manufactures a special device, Apple Time Capsule, which can be used specifically as a network drive for storing Time Machine backup files. Time Machine can be used with any USB or eSATA disk. When you start it for the first time, Time Machine creates a folder on the specified backup (reserve) disk to contain all the data.&lt;/p&gt;

&lt;p&gt;Later, Time Machine will only copy the modified files. Generally speaking, as long as Time Machine is used for a disk, recovering lost data is not much of a problem.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--QKkAUuBW--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qwp7ajjoesaqk2dpnm6v.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--QKkAUuBW--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qwp7ajjoesaqk2dpnm6v.png" alt="Image description" width="578" height="380"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#recovery-algorithm-for-hfs"&gt;
  &lt;/a&gt;
  Recovery algorithm for HFS+
&lt;/h2&gt;

&lt;p&gt;Recovering data from HFS+ file system is more difficult than from other file systems. One of the aspects causing difficulties is that HFS+ uses B-trees to store most volume metadata on allocation of files. After a file is removed, the B-tree is updated immediately, so the information on where the removed file was located is lost at once.&lt;/p&gt;

&lt;p&gt;Our program lets you see the storage device and the HFS+ structure in Windows without using any extra software or drivers.&lt;/p&gt;

&lt;p&gt;While running a full analysis scan, its algorithm allows to exclude these elements when searching for lost data, and recover the information we need.&lt;/p&gt;

&lt;p&gt;If you choose a fast scan, the program reads the Volume Header or its copy, gains access to the Catalog file and location of the journal on the disk. If the blocks related to such files are not overwritten yet, it will read them and recover the data.&lt;/p&gt;

&lt;p&gt;If blocks of the removed file haven‚Äôt been overwritten, this method lets you recover the file completely. And even if overwriting has taken place, the file data can still be found in the journal, or the file will be recovered partially.&lt;/p&gt;

&lt;p&gt;The algorithm beyond full analysis allows the program to exclude certain elements while searching for deleted data, so the program will rebuild the disk structure and display the deleted files, even if Volume Header and Catalog file have been overwritten partially.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--nv33LQw2--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/o3w7b3yl533mh8k4rqu5.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--nv33LQw2--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/o3w7b3yl533mh8k4rqu5.png" alt="Image description" width="788" height="596"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#conclusion"&gt;
  &lt;/a&gt;
  Conclusion
&lt;/h2&gt;

&lt;p&gt;All in all, it can be said that HFS+ is an obsolete file system, which has come to exist as the optimized version of an even older file system, HFS. At the moment, HFS+ is replaced by Apple File System.&lt;/p&gt;

&lt;p&gt;In terms of performance, safety and reliability, HFS+ fall behind APFS considerably, so the issue of data recovery becomes especially relevant. Information rarely disappears without trace, so knowing well how the file system works, you may hope to recover even the elements initially considered to have been lost for good.&lt;/p&gt;

&lt;p&gt;YouTube: &lt;iframe width="710" height="399" src="https://www.youtube.com/embed/e1xg5hANli0"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;Read the &lt;a href="https://hetmanrecovery.com/recovery_news/hfs-file-system-data-recovery-algorithm.htm"&gt;full article&lt;/a&gt; with all additional video tutorials. Also visit our &lt;a href="https://www.youtube.com/channel/UCu-D9QnPsAPn7AtxL4HXLUg"&gt;Youtube channel&lt;/a&gt;, there are over 400 video tutorials.&lt;/p&gt;

</description>
      <category>beginners</category>
      <category>testing</category>
      <category>tutorial</category>
      <category>security</category>
    </item>
  </channel>
</rss>
