<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>DEV Community</title>
    <author>DEV Community</author>
    <description>A constructive and inclusive social network for software developers. With you every step of your journey.</description>
    <link>https://dev.to</link>
    <language>en</language>
    <item>
      <title>AWS Certified SysOps Administrator SOA-C02 Exam Questions Part 2</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 15:30:18 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-sysops-administrator-soa-c02-exam-questions-part-2-3l85</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-sysops-administrator-soa-c02-exam-questions-part-2-3l85</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A fleet of servers must send local logs to Amazon Cloudwatch. How should the servers be configured to meet these requirements ?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Configure AWS Config to forward events to cloudwatch
B. Configure a simple network management protocol (SNMP) agent to forward events to Cloudwatch
C. Install and configure the unified Cloudwatch agent
D. Install and configure the Amazon Inspector agent
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company data retention policy dictates that backups be stored for exactly two years. After that the data must be deleted. How can Amazon EBS snapshots be managed to conform to this data retention policy?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use an Amazon S3 lifecycle policy to delete snapshots older than two years
B. Configure Amazon Inspector to find and delete old EBS Snapshots
C. Schedule an AWS Lambda function using Cloudwatch events to periodically run a scripts to delete old snapshots
D. Configure an Amazon Cloudwatch Alarm to trigger the launch of an AWS Cloudformation template that will clean the older snapshots
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;In configuring an Amazon Route 53 health check, a SysOps Administrator selects ‘Yes’ to the String Matching option in the Advanced Configuration section. In the Search String box, the Administrator types the following text: /html. This is to ensure that the entire page is loading during the health check. Within 5 minutes of enabling the health check, the Administrator receives an alert stating that the check failed. However, when the Administrator navigates to the page, it loads successfully. What is the Most likely cause of this false alarm?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The search string is not HTML encoded
B. The search string must be put in quotes
C. The search string must be escaped with a backslash (\) before the forward slash (/)
D. The search string is not in the first 5120 bytes of the tested page
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator must ensure that AWS Cloudformation deployment changes are properly backend for governance. Which AWS Service should be used to accomplish this?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. AWS Artifact
B. AWS Config
C. Amazon Inspector
D. AWS Trusted Advisor
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Developer created an AWS Lambda function and has asked the SysOps Administrator to make the function run in every 15 minutes . What is the MOST efficient way to accomplish this request?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create an Amazon EC2 instance and schedule a cron to invoke the Lambda function
B. Create a repeat time variable inside the Lambda function to invoke the Lambda function
C. Create a second Lambda function to monitor and invoke the first Lambda function
D. Create an Amazon Cloudwatch scheduled event to invoke the Lambda function
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is analyzing how Reserved Instance discounts are allocated to Amazon EC2 instances across multiple AWS Account. Which AWS tool will provide the details necessary to understand the billing charges?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. AWS Budgets
B. AWS Cost and Usage report
C. AWS Trusted Advisor
D. AWS Organizations
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator wants to prevent Developer from accidentally terminating Amazon EC2 instance. How can this be accomplished?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use AWS Systems Manager to restrict EC2 termination
B. Use AWS Config to restrict EC2 termination
C. Application Amazon Cloudwatch event to prevent EC2 termination
D. Enable termination protection on EC2 instances
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization has developed a new memory intensive application that is deployed to a large Amazon EC2. The application is exhaustion, so the development team wants to monitor memory usage by using Amazon Cloudwatch. What is the MOST efficient way to accomplish this goal?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Deploy the solution to memory-optimized EC2 instances and use the cloudwatch MemoryUtilization metrics
B. Enable the memory monitoring option by using AWS Config
C. Install the AWS System Manager agent on applicable EC2 instances to monitor memory
D. Monitor memory by using a script within the instance and send it to cloudwatch as a custom metric
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization has been running their website on several m2 Linux instances behind a classic load balancer for two years. Application load has been constant and predictable. What should the organization do to reduce costs?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Purchase Reserved instances for the specific m2.instances
B. Change the m2 instances to equivalent m5 types, and purchase Reserved instances for the specific m5 instances
C. Change the classic load balancer to an application load balancer and purchase reserved instances for the specific m2 instances
D. Purchase Spot instances for the specific m2 instances
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator has written an AWS Lambda function to launch new Amazon EC2 instances and deployed it in the us-east-1 region. The Administrator tested it by launching a new t2 nano instance in the us-east-1 region and it performed as expected. However, when the region name was updated in the Lambda function to launch an EC2 instance in the us-west-1 region, it failed. What is causing this error?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The AMI ID must be updated for the us-west-1 region in the Lambda function as well
B. The Lambda function can only launch EC2 instances in the same region where it is deployed
C. The Lambda function does not have the necessary IAM permission to launch more than one EC2 instance
D. The instance type defined in the Lambda function is not available in the us-west-1 region
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company backs up data from data center using a tape gateway on AWS Storage Gateway. The SysOps Administrator must stop a running storage gateway. What process will protect data integrity?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Stop storage gateway and reboot the virtual machine, then restart Storage Gateway
B. Reboot the virtual machine then restart storage gateway
C. Reboot the virtual machine
D. Shutdown the virtual machine and stop storage gateway then turn the virtual machine
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is responsible for a legacy, CPU heavy application. The application can only be scaled vertical. Currently application running on t2.large Amazon EC2 instance. The system is showing 90% CPU usage and significant performance latency. What change should be made to alleviate the performance problem?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Change the EBS volume to provisioned IOPS
B. Upgrade to a compute-optimized instance
C. Add additional t2.large instances to the application
D. Purchase the Reserved Instance
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator runs a web application that is using a microservices approach whereby different responsibilities of the application have been divided in a separate microservice running on a different Amazon EC2 instance. The Administrator has been tasked with reconfiguring the infrastructure to support this approach. How can the Administrator accomplish this with the LEAST administrative overhead?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use Amazon Cloudfront to log the URL and forward the request
B. Use Amazon Cloudfront to rewrite the header base on the micro service and forward the request
C. Use an Application Load Balancer (ALB) and do path-based routing
D. Use a Network Load Balancer (NLB) and do path-based routing
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization is concerned that its Amazon RDS databases are not protected. The solution to address this issue must be low cost, protect against table corruption that could be overlooked for several days, and must offer a 30-day window of protection. How can these requirement must be met?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Enable multi-AZ on the RDS Instance to maintain the data in a second Availability Zone
B. Create a Read Replica of the RDS Instance to maintain the data in a second region
C. Ensure that automated backups are enabled and set the appropriate retention period
D. Enable versioning in RDS to recover altered table data when needed
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company with a dozens of AWS Account wants to ensure that governance rules are being applied across all accounts. The CIO has recommended that AWS Config rules be deployed using an AWS Cloudformation template. How should the requirements be met?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create the Cloudformation stack set then select Cloudformation template and use it to configure the AWS accounts
B. Write a script that iterates over the Company AWS accounts and executes the Cloudformation template in each account
C. Use AWS Organizations to execute the Cloudformation template in all accounts
D. Create a Cloudformation template in the master account of AWS. Organizations and execute the Cloudformation template to create AWS Config rules in all accounts
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company must ensures that any objects uploaded to an s3 bucket must be encrypted. Which of the following actions will meet the requirement? ( SELECT TWO)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Implement AWS Shield to protect again unencrypted objects stored in s3 buckets
B. Implement Object access control list (ACL) to deny unencrypted objects from being uploaded to the S3 bucket
C. Implement Amazon S3 default encryption to make sure that any object being uploaded is encrypted before it is stored
D. Implement Amazon Inspector to inspect objects uploaded to s3 bucket to make sure that they are encrypted
E. Implement S3 bucket policies to deny unencrypted objects from being uploaded to the buckets
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Based on the AWS Shared Responsibility Model, which of the following actions are the responsibility of the customer for an Aurora database?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Performing underlying OS updates
B. Provisioning of storage for database
C. Scheduling maintenance, patches and other updates
D. Executing maintenance, patches and other updates
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company would like to review each change in the infrastructure before deploying updates in its AWS Cloudformation stacks. Which action will allow an Administrator to understand the impact of these changes before implement?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Implement a blue/green strategy using AWS Elastic Beanstalk
B. Perform a canary deployment using a Application Load Balancer and target groups
C. Create a change set for the running stack
D. Submit the update using UpdateStack API call.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization is running multiple applications for their customers. Each application is deployed by running a base AWS CloudFormation template that configures a new VPC. All applications are run in the same AWS account and AWS Region. A SysOps Administrator has noticed that when trying to deploy the same AWS CloudFormation stack, it fails to deploy. What is likely to be the problem?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The Amazon Machine Image used is not Available in that region
B. The AWS Cloudformation template needs to be update to the latest version
C. The VPC configurations parameters have changed and must be updated in the template
D. The account has reached the default limit for VPCs allowed
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator found that newly-deployed Amazon EC2 application server is unable to connect to an Amazon RDS database. VPC Flow Logs and confirming that the flow log is active on the console, the log group cannot be located on Amazon Cloudwatch. What are the MOST likely reasons for this situation? (SELECT TWO)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The Administrator must configure the VPC Flow Logs to have them sent to AWS CloudTrail
B. The Administrator has waited less than ten minutes for the log group to be created in Cloudwatch
C. The account VPC Flow Logs have been disabled by using a service control policy
D. No relevant traffic has been sent since the VPC Flow Logs were created
E. The account has Amazon Guard Duty enabled.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has mandated the use of multi-factor authentication (MFA) for all IAM users, and requires users to make all API-calls using the CLI. However, users are not prompted to enter MFA tokens, and are able to run CLI commands without MFA. In an attempt to enforce MFA, the company attached an IAM policy to all users that denies API calls that have not been authenticated with MFA. What additional step must be taken to ensure that API calls are authenticated using MFA?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Enable MFA on IAM roles and require IAM users to use role credentials to sign API calls.
B. Ask the IAM users to log into the AWS Management Console with MFA before making API calls using the CLI
C. Restricts the IAM users to use of the console, as MFA is not supported for CLI use
D. Require user to use temporary credentials from the get sessions token command to sign API calls
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;p&gt;62.An application running on Amazon EC2 allows users to launch batch jobs for data analysis. The jobs are run asynchronously, and the user is notified when they are complete. While multiple jobs can run concurrently, a user’s request need not be fulfilled for up to 24 hours. To run a job, the application launches an additional EC2 instance that performs all the analytics calculations. A job takes between 75 and 110 minutes to complete and cannot be interrupted. What is the MOST cost-effective way to run this workload?&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;    A. Run the application on-Demand EC2 instances. Run the jobs on spot instances with a specified duration
    B. Run the application on Reserved instance EC2 instances. Run the jobs on AWS Lambda
    C. Run the application on On-Demand EC2 instances. Run the jobs on On-Demand EC2 instances
    D. Run the application on Reserved instance EC2 instances. Run the jobs on spot instances with a specified duration
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization has two AWS accounts Development and Production. A SysOps Administrator manages access via IAM. Users require in Development should have access to certain resource in Production. How can this be accomplished?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create an IAM role in Production account with the Development account as a trusted entity and then allow those users from Development account to assume the Production account IAM role
B. Create a group of IAM users in the Development account and add Production account service ARNs as resources in the IAM policy
C. Establish a federation between the two accounts using the on-premises Microsoft Active Directory and allow development account to access the Production account through this federation
D. Establish an Amazon Cognito Federated Identity between the two accounts and allow the Development account to access the Production account through this federation
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator has been able to consolidate multiple secure websites onto a single servers and each site is running on a different port. The Administrator now wants to start a duplicate server in a second Availability Zone and put both behind a Load Balancer for high availability. What would be the command line necessary to deploy one of the sites certificates to the load balancer?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. aws kms modify-listener –loadbalancer-name my-loadbalancer –certificates CertificateARN arn:aws:iam::123456:server-certificate/my-new-server-cert
B. aws elb set-load-balancer-listener-ssl-cerficate –load-balancer-name my-load-balancer –load-balaner-port 443 –ssl-cerficate-id arn:aws:iam::123456:server-certificate/new-server-cert
C. aws ec2 put-ssl-certificate –loadbalancer-name my-loadbalancer –load-balaner-port 443 –ssl-cerficate-id arn:aws:iam::123456:server-certificate/new-server-cert
D. aws acm put-ssl-cerficate –loadbalancer-name my-loadbalancer –load-balaner-port 443 –ssl-cerficate-id arn:aws:iam::123456:server-certificate/new-server-cert
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An application resides on multiple EC2 instances in public subnets in two Availability Zones. To improve security Application Load Balancer (ALB) in separate subnets and pointed the DNS at the ALB instead of EC2 instances. After the change, traffic is not reaching the instances and an error is being returned from the ALB. What steps must a SysOps Administrator take to resolve this issue and improve the security of the application? (SELECT TWO)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Add the EC2 instances to the ALB target group, configure the health check and ensure that the instances report healthy
B. Add the EC2 instances to an Auto Scaling group, configure the health check to ensure that the instances report healthy and remove the public IPs from the instances
C. Create a new subnet in which EC2 instances and ALB will reside to ensure that they can communicate and remove the public IPs from the instances
D. Change the security group for the EC2 instances to allow access from only the ALB security group and remove the public IPs from the instances
E. Change the security group to allow access from 0.0.0.0/0 which permits access from the ALB
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is received its latest bill with a large increase in the number of request against Amazon SQS as API call action. Admin need to know of any major changes in it SQS usage. The company is concerned about the cost increase and who or what was missing the calls. What should the SysOps Administrator use to validate the calls made to SQS?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Amazon Cloudtrail
B. Amazon Cloudwatch
C. AWS Cost Explorer
D. Amazon S3 Access logs
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;After a particularly high AWS bill, an organization wants to review the use of AWS Services&lt;br&gt;
What AWS Service will allow the SysOps Administrator to quickly view this information to share it and will also forecast equipment ?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. AWS Trusted Advisor
B. Amazon QuickSight
C. AWS Cost and Usage Report
D. AWS Cost Explorer
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator must find a way to setup alerts when Amazon EC2 service limit are close to being reached? How can the Administrator achieve this requirement?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use Amazon Inspector and Amazon Cloudwatch Events
B. Use AWS Trusted Advisor and Amazon Cloudwatch Events
C. Use the Personal Health Dashboard and Cloudwatch Events
D. Use AWS CloudTrail and Cloudwatch Events
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is reviewing AWS Trusted Advisor warnings and encounters a warning for an S3 bucket policy that discussing the issue with the bucket owner, the Administrator realizes the S3 bucket is an origin for an Amazon Cloudfront Which action should the Administrator take to ensure that users access objects in Amazon S3 by using only Cloudfront URL?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Encrypt the S3 bucket content with Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)
B. Create an Origin access identity and grand it permissions to read objects in the S3 buckets
C. Assign an IAM user to the Cloudfront distribution and whitelist the IAM user in the S3 bucket policy
D. Assign an IAM Role to the Cloudfront distribution and whitelist the IAM role in the S3 bucket policy
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An Auto Scaling group scales up and down based on Average CPU Utilization. The alarms is set to trigger a scaling when CPU exceeds 80% for 5 minutes. Currently, the average CPU has been 95% for over two hours and new instances are not being added What could be the issue?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. A Scheduled scaling action has not been defined
B. In the field suspend process “ ReplacesUnhealthy” has been selected
C. The maximum size of the Auto Scaling Group is below or at the current group size
D. The HealthCheck Grace Period is set to less than 300 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The Database Administrator team is interested in performing manual backups of an Amazon RDS Oracle DB instance. What step should be taken to perform the backups?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Attach Amazon EBS Volume with Oracle RMAN installed to the RDS Instance
B. Take a snapshot of the EBS volume that is attached to the DB instance
C. Install Oracle Secure backup on the RDS instance and backup the Oracle database to Amazon S3
D. Take a snapshot of the DB Instance
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company has created a separate AWS account for all development work to protect the production environment. In the development environment users request permission to manipulate IAM policies and roles. Corporate policies require that developers are blocked from accessing services. What is the BEST way to grant the developers privileges in the development account while still complying with corporate policies?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create a service control policy in AWS Organizations and apply it to the development account
B. Create a customer managed policy in IAM and apply it in to all users within the development account
C. Create a job function policy in IAM and apply it to all users within the development account
D. Create an IAM Policy and apply it in API Gateway to restrict the development account
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company has a web application that runs on both on-premises and on Amazon EC2 instances. Over time both the on-premises server and EC2 instances is crashing. A SysOps Administrator suspects a memory leak in the application and wants unified method to monitor memory utilization. How can the Administrator track both the EC2 memory utilization and on-premises server memory utilization over time?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Write a script or use a third-party application to report memory utilization for both EC2 instances and on-premises servers.
B. Use Amazon Cloudwatch agent for both Amazon EC2 instances and on-premises servers to report MemoryUtilization metrics to Cloudwatch and set a Cloudwatch alarm for notifications
C. Use Cloudwatch agent for Amazon EC2 instances to report memory Utilization to Cloudwatch and set Cloudwatch Alarms for notifications. Use a third-party application for the on-premises servers.
D. Configure a load balancer to route traffic to both on-premises servers and EC2 instances, then use cloudwatch as the unified view of the metrics for the load balancer.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is using AWS Cloudformation to deploy resources but would like to manually address any errors the template encounters. What should the Administrator add to the template to support the requirement?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Enable Termination Protection on the Stack
B. Set the OnFailure parameter to “DO_NOTHING”
C. Restrict the IAM permissions for CloudFormation to delete resources
D. Set the DeleteStack API action to “NO”
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company’s application stores documents within an Amazon S3 bucket. The application is running on Amazon EC2 in a VPC. A recent change in security requirements states that traffic between the company’s application and the S3 bucket must never leave the Amazon network. What AWS feature can provide this functionality?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Security Groups
B. NAT gateways
C. Virtual private gateway
D. Amazon VPC endpoints
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization with a large IT department has decided to migrate to AWS . With different jobs functions in departments and is not desirable to give all users access to all AWS resources. Currently the organization handles access via LDAP group membership. What the best method to allow access using current LDAP credentials ?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create an AWS directory service simple AD . Replicate the onpremise LDAP directory to simple AD
B. Create Lambda function to read LDAP groups and automate the creation of IAM users
C. Use AWS Cloud Formations to create IAM roles . Deploy direct connect to allow access to the on-premises LDAP server.
D. Federate the LDAP directory with IAM using SAML. Create different IAM roles correspond to different LDAP group to limit permissions.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An Sysops Administrator must set up notifications for whenever combined billing exceeds a certain threshold for all AWS account within company. The Administrator has set up AWS Organizations and enabled Consolidate billing. Which additionals steps must the Administrator perform to setup the billing alerts?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. On the payer account Enable billing alerts in the Billing and Cost management console ; publish an Amazon SNS message when the billing alerts triggers.
B. On each account Enable billing alerts in the billing and cost management console ; setup a billing alarm in Amazon Cloudwatch; publish an SNS message when the alarm triggers.
C. On the payer account Enable billing alerts in the billing and cost management console; setup a billing alarm in the billing and cost management console to publish an SNS message when the alarm triggers.
D. On the payer account Enable billing alerts in the billing and cost management console; setup billing alarm in Amazon Cloudwatch , publish an SNS message when the alarm triggers.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization has been running their website on several m2 Linux instance behind a classic load balancer for more than two years. Traffic and utilization have been constant and predictable. What should the organization do to reduce cost ?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Purchase reserved instances for the specific m2 instances.
B. Change the m2 instances type to equivalent m5 types and purchase reserved instances for specific m5 instances.
C. Change the classic load balancer to an application load balancer and purchase reserved instances for the specific m2 instances.
D. Purchase spot instances for the specific m2 instances.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An application developers are reporting access denied errors when trying to list the content in s3 bucket with IAM Role ARN “arn:aws:iam:11111111:user/application”. The following s3 bucket policy:&lt;br&gt;
    {&lt;br&gt;
    “Id”: “S3BucketPolicy”,&lt;br&gt;
    “Versions”: “2012-10-17”,&lt;br&gt;
    “Statement”: [&lt;br&gt;
    {&lt;br&gt;
    “Sid”: “List”,&lt;br&gt;
    “Action”: {&lt;br&gt;
    “s3: List*”&lt;br&gt;
    },&lt;br&gt;
    “Effect”: “Allow”,&lt;br&gt;
    “Resources”: {&lt;br&gt;
    “arn:aws:s3:::bucketname/*”&lt;br&gt;
    },&lt;br&gt;
    “Principal”: {&lt;br&gt;
    “AWS”: {&lt;br&gt;
    “arn:aws:iam::11111111:user/application”&lt;br&gt;
    }&lt;br&gt;
    }&lt;br&gt;
    }&lt;br&gt;
    }&lt;br&gt;
    }&lt;br&gt;
How should a SysOps Administrator modify the S3 bucket policy to fix the issue?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Change the “Effect” from “Allow” to “Deny”
B. Change the “Action” from “S3:List*” to “S3:ListBucket”
C. Change the “Resource” from “arn:aws:s3:::bucketname/*” to “arn:aws:s3:::bucketname”
D. Change the “Principal” from “arn:aws:iam::11111111:user/application” to “arn:aws:iam:1111111:role/application”
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company creates custom AMI images by launching new Amazon EC2 instance from an Amazon Cloudformation template. AMI images is installed software through AWS OpsWorks and take image of each EC2 instance. The process of installing software take a long times, the process stalls due to installations errors. The SysOps administrator must modify the Cloudformation Template so if the process stalls, stacks will rollback. Based on the requirements, what should be added to the template?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Conditions with a timeout set to 4 hours
B. CreationPolicy with a timeout set to 4 hours
C. DependOn with a timeout set to 4 hours
D. MetaData with a timeout set to 4 hours
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

</description>
      <category>aws</category>
      <category>googlecloud</category>
      <category>awslagi</category>
    </item>
    <item>
      <title>AWS Certified Solutions Architect Professional SAP-C01 Exam Questions Part 5</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 14:58:33 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-5-3kog</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-5-3kog</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;For AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company plans to migrate to AWS. A solutions architect uses AWS Application Discovery Service over the fleet and discovers that there is an Oracle data warehouse and several PostgreSQL databases. Which combination of migration patterns will reduce licensing costs and operational overhead? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Lift and shift the Oracle data warehouse to Amazon EC2 using AWS DMS.
       B. Migrate the Oracle data warehouse to Amazon Redshift using AWS SCT and AWS DMS
       C. Lift and shift the PostgreSQL databases to Amazon EC2 using AWS DMS.
       D. Migrate the PostgreSQL databases to Amazon RDS for PostgreSQL using AWS DMS.
       E. Migrate the Oracle data warehouse to an Amazon EMR managed cluster using AWS DMS.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A solutions architect needs to define a reference architecture for a solution for three-tier applications with web, application, and NoSQL data layers. The reference architecture must meet the following requirements:&lt;br&gt;
– High availability within an AWS Region.&lt;br&gt;
– Able to fail over in 1 minute to another AWS Region for disaster recovery.&lt;br&gt;
– Provide the most efficient solution while minimizing the impact on the user experience.&lt;br&gt;
Which combination of steps will meet these requirements? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use an Amazon Route 53 weighted routing policy set to 100/0 across the two selected Regions. Set Time to Live (TTL) to 1 hour.
       B. Use an Amazon Route 53 failover routing policy for failover from the primary Region to the disaster recovery Region. Set Time to Live (TTL) to 30 seconds.
       C. Use a global table within Amazon DynamoDB so data can be accessed in the two selected Regions.
       D. Back up data from an Amazon DynamoDB table in the primary Region every 60 minutes and then write the data to Amazon S3. Use S3 cross-Region replication to copy the data from the primary Region to the disaster recovery Region. Have a script import the data into DynamoDB in a disaster recovery scenario.
       E. Implement a hot standby model using Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use zonal Reserved Instances for the minimum number of servers and On-Demand Instances for any additional resources. Use Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use Spot Instances for the required resources.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A D E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has a Microsoft SQL Server database in its data center and plans to migrate data to Amazon Aurora MySQL. The company has already used the AWS Schema Conversion Tool to migrate triggers, stored procedures and other schema objects to Aurora MySQL. The database contains 1 TB of data and grows less than 1 MB per day. The company’s data center is connected to AWS through a dedicated 1Gbps AWS Direct Connect connection. The company would like to migrate data to Aurora MySQL and perform reconfigurations with minimal downtime to the applications. Which solution meets the company’s requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Shut down applications over the weekend. Create an AWS DMS replication instance and task to migrate existing data from SQL Server to Aurora MySQL. Perform application testing and migrate the data to the new database endpoint.
       B. Create an AWS DMS replication instance and task to migrate existing data and ongoing replication from SQL Server to Aurora MySQL. Perform application testing and migrate the data to the new database endpoint.
       C. Create a database snapshot of SQL Server on Amazon S3. Restore the database snapshot from Amazon S3 to Aurora MySQL. Create an AWS DMS replication instance and task for ongoing replication from SQL Server to Aurora MySQL. Perform application testing and migrate the data to the new database endpoint.
       D. Create a SQL Server native backup file on Amazon S3. Create an AWS DMS replication instance and task to restore the SQL Server backup file to Aurora MySQL. Create another AWS DMS task for ongoing replication from SQL Server to Aurora MySQL. Perform application testing and migrate the data to the new database endpoint.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company runs an application on a fleet of Amazon EC2 instances. The application requires low latency and random access to 100 GB of data. The application must be able to access the data at up to 3.000 IOPS. A Development team has configured the EC2 launch template to provision a 100-GB Provisioned IOPS (PIOPS) Amazon EBS volume with 3 000 IOPS provisioned. A Solutions Architect is tasked with lowering costs without impacting performance and durability. Which action should be taken?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an Amazon EFS file system with the performance mode set to Max I/O. Configure the EC2 operating system to mount the EFS file system.
       B. Create an Amazon EFS file system with the throughput mode set to Provisioned. Configure the EC2 operating system to mount the EFS file system.
       C. Update the EC2 launch template to allocate a new 1-TB EBS General Purpose SSO (gp2) volume.                
       D. Update the EC2 launch template to exclude the PIOPS volume. Configure the application to use local instance storage.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company recently transformed its legacy infrastructure provisioning scripts to AWS CloudFormation templates. The newly developed templates are hosted in the company’s private GitHub repository. Since adopting CloudFormation, the company has encountered several issues with updates to the CloudFormation templates, causing execution or creating an environment. Management is concerned by the increase in errors and has asked a Solutions Architect to design the automated testing of CloudFormation template updates. What should the Solution Architect do to meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use AWS CodePipeline to create a change set from the CloudFormation templates stored in the private GitHub repository. Execute the change set using AWS CodeDeploy. Include a CodePipeline action to test the deployment with testing scripts run by AWS CodeBuild.
       B. Mirror the GitHub repository to AWS CodeCommit using AWS Lambda. Use AWS CodeDeploy to create a change set from the CloudFormation templates and execute it. Have CodeDeploy test the deployment with testing scripts run by AWS CodeBuild.
       C. Use AWS CodePipeline to create and execute a change set from the CloudFormation templates stored in the GitHub repository. Configure a CodePipeline action to be deployed with testing scripts run by AWS CodeBuild.
       D. Mirror the GitHub repository to AWS CodeCommit using AWS Lambda. Use AWS CodeBuild to create a change set from the CloudFormation templates and execute it. Have CodeBuild test the deployment with testing scripts.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has several Amazon EC2 instances to both public and private subnets within a VPC that is not connected to the corporate network. A security group associated with the EC2 instances allows the company to use the Windows remote desktop protocol (RDP) over the internet to access the instances. The security team has noticed connection attempts from unknown sources. The company wants to implement a more secure solution to access the EC2 instances. Which strategy should a solutions architect implement?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Deploy a Linux bastion host on the corporate network that has access to all instances in the VPC.
       B. Deploy AWS Systems Manager Agent on the EC2 instances. Access the EC2 instances using Session Manager restricting access to users with permission.
       C. Deploy a Linux bastion host with an Elastic IP address in the public subnet. Allow access to the bastion host from 0.0.0.0/0.
       D. Establish a Site-to-Site VPN connecting the corporate network to the VPC. Update the security groups to allow access from the corporate network only.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A retail company has a custom .NET web application running on AWS that uses Microsoft SQL Server for the database. The application servers maintain a user’s session locally. Which combination of architecture changes are needed to ensure all tiers of the solution are highly available? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Refactor the application to store the user’s session in Amazon ElastiCache. Use Application Load Balancers to distribute the load between application instances.
       B. Set up the database to generate hourly snapshots using Amazon EBS. Configure an Amazon CloudWatch Events rule to launch a new database instance if the primary one fails.
       C. Migrate the database to Amazon RDS for SQL Server. Configure the RDS instance to use a MultiAZ deployment.
       D. Move the .NET content to an Amazon S3 bucket. Configure the bucket for static website hosting.                
       E. Put the application instances in an Auto Scaling group. Configure the Auto Scaling group to create new instances if an instance becomes unhealthy.
       F. Deploy Amazon CloudFront in front of the application tier. Configure CloudFront to serve content from healthy application instances only.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A B E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to improve cost awareness for its Amazon EMR platform. The company has allocated budgets for each team’s Amazon EMR usage. When a budgetary threshold is reached, a notification should be sent by email to the budget office’s distribution list. Teams should be able to view their EMR cluster expenses to date. A solutions architect needs to create a solution that ensures the policy is proactively and centrally enforced in a multi-account environment. Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Update the AWS CloudFormation template to include the AWS::Budgets::Budget::resource with the NotificationsWithSubscribers property.
       B. Implement Amazon CloudWatch dashboards for Amazon EMR usage.
       C. Create an EMR bootstrap action that runs at startup that calls the Cost Explorer API to set the budget on the cluster with the GetCostForecast and NotificationsWithSubscribers actions.
       D. Create an AWS Service Catalog portfolio for each team. Add each team’s Amazon EMR cluster as an AWS CloudFormation template to their Service Catalog portfolio as a Product.
       E. Create an Amazon CloudWatch metric for billing. Create a custom alert when costs exceed the budgetary threshold.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is migrating its on-premises systems to AWS. The user environment consists of the following systems:&lt;br&gt;
– Windows and Linux virtual machines running on VMware.&lt;br&gt;
– Physical servers running Red Hat Enterprise Linux.&lt;br&gt;
– The company wants to be able to perform the following steps before migrating to AWS:&lt;br&gt;
– Identify dependencies between on-premises systems.&lt;br&gt;
– Group systems together into applications to build migration plans.&lt;br&gt;
– Review performance data using Amazon Athena to ensure that Amazon EC2 instances are right-sized.&lt;br&gt;
How can these requirements be met?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Populate the AWS Application Discovery Service import template with information from an on premises configuration management database (CMDB). Upload the completed import template to Amazon S3, then import the data into Application Discovery Service.
       B. Install the AWS Application Discovery Service Discovery Agent on each of the on-premises systems. Allow the Discovery Agent to collect data for a period of time.
       C. Install the AWS Application Discovery Service Discovery Connector on each of the on-premises systems and in VMware vCenter. Allow the Discovery Connector to collect data for one week.
       D. Install the AWS Application Discovery Service Discovery Agent on the physical on-pre-map servers. Install the AWS Application Discovery Service Discovery Connector in VMware vCenter. Allow the Discovery Agent to collect data for a period of time.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to migrate its on-premises data center to the AWS Cloud. This includes thousands of virtualized Linux and Microsoft Windows servers, SAN storage, Java and PHP applications with MYSQL, and Oracle databases. There are many department services hosted either in the same data center or externally. The technical documentation is incomplete and outdated. A solutions architect needs to understand the current environment and estimate the cloud resource costs after the migration. Which tools or services should solutions architects use to plan the cloud migration (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. AWS Application Discovery Service
       B. AWS SMS
       C. AWS x-Ray
       D. AWS Cloud Adoption Readness Tool (CART)
       E. Amazon Inspector
       F. AWS Migration Hub
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B C F&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company decided to purchase Amazon EC2 Reserved Instances. A solutions architect is tasked with implementing a solution where only the master account in AWS Organizations is able to purchase the Reserved Instances. Current and future member accounts should be blocked from purchasing Reserved Instances. Which solution will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an SCP with the Deny effect on the ec2:PurchaseReservedInstancesOffering action. Attach the SCP to the root of the organization.
       B. Create a new organizational unit (OU) Move all current member accounts to the new OU. Create an SCP with the Deny effect on the ec2:PurchaseReservedInstancesOffering action. Attach the SCP to the new OU.
       C. Create an AWS Config rule event that triggers automation that will terminate any Reserved Instances launched by member accounts.
       D. Create two new organizational units (OUs): OU1 and OU2. Move all member accounts to OU2 and the master account to OU1. Create an SCP with the Allow effect on the ec2:PurchaseReservedInstancesOffering action. Attach the SCP to OU1.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is using multiple AWS accounts. The DNS records are stored in a private hosted zone for Amazon Route 53 in Account                A. The company’s applications and databases are running in Account                B. A solutions architect will deploy a two-tier application in a new VPC. To simplify the configuration, the db.example.com CNAME record set for the Amazon RDS endpoint was created in a private hosted zone for Amazon Route 53. During deployment the application failed to start. Troubleshooting revealed that db.example.com is not resolvable on the Amazon EC2 instance. The solutions architect confirmed that the record set was created correctly in Route 53. Which combination of steps should the solutions architect take to resolve this issue? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Deploy the database on a separate EC2 instance in the new VPC. Create a record set for the instance’s private IP in the private hosted zone.
       B. Use SSH to connect to the application tier EC2 instance. Add an RDS endpoint IP address to the /etc/resolv conf file.
       C. Create an authorization to associate the private hosted zone in Account A with the new VPC in Account                B.
       D. Create a private hosted zone for the example com domain in Account                B. Configure Route 53 replication between AWS accounts.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A solutions architect needs to advise a company on how to migrate its on-premises data processing application to the AWS Cloud. Currently, users upload input files through a web portal. The web server then stores the uploaded files on NAS and messages the processing server over a message queue. Each media file can take up to 1 hour to process. The company has determined that the number of media files awaiting processing is significantly higher during business hours, with the number of files rapidly declining after business hours. What is the MOST cost-effective migration recommendation?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in an Amazon S3 bucket.
       B. Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, create a new Amazon EC2 instance to pull requests from the queue and process the files. Store the processed files in Amazon EFS. Shut down the EC2 instance after the task is complete.
       C. Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in Amazon EFS.
       D. Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. Use Amazon EC2 instances in an EC2 Auto Seating group to pull requests from the queue and process the files. Scale the EC2 instances based on the SQS queue length. Store the processed files in an Amazon S3 bucket.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A utility company wants to collect usage data every 5 minutes from its smart meters to facilitate time-of-use metering. When a meter sends data to AWS, the data is sent to Amazon API Gateway, processed by an AWS Lambda function and stored in an Amazon DynamoDB table. During the pilot phase, the Lambda functions took from 3 to 5 seconds to complete. As more smart meters are deployed, the Engineers notice the Lambda functions are taking from 1 to 2 minutes to complete. The functions are also increasing in duration as new types of metrics are collected from the devices. There are many ProvisionedThroughputExceededException errors while performing PUT operations on DynamoDB, and there are also many TooManyRequestsException errors from Lambda. Which combination of changes will resolve these issues? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Increase the write capacity units to the DynamoDB table.
       B. Increase the memory available to the Lambda functions.
       C. Increase the payload size from the smart meters to send more data.
       D. Stream the data into an Amazon Kinesis data stream from API Gateway and process the data in batches.
       E. Collect data in an Amazon SQS FIFO queue, which triggers a Lambda function to process each message.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An AWS partner company is building a service in AWS Organizations using its organization named org1. This service requires the partner company to have access to AWS resources in a customer account, which is in a separate organization named org2. The company must establish least privilege security access using an API or command line tool to the customer account. What is the MOST secure way to allow org1 to access resources in org2?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. The customer should provide the partner company with their AWS account access keys to log in and perform the required tasks.
       B. The customer should create an IAM user and assign the required permissions to the IAM user. The customer should then provide the credentials to the partner company to log in and perform the required tasks.
       C. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role’s Amazon Resource Name (ARN) when requesting access to perform the required tasks.
       D. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role’s Amazon Resource Name (ARN), including the external ID in the IAM role’s trust policy, when requesting access to perform the required tasks.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company’s security compliance requirements state that all Amazon EC2 images must be scanned for vulnerabilities and must pass a CVE assessment. A solutions architect is developing a mechanism to create security- approved AMIs that can be used by developers. Any new AMIs should go through an automated assessment process and be marked as approved before developers can use them. The approved images must be scanned every 30 days to ensure compliance. Which combination of steps should the solutions architect take to meet these requirements while following best practices? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use the AWS Systems Manager EC2 agent to run the CVE assessment on the EC2 instances launched from the AMIs that need to be scanned.
       B. Use AWS Lambda to write automatic approval rules. Store the approved AMI list in AWS Systems Manager Parameter Store. Use Amazon EventBridge to trigger an AWS Systems Manager Automation document on all EC2 instances every 30 days.
       C. Use Amazon Inspector to run the CVE assessment on the EC2 instances launched from the AMIs that need to be scanned.
       D. Use AWS Lambda to write automatic approval rules. Store the approved AMI list in AWS Systems Manager Parameter Store. Use a managed AWS Config rule for continuous scanning on all EC2 instances, and use AWS Systems Manager Automation documents for remediation.
       E. Use AWS CloudTrail to run the CVE assessment on the EC2 instances launched from the AMIs that need to be scanned.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company uses AWS Organizations with a single OU named Production to manage multiple accounts. All accounts are members of the Production OU. Administrators use deny list SCPs in the root of the organization to manage access to restricted services. The company recently acquired a new business unit and invited the new unit’s existing AWS account to the organization. Once onboarded, the administrators of the new business unit discovered that they are not able to update existing AWS Config rules to meet the company’s policies. Which option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Remove the organization’s root SCPs that limit access to AWS Config. Create AWS Service Catalog products for the company’s standard AWS Config rules and deploy them throughout the organization, including the new account.
       B. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the new account to the Production OU when adjustments to AWS Config are complete.
       C. Convert the organization’s root SCPs from deny list SCPs to allow list SCPs to allow the required services only. Temporally apply an SCP to the organization’s root that allows AWS Config actions for principals only in the new account.
       D. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization’s root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is launching a web-based application in multiple regions around the world. The application consists of both static content stored in a private Amazon S3 bucket and dynamic content hosted in Amazon ECS containers content behind an Application Load Balancer (ALB). The company requires that the static and dynamic application content be accessible through Amazon CloudFront only. Which combination of steps should a solutions architect recommend to restrict direct content access to CloudFront? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a web ACL in AWS WAF with a rule to validate the presence of a custom header and associate the web ACL with the ALB.
       B. Create a web ACL in AWS WAF with a rule to validate the presence of a custom header and associate the web ACL with the CloudFront distribution.
       C. Configure CloudFront to add a custom header to origin requests.
       D. Configure the ALB to add a custom header to HTTP requests.
       E. Update the S3 bucket ACL to allow access from the CloudFront distribution only.
       F. Create a CloudFront Origin Access Identity (OAI) and add it to the CloudFront distribution. Update the S3 bucket policy to allow access to the OAI only.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A D F&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An ecommerce website running on AWS uses an Amazon RDS for MySQL DB instance with General Purpose SSD storage. The developers chose an appropriate instance type based on demand, and configured 100 GB of storage with a sufficient amount of free space. The website was running smoothly for a few weeks until a marketing campaign launched. On the second day of the campaign, users reported long wait times and time outs. Amazon CloudWatch metrics indicated that both reads and writes to the DB instance were experiencing long response times. The CloudWatch metrics show 40% to 50% CPU and memory utilization, and sufficient free storage space is still available. The application server logs show no evidence of database connectivity issues. What could be the root cause of the issue with the marketing campaign?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. It exhausted the I/O credit balance due to provisioning low disk storage during the setup phase.
       B. It caused the data in the tables to change frequently, requiring indexes to be rebuilt to optimize queries.
       C. It exhausted the maximum number of allowed connections to the database instance.
       D. It exhausted the network bandwidth available to the RDS for MySQL DB instances.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A solutions architect has been assigned to migrate a 50 TB Oracle data warehouse that contains sales data from on-premises to Amazon Redshift. Major updates to the sales data occur on the final calendar day of the month. For the remainder of the month, the data warehouse only receives minor daily updates and is primarily used for reading and reporting. Because of this, the migration process must start on the first day of the month and must be complete before the next set of updates occur. This provides approximately 30 days to complete the migration and ensure that the minor daily changes have been synchronized with the Amazon Redshift data warehouse. Because the migration cannot impact normal business network operations, the bandwidth allocated to the migration for moving data over the internet is 50 Mbps. The company wants to keep data migration costs low. Which steps will allow the solutions architect to perform the migration within the specified timeline?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Install Oracle database software on an Amazon EC2 instance. Configure VPN connectivity between AWS and the company’s data center. Configure the Oracle database running on Amazon EC2 to join the Oracle Real Application Clusters (RAC). When the Oracle database on Amazon EC2 finishes synchronizing, create an AWS DMS ongoing replication task to migrate the data from the Oracle database on Amazon EC2 to Amazon Redshift. Verify the data migration is complete and perform the cut over to Amazon Redshift.
       B. Create an AWS Snowball import job. Export a backup of the Oracle data warehouse. Copy the exported data to the Snowball device. Return the Snowball device to AWS. Create an Amazon RDS for Oracle database and restore the backup file to that RDS instance. Create an AWS DMS task to migrate the data from the RDS for Oracle database to Amazon Redshift. Copy daily incremental backups from Oracle in the data center to the RDS for Oracle database over the internet. Verify the data migration is complete and perform the cut over to Amazon Redshift.
       C. Install Oracle database software on an Amazon EC2 instance. To minimize the migration time, configure VPN connectivity between AWS and the company’s data center by provisioning a 1 Gbps AWS Direct Connect connection. Configure the Oracle database running on Amazon EC2 to be a read replica of the data center Oracle database. Start the synchronization process between the company’s on-premises data center and the Oracle database on Amazon EC2. When the Oracle database on Amazon EC2 is synchronized with the on-premises database, create an AWS DMS ongoing replication task to migrate the data from the Oracle database read replica that is running on Amazon EC2 to Amazon Redshift. Verify the data migration is complete and perform the cut over to Amazon Redshift.
       D. Create an AWS Snowball import job. Configure a server in the company’s data center with an extraction agent. Use AWS SCT to manage the extraction agent and convert the Oracle schema to an Amazon Redshift schema. Create a new project in AWS SCT using the registered data extraction agent. Create a local task and an AWS DMS task in AWS SCT with replication of ongoing changes. Copy data to the Snowball device and return the Snowball device to AWS. Allow AWS DMS to copy data from Amazon S3 to Amazon Redshift. Verify that the data migration is complete and perform the cut over to Amazon Redshift.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

</description>
      <category>awslagi</category>
      <category>googlecloud</category>
      <category>aws</category>
    </item>
    <item>
      <title>AWS Certified Solutions Architect Professional SAP-C01 Exam Questions Part 4</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 14:54:28 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-4-407b</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-4-407b</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;For AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A solutions architect is designing a disaster recovery strategy for a three-tier application. The application has an RTO of 30 minutes and an RPO of 5 minutes for the data tier. The application and web tiers are stateless and leverage a fleet of Amazon EC2 instances. The data tier consists of a 50 TB Amazon Aurora database. Which combination of steps satisfies the RTO and RPO requirements while optimizing costs? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create daily snapshots of the EC2 instances and replicate the snapshots to another Region.
       B. Deploy a hot standby of the application to another Region.
       C. Create snapshots of the Aurora database every 5 minutes.
       D. Create a cross-Region Aurora Replica of the database.
       E. Create an AWS Backup job to replicate data to another Region.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has a primary Amazon S3 bucket that receives thousands of objects every day. The company needs to replicate these objects into several other S3 buckets from various AWS accounts. A solutions architect is designing a new AWS Lambda function that is triggered when an object is created in the main bucket and replicates the object into the target buckets. The objects do not need to be replicated in real time. There is concern that this function may impact other critical Lambda functions due to Lambda’s regional concurrency limit. How can the solutions architect ensure this new Lambda function will not impact other critical Lambda functions?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Set the new Lambda function reserved concurrency limit to ensure the executions do not impact other critical Lambda functions. Monitor existing critical Lambda functions with Amazon CloudWatch alarms for the Throttles Lambda metric.
       B. Increase the execution timeout of the new Lambda function to 5 minutes. Monitor existing critical Lambda functions with Amazon CloudWatch alarms for the Throttles Lambda metric.
       C. Configure S3 event notifications to add events to an Amazon SQS queue in a separate account. Create the new Lambda function in the same account as the SQS queue and trigger the function when a message arrives in the queue.
       D. Ensure the new Lambda function implements an exponential backoff algorithm. Monitor existing critical Lambda functions with Amazon CloudWatch alarms for the Throttles Lambda metric.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to run a serverless application on AWS. The company plans to provision its application in Docker containers running in an Amazon ECS cluster. The application requires a MySQL database and the company plans to use Amazon RDS. The company has documents that need to be accessed frequently for the first 3 months, and rarely after that. The document must be retained for 7 years. What is the MOST cost-effective solution to meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an ECS cluster using On-Demand Instances. Provision the database and its read replicas in Amazon RDS using Spot Instances. Store the documents in an encrypted EBS volume, and create a cron job to delete the documents after 7 years.
       B. Create an ECS cluster using a fleet of Spot Instances, with Spot Instance draining enabled. Provision the database and its read replicas in Amazon RDS using Reserved Instances. Store the documents in a secured Amazon S3 bucket with a lifecycle policy to move the documents that are older than 3 months to Amazon S3 Glacier, then delete the documents from Amazon S3 Glacier that are more than 7 years old.
       C. Create an ECS cluster using On-Demand Instances. Provision the database and its read replicas in Amazon RDS using On-Demand Instances. Store the documents in Amazon EFS. Create a cron job to move the documents that are older than 3 months to Amazon S3 Glacier. Create an AWS Lambda function to delete the documents in S3 Glacier that are older than 7 years.
       D. Create an ECS cluster using a fleet of Spot Instances with Spot Instance draining enabled. Provision the database and its read replicas in Amazon RDS using On-Demand Instances. Store the documents in a secured Amazon S3 bucket with a lifecycle policy to move the documents that are older than 3 months to Amazon S3 Glacier, then delete the documents in Amazon S3 Glacier after 7 years.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A financial services company receives a regular data feed from its credit card servicing partner. Approximately 5,000 records are sent every 15 minutes in plaintext, delivered over HTTPS directly into an Amazon S3 bucket with server-side encryption. This feed contains sensitive credit card primary account number (PAN) data. The company needs to automatically mask the PAN before sending the data to another S3 bucket for additional internal processing. The company also needs to remove and merge specific fields, and then transform the record into JSON format. Additionally, extra feeds are likely to be added in the future, so any design needs to be easily expandable. Which solutions will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Trigger an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Trigger another Lambda function when new messages arrive in the SQS queue to process the records, writing the results to a temporary location in Amazon S3. Trigger a final Lambda function once the SQS queue is empty to transform the records into JSON format and send the results to another S3 bucket for internal processing.
       B. Trigger an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Configure an AWS Fargate container application to automatically scale to a single instance when the SQS queue contains messages. Have the application process each record, and transform the record into JSON format. When the queue is empty, send the results to another S3 bucket for internal processing and scale down the AWS Fargate instance.
       C. Create an AWS Glue crawler and custom classifier based on the data feed formats and build a table definition to match. Trigger an AWS Lambda function on file delivery to start an AWS Glue ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, have the ETL job send the results to another S3 bucket for internal processing.
       D. Create an AWS Glue crawler and custom classifier based upon the data feed formats and build a table definition to match. Perform an Amazon Athena query on file delivery to start an Amazon EMR ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, send the results to another S3 bucket for internal processing and scale down the EMR cluster.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A media company is serving video files stored in Amazon S3 using Amazon CloudFront. The development team needs access to the logs to diagnose faults and perform service monitoring. The log files from CloudFront may contain sensitive information about users. The company uses a log processing service to remove sensitive information before making the logs available to the development team. The company has the following requirements for the unprocessed logs:&lt;br&gt;
– The logs must be encrypted at rest and must be accessible by the log processing service only.&lt;br&gt;
– Only the data protection team can control access to the unprocessed log files.&lt;br&gt;
– AWS CloudFormation templates must be stored in AWS CodeCommit.&lt;br&gt;
– AWS CodePipeline must be triggered on commit to perform updates made to CloudFormation templates.&lt;br&gt;
– CloudFront is already writing the unprocessed logs to an Amazon S3 bucket, and the log processing service is operating against this S3 bucket.&lt;br&gt;
Which combination of steps should a solutions architect take to meet the company’s requirements? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an AWS KMS key that allows the AWS Logs Delivery account to generate data keys for encryption Configure S3 default encryption to use server-side encryption with KMS managed keys (SSEKMS) on the log storage bucket using the new KMS key. Modify the KMS key policy to allow the log processing service to perform decrypt operations.
       B. Create an AWS KMS key that follows the CloudFront service role to generate data keys for encryption Configure S3 default encryption to use KMS managed keys (SSE-KMS) on the log storage bucket using the new KMS key Modify the KMS key policy to allow the log processing service to perform decrypt operations.
       C. Configure S3 default encryption to use AWS KMS managed keys (SSE-KMS) on the log storage bucket using the AWS Managed S3 KMS key. Modify the KMS key policy to allow the CloudFront service role to generate data keys for encryption Modify the KMS key policy to allow the log processing service to perform decrypt operations.
       D. Create a new CodeCommit repository for the AWS KMS key template. Create an IAM policy to allow commits to the new repository and attach it to the data protection team’s users. Create a new CodePipeline pipeline with a custom IAM role to perform KMS key updates using CloudFormation Modify the KMS key policy to allow the CodePipeline IAM role to modify the key policy.
       E. Use the existing CodeCommit repository for the AWS KMS key template. Create an IAM policy to allow commits to the new repository and attach it to the data protection team’s users. Modify the existing CodePipeline pipeline to use a custom IAM role and to perform KMS key updates using CloudFormation. Modify the KMS key policy to allow the CodePipeline IAM role to modify the key policy.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company’s service for video game recommendations has just gone viral. The company has new users from all over the world. The website for the service is hosted on a set of Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The website consists of static content with different resources being loaded depending on the device type. Users recently reported that the load time for the website has increased. Administrators are reporting high loads on the EC2 instances that host the service. Which set actions should a solutions architect take to improve response times?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create separate Auto Scaling groups based on device types. Switch to Network Load Balancer (NLB). Use the User-Agent HTTP header in the NLB to route to a different set of EC2 instances.
       B. Move content to Amazon S3. Create an Amazon CloudFront distribution to serve content out of the S3 bucket. Use Lambda@Edge to load different resources based on the User-Agent HTTP header.
       C. Create a separate ALB for each device type. Create one Auto Scaling group behind each ALB. Use Amazon Route 53 to route to different ALBs depending on the User-Agent HTTP header.
       D. Move content to Amazon S3. Create an Amazon CloudFront distribution to serve content out of the S3 bucket. Use the User-Agent HTTP header to load different content.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is planning a large event where a promotional offer will be introduced. The company’s website is hosted on AWS and backed by an Amazon RDS for PostgreSQL DB instance. The website explains the promotion and includes a sign-up page that collects user information and preferences. Management expects large and unpredictable volumes of traffic periodically, which will create many database writes. A solutions architect needs to build a solution that does not change the underlying data model and ensures that submissions are not dropped before they are committed to the database. Which solution meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Immediately before the event, scale up the existing DB instance to meet the anticipated demand. Then scale down after the event.
       B. Use Amazon SQS to decouple the application and database layers. Configure an AWS Lambda function to write items from the queue into the database.
       C. Migrate to Amazon DynamoDB and manage throughput capacity with automatic scaling.
       D. Use Amazon ElastiCache for Memcached to increase write capacity to the DB instance.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A mobile app has become very popular, and usage has gone from a few hundred to millions of users. Users capture and upload images of activities within a city, and provide ratings and recommendations. Data access patterns are unpredictable. The current application is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The application is experiencing slowdowns and costs are growing rapidly. Which changes should a solutions architect make to the application architecture to control costs and improve performance?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an Amazon CloudFront distribution and place the ALB behind the distribution. Store static content in Amazon S3 in an Infrequent Access storage class.
       B. Store static content in an Amazon S3 bucket using the Intelligent Tiering storage class. Use an Amazon CloudFront distribution in front of the S3 bucket and the ALB.
       C. Place AWS Global Accelerator in front of the ALB. Migrate the static content to Amazon EFS, and then run an AWS Lambda function to resize the images during the migration process.
       D. Move the application code to AWS Fargate containers and swap out the EC2 instances with the Fargate containers.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A financial company with multiple departments wants to expand its on-premises environment to the AWS Cloud. The company must retain centralized access control using an existing on premises Active Directory (AD) service. Each department should be allowed to create AWS accounts with preconfigured networking and should have access to only a specific list of approved services. Departments are not permitted to have account administrator permissions. What should a solutions architect do to meet these security requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure AWS Identity and Access Management (IAM) with a SAML identity provider (IdP) linked to the on-premises Active Directory, and create a role to grant access. Configure AWS Organizations with SCPs and create new member accounts. Use AWS CloudFormation templates to configure the member account networking.
       B. Deploy an AWS Control Tower landing zone. Create an AD Connector linked to the on-premises Active Directory. Change the identity source in AWS Single Sign-On to use Active Directory. Allow department administrators to use Account Factory to create new member accounts and networking. Grant the departments AWS power user permissions on the created accounts.
       C. Deploy an Amazon Cloud Directory. Create a two-way trust relationship with the on-premises Active Directory, and create a role to grant access. Set up an AWS Service Catalog to use AWS CloudFormation templates to create the new member accounts and networking. Use IAM roles to allow access to approved AWS services.
       D. Configure AWS Directory Service for Microsoft Active Directory with AWS Single Sign-On. Join the service to the on-premises Active Directory. Use AWS CloudFormation to create new member accounts and networking. Use IAM roles to allow access to approved AWS services.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A large financial company is deploying applications that consist of Amazon EC2 and Amazon RDS instances to the AWS Cloud using AWS CloudFormation. The CloudFormation stack has the following stack policy:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The company wants to ensure that developers do not lose data by accidentally removing or replacing RDS instances when updating the CloudFormation stack. Developers also still need to be able to modify or remove EC2 instances as needed. How should the company change the stack policy to meet these requirements?&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;           A. Modify the statement to specify “Effect”: “Deny”, “Action”:[“Update:*”] for all logical RDS resources.                B. Modify the statement to specify “Effect”: “Deny”, “Action”:[“Update:Delete”] for all logical RDS resources.
           C. Add a second statement that specifies “Effect”: “Deny”, “Action”:[“Update:Delete”, “Update:Replace”] for all logical RDS resources.
           D. Add a second statement that specifies “Effect”: “Deny”, “Action”:[“Update:*”] for all logical RDS resources.
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is currently in the design phase of an application that will need an RPO of less than 5 minutes and an RTO of less than 10 minutes. The solutions architecture team is forecasting that the database will store approximately 10 TB of data. As part of the design, they are looking for a database solution that will provide the company with the ability to fail over to a secondary Region. Which solution will meet these business requirements at the LOWEST cost?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Deploy an Amazon Aurora DB cluster and take snapshots of the cluster every 5 minutes. Once a snapshot is complete, copy the snapshot to a secondary Region to serve as a backup in the event of a failure.
       B. Deploy an Amazon RDS instance with a cross-Region read replica in a secondary Region. In the event of a failure, promote the read replica to become the primary.
       C. Deploy an Amazon Aurora DB cluster in the primary Region and another in a secondary Region. Use AWS DMS to keep the secondary Region in sync.
       D. Deploy an Amazon RDS instance with a read replica in the same Region. In the event of a failure, promote the read replica to become the primary.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has a web application that uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. A recent marketing campaign has increased demand. Monitoring software reports that many requests have significantly longer response times than before the marketing campaign. A solutions architect enabled Amazon CloudWatch Logs for API Gateway and noticed that errors are occurring on 20% of the requests. In CloudWatch, the Lambda function Throttles metric represents 1% of the requests and the Errors metric represents 10% of the requests. Application logs indicate that, when errors occur, there is a call to DynamoDB. What change should the solutions architect make to improve the current response times as the web application becomes more popular?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Increase the concurrency limit of the Lambda function
       B. Implement DynamoDB auto scaling on the table
       C. Increase the API Gateway throttle limit
       D. Re-create the DynamoDB table with a better-partitioned primary index
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A European online newspaper service hosts its public-facing WordPress site in a collocated data center in London. The current WordPress infrastructure consists of a load balancer, two web servers, and one MySQL database server. A solutions architect is tasked with designing a solution with the following requirements:&lt;br&gt;
– Improve the website’s performance&lt;br&gt;
– Make the web tier scalable and stateless&lt;br&gt;
– Improve the database server performance for read-heavy loads&lt;br&gt;
– Reduce latency for users across Europe and the US Design the new architecture with a goal of 99.9% availability&lt;br&gt;
Which solution meets these requirements while optimizing operational efficiency?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use an Application Load Balancer (ALB) in front of an Auto Scaling group of WordPress Amazon EC2 instances in one AWS Region and three Availability Zones. Configure an Amazon ElastiCache cluster in front of a Multi-AZ Amazon Aurora MySQL DB cluster. Move the WordPress shared files to Amazon EFS. Configure Amazon CloudFront with the ALB as the origin, and select a price class that includes the US and Europe.
       B. Use an Application Load Balancer (ALB) in front of an Auto Scaling group of WordPress Amazon EC2 instances in two AWS Regions and two Availability Zones in each Region. Configure an Amazon ElastiCache cluster in front of a global Amazon Aurora MySQL database. Move the WordPress shared files to Amazon EFS. Configure Amazon CloudFront with the ALB as the origin, and select a price class that includes the US and Europe. Configure EFS cross-Region replication.
       C. Use an Application Load Balancer (ALB) in front of an Auto Scaling group of WordPress Amazon EC2 instances in one AWS Region and three Availability Zones. Configure an Amazon DocumentDB table in front of a Multi-AZ Amazon Aurora MySQL DB cluster. Move the WordPress shared files to Amazon EFS. Configure Amazon CloudFront with the ALB as the origin, and select a price class that includes all global locations.
       D. Use an Application Load Balancer (ALB) in front of an Auto Scaling group of WordPress Amazon EC2 instances in two AWS Regions and three Availability Zones in each Region. Configure an Amazon ElastiCache cluster in front of a global Amazon Aurora MySQL database. Move the WordPress shared files to Amazon FSx with cross-Region synchronization. Configure Amazon CloudFront with the ALB as the origin and a price class that includes the US and Europe.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company built an ecommerce website on AWS using a three-tier web architecture. The application is Java-based and composed of an Amazon CloudFront distribution, an Apache web server layer of Amazon EC2 instances in an Auto Scaling group, and a backend Amazon Aurora MySQL database. Last month, during a promotional sales event, users reported errors and timeouts while adding items to their shopping carts. The operations team recovered the logs created by the web servers and reviewed Aurora DB cluster performance metrics. Some of the web servers were terminated before logs could be collected and the Aurora metrics were not sufficient for query performance analysis. Which combination of steps must the solutions architect take to improve application performance visibility during peak traffic events? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure the Aurora MySQL DB cluster to publish slow query and error logs to Amazon CloudWatch Logs.
       B. Implement the AWS X-Ray SDK to trace incoming HTTP requests on the EC2 instances and implement tracing of SQL queries with the X-Ray SDK for Java.
       C. Configure the Aurora MySQL DB cluster to stream slow query and error logs to Amazon Kinesis
       D. Install and configure an Amazon CloudWatch Logs agent on the EC2 instances to send the Apache logs to CloudWatch Logs.
       E. Enable and configure AWS CloudTrail to collect and analyze application activity from Amazon EC2 and Aurora.                
       F. Enable Aurora MySQL DB cluster performance benchmarking and publish the stream to AWS XRay.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B C E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A solutions architect has an operational workload deployed on Amazon EC2 instances in an Auto Scaling group. The VPC architecture spans two Availability Zones (AZ) with a subnet in each that the Auto Scaling group is targeting. The VPC is connected to an on-premises environment and connectivity cannot be interrupted. The maximum size of the Auto Scaling group is 20 instances in service. The VPC IPv4 addressing is as follows:&lt;br&gt;
– VPC CIDR: 10.0.0.0/23&lt;br&gt;
– AZ1 subnet CIDR: 10.0.0.0/24&lt;br&gt;
– AZ2 subnet CIDR: 10.0.1.0/24&lt;br&gt;
Since deployment, a third AZ has become available in the Region. The solutions architect wants to adopt the new AZ without adding additional IPv4 address space and without service downtime. Which solution will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Update the Auto Scaling group to use the AZ2 subnet only. Delete and re-create the AZ1 subnet using half the previous address space. Adjust the Auto Scaling group to also use the new AZ1 subnet. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Remove the current AZ2 subnet. Create a new AZ2 subnet using the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets.
       B. Terminate the EC2 instances in the AZ1 subnet. Delete and re-create the AZ1 subnet using half the address space. Update the Auto Scaling group to use this new subnet. Repeat this for the second AZ. Define a new subnet in AZ3, then update the Auto Scaling group to target all three new subnets.
       C. Create a new VPC with the same IPv4 address space and define three subnets, with one for each AZ. Update the existing Auto Scaling group to target the new subnets in the new VPC.
       D. Update the Auto Scaling group to use the AZ2 subnet only. Update the AZ1 subnet to have the previous address space. Adjust the Auto Scaling group to also use the AZ1 subnet again. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Update the current AZ2 subnet and assign the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is storing data on premises on a Windows file server. The company produces 5 GB of new data daily. The company migrated part of its Windows-based workload to AWS and needs the data to be available on a file system in the cloud. The company already has established an AWS Direct Connect connection between the on-premises network and AWS. Which data migration strategy should the company use?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use the file gateway option in AWS Storage Gateway to replace the existing Windows file server, and point the existing file share to the new file gateway
       B. Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon FSx
       C. Use AWS Data Pipeline to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS)
       D. Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS)
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company uses AWS Organizations to manage one parent account and nine member accounts. The number of member accounts is expected to grow as the business grows. A security engineer has requested consolidation of AWS CloudTrail logs into the parent account for compliance purposes. Existing logs currently stored in Amazon S3 buckets in each individual member account should not be lost. Future member accounts should comply with the logging strategy. Which operationally efficient solution meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an AWS Lambda function in each member account with a cross-account role. Trigger the Lambda functions when new CloudTrail logs are created and copy the CloudTrail logs to a centralized S3 bucket. Set up an Amazon CloudWatch alarm to alert if CloudTrail is not configured properly.
       B. Configure CloudTrail in each member account to deliver log events to a central S3 bucket. Ensure the central S3 bucket policy allows PutObject access from the member accounts. Migrate existing logs to the central S3 bucket. Set up an Amazon CloudWatch alarm to alert if CloudTrail is not configured properly.
       C. Configure an organization-level CloudTrail in the parent account to deliver log events to a central S3 bucket. Migrate the existing CloudTrail logs from each member account to the central S3 bucket. Delete the existing CloudTrail and logs in the member accounts.
       D. Configure an organization-level CloudTrail in the parent account to deliver log events to a central S3 bucket. Configure CloudTrail in each member account to deliver log events to the central S3 bucket.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A weather service provides high-resolution weather maps from a web application hosted on AWS in the eu-west-1 Region. The weather maps are updated frequently and stored in Amazon S3 along with static HTML content. The web application is fronted by Amazon CloudFront. The company recently expanded to serve users in the us-east-1 Region, and these new users report that viewing their respective weather maps is slow from time to time. Which combination of steps will resolve the us-east-1 performance issues? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure the AWS Global Accelerator endpoint for the S3 bucket in eu-west-1. Configure endpoint groups for TCP ports 80 and 443 in us-east-1.
       B. Create a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1.
       C. Use Lambda@Edge to modify requests from North America to use the S3 Transfer Acceleration endpoint in us-east-1.
       D. Use Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1.
       E. Configure the AWS Global Accelerator endpoint for us-east-1 as an origin on the CloudFront distribution. Use Lambda@Edge to modify requests from North America to use the new origin.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is deploying a public-facing global application on AWS using Amazon CloudFront. The application communicates with an external system. A solutions architect needs to . Which combination of steps will satisfy these requirements? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a public certificate for the required domain in AWS Certificate Manager and deploy it to CloudFront, an Application Load Balancer, and Amazon EC2 instances.
       B. Acquire a public certificate from a third-party vendor and deploy it to CloudFront, an Application Load Balancer, and Amazon EC2 instances.
       C. Provision Amazon EBS encrypted volumes using AWS KMS and ensure explicit encryption of data when writing to Amazon EBS.
       D. Provision Amazon EBS encrypted volumes using AWS KMS.
       E. Use SSL or encrypt data while communicating with the external system using a VPN.
       F. Communicate with the external system using plaintext and use the VPN to encrypt the data in transit.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A C E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company provides a centralized Amazon EC2 application hosted in a single shared VPC. The centralized application must be accessible from client applications running in the VPCs of other business units. The centralized application front end is configured with a Network Load Balancer (NLB) for scalability. Up to 10 business unit VPCs will need to be connected to the shared VPC. Some of the business unit VPC CIDR blocks overlap with the shared VPC, and some overlap with each other. Network connectivity to the centralized application in the shared VPC should be allowed from authorized business unit VPCs only. Which network configuration should a solutions architect use to provide connectivity from the client applications in the business unit VPCs to the centralized application in the shared VPC?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an AWS Transit Gateway. Attach the shared VPC and the authorized business unit VPCs to the transit gateway. Create a single transit gateway route table and associate it with all of the attached VPCs. Allow automatic propagation of routes from the attachments into the route table. Configure VPC routing tables to send traffic to the transit gateway
       B. Create a VPC endpoint service using the centralized application NLB and enable the option to require endpoint acceptance. Create a VPC endpoint in each of the business unit VPCs using the service name of the endpoint service. Accept authorized endpoint requests from the endpoint service console.
       C. Create a VPC peering connection from each business unit VPC to the shared VPC. Accept the VPC peering connections from the shared VPC console. Configure VPC routing tables to send traffic to the VPC peering connection.
       D. Configure a virtual private gateway for the shared VPC and create customer gateways for each of the authorized business unit VPCs. Establish a Site-to-Site VPN connection from the business unit VPCs to the shared VPC. Configure VPC routing tables to send traffic to the VPN connection.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

</description>
      <category>awslagi</category>
      <category>aws</category>
      <category>googlecloud</category>
    </item>
    <item>
      <title>Best 2021 Tailwind CSS components collections</title>
      <author>Kroustof</author>
      <pubDate>Sun, 22 Aug 2021 14:31:25 +0000</pubDate>
      <link>https://dev.to/kroustof/best-2021-tailwind-css-components-collections-7ei</link>
      <guid>https://dev.to/kroustof/best-2021-tailwind-css-components-collections-7ei</guid>
      <description>&lt;p&gt;Tailwind CSS is definetly the CSS framework you should keep an eye on ! This utility-first framework has been created for rapidly building custom UI components. Since its launch in 2017, Tailwind CSS has gotten more and more popular. The proof of this growing success ? Tailwind CSS has been ranked #1 at the &lt;a href="https://2020.stateofcss.com/en-US/technologies/css-frameworks/"&gt;State of CSS report&lt;/a&gt; for the past two years (2020 and 2021). This ranking is based on satisfaction, interest, usage, and awareness in the developers community.&lt;/p&gt;

&lt;p&gt;Tailwind’s syntax gives an incredible development speed to the developer. If Tailwind CSS is often the good choice to get the work done effectively and quickly, its learning curve can be a bit steep for novice developers.&lt;/p&gt;

&lt;p&gt;So for those in lack of time or inspiration Tailwind CSS component libraries and collections are there to help you ! Their components are fully responsive and page templates allow you to copy and paste HTML snippets directly into your codebase to use or modify. Built by other developers and organizations, these collections allow you to get started on your project quickly by seeing the potential outcome.&lt;/p&gt;

&lt;p&gt;Some of these collections are totally free and others offer free and premium components. Here is the list of the best 2021 Tailwind CSS component libraries that worth your attention. Enjoy !&lt;/p&gt;

&lt;h2&gt;1-&lt;a href="https://tailwindui.com/"&gt; Tailwind UI&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--x9UP61_K--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/xhp7eosk41ggratddw59.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--x9UP61_K--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/xhp7eosk41ggratddw59.png" alt="Tailwind UI website"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Probably the must know collection of Tailwind CSS components which is made by the makers of Tailwind CSS. You will find over 400+ basic components in React, Vue and HTML organized in 3 main categories : Marketing, Application UI and Ecommerce, to fit to your project. No Javascript included. &lt;br&gt;
PREMIUM CONTENT and few free components.&lt;/p&gt;

&lt;h2&gt;2-&lt;a href="https://fancytailwind.com/"&gt; Fancy Tailwind&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--hslRhpl6--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1rpincv50h9gwweq52wz.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--hslRhpl6--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1rpincv50h9gwweq52wz.png" alt="Fancy Tailwind website"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This one is really new. Created by a single frontend developer Fancy Tailwind has been released in August of 2021 and stands out from the rest of Tailwind CSS collections because it offers really cool components with animations and effects (Javascript included). &lt;br&gt;
No other Tailwind CSS collections offers that ! &lt;br&gt;
For now the collection is over 300+ components (made in React) and its owner announce that he will add new categories and components every 2-3 weeks. &lt;br&gt;
PREMIUM CONTENT and around 20% of the components are free (Early access discount available for premium access).&lt;/p&gt;

&lt;h2&gt;3-&lt;a href="http://tailblocks.cc/"&gt; Tailblocks&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--I96skZbX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dxj3rx1yq13k53dau8k0.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--I96skZbX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dxj3rx1yq13k53dau8k0.png" alt="Tailblocks website"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Tailblocks is a collection of 60+ minimalist layout blocks which are used as starters and can be easily customized. You will find multiple categories like testimonial, team, steps, statistics, pricing, hero, header, gallery, footer, feature, and ecommerce. The Tailblocks components are cool because you can switch them to dark mode and also change the primary colors from the interface.&lt;br&gt;
TOTALLY FREE collection !&lt;/p&gt;

&lt;h2&gt;4-&lt;a href="https://tailwindcomponents.com/"&gt; Tailwind Components&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--AkDdWmCd--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vuy2y22ebfpod6bwiouh.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--AkDdWmCd--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vuy2y22ebfpod6bwiouh.png" alt="Tailwind Components website"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Tailwind Components is an open source Tailwind CSS components and templates collection that will help you to bootstrap your new apps, projects or landing sites. The components are essentially made and shared by the community. The amount of components is not that large but you could find some gems hidden in there.&lt;br&gt;
TOTALLY FREE collection !&lt;/p&gt;

&lt;h2&gt;5-&lt;a href="https://merakiui.com/"&gt; Meraki UI&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--VaRkvUP3--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/emo5iqdh446nlepxr7ik.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--VaRkvUP3--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/emo5iqdh446nlepxr7ik.png" alt="Meraki UI website"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Meraki UI is a small collection of beautiful Tailwind CSS components that supports right-to-left (RTL) languages. This means that if a user’s default browser language is set to an RTL language, Meraki UI will reverse everything, including text, scroll, progress indicators, buttons, etc.&lt;br&gt;
TOTALLY FREE collection !&lt;/p&gt;

&lt;h2&gt;6-&lt;a href="https://daisyui.com/"&gt; Daisy UI&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--jJLEgPiT--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qmhdoh58vo7zhcz8e1gj.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--jJLEgPiT--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qmhdoh58vo7zhcz8e1gj.png" alt="Daisy UI website"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Daidy UI is an npm package that can be viewed as a Bootstrap of Tailwind CSS. It is basically a layer to Tailwind CSS with premade classes that give you access to Daisy UI collections of nice made components. You will find a lot of components that could fit to your projects even though some of you will prefer to have a hand on all the utility classes of Tailwind CSS.&lt;br&gt;
TOTALLY FREE collection ! Need an npm installation.&lt;/p&gt;

&lt;h2&gt;7-&lt;a href="https://www.tailwindtoolbox.com/"&gt; Tailwind toolbox&lt;/a&gt;
&lt;/h2&gt;
 

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--f7ZnXkTD--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/y3tw9kxyapwpfkolz22i.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--f7ZnXkTD--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/y3tw9kxyapwpfkolz22i.png" alt="Tailwind toolbox website"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Tailwind toolbox is another open source, community-contributed collections of Tailwind CSS components. However Tailwind toolbox is focused on starters rather than single unit components. You will find there more than 45 starters and 18 components.&lt;br&gt;
TOTALLY FREE collection !&lt;/p&gt;

&lt;h2&gt;8-&lt;a href="https://tailwinduikit.com/"&gt; TUK&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--Uu7KzFXR--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/tsdo1inp7xcsmlee41al.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Uu7KzFXR--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/tsdo1inp7xcsmlee41al.png" alt="TUK website"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Tailwind UI Kit (TUK) is with Tailwind UI the biggest collection of Tailwind CSS components. They have over 1000+ components accesible in React, Angular and Vue, and organized in various categories (Marketing, Application UI and Ecommerce). Like Tailwind UI there is no interaction with the components and you will have to add your own Javascript recipes.&lt;br&gt;
PREMIUM CONTENT and free components (subscription needed to access free components).&lt;/p&gt;

&lt;h2&gt;9-&lt;a href="https://lofiui.co/"&gt; Lofi UI&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--eVTMzCUJ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2dnwh39e0s0ocpr3yr9x.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--eVTMzCUJ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2dnwh39e0s0ocpr3yr9x.png" alt="Lofi UI website"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Lofi UI is a small but well done set of Tailwind CSS components (40 components). The components are structured to be reusable and have minimal styles, so you can add your own and customize your designs to suit your personal taste.&lt;br&gt;
TOTALLY FREE collection !&lt;/p&gt;

&lt;h2&gt;10-&lt;a href="https://blocks.wickedtemplates.com/"&gt; Wicked Blocks&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--60fwqwn0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/eeh166ih8z071qpwkdpi.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--60fwqwn0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/eeh166ih8z071qpwkdpi.png" alt="Wicked Templates website"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Wicked Blocks is the last collection of Tailwind CSS UI components that I will present you. It is a really nice collection of 120+ unique component designs, including alerts, modals, cards, forms, navigation, and footers.&lt;br&gt;
TOTALLY FREE collection !&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;THAT'S IT GUYS !&lt;br&gt;
I hope this list will help you. With these collections of components and templates, you will be able to easily style your Tailwind CSS projects. So take the time to explore them !&lt;/p&gt;

</description>
      <category>tailwindcss</category>
      <category>css</category>
      <category>components</category>
      <category>librairies</category>
    </item>
    <item>
      <title>Effortlessly install TailwindCss in a Rails app with Webpack (minimum configuration)</title>
      <author>Vernes</author>
      <pubDate>Sun, 22 Aug 2021 14:26:28 +0000</pubDate>
      <link>https://dev.to/wizardhealth/effortlessly-install-tailwindcss-in-a-rails-app-with-webpack-minimum-configuration-14gg</link>
      <guid>https://dev.to/wizardhealth/effortlessly-install-tailwindcss-in-a-rails-app-with-webpack-minimum-configuration-14gg</guid>
      <description>&lt;p&gt;A while back &lt;a href="https://twitter.com/dhh"&gt;DHH&lt;/a&gt; decided to created a &lt;a href="https://github.com/rails/tailwindcss-rails"&gt;gem&lt;/a&gt; for easily installing TailwindCss into rails apps 🙌🏻. This gem could be used to install Tailwind through the asset pipeline as well as using webpack. This &lt;a href="https://github.com/rails/tailwindcss-rails/issues/62#issuecomment-900193727"&gt;changed&lt;/a&gt; later on as contributors wanted to focus their attention to what was the heart of the gem 🥺.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://i.giphy.com/media/d10dMmzqCYqQ0/giphy.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://i.giphy.com/media/d10dMmzqCYqQ0/giphy.gif"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Well, since a lot of people used this gem to get a new Rails app going with Tailwind without the hassle of configuring everything from scratch (including colleagues from my company), &lt;a href="https://dev.to/wizardhealth"&gt;we decided&lt;/a&gt; to create a &lt;a href="https://github.com/WizardComputer/tailwindcss-rails-webpacker"&gt;new gem&lt;/a&gt; 🎉. This gem installs Tailwind with Webpack and has production purging enabled. Other Webpack specific problems are to be addressed as well.&lt;/p&gt;

</description>
      <category>tailwindcss</category>
      <category>webpack</category>
      <category>rails</category>
      <category>gem</category>
    </item>
    <item>
      <title>AWS Certified Solutions Architect Professional SAP-C01 Exam Questions Part 3</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 14:20:03 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-3-3leg</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-3-3leg</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;For AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has an on-premises monitoring solution using a PostgreSQL database for persistence of events. The database is unable to scale due to heavy ingestion and it frequently runs out of storage. The company wants to create a hybrid solution and has already set up a VPN connection between its network and AWS. The solution should include the following attributes:&lt;br&gt;
– Managed AWS services to minimize operational complexity.&lt;br&gt;
– A buffer that automatically scales to match the throughput of data and requires no ongoing administration.&lt;br&gt;
– A visualization tool to create dashboards to observe events in near-real time. Support for semi-structured JSON data and dynamic schemas.&lt;br&gt;
Which combination of components will enable the company to create a monitoring solution that will satisfy these requirements? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events.
       B. Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events.
       C. Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.
       D. Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards.
       E. Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A life sciences company is using a combination of open source tools to manage data analysis workflows and Docker containers running on servers in its on-premises data center to process genomics data. Sequencing data is generated and stored on a local storage area network (SAN), and then the data is processed. The research and development teams are running into capacity issues and have decided to re-architect their genomics analysis platform on AWS to scale based on workload demands and reduce the turnaround time from weeks to days. The company has a high-speed AWS Direct Connect connection. Sequencers will generate around 200 GB of data for each genome, and individual jobs can take several hours to process the data with ideal compute capacity. The end result will be stored in Amazon S3. The company is expecting 10-15 job requests each day. Which solution meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use regularly scheduled AWS Snowball Edge devices to transfer the sequencing data into AWS. When AWS receives the Snowball Edge device and the data is loaded into Amazon S3, use S3 events to trigger an AWS Lambda function to process the data.
       B. Use AWS Data Pipeline to transfer the sequencing data to Amazon S3. Use S3 events to trigger an Amazon EC2 Auto Scaling group to launch custom-AMI EC2 instances running the Docker containers to process the data.
       C. Use AWS DataSync to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow. Store the Docker images in Amazon Elastic Container Registry (Amazon ECR) and trigger AWS Batch to run the container and process the sequencing data.
       D. Use an AWS Storage Gateway file gateway to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Batch job that executes on Amazon EC2 instances running the Docker containers to process the data.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has five physical data centers in specific locations around the world. Each data center has hundreds of physical servers with a mix of Windows and Linux-based applications and database services. Each data center also has an AWS Direct Connect connection of 10 Gbps to AWS with a company-approved VPN solution to ensure that data transfer is secure. The company needs to shut down the existing data centers as quickly as possible and migrate the servers and applications to AWS. Which solution meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Install the AWS Server Migration Service (AWS SMS) connector onto each physical machine. Use the AWS Management Console to select the servers from the server catalog, and start the replication. Once the replication is complete, launch the Amazon EC2 instances created by the service.
       B. Install the AWS DataSync agent onto each physical machine. Use the AWS Management Console to configure the destination to be an AMI, and start the replication. Once the replication is complete, launch the Amazon EC2 instances created by the service.
       C. Install the CloudEndure Migration agent onto each physical machine. Create a migration blueprint, and start the replication. Once the replication is complete, launch the Amazon EC2 instances in cutover mode.
       D. Install the AWS Application Discovery Service agent onto each physical machine. Use the AWS Migration Hub import option to start the replication. Once the replication is complete, launch the Amazon EC2 instances created by the service.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A security engineer determined that an existing application retrieves credentials to an Amazon RDS for MySQL database from an encrypted file in Amazon S3. For the next version of the application, the security engineer wants to implement the following application design changes to improve security:&lt;br&gt;
– The database must use strong, randomly generated passwords stored in a secure AWS managed service.&lt;br&gt;
– The application resources must be deployed through AWS CloudFormation.&lt;br&gt;
– The application must rotate credentials for the database every 90 days.&lt;br&gt;
A solutions architect will generate a CloudFormation template to deploy the application. Which resources specified in the CloudFormation template will meet the security engineer’s requirements with the LEAST amount of operational overhead?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Specify a Secrets Manager RotationSchedule resource to rotate the database password every 90 days.
       B. Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Create an AWS Lambda function resource to rotate the database password. Specify a Parameter Store RotationSchedule resource to rotate the database password every 90 days.
       C. Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Create an Amazon EventBridge scheduled rule resource to trigger the Lambda function password rotation every 90 days.
       D. Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Specify an AWS AppSync DataSource resource to automatically rotate the database password every 90 days.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has a three-tier application running on AWS with a web server, an application server, and an Amazon RDS MySQL DB instance. A solutions architect is designing a disaster recovery (DR) solution with an RPO of 5 minutes. Which solution will meet the company’s requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure AWS Backup to perform cross-Region backups of all servers every 5 minutes. Reprovision the three tiers in the DR Region from the backups using AWS CloudFormation in the event of a disaster.
       B. Maintain another running copy of the web and application server stack in the DR Region using AWS CloudFormation drift detection. Configure cross-Region snapshots of the DB instance to the DR Region every 5 minutes. In the event of a disaster, restore the DB instance using the snapshot in the DR Region.
       C. Use Amazon EC2 Image Builder to create and copy AMIs of the web and application server to both the primary and DR Regions. Create a cross-Region read replica of the DB instance in the DR Region. In the event of a disaster, promote the read replica to become the master and reprovision the servers with AWS CloudFormation using the AMIs.
       D. Create AMIs of the web and application servers in the DR Region. Use scheduled AWS Glue jobs to synchronize the DB instance with another DB instance in the DR Region. In the event of a disaster, switch to the DB instance in the DR Region and reprovision the servers with AWS CloudFormation using the AMIs.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to migrate its corporate data center from on premises to the AWS Cloud. The data center includes physical servers and VMs that use VMware and Hyper-V. An administrator needs to select the correct services to collect data for the initial migration discovery process. The data format should be supported by AWS Migration Hub. The company also needs the ability to generate reports from the data. Which solution meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use the AWS Agentless Discovery Connector for data collection on physical servers and all VMs. Store the collected data in Amazon S3. Query the data with S3 Select. Generate reports by using Kibana hosted on Amazon EC2.
       B. Use the AWS Application Discovery Service agent for data collection on physical servers and all VMs. Store the collected data in Amazon Elastic File System (Amazon EFS). Query the data and generate reports with Amazon Athena.
       C. Use the AWS Application Discovery Service agent for data collection on physical servers and Hyper-V. Use the AWS Agentless Discovery Connector for data collection on VMware. Store the collected data in Amazon S3. Query the data with Amazon Athena. Generate reports by using Amazon QuickSight.
       D. Use the AWS Systems Manager agent for data collection on physical servers. Use the AWS Agentless Discovery Connector for data collection on all VMs. Store, query, and generate reports from the collected data by using Amazon Redshift.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is using Amazon Aurora MySQL for a customer relationship management (CRM) application. The application requires frequent maintenance on the database and the Amazon EC2 instances on which the application runs. For AWS Management Console access, the system administrators authenticate against AWS Identity and Access Management (IAM) using an internal identity provider. For database access, each system administrator has a user name and password that have previously been configured within the database. A recent security audit revealed that the database passwords are not frequently rotated. The company wants to replace the passwords with temporary credentials using the company’s existing AWS access controls. Which set of options will meet the company’s requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a new AWS Systems Manager Parameter Store entry for each database password. Enable parameter expiration to invoke an AWS Lambda function to perform password rotation by updating the parameter value. Create an IAM policy allowing each system administrator to retrieve their current password from the Parameter Store. Use the AWS CLI to retrieve credentials when connecting to the database.
       B. Create a new AWS Secrets Manager entry for each database password. Configure password rotation for each secret using an AWS Lambda function in the same VPC as the database cluster. Create an IAM policy allowing each system administrator to retrieve their current password. Use the AWS CLI to retrieve credentials when connecting to the database.
       C. Enable IAM database authentication on the database. Attach an IAM policy to each system administrator’s role to map the role to the database user name. Install the Amazon Aurora SSL certificate bundle to the system administrators’ certificate trust store. Use the AWS CLI to generate an authentication token used when connecting to the database.
       D. Enable IAM database authentication on the database. Configure the database to use the IAM identity provider to map the administrator roles to the database user. Install the Amazon Aurora SSL certificate bundle to the system administrators’ certificate trust store. Use the AWS CLI to generate an authentication token used when connecting to the database.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company’s AWS architecture currently uses access keys and secret access keys stored on each instance to access AWS services. Database credentials are hard-coded on each instance. SSH keys for command-line remote access are stored in a secured Amazon S3 bucket. The company has asked its solutions architect to improve the security posture of the architecture without adding operational complexity. Which combination of steps should the solutions architect take to accomplish this? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use Amazon EC2 instance profiles with an IAM role
       B. Use AWS Secrets Manager to store access keys and secret access keys
       C. Use AWS Systems Manager Parameter Store to store database credentials
       D. Use a secure fleet of Amazon EC2 bastion hosts for remote access
       E. Use AWS KMS to store database credentials                
       F. Use AWS Systems Manager Session Manager for remote access
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A B D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to change its internal cloud billing strategy for each of its business units. Currently, the cloud governance team shares reports for overall cloud spending with the head of each business unit. The company uses AWS Organizations to manage the separate AWS accounts for each business unit. The existing tagging standard in Organizations includes the application, environment, and owner. The cloud governance team wants a centralized solution so each business unit receives monthly reports on its cloud spending. The solution should also send notifications for any cloud spending that exceeds a set threshold. Which solution is the MOST cost-effective way to meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in each account to create monthly reports for each business unit.
       B. Configure AWS Budgets in the organization’s master account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in the organization’s master account to create monthly reports for each business unit.
       C. Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use the AWS Billing and Cost Management dashboard in each account to create monthly reports for each business unit.
       D. Enable AWS Cost and Usage Reports in the organization’s master account and configure reports grouped by application, environment, and owner. Create an AWS Lambda function that processes AWS Cost and Usage Reports, sends budget alerts, and sends monthly reports to each business unit’s email list
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is configuring connectivity to a multi-account AWS environment to support application workloads that serve users in a single geographic region. The workloads depend on a highly available, on-premises legacy system deployed across two locations. It is critical for the AWS workloads to maintain connectivity to the legacy system, and a minimum of 5 Gbps of bandwidth is required. All application workloads within AWS must have connectivity with one another. Which solution will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from a DX partner for each on–premises location. Create private virtual interfaces on each connection for each AWS account VPC. Associate the private virtual interface with a virtual private gateway attached to each VPC.
       B. Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from two DX partners for each on-premises location. Create and attach a virtual private gateway for each AWS account VPC. Create a DX gateway in a central network account and associate it with the virtual private gateways. Create a public virtual interface on each DX connection and associate the interface with the DX gateway.
       C. Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from two DX partners for each on-premises location. Create a transit gateway and a DX gateway in a central network account. Create a transit virtual interface for each DX interface and associate them with the DX gateway. Create a gateway association between the DX gateway and the transit gateway.
       D. Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from a DX partner for each on-premises location. Create and attach a virtual private gateway for each AWS account VPC. Create a transit gateway in a central network account and associate it with the virtual private gateways. Create a transit virtual interface on each DX connection and attach the interface to the transit gateway.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A financial company needs to create a separate AWS account for a new digital wallet application. The company uses AWS Organizations to manage its accounts. A solutions architect uses the IAM user Support1 from the master account to create a new member account with &lt;a href="mailto:finance1@example.com"&gt;finance1@example.com&lt;/a&gt; as the email address. What should the solutions architect do to create IAM users in the new member account?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Sign in to the AWS Management Console with AWS account root user credentials by using the 64- character password from the initial AWS Organizations email sent to finance1@example.com. Set up the IAM users as required.
       B. From the master account, switch roles to assume the OrganizationAccountAccessRole role with the account ID of the new member account. Set up the IAM users as required.
       C. Go to the AWS Management Console sign-in page. Choose “Sign in using root account credentials.” Sign in by using the email address finance1@example.com and the master account’s root password. Set up the IAM users as required.
       D. Go to the AWS Management Console sign-in page. Sign in by using the account ID of the new member account and the Support1 IAM credentials. Set up the IAM users as required.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A financial company needs to create a separate AWS account for a new digital wallet application. The company uses AWS Organizations to manage its accounts. A solutions architect uses the IAM user Support1 from the master account to create a new member account with &lt;a href="mailto:finance1@example.com"&gt;finance1@example.com&lt;/a&gt; as the email address. What should the solutions architect do to create IAM users in the new member account?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Sign in to the AWS Management Console with AWS account root user credentials by using the 64- character password from the initial AWS Organizations email sent to finance1@example.com. Set up the IAM users as required.
       B. From the master account, switch roles to assume the OrganizationAccountAccessRole role with the account ID of the new member account. Set up the IAM users as required.
       C. Go to the AWS Management Console sign-in page. Choose “Sign in using root account credentials.” Sign in by using the email address finance1@example.com and the master account’s root password. Set up the IAM users as required.
       D. Go to the AWS Management Console sign-in page. Sign in by using the account ID of the new member account and the Support1 IAM credentials. Set up the IAM users as required.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is designing a data processing platform to process a large number of files in an Amazon S3 bucket and store the results in Amazon DynamoDB. These files will be processed once and must be retained for 1 year. The company wants to ensure that the original files and resulting data are highly available in multiple AWS Regions. Which solution will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an S3 CreateObject event notification to copy the file to Amazon Elastic Block Store (Amazon EBS). Use AWS DataSync to sync the files between EBS volumes in multiple Regions. Use an Amazon EC2 Auto Scaling group in multiple Regions to attach the EBS volumes. Process the files and store the results in a DynamoDB global table in multiple Regions. Configure the S3 bucket with an S3 Lifecycle policy to move the files to S3 Glacier after 1 year.
       B. Create an S3 CreateObject event notification to copy the file to Amazon Elastic File System (Amazon EFS). Use AWS DataSync to sync the files between EFS volumes in multiple Regions. Use an AWS Lambda function to process the EFS files and store the results in a DynamoDB global table in multiple Regions. Configure the S3 buckets with an S3 Lifecycle policy to move the files to S3 Glacier after 1 year.
       C. Copy the files to an S3 bucket in another Region by using cross-Region replication. Create an S3 CreateObject event notification on the original bucket to push S3 file paths into Amazon EventBridge (Amazon CloudWatch Events). Use an AWS Lambda function to poll EventBridge (CloudWatch Events) to process each file and store the results in a DynamoDB table in each Region. Configure both S3 buckets to use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class and an S3 Lifecycle policy to delete the files after 1 year.
       D. Copy the files to an S3 bucket in another Region by using cross-Region replication. Create an S3 CreateObject event notification on the original bucket to execute an AWS Lambda function to process each file and store the results in a DynamoDB global table in multiple Regions. Configure both S3 buckets to use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class and an S3 Lifecycle policy to delete the files after 1 year.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is running an Apache Hadoop cluster on Amazon EC2 instances. The Hadoop cluster stores approximately 100 TB of data for weekly operational reports and allows occasional access for data scientists to retrieve data. The company needs to reduce the cost and operational complexity for storing and serving this data. Which solution meets these requirements in the MOST cost-effective manner?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Move the Hadoop cluster from EC2 instances to Amazon EMR. Allow data access patterns to remain the same.
       B. Write a script that resizes the EC2 instances to a smaller instance type during downtime and resizes the instances to a larger instance type before the reports are created.
       C. Move the data to Amazon S3 and use Amazon Athena to query the data for reports. Allow the data scientists to access the data directly in Amazon S3.
       D. Migrate the data to Amazon DynamoDB and modify the reports to fetch data from DynamoDB. Allow the data scientists to access the data directly in DynamoDB.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is building a sensor data collection pipeline in which thousands of sensors write data to an Amazon Simple Queue Service (Amazon SQS) queue every minute. The queue is processed by an AWS Lambda function that extracts a standard set of metrics from the sensor data. The company wants to send the data to Amazon CloudWatch. The solution should allow for viewing individual and aggregate sensor metrics and interactively querying the sensor log data using CloudWatch Logs Insights. What is the MOST cost-effective solution that meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Write the processed data to CloudWatch Logs in the CloudWatch embedded metric format.
       B. Write the processed data to CloudWatch Logs. Then write the data to CloudWatch by using the PutMetricData API call.
       C. Write the processed data to CloudWatch Logs in a structured format. Create a CloudWatch metric filter to parse the logs and publish the metrics to CloudWatch with dimensions to uniquely identify a sensor.
       D. Configure the CloudWatch Logs agent for AWS Lambda. Output the metrics for each sensor in statsd format with tags to uniquely identify a sensor. Write the processed data to CloudWatch Logs.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A car rental company has built a serverless REST API to provide data to its mobile app. The app consists of an Amazon API Gateway API with a Regional endpoint, AWS Lambda functions, and an Amazon Aurora MySQL Serverless DB cluster. The company recently opened the API to mobile apps of partners. A significant increase in the number of requests resulted, causing sporadic database memory errors. Analysis of the API traffic indicates that clients are making multiple HTTP GET requests for the same queries in a short period of time. Traffic is concentrated during business hours, with spikes around holidays and other events. The company needs to improve its ability to support the additional usage while minimizing the increase in costs associated with the solution. Which strategy meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Convert the API Gateway Regional endpoint to an edge-optimized endpoint. Enable caching in the production stage.
       B. Implement an Amazon ElastiCache for Redis cache to store the results of the database calls. Modify the Lambda functions to use the cache.
       C. Modify the Aurora Serverless DB cluster configuration to increase the maximum amount of available memory.
       D. Enable throttling in the API Gateway production stage. Set the rate and burst values to limit the incoming calls.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has application services that have been containerized and deployed on multiple Amazon EC2 instances with public IPs. An Apache Kafka cluster has been deployed to the EC2 instances. A PostgreSQL database has been migrated to Amazon RDS for PostgreSQL. The company expects a significant increase of orders on its platform when a new version of its flagship product is released. What changes to the current architecture will reduce operational overhead and support the product release?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an EC2 Auto Scaling group behind an Application Load Balancer. Create additional read replicas for the DB instance. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3.
       B. Create an EC2 Auto Scaling group behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3.
       C. Deploy the application on a Kubernetes cluster created on the EC2 instances behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution.
       D. Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate and enable auto scaling behind an Application Load Balancer. Create additional read replicas for the DB instance. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company recently completed a large-scale migration to AWS. Development teams that support various business units have their own accounts in AWS Organizations. A central cloud team is responsible for controlling which services and resources can be accessed, and for creating operational strategies for all teams within the company. Some teams are approaching their account service quotas. The cloud team needs to create an automated and operationally efficient solution to proactively monitor service quotas. Monitoring should occur every 15 minutes and send alerts when a team exceeds 80% utilization. Which solution will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a scheduled AWS Config rule to trigger an AWS Lambda function to call the GetServiceQuota API. If any service utilization is above 80%, publish a message to an Amazon Simple Notification Service (Amazon SNS) topic to alert the cloud team. Create an AWS CloudFormation template and deploy the necessary resources to each account.
       B. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that triggers an AWS Lambda function to refresh the AWS Trusted Advisor service limits checks and retrieve the most current utilization and service limit data. If the current utilization is above 80%, publish a message to an Amazon Simple Notification Service (Amazon SNS) topic to alert the cloud team. Create AWS CloudFormation StackSets that deploy the necessary resources to all Organizations accounts.
       C. Create an Amazon CloudWatch alarm that triggers an AWS Lambda function to call the Amazon CloudWatch GetInsightRuleReport API to retrieve the most current utilization and service limit data. If the current utilization is above 80%, publish an Amazon Simple Email Service (Amazon SES) notification to alert the cloud team. Create AWS CloudFormation StackSets that deploy the necessary resources to all Organizations accounts.
       D. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that triggers an AWS Lambda function to refresh the AWS Trusted Advisor service limits checks and retrieve the most current utilization and service limit data. If the current utilization is above 80%, use Amazon Pinpoint to send an alert to the cloud team. Create an AWS CloudFormation template and deploy the necessary resources to each account.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An AWS customer has a web application that runs on premises. The web application fetches data from a third-party API that is behind a firewall. The third party accepts only one public CIDR block in each client’s allow list. The customer wants to migrate their web application to the AWS Cloud. The application will be hosted on a set of Amazon EC2 instances behind an Application Load Balancer (ALB) in a VPC. The ALB is located in public subnets. The EC2 instances are located in private subnets. NAT gateways provide internet access to the private subnets. How should a solutions architect ensure that the web application can continue to call the third party API after the migration?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Associate a block of customer-owned public IP addresses to the VPC. Enable public IP addressing for public subnets in the VPC.
       B. Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC.
       C. Create Elastic IP addresses from the block of customer-owned IP addresses. Assign the static Elastic IP addresses to the ALB.
       D. Register a block of customer-owned public IP addresses in the AWS account. Set up AWS Global Accelerator to use Elastic IP addresses from the address block. Set the ALB as the accelerator endpoint.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is using AWS Organizations to manage multiple AWS accounts. For security purposes, the company requires the creation of an Amazon Simple Notification Service (Amazon SNS) topic that enables integration with a third-party alerting system in all the Organizations member accounts. A solutions architect used an AWS CloudFormation template to create the SNS topic and stack sets to automate the deployment of CloudFormation stacks. Trusted access has been enabled in Organizations. What should the solutions architect do to deploy the CloudFormation StackSets in all AWS accounts?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a stack set in the Organizations member accounts. Use service-managed permissions. Set deployment options to deploy to an organization. Use CloudFormation StackSets drift detection.
       B. Create stacks in the Organizations member accounts. Use self-service permissions. Set deployment options to deploy to an organization. Enable the CloudFormation StackSets automatic deployment.
       C. Create a stack set in the Organizations master account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets automatic deployment.
       D. Create stacks in the Organizations master account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets drift detection.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to provide a desktop as a service (DaaS) to a number of employees using Amazon WorkSpaces. WorkSpaces will need to access files and services hosted on premises with authorization based on the company’s Active Directory. Network connectivity will be provided through an existing AWS Direct Connect connection. The solution has the following requirements: Credentials from Active Directory should be used to access on-premises files and services. Credentials from Active Directory should not be stored outside the company. End users should have a single sign-on (SSO) to on-premises files and services once connected to WorkSpaces. Which strategy should the solutions architect use for end user authentication?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) directory within the WorkSpaces VPC. Use the Active Directory Migration Tool (ADMT) with the Password Export Server to copy users from the on-premises Active Directory to AWS Managed Microsoft AD. Set up a one-way trust allowing users from AWS Managed Microsoft AD to access resources in the on-premises Active Directory. Use AWS Managed Microsoft AD as the directory for WorkSpaces.
       B. Create a service account in the on-premises Active Directory with the required permissions. Create an AD Connector in AWS Directory Service to be deployed on premises using the service account to communicate with the on-premises Active Directory. Ensure the required TCP ports are open from the WorkSpaces VPC to the on-premises AD Connector. Use the AD Connector as the directory for WorkSpaces.
       C. Create a service account in the on-premises Active Directory with the required permissions. Create an AD Connector in AWS Directory Service within the WorkSpaces VPC using the service account to communicate with the on-premises Active Directory. Use the AD Connector as the directory for WorkSpaces.
       D. Create an AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) directory in the AWS Directory Service within the WorkSpaces VPC. Set up a one-way trust allowing users from the on-premises Active Directory to access resources in the AWS Managed Microsoft AD. Use AWS Managed Microsoft AD as the directory for WorkSpaces. Create an identity provider with AWS Identity and Access Management (IAM) from an on-premises ADFS server. Allow users from this identity provider to assume a role with a policy allowing them to run WorkSpaces.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

</description>
      <category>awslagi</category>
      <category>aws</category>
      <category>googlecloud</category>
    </item>
    <item>
      <title>AWS Certified Solutions Architect Professional SAP-C01 Exam Questions Part 1</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 14:15:25 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-1-3jk7</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-1-3jk7</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;For AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt;&lt;br&gt;&lt;br&gt;
For GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;1.Company A has hired you to assist with the migration of an interactive website that allows registered users to rate local restaurants. Updates to the ratings are displayed on the home page, and ratings are updated in real time. Althoughthe website is not very popular today, the company anticipates that it willgrow rapidly over the next few weeks. Theywant the site to be highly available. The current architecture consists of a single Windows Server 2008 R2 web server and a MySQL database running on Linux. Both reside inside an on-premises hypervisor. What would be the most efficient way to transfer the application to AWS, ensuring performance and high-availability?&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;           A. Use AWS VM Import/Export to create an Amazon Elastic Compute Cloud (EC2) Amazon Machine Image (AMI) of the web server. Configure Auto Scaling to launch two web servers in us-west-1a and two in us-est-1b. Launch a Multi-AZ MySQL Amazon Relational Database Service (RDS) instance in us-west-1b. Import the data into Amazon RDS from the latest MySQL backup. Use Amazon Route_53 to create a hosted zone and point an A record to the elastic load balancer
           B. Export web files to an Amazon S3 bucket in us-west-1. Run the website directly out of Amazon S3. Launch a multi-AZ MySQL Amazon RDS instance in us-west-1a. Import the data into Amazon RDS from the latest MySQL backup. Use Route 53 and create an alias record pointing to the elastic load balancer
           C. Use AWS VM Import/Export to create an Amazon EC2 AMI of the web server. Configure auto-scaling to launch two web servers in us-west-1a and two in us-west-1b. Launch a multi-AZ MySQL Amazon RDS instance in us-west-1a. Import the data into Amazon RDS from the latest MySQL backup. Create an elastic load balancer to front your web servers. Use Amazon Route 53 and create an A record pointing to the elastic load balancer
           D. Launch two Windows Server 2008 R2 instances in us-west-1b and two in Us-west-1a. Copy the web files from on premises web server to each Amazon EC2 web server, using Amazon S3 as the repository. Launch a multi-AZ MySQL Amazon RDS instance in us-west-2a. Import the data into Amazon RDS from the latest MySQL backup. Create an elastic load balancer to front your web servers. Use Route 53 and create an alias record pointing to the elastic load balancer.
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A marketing research company has developed a tracking system that collects user behavior during web marketing campaigns on behalf of their customers all over the world. The tracking system consists of an auto-scaled group of Amazon Elastic Compute Cloud (EC2) instances behind an elastic load balancer (ELB), and the collected data is stored in Amazon DynamoDB. After the campaign is terminated, the tracking system is torn down and the data is moved to Amazon Redshift, where it is aggregated, analyzed and used to generate detailed reports. The company wants to be able to instantiate new tracking systems in any region without any manual intervention and therefore adopted AWS CloudFormation. What needs to be done to make sure that the AWS CloudFormation template works in every AWS region? (Choose 2 answers)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Avoid using DeletionPolicies for EBS snapshots
       B. The names of the Amazon DynamoDB tables must be different in every target region
       C. Use the built-in Mappings and FindInMap functions of AWS CloudFormation to refer to the AMI ID set in the ImageId attribute of the Auto Scaling::LaunchConfiguration resource
       D. IAM users with the right to start AWS CloudFormation stacks must be defined for every target region.
       E. Use the built-in function of AWS CloudFormation to set the AvailabilityZone attribute of the ELB resource
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A development team that is currently doing a nightly six-hour build which is lengthening over time on-premises with a large and mostly underutilized server would like to transition to a continuous integration model of development on AWS with multiple builds triggered within the same day. However, they areare concerned about cost, security, and how to integrate with existing on-premises applications such as their LDAP and email servers which cannot move off-premises. The development environment needs a source code repository, a project management system with a MySQL database, resources for performing the builds, and a storage location for QA to pick up builds from. What AWS services combination would you recommend to meet the development team’s requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. A Bastion host Amazon Elastic Compute Cloud (EC2) instance running a VPN server for access from on-premises, Amazon EC2 for the source code repository with attached Amazon Elastic Block Store (EBS) volumes, Amazon EC2 and Amazon Relational Database Service (RDS) MySQL for the project management system, EIPs for the source code repository and project management system, Amazon Simple Queue Service (SQS) for a build queue, An Auto Scaling group of Amazon EC2 instances for performing builds, and Amazon Simple Email Service for sending the build output
       B. An AWS Storage Gateway for connecting on-premises software applications with cloud-based storage securely, Amazon EC2 for the source code repository with attached Amazon EBS volumes, Amazon EC2 and Amazon RDS MySQL for the project management system, EIPs for the source code repository and project management system, Amazon Simple Notification Service (SNS) for a notification-initiated build, An Auto Scaling group of Amazon EC2 instances for performing builds, and Amazon S3 for the build output.
       C. An AWS Storage Gateway for connecting on-premises software applications with cloud-based storage securely, Amazon EC2 for the source code repository with attached Amazon EBS volumes, Amazon EC2 and Amazon RDS MySQL for the project management system, EIPs for the source code repository and project management system, Amazon SQS for a build queue, An Amazon Elastic MapReduce (EMR) cluster of Amazon EC2 instances for performing builds, and Amazon CloudFront for the build output.
       D. A VPC with a VPN Gateway back to their on-premises servers, Amazon EC2 for the source-code repository with attached Amazon EBS volumes, Amazon EC2 and Amazon RDS MySQL for the project management system, EIPs for the source code repository and project management system, SQS for a build queue, An Auto Scaling group of EC2 instances for performing builds, and S3 for the build output
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A large enterprise wants to adopt CloudFormation to automate administrative tasks and implement the security principles of least priviledge and separation of duties. They have identified the following roles with the corresponding tasks in the company:&lt;/li&gt;
&lt;li&gt; Network administrators: create, modify and delete VPCs, subnets, NACLs, routing tables, and security groups application operators: deploy complete application stacks (ELB, Auto-Scaling groups, RDS) whereas all resources must be deployed in the VPCs managed by the network administrators.&lt;/li&gt;
&lt;li&gt;Both groups must maintain their own CloudFormation templates and should be able to create, update and delete only their own CloudFormation stacks. &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The company has followed your advice to create two IAM groups, one for applications and one for - networks. Both IAM groups are attached to IAM policies that grant rights to perform the necessary task of each group as well as the creation, update and deletion of CloudFormation stacks. &lt;br&gt;
Given setup and requirements, which statements represent valid design considerations? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Network stack updates will fail upon attempts to delete a subnet with EC2 instances
       B. Restricting the launch of EC2 instances into VPCs requires resource level permissions in the IAM policy of the application group
       C. Nesting network stacks within application stacks simplifies management and debugging, but requires resource level permissions in the IAM policy of the network group
       D. Unless resource level permissions are used on the cloudformation:DeleteStack action, network administrators could tear down application stacks
       E. The application stack cannot be deleted before all network stacks are deleted
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To enable end-to-end HTTPS connections from the userˈs browser to the origin via CloudFront, which of the following options would be valid? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use a self signed certificate in the origin and CloudFront default certificate in CloudFront
       B. Use the CloudFront default certificate in both the origin and CloudFront
       C. Use third-party CA certificate in the origin and CloudFront default certificate in CloudFront
       D. Use third-party CA certificate in both the origin and CloudFront
       E. Use a self signed certificate in both the origin and CloudFront
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A customer is runningan application in US-West (Northern California) region and wants to setup disaster recovery failover to the Asian Pacific (Singapore) region.The customer isinterested in achieving a low Recovery Point Objective (RPO) foran Amazon Relational DatabaseService(RDS) multi-AZ MySQL database instance. Which approach is best suited to this need?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Synchronous replication
       B. Asynchronous replication
       C. Route53 health checks
       D. Copying of RDS incremental snapshots
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A document storage company is deploying their application to AWS and changing their business model to support both Free Tier and Premium Tier users. The Premium Tier &lt;br&gt;
users will be allowed to store up to 200GB of data and Free Tier customers will be allowed to store only 5GB. The customer expects that billions of files will be stored. All users need to be alerted when approaching 75 percent quota utilization and again at 90 percent quota use. To support the Free Tier and Premium Tier users, how should they architect their application?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. The company should utilize an Amazon Simple Workflow Service activity worker that updates the userˈs used data counter in Amazon DynamoDB. The Activity Worker will use Simple Email Service to send an email if the counter increases above the appropriate thresholds.
       B. The company should deploy an Amazon Relational Database Service (RDS) relational database with a stored objects table that has a row for each stored object along with the size of each object. The upload server will query the aggregate consumption of the user in question (by first determining the files stored by the user, and then querying the stored objects table for respective file sizes) and send an email via Amazon Simple Email Service if the thresholds are breached.
       C. The company should write both the content length and the username of the files owner as S3 metadata for the object. They should then create a a file watcher to iterate over each object and aggregate the size for each user and send a notification via Amazon Simple Queue Service to an emailing service if the storage threshold is exceeded
       D. The company should create two separate Amazon Simple Storage Service buckets, one for data storage for Free Tier Users, and another for data storage for Premium Tier users. An Amazon Simple Workflow Service activity worker will query all objects for a given user based on the bucket the data is stored in and aggregate storage. The activity worker will notify the user via Amazon Simple Notification Service when necessary.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A public archives organization is about to move a pilot application they are running on AWS into production. You have been hired to analyze their application architecture and give cost-saving recommendations. The application displays scanned historical documents. Each document is split into individual image tiles at multiple zoom levels to improve responsiveness and ease of use for the end users. At maximum zoom level the average document will be 8000x 6000 pixels in size, split into multiple 40pxx 40px image tiles. The tiles are batch processed by Amazon Elastic Compute Cloud (EC2) instances, and put into an Amazon Simple Storage Service(S3) bucket.A browser-based JavaScript viewer fetches tiles from the Amazon (S3) bucket and displays them to users as they zoom and pan around each document. The average storage size of all zoom levels for a document is approximately 30MB of JPEG tiles. Originals of each document are archived in Amazon Glacier. The company expects to process and host over 500,000 scanned documents in the first year. What are your recommendations? Choose 3 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Deploy an Amazon CloudFront distribution in front of the Amazon S3 tiles bucket
       B. Increase the size (width/height) of the individual tiles at the maximum zoom level
       C. Store the maximum zoom level in the low cost Amazon S3 Glacier option and only retrieve the most frequently access tiles as they are requested by users.
       D. Use Amazon S3 Reduced Redundancy Storage for each zoom level.
       E. Decrease the size (width/height) of the individual tiles at the maximum zoom level.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A B D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Your multi-national customer wants to rewrite a website portal to “take advantage of AWS best practices”. Other information that you have for this large Enterprise customer is as follows:&lt;/li&gt;
&lt;li&gt;Part of the portal is an employee-only section, and authentication must be against the corporate Active Directory.
• You used a web analytics website to discover that on average there were 140,000 visitors per month over the past year, a peak of 187,000 unique visitors last month, and a minimum of 109,000 unique visitors two months ago. You have no information about what percentage of these visitors represents employees who signed into the portal.&lt;/li&gt;
&lt;li&gt; The web analytics website also revealed that traffic breakdown is 40 percent South America, 50 percent North America, and 10 percent other.&lt;/li&gt;
&lt;li&gt;The customer’s primary data center is located in Sao Paulo Brazil.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Their chief technology officer believes that response time for logging in to the employee portal is a primary metric, because employees complain that the current website is too slow in this regard.&lt;br&gt;
When you present your proposed application architecture to the customer, which of the following should you propose as part of the architecture? Choose 3 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. A three-subnet VPC, with an AD controller in the AWS region. The AWS AD controller will be part of the primary AD controller’s forest, and will synchronize with the corporate controller over a dedicated pipe to the corporate data center
       B. Do not use Amazon CloudFront, because the employees who log in to the portal have unique (private) session data that should not be cached in a content delivery network.
       C. A three-subnet VPC, with all AD calls traversing a dedicated pipe to the corporate data center
       D. Establish the AWS presence in the US-EAST region, with a dedicated pipe to the corporate data center.
       E. Establish the AWS presence in multiple regions: SA-EAST, and also US-EAST, with a dedicated pipe from both SA-EAST and US-EAST to the corporate data center – and also a dedicated connection between regions. Replicate data as needed between the regions. Use a geo load balancer to determine which region is primary for a given user.
       F. Use Amazon CloudFront to cache pages for users at the nearest edge location.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A D F&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;For a 3-tier, customer facing, inclement weather site utilizing a MySQL database running in a Region which has two AZs (Availability Zone), which architecture provides fault tolerance within the Region for the application that minimally requires 6 web tier servers and 6 application tier servers running in the web and application tiers and one MySQL database?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. A web tier deployed in 2 AZs with 6 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (Elastic Load Balancer), and an application tier deployed in 2 AZs with 6 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB, and a Multi-AZ RDS (Relational Database Service) deployment
       B. A web tier deployed in 2 AZs with 3 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (Elastic Load Balancer), and an application tier deployed in 2 AZs with 3 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB, and a Multi-AZ RDS (Relational Database Service) deployment.
       C. A web tier deployed in 2 AZs with 3 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (Elastic Load Balancer), and an application tier deployed in 2 AZs with 6 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB, and one RDS (Relational Database Service) instance deployed with read replicas in the other AZ.
       D. A web tier deployed in 1 AZ with 6 EC2 (Elastic Compute Cloud) instances inside an Auto Scaling Group behind an ELB (Elastic Load Balancer), and an application tier deployed in the same AZ with 6 EC2 instances inside an Auto Scaling Group behind an ELB, and a Multi-AZ RDS (Relational Database Service) deployment, with 6 stopped web tier EC2 instances and 6 stopped application tier EC2 instances all in the other AZ ready to be started if any of the running instances in the first AZ fails.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A gaming company adopted AWS Cloud Formation to automate load-testing of their games. They have created an AWS Cloud Formation template for each gaming environment and one for the load-testing stack. The load-testing stack creates an Amazon Relational Database Service (RDS) Postgres database and two web servers running on Amazon Elastic Compute Cloud (EC2) that send HTTP requests, measure response times, and write the results into the database. A test run usually takes between 15 and 30 minutes. Once the tests are done, the AWS CloudFormation stacks are torn down immediately. The test results written to the Amazon RDS database must remain accessible for visualization and analysis. Select possible solutions that allow access to the test results after the AWS Cloud Formation load-testing stack is deleted. Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Define an update policy to prevent deletion of the Amazon RDS database after the AWS CloudFormation stack is deleted.
       B. Define a deletion policy of type Snapshot for the Amazon RDS resource to assure that the RDS database can be restored after the AWS CloudFormation stack is deleted.
       C. Define automated backups with a backup retention period of 30 days for the Amazon RDS database and perform point-in-time recovery of the database after the AWS CloudFormation stack is deleted.
       D. Define an Amazon RDS Read-Replica in the load-testing AWS CloudFormation stack and define a dependency relation between master and replica via the DependsOn attribute
       E. Define a deletion policy of type Retain for the Amazon RDS resource to assure that the RDS database is not deleted with the AWS CloudFormation stack.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You are an architect for a news-sharing mobile application. Anywhere in the world, your users can see local news on topics they choose. They can post pictures and videos from inside the application. Since the application is being used on a mobile phone, connection stability is required for uploading content, and delivery should be quick. Content is accessed a lot in the first minutes after it has been posted, but is quickly replaced by new content before disappearing. The local nature of the news means that 90 percent of the uploaded content is then read locally (less than a hundred kilometers from where it was posted). What solution will optimize the user experience when users upload and view content (by minimizing page load times and minimizing upload times)?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Upload and store the content in a central Amazon Simple Storage Service (S3) bucket, and use an Amazon CloudFront Distribution for content delivery.
       B. Upload and store the content in an Amazon Simple Storage Service (S3) bucket in the region closest to the user, and use multiple Amazon CloudFront distributions for content delivery
       C. Upload the content to an Amazon Elastic Compute Cloud (EC2) instance in the region closest to the user, send the content to a central Amazon Simple Storage Service (S3) bucket, and use an Amazon CloudFront distribution for content delivery.
       D. Use an Amazon CloudFront distribution for uploading the content to a central Amazon Simple Storage Service (S3) bucket and for content delivery.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A customer is deploying an SSL enabled Web application to AWS and would like to implement a separation of roles between the EC2 service administrators that are entited to login to Instances as well as making API calls and the security officers who will maintain and have exclusive access to the applicationˈs X.509 certificate that contains the private key. Which configuration option could satisfy the above requirement?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure the web servers to retrieve the certificate upon boot from an CloudHSM that is managed by the security officers.
       B. Configure system permissions on the web servers to restrict access to the certificate only to the authorized security officers.
       C. Configure IAM policies authorizing access to the certificate store only to the security officers and terminate SSL on an ELB.
       D. Upload the certificate on an S3 bucket owned by the security officers and accessible only by the EC2 Role of the web servers.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You are designing security inside your VPC. You are considering the options for establishing separate security zones, and enforcing network traffic rules across the different zones to limit which instances can communicate. How would you accomplish these requirements? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure multiple subnets in your VPC, one for each zone. Configure routing within your VPC in such a way that each subnet only has routes to other subnets with which it needs to communicate, and doesnˈt have routes to subnets with which it shouldnˈt be able to communicate.
       B. Configure your instances to use pre-set IP addresses with an IP address range for every security zone. Configure NACLs to explicitly allow or deny communication between the different IP address ranges, as required for interzone communication.
       C. Configure a security group for every zone. Configure a default allow all rule. Configure explicit deny rules for the zones that shouldnˈt be able to communicate with one another
       D. Configure a security group for every zone. Configure allow rules only between zones that need to be able to communicate with one another. Use the implicit deny all rule to block any other traffic.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your company currently has a highly available web application running in production. The application’s web front-end utilizes an Elastic Load Balancerand Auto Scaling across three Availability Zones.During peak load, your web servers operate at 90% utilization and leverage a combination of Heavy Utilization Reserved Instances for steady state load and On-Demand and Spot Instances for peak load. You are tasked with designing a cost effective architecture to allow the application to recover quickly in the event that an Availability Zoneis unavailable during peak load. Which option provides the most cost effective high availability architectural design for this application?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Continue to run your web front-end at 90% utilization, but leverage a high bid price strategy to cover the loss of any of the other Availability Zones during peak load.
       B. Increase use of spot instances to cost effectively scale the web front-end across all Availability Zones to lower aggregate utilization levels that will allow an Availability Zone to fail during peak load without affecting the application’s availability.
       C. Increase Auto Scaling capacity and scaling thresholds to allow the web front-end to cost effectively scale across all Availability Zones to lower aggregate utilization levels that will allow an Availability Zone to fail during peak load without affecting the application’s availability.
       D. Continue to run your web front-end at 90% utilization, but purchase an appropriate number of light utilization RIs in each Availability Zone to cover the loss of any of the other Availability Zones during peak load.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An Enterprise customer is starting their migration to the cloud, their main reason for migrating is agility, and they want to make their internal Microsoft Active Directory available to any applications running on AWS; this is so internal users only have to remember one set of credentials and as a central point of user control for leavers and joiners. How could they make their Active Directory secure, and highly available, with minimal on-premises infrastructure changes, in the most cost and time-efficient way? Choose the most appropriate:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Using Amazon Elastic Compute Cloud (EC2), they could create a DMZ using a security group; within the security group they could provision two smaller Amazon EC2 instances that are running Openswan for resilient IPSEC tunnels, and two larger instances that are domain controllers; they would use multiple Availability Zones
       B. Using VPC, they could create an extension to their data center and make use of resilient hardware IPSEC tunnels; they could then have two domain controller instances that are joined to their existing domain and reside within different subnets, in different Availability Zones.
       C. Within the customerˈs existing infrastructure, they could provision new hardware to run Active Directory Federation Services; this would present Active Directory as a SAML2 endpoint on the internet; any new application on AWS could be written to authenticate using SAML2.
       D. The customer could create a stand-alone VPC with its own Active Directory Domain Controllers; two domain controller instances could be configured, one in each Availability Zone; new applications would authenticate with those domain controllers.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An AWS customer is deploying a web application that is composed of a front-end running on Amazon EC2 and confidential data that is stored on Amazon S3. The customers security policy requires that the all access operations to this sensitive data must be authenticated and authorized by a centralized access management system that is operated by a separate security team. In addition, the web application team that owns and administers the EC2 web front-end instances is prohibited from having any ability to access the data that circumvents this centralized access management system. Which of the following configurations will support these requirements:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure the web application to authenticate end-users against the centralized access management system. Have the web application provision trusted users STS tokens entitling the download of approved data directly from Amazon S3.
       B. Encrypt the data on Amazon S3 using a CloudHSM that is operated by the separate security team. Configure the web application to integrate with the CloudHSM for decrypting approved data access operations for trusted end-users.
       C. Configure the web application to authenticate end-users against the centralized access management system using SAML. Have the end-users authenticate to IAM using their SAML token and download the approved data directly from Amazon S3.
       D. Have the separate security team create an IAM Role that is entitled to access the data on Amazon S3. Have the web application team provision their instances with this Role while denying their IAM users access to the data on Amazon S3.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You have been asked to design network connectivity between your existing data centers and AWS. Your application’s EC2 instances must be able to connect to existing backend resources located in your data center. Network traffic between AWS and your data centers will start small, but ramp up to 10s of GB per second over the course of several months. The success of your application is dependent upon getting to market quickly. Which of the following design options will allow you to meet your objectives?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Quickly submit a DirectConnect request to provision a 1 Gbps cross connect between your data center and VPC, then increase the number or size of your DirectConnect connections as needed
       B. Quickly create an internal ELB for your backend applications, submit a DirectConnect request to provision a 1 Gbps cross connect between your data center and VPC, then increase the number or size of your DirectConnect connections as needed
       C. Allocate EIPs and an Internet Gateway for your VPC instances to use for quick, temporary access to your backend applications, then provision a VPN connection between a VPC and existing on-premises equipment.
       D. Provision a VPN connection between a VPC and existing on-premises equipment, submit a DirectConnect partner request to provision cross connects between your data center and the DirectConnect location, then cut over from the VPN connection to one or more DirectConnect connections as needed.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You have an application running on an EC2 instance which will allow users to download files from a private S3 bucket using a pre-signed URL. Before generating the URL, the application should verify the existence of the file in S3. How should the application use AWS credentials to access the S3 bucket securely?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an IAM user for the application with permissions that allow list access to the S3 bucket; launch the instance as the IAM user, and retrieve the IAM user’s credentials from the EC2 instance user data.
       B. Create an IAM role for EC2 that allows list access to objects in the S3 bucket; launch the instance with the role, and retrieve the role’s credentials from the EC2 instance metadata
       C. Use the AWS account access keys; the application retrieves the credentials from the source code of the application.
       D. Create an IAM user for the application with permissions that allow list access to the S3 bucket; the application retrieves the IAM user credentials from a temporary directory with permissions that allow read access only to the application user.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A startup deploys its photo-sharing site in a VPC. An elastic load balancer distributes web traffic across two subnets. The load balancer session stickiness is configured to use the AWS-generated session cookie, with a session TTL of 5 minutes. The web server Auto Scaling group is configured as min-size=4, max-size=4. The startup is preparing for a public launch, by running load-testing software installed on a single Amazon Elastic Compute Cloud (EC2) instance running in us-west-2a. After 60 minutes of load-testing, the web server logs show the following:&lt;br&gt;
+———————-+————————-+&lt;br&gt;
| # of HTTP requests | # of HTTP requests |&lt;br&gt;
WEBSERVER LOGS | from load-tester | from private beta users |&lt;br&gt;
+—————————————|———————-|————————-+&lt;br&gt;
| webserver #1 (subnet in us-west-2a): | 19,210 | 434 |&lt;br&gt;
| webserver #2 (subnet in us-west-2a): | 21,790 | 490 |&lt;br&gt;
| webserver #3 (subnet in us-west-2b): | 0 | 410 |&lt;br&gt;
| webserver #4 (subnet in us-west-2b): | 0 | 428 |&lt;br&gt;
+—————————————+———————-+————————-+&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Which recommendations can help ensure that load-testing HTTP requests are evenly distributed across the four webservers?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Choose 2 answers&lt;br&gt;
           B. Launch and run the load-tester Amazon EC2 instance from us-east-1 instead.&lt;br&gt;
           C. Use a third-party load-testing service which offers globally distributed test clients.&lt;br&gt;
           D. Configure Elastic Load Balancing and Auto Scaling to distribute across us-west-2a and us-west-2b.&lt;br&gt;
           E. Configure Elastic Load Balancing session stickiness to use the app-specific session cookie&lt;br&gt;
           F. Re-configure the load-testing software to re-resolve DNS for each web request.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C F&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To meet regulatory requirements, a pharmaceuticals company needs to archive data after a drug trial test is concluded. Each drug trial test may generate up to several thousands of files, with compressed file sizes ranging from 1 byte to 100MB. Once archived, data rarely needs to be restored, and on the rare occasion when restoration is needed, the company has 24 hours to restore specific files that match certain metadata. Searches must be possible by numeric file ID, drug name, participant names, date ranges, and other metadata. Which is the most cost-effective architectural approach that can meet the requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Store individual compressed files and search metadata in Amazon Simple Storage Service (S3). Create a lifecycle rule to move the data to Amazon Glacier, after a certain number of days. When restoring data, query the Amazon S3 bucket for files matching the search criteria, and retrieve the file to S3 reduced redundancy in order to move it back to S3 Standard class.
       B. Store individual files in Amazon Glacier, using the file ID as the archive name. When restoring data, query the Amazon Glacier vault for files matching the search criteria.
       C. First, compress and then concatenate all files for a completed drug trial test into a single Amazon Glacier archive. Store the associated byte ranges for the compressed files along with other search metadata in an Amazon RDS database with regular snapshotting. When restoring data, query the RDS database for files that match the search criteria, and create restored files from the retrieved byte ranges
       D. Store individual files in Amazon S3, and store search metadata in an Amazon Relational Database Service (RDS) multi-AZ database. Create a lifecycle rule to move the data to Amazon Glacier after a certain number of days. When restoring data, query the Amazon RDS database for files matching the search criteria, and move the files matching the search criteria back to S3 Standard class.
       E. Store individual files in Amazon Glacier, and store the search metadata in an Amazon RDS multi-AZ database. When restoring data, query the Amazon RDS database for files matching the search criteria, and retrieve the archive name that matches the file ID returned from the database query.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You have been asked to virtually extend two existing data centers into AWS to support a highly available application that depends on existing, on-premises resources located in multiple data centers and static content that is served from an Amazon Simple Storage Service (S3) bucket. Your design currently includes a dual-tunnel VPN connection between your CGW and VGW. Which component of your architecture represents a potential single point of failure that you should consider changing to make the solution more highly available?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Add another CGW in a different data center and create another dual-tunnel VPN connection
       B. Add a second VGW in a different Availability Zone, and a CGW in a different data center, and create another dual-tunnel.
       C. No changes are necessary: the network architecture is currently highly available
       D. Add another VGW in a different Availability Zone and create another dual-tunnel VPN connection
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your company has recently extended its datacenter into a VPC on AWS to add burst computing capacity as needed. Members of your Network Operations Center need to be able to go to the AWS Management Console and administer Amazon EC2 instances as necessary. You donˈt want to create new IAM users for each NOC member and make those users sign in again to the AWS Management Console. Which option below will meet the needs for your NOC members&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use Web Identity Federation to retrieve AWS temporary security credentials to enable your NOC members to sign in to the AWS Management Console.
       B. Use your on-premises SAML 2.0-compliant identity provider (IdP) to retrieve temporary security credentials to enable NOC members to sign in to the AWS Management Console.
       C. Use OAuth 2.0 to retrieve temporary AWS security credentials to enable your NOC members to sign in to the AWS Management Console.
       D. Use your on-premises SAML 2.0-compliant identity provider (IdP) to grant the NOC members federated access to the AWS Management Console via the AWS single sign-on (SSO) endpoint.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A media production company wants to deliver high-definition raw video material for preproduction and dubbing to customers all around the world. They would like to use Amazon CloudFront for their scenario, and they require the ability to limit downloads per customer and video file to a configurable number. A CloudFront download distribution with TTL = 0 was already setup to make sure all client HTTP requests hit an authentication backend on Amazon Elastic Compute Cloud (EC2)/Amazon Relational Database Service (RDS) first, which is responsible for restricting the number of downloads. Content is stored in Amazon Simple Storage Service (S3) and configured to be accessible only via CloudFront. What else needs to be done to achieve an architecture that meets the requirements? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Enable CloudFront logging into an Amazon S3 bucket, leverage Amazon Elastic MapReduce (EMR) to analyze CloudFront logs to determine the number of downloads per customer, and return the content S3 URL unless the download limit is reached.
       B. Enable CloudFront logging into an Amazon S3 bucket, let the authentication backend determine the number of downloads per customer by parsing those logs, and return the content S3 URL unless the download limit is reached
       C. Enable URL parameter forwarding, let the authentication backend count the number of downloads per customer in Amazon RDS, and return the content S3 URL unless the download limit is reached
       D. Configure a list of trusted signers, let the authentication backend count the number of download requests per customer in Amazon RDS, and return a dynamically signed URL unless the download limit is reached.
       E. Enable URL parameter forwarding, let the authentication backend count the number of downloads per customer in Amazon RDS, and invalidate the CloudFront distribution as soon as the download limit is reached.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your customer is implementing a video on-demand streaming platform on AWS. The requirements are; support for multiple devices such as iOS, Android, and PC as client devices, using a standard client player, using streaming technology (not download,) and scalable architecture with cost effectiveness. Which architecture meets the requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Store the video contents to Amazon Simple Storage Service (S3) as an origin server. Configure the Amazon CloudFront distribution with a streaming option to stream the video contents
       B. Store the video contents to Amazon S3 as an origin server. Configure the Amazon CloudFront distribution with a download option to stream the video contents.
       C. Launch a streaming server on Amazon Elastic Compute Cloud (EC2) (for example, Adobe Media Server), and store the video contents as an origin server. Configure the Amazon CloudFront distribution with a download option to stream the video contents.
       D. Launch a streaming server on Amazon EC2(for example, Adobe Media Server), and store the video contents as an origin server. Launch and configure the required amount of streaming servers on Amazon EC2 as an edge server to stream the video contents.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A research scientist is planning for the one-time launch of an Elastic MapReduce cluster and is encouraged by her manager to minimize costs. The cluster is designed to ingest 200TB of genomics data with a total of 100 Amazon Elastic Compute Cloud (EC2) instances and is expected to run for around four hours. The resulting data set must be stored temporarily until archived into an Amazon Relational Database Service (RDS) Oracle instance. Which option will help save the mostmoney while meeting requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Deploy on-demand master, core and task nodes and store ingest and output files in Amazon Simple Storage Service (S3) Reduced Redundancy Storage (RRS).
       B. Store the ingest files in Amazon S3 RRS and store the output files in S3. Deploy Reserved Instances for the master, and core nodes and on-demand for the task nodes.
       C. Store ingest and output files in Amazon S3. Deploy on-demand for the master, and core nodes and spot for the task nodes.
       D. Optimize by deploying a combination of on-demand, RI, and spot-pricing models for the master, core, and task nodes. Store ingest and output files in Amazon S3 with a lifecycle policy that archives them to Amazon Glacier.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your social media monitoring application uses a Python app running on AWS Elastic Beanstalk to inject tweets, Facebook updates and RSS feeds into an Amazon Kinesis stream. A second AWS Elastic Beanstalk app generates key performance indicators into an Amazon DynamoDB table and powers a dashboard application.&lt;br&gt;
What is the most efficient option to prevent any data loss for this application?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Add a second Amazon Kinesis stream in another Availability Zone and use AWS data pipeline to replicate data across Kinesis streams.
       B. Add a third AWS Elastic Beanstalk app that uses the Amazon Kinesis S3 connector to archive data from Amazon Kinesis into Amazon S3.
       C. Use AWS Data Pipeline to replicate your DynamoDB tables into another region.
       D. Use the second AWS Elastic Beanstalk app to store a backup of Kinesis data onto Amazon Elastic Block Store (EBS), and then create snapshots from your Amazon EBS volumes.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You tried to integrate two subsystems (front-end and back-end) with an HTTP interface to one large system. These subsystems don’t store any state inside. All state is stored in an Amazon DynamoDB table. You have launched each of these two subsystems from a separate AMI. Black box testing has shown that these servers have stopped running and are issuing malformed requests that do not meet HTTP specifications from the client. Your developers have discover and fixed this issue, and you deploy the fix to the two subsystems as soon as possible without service disruption. What are the most effective options to deploy the fixes? Choose 3 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use VPC.
       B. Use AWS OpsWorks auto healing for both the front-end and back-end instance pair
       C. Use Elastic Load Balancing in front of the front-end subsystem and Auto Scaling to keep the specified number of instances
       D. Use Elastic Load Balancing in front of the back-end subsystem and Auto Scaling to keep specified number of instances.
       E. Use Amazon CloudFront which accesses the front-end server when origin fetch
       F. Use Amazon Simple Queue Service SQS between the front-end and back-end subsystems
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B C D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;When deploying a highly available 2-tier web application on AWS, which combination of AWS Services meets the requirements?&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;AWS Direct Connect&lt;br&gt;
2.Amazon Route 53&lt;br&gt;
3.AWS Storage Gateway&lt;br&gt;
4.Elastic Load Balancing&lt;br&gt;
5.Amazon EC2&lt;br&gt;
6.Auto Scaling&lt;br&gt;
7.Amazon VPC&lt;br&gt;
8.AWS Cloud Trail&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. 2,4,5 and 6
       B. 3,4,5 and 8
       C. 1,2,5 and 6
       D. 1 through 8
       E. 1,3,5 and 7
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your customer needs to create an application to allow contractors to upload videos to Amazon Simple Storage Service (S3) so they can be transcoded into a different format. She creates AWS Identity and Access Management (IAM) users for her application developers, and in just one week, they have the application hosted on a fleet of Amazon Elastic Compute Cloud (EC2) instances. The attached IAM role is assigned to the instances. As expected, a contractor who authenticates to the application is given a pre-signed URL that points to the location for video upload. However, contractors are reporting that they cannot upload their videos. Which of the following are valid reasons for this behavior? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. The IAM role does not explicitly grant permission to upload the object
       B. The contractorsˈ accounts have not been granted “write” access to the S3 bucket.
       C. The application is not using valid security credentials to generate the pre-signed URL.
       D. The developers do not have access to upload objects to the S3 bucket
       E. The S3 bucket still has the associated default permissions
       F. The pre-signed URL has expired.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C F&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your company runs a complex customer relations management system that consists of around 10 different software components all backed by the same Amazon Relational Database Service (RDS) database. You adopted AWS OpsWorks to simplify management and deployment of that application and created an AWS OpsWorks stack with layers for each of the individual components. An internal security policy requires that all instances should run on the latest Amazon Linux AMI and that instances must be replaced within one month after the latest Amazon Linux AMI has been released. AMI replacements should be done without incurring application downtime or capacity problems. You decide to write a script to be run as soon as a new Amazon Linux AMI is released. Which solutions support the security policy and meet your requirements? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a new stack and layers with identical configuration, add instances with the latest Amazon Linux AMI specified as a custom AMI to the new layers, switch DNS to the new stack, and tear down the old stack
       B. Identify all Amazon Elastic Compute Cloud (EC2) instances of your AWS OpsWorks stack, stop each instance, replace the AMI ID property with the ID of the latest Amazon Linux AMI ID, and restart the instance. To avoid down time, make sure not more than one instance is stopped at the same time.
       C. Specify the latest Amazon Linux AMI as a custom AMI at the stack level, terminate instances of the stack and let AWS OpsWorks launch new instances with the new AMI.
       D. Add new instances with the latest Amazon Linux AMI specified as a custom AMI to all AWS OpsWorks layers of your stack, and terminate the old ones.
       E. Assign a custom recipe to each layer which replaces the underlying AMI. Use AWS OpsWorks life-cycle events to incrementally execute this custom recipe and update the instances with the new AMI.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A utility company is building an application that stores data coming from more than 10,000 sensors. Each sensor has a unique ID and will send a datapoint (approximately 1 KB) every 10 minutes throughout the day. Each datapoint contains the information coming from the sensor as well as a timestamp. This company would like to query information coming from a particular sensor for the past week very rapidly and would like to delete all data that is older thanfour weeks. Using Amazon DynamoDB for its scalability and rapidity, how would you implement this in the most cost-effective way?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. One table for each week, with a primary key that is the concatenation of the sensor ID and the timestamp
       B. One table for each week, with a primary key that is the sensor ID, and a hash key that is the timestamp
       C. One table, with a primary key that is the concatenation of the sensor ID and the timestamp
       D. One table, with a primary key that is the sensor ID, and a hash key that is the timestamp
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your company sells consumer devices and needs to record the first activation of all sold devices. Devices are not activated until the information is written on a persistent database. Activation data is very important for your company and must be analyzed daily with a MapReduce job. The execution time of the data analysis process must be less than three hours per day. Devices are usually sold evenly during the year, but when a new device model is out, there is a predictable peak in activations, that is, for a few days there are 10 times or even 100 times more activations than in the average day. Which of the following databases and analysis framework would you implement to better optimize costs and performance for this workload?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Amazon Relational Database Service and Amazon Elastic MapReduce with Spot Instances
       B. Amazon DynamoDB and Amazon Elastic MapReduce with Spot Instances
       C. Amazon Relational Database Service and Amazon Elastic MapReduce with Reserved Instances
       D. Amazon DynamoDB and Amazon Elastic MapReduce with Reserved Instances
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You are moving an existing traditional system to AWS, and during the migration discover that there is a master server which is a single point of failure. Having examined the implementation of the master server you realize there is not enough time during migration to re-engineer it to be highly available, though you do discover that it stores its state in a local MySQL database. In order to minimize down-time you select RDS to replace the local database and configure master to use it, what steps would best allow you to create aself-healing architecture:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Replicate the local database into a RDS Read Replica. Place the master node into a multi-AZ auto-scaling group with a minimum of one and a maximum of one with health checks.
       B. Migrate the local database into a multi-AZ RDS database. Place the master node into a Cross-Zone ELB with a minimum of one and a maximum of one with health checks.
       C. Replicate the local database into a RDS Read Replica. Place the master node into a Cross-Zone ELB with a minimum of one and a maximum of one with health checks.
       D. Migrate the local database into a multi-AZ RDS database. Place the master node into a multi-AZ auto-scaling group with a minimum of one and a maximum of one with health checks.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A customer is in the process of deploying multiple applications to AWS that are owned and operated by different development teams. Each development team maintains the authorization offits users independently from other teams. The customerˈs information security team would like to be able to delegate user authorization to the individual development teams but independently apply restrictions to the users permissions based on factors such as the userˈs device and location . For example, the information security team would like to grant read-only permissions to a user who is defined by the development team as read/write whenever the user is authenticating from outside the corporate network. What steps can the information security team take to implement this capability?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Operate an authentication service that generates AWS Security Token Service (STS) tokens with IAM policies from application-defined IAM roles.
       B. Add additional IAM policies to the application IAM roles that deny user privileges based on information security policy.
       C. Enable federation with the internal LDAP directory and grant the application teams permissions to modify users.
       D. Configure IAM policies that restrict modification of the application IAM roles only to the information security team.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You are designing a file-sharing service. This service will have millions of files in it. Revenue for the service will come from fees based on how much storage a user is using. You also want to store metadata on each file, such as title, description and whether the object is public or private. How do you achieve all of these goals in a way that is economical and can scale to millions of users?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Store all files in Amazon Simple Storage Service (S3). Create a bucket for each user. Store metadata in the filename of each object, and access it with LIST commands against the S3 API.
       B. Store all files in Amazon S3. Create Amazon DynamoDB tables for the corresponding key-value pairs on the associated metadata, when objects are uploaded.
       C. Create a striped set of 4000 IOPS Elastic Load Balancing volumes to store the data. Use a database running in Amazon Relational Database Service (RDS) to store the metadata.
       D. Create a striped set of 4000 IOPS Elastic Load Balancing volumes to store the data. Create Amazon DynamoDB tables for the corresponding key-value pairs on the associated metadata, when objects are uploaded.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your company has been contracted to develop and operate a website that tracks NBA basketball statistics. Statistical data to derive reports like “best game-winning shots from the regular season” and more frequently built reports like “top shots of the game” need to be stored durably for repeated lookup. Leveraging social media techniques, NBA fans submit and vote on new report types from the existing data set so the system needs to accommodate variability in data queries and new static reports must be generated and posted daily. Initial research in the design phase indicates that there will be over 3 million report queries on game day by end users and other applications that use this application as a data source. It is expected that this system will gain in popularity over time and reach peaks of 10-15 million report queries of the system on game days. Select the answer that will allow your application to best meet these requirements while minimizing costs.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Launch a multi-AZ MySQL Amazon Relational Database Service (RDS) Read Replica connected to your multi AZ master database and generate reports by querying the Read Replica. Perform a daily table cleanup.
       B. Generate reports from a multi-AZ MySQL Amazon RDS deployment and have an offline task put reports in Amazon Simple Storage Service (S3) and use CloudFront to cache the content. Use a TTL to expire objects daily.
       C. Implement a multi-AZ MySQL RDS deployment and have the application generate reports from Amazon ElastiCache for in-memory performance results. Utilize the default expire parameter for items in the cache.
       D. Query a multi-AZ MySQL RDS instance and store the results in a DynamoDB table. Generate reports from the DynamoDB table. Remove stale tables daily.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Youˈve been tasked with moving an ecommerce web application from a customerˈs datacenter into a VPC. The application must be fault tolerant and well as highly scalable. Moreover, the customer is adamant that service interruptions not affect the user experience. As you near launch, you discover that the application currently uses multicast to share session state between web servers. In order to handle session state within the VPC, you choose to:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Store session state in Amazon ElastiCache for Redis
       B. Enable session stickiness via Elastic Load Balancing
       C. Create a mesh VPN between instances and allow multicast on it.
       D. Store session state in Amazon Relational Database Service
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Your application is leveraging IAM Roles for EC2 for accessing objects stored in S3. Which two of the following IAM policies control access to your S3 objects?&lt;br&gt;
Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. An IAM trust policy allows the EC2 instance to assume an EC2 instance role
       B. An IAM access policy allows the EC2 role to access S3 objects
       C. An IAM bucket policy allows the EC2 role to access S3 objects
       D. An IAM trust policy allows applications running on the EC2 instance to assume an EC2 role
       E. An IAM trust policy allows applications running on the EC2 instance to access S3 objects
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You are designing Internet connectivity for your VPC. The Web servers must be available on the Internet. The application must have a highly available architecture. Which alternatives should you consider? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure ELB with an EIP. Place all your Web servers behind ELB. Configure a Route53 A record that points to the EIP.
       B. Place all your Web servers behind ELB. Configure a Route53 CNAME to point to the ELB DNS name.
       C. Configure a CloudFront distribution and configure the origin to point to the private IP addresses of your Web servers. Configure a Route53 CNAME record to your CloudFront distribution.
       D. Configure a NAT instance in your VPC. Create a default route via the NAT instance and associate it with all subnets. Configure a DNS A record that points to the NAT Instance public IP address.
       E. Assign EIPs to all Web servers. Configure a Route53 record set with all EIPs, with health checks and DNS failover.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B E&lt;/p&gt;

</description>
      <category>awslagi</category>
      <category>aws</category>
      <category>googlecloud</category>
    </item>
    <item>
      <title>Injecting pre-rendered widgets/content</title>
      <author>dave </author>
      <pubDate>Sun, 22 Aug 2021 14:03:16 +0000</pubDate>
      <link>https://dev.to/davethebrit/injecting-pre-rendered-widgets-content-2bjb</link>
      <guid>https://dev.to/davethebrit/injecting-pre-rendered-widgets-content-2bjb</guid>
      <description>&lt;p&gt;Hi! First time member and post. I had a question that I tried to find an answer for, but likely wasn't searching the right terms.&lt;/p&gt;

&lt;p&gt;We have two teams building React experiences. One who is building the core page, and another that owns widget experiences (think carousels that are componentized, have data and specific logic attached). Is there a concept within React of "injecting" these carousels into the body of a page? Does this impact SSR, performance or security?&lt;/p&gt;

&lt;p&gt;Many thanks in advance&lt;/p&gt;

</description>
      <category>react</category>
    </item>
    <item>
      <title>How to Master Vim</title>
      <author>PythonBasics</author>
      <pubDate>Sun, 22 Aug 2021 13:50:08 +0000</pubDate>
      <link>https://dev.to/basicspython/how-to-master-vim-4kp2</link>
      <guid>https://dev.to/basicspython/how-to-master-vim-4kp2</guid>
      <description>&lt;p&gt;Vim is a text editor that I use every day. It's a program that most people know about, but not many mastered.Here are some tips and tricks to get you started.&lt;/p&gt;

&lt;p&gt;Vim is part of the UNIX family of programs, which means that it's an old program with a lot of history and power. Vim is not just a text editor, it's a "programmer's editor". That means it has some pretty advanced features under the hood.&lt;/p&gt;

&lt;p&gt;First thing you need to do is get Vim installed on your computer, where I will assume that you are using Linux. If not, you can download it at &lt;a href="https://vim.org"&gt;vim.org&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;One thing to know about vim is that it's &lt;strong&gt;keyboard oriented&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Every function has a keyboard shotcut. For example:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;To quit without saving, press &lt;code&gt;ZQ&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;To save press &lt;code&gt;ESC&lt;/code&gt; &lt;code&gt;:w&lt;/code&gt;
&lt;/li&gt;
&lt;li&gt;To start typing press &lt;code&gt;i&lt;/code&gt;
&lt;/li&gt;
&lt;li&gt;To go back to option mode, press &lt;code&gt;ESC&lt;/code&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And so in order to master vim, you need to master the keyboard shortcuts. &lt;/p&gt;

&lt;p&gt;To start learning vim, this &lt;a href="https://vim.is/"&gt;tutorial&lt;/a&gt; is a good staring point. It takes you from beginner to master level.&lt;/p&gt;

&lt;p&gt;Alternatively, you can grab a cheat sheet and start editing text. But that can be a frustrating process, as you will need to learn the keyboard shortcuts while at the same time doing your work.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#why-vim"&gt;
  &lt;/a&gt;
  Why vim?
&lt;/h2&gt;

&lt;p&gt;As software developers, we often spend all day inside looking at computers and writing code. &lt;/p&gt;

&lt;p&gt;So many people like me start a terminal to type vim. The reason is that vim is a good tool. Users can edit files, run simple shell commands, compile programs&lt;/p&gt;

&lt;p&gt;What makes the tool good? I created some list of features that are very important for me:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Fullscreen&lt;/strong&gt; is the best way to focus. How to close windows, how to maximize them, how to split them - you don't need to think about how to use vim. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Keyboard control&lt;/strong&gt; because as a coder your hands are already on the keyboard. You can just do everything what you need with keyboard. It's similar to "Windows" keyboard shortcuts, but much more powerful.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Keyboard macros&lt;/strong&gt; easily record and replay keystrokes. This feature saves you a lot of time&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <category>vim</category>
      <category>linux</category>
    </item>
    <item>
      <title>Sharpen your Ruby: Part 1</title>
      <author>Eric The Coder</author>
      <pubDate>Sun, 22 Aug 2021 13:47:49 +0000</pubDate>
      <link>https://dev.to/ericchapman/sharpen-your-ruby-part-1-18f</link>
      <guid>https://dev.to/ericchapman/sharpen-your-ruby-part-1-18f</guid>
      <description>&lt;p&gt;&lt;a href="https://twitter.com/EricTheCoder_?ref_src=twsrc%5Etfw"&gt;Follow me on Twitter @EricTheCoder_&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;


&lt;p&gt;I develop in Javascript, Python, PHP, and Ruby. By far Ruby is my favorite programming language. Together let start a journey and revisit our Ruby foundations. &lt;/p&gt;

&lt;h2&gt;You want to sharpen your Ruby?&lt;/h2&gt; 

&lt;p&gt;In this series, we will start from the beginning and will discover every aspect of Ruby one step at a time. &lt;/p&gt;

&lt;p&gt;Each post will include some theory but also exercise and solution.&lt;/p&gt;

&lt;p&gt;If you have any questions/comments or you are new and need help, you can comment below or send me a message.&lt;/p&gt;

&lt;h2&gt;Run your Ruby code&lt;/h2&gt;

&lt;p&gt;No need to go through a complete install. Just go to this website &lt;a href="https://replit.com/languages/ruby"&gt;https://replit.com/languages/ruby&lt;/a&gt; and start learning right now. You will have plenty of time to figure out the Ruby installation on your local machine later on...&lt;/p&gt;

&lt;h1&gt;Ruby Variables&lt;/h1&gt;

&lt;p&gt;If you’re new to programming, variables are the fundamental building blocks of a programming language as they are used to store different values that you want to process in your code.&lt;/p&gt;

&lt;p&gt;Once the variable is store in program memory, it can be used later on.&lt;/p&gt;

&lt;p&gt;For example, let say you want to store the user name you can use a variable call name and set its value to Mike Taylor.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;span class="nb"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'Mike Taylor'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;In Ruby string is enclosed with quotation marks.&lt;/p&gt;

&lt;p&gt;The variable name we just created is a string variable. In Ruby, we don't have to specify the variable type. &lt;/p&gt;

&lt;p&gt;Ruby is a Just-in-time (JIT) interpreted language. Which automatically recognizes the data type based on what variables are stored.&lt;/p&gt;

&lt;p&gt;Here are some Ruby basic variables types and how to create them&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;span class="c1"&gt;# string&lt;/span&gt;
&lt;span class="n"&gt;full_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'Mike Taylor'&lt;/span&gt;

&lt;span class="c1"&gt;# integer number&lt;/span&gt;
&lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;

&lt;span class="c1"&gt;# float number&lt;/span&gt;
&lt;span class="n"&gt;book_price&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;15.80&lt;/span&gt;

&lt;span class="c1"&gt;# booleans&lt;/span&gt;
&lt;span class="n"&gt;active?&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;true&lt;/span&gt;
&lt;span class="n"&gt;admin_user?&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Ruby also has more advanced variables types like an array, hash, structure, and class. We will cover all of those in detail later.&lt;/p&gt;

&lt;h1&gt;Output&lt;/h1&gt;

&lt;p&gt;In Ruby, it is possible to output information to the console/terminal.&lt;/p&gt;

&lt;p&gt;For example, let's send our name variable to the console&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;span class="nb"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'Mike Taylor'&lt;/span&gt;
&lt;span class="nb"&gt;puts&lt;/span&gt; &lt;span class="nb"&gt;name&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;The puts method will take any value we give him and print it to the console...&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;Mike Taylor
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Others example&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;span class="nb"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'Mike Taylor'&lt;/span&gt;
&lt;span class="nb"&gt;puts&lt;/span&gt; &lt;span class="s1"&gt;'Hello World'&lt;/span&gt;
&lt;span class="nb"&gt;puts&lt;/span&gt; &lt;span class="s1"&gt;'Hello'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;name&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;Hello World
Hello
Mike Taylor
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;As you can see we can send multiple values to puts method and he will display all of them.&lt;/p&gt;

&lt;p&gt;Another Ruby method very similar to puts is the method print. Print can display something to the console but will not send the line break after each print. Example:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;span class="nb"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'Mike Taylor'&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;'Hello '&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;name&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;Hello Mike Taylor
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h1&gt;Input&lt;/h1&gt;

&lt;p&gt;How about getting info from the user. In Ruby we use the method gets to do just that&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;'Enter user name: '&lt;/span&gt;
&lt;span class="nb"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;gets&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;The console will then wait for user input:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;Enter user name: _
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;The gets method will return everything you type plus a line break characters. If you don't want to read the line break characters use the chomp method to remove that last character&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;'Enter user name: '&lt;/span&gt;
&lt;span class="nb"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;gets&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;chomp&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;Everything in Ruby is an object&lt;/h2&gt;

&lt;p&gt;The following concept will be a bit more advance. Beginners could have hard time to fully understand this concept. (and it is ok).&lt;/p&gt;

&lt;p&gt;In Ruby, everything is an object. Event types like integer, string, array are all objects. All Ruby objects inherit properties and methods from their parent object. &lt;/p&gt;

&lt;p&gt;Here an example&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;span class="nb"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'Mike'&lt;/span&gt;
&lt;span class="nb"&gt;puts&lt;/span&gt; &lt;span class="nb"&gt;name&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;upcase&lt;/span&gt; &lt;span class="c1"&gt;# MIKE&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;In this example, we use the method upcase to convert our string to upper case. How is that's possible? The name variable is a string and the string in Ruby is an object. All string objects have already many pre-build methods to help the developer.&lt;/p&gt;

&lt;p&gt;The .upcase method is not the only method provided by the string object. There are many more methods available. We will discover some of those in future posts.&lt;/p&gt;

&lt;p&gt;If you want to know all available methods for string or any other Ruby objects you can consult Ruby official documentation: &lt;a href="https://www.ruby-lang.org/en/documentation/"&gt;https://www.ruby-lang.org/en/documentation/&lt;/a&gt;  &lt;/p&gt;

&lt;p&gt;We will come back later to this concept, but just keep it in your mind: Everything in Ruby is an object.&lt;/p&gt;

&lt;h1&gt;Exercise&lt;/h1&gt;

&lt;p&gt;Create a little program that asks for the user name and age and save the result in the name and age variable. &lt;/p&gt;

&lt;p&gt;Then display name and age variable in the console&lt;/p&gt;

&lt;p&gt;Solution&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;'Enter user name: '&lt;/span&gt;
&lt;span class="nb"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;gets&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;chomp&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;'Enter user age: '&lt;/span&gt;
&lt;span class="n"&gt;age&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;gets&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;chomp&lt;/span&gt;

&lt;span class="nb"&gt;puts&lt;/span&gt; &lt;span class="s1"&gt;'The user name is: '&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;name&lt;/span&gt;
&lt;span class="nb"&gt;puts&lt;/span&gt; &lt;span class="s1"&gt;'The user age is: '&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;age&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;That's it for today. The journey just started, stay tuned for the next post very soon. &lt;/p&gt;

&lt;p&gt;If you have any comments or questions please do so here or send me a message on Twitter. &lt;/p&gt;

&lt;p&gt;Follow me: &lt;a href="https://twitter.com/EricTheCoder_?ref_src=twsrc%5Etfw"&gt;@EricTheCoder_&lt;/a&gt;&lt;/p&gt;

</description>
      <category>ruby</category>
      <category>rails</category>
      <category>beginners</category>
      <category>tutorial</category>
    </item>
    <item>
      <title>Meetings don't have to suck - Part 1</title>
      <author>Abhinav Pandey</author>
      <pubDate>Sun, 22 Aug 2021 13:42:08 +0000</pubDate>
      <link>https://dev.to/abh1navv/meetings-don-t-have-to-suck-part-1-4lh0</link>
      <guid>https://dev.to/abh1navv/meetings-don-t-have-to-suck-part-1-4lh0</guid>
      <description>&lt;p&gt;We have all been through this phase lately where the number of meetings being added to our calendars has increased. Some of us are still working remotely and having long tiring communications throughout the day. But this is not what meetings are supposed to be.&lt;/p&gt;

&lt;p&gt;Meetings are &lt;strong&gt;an opportunity to have effective communication&lt;/strong&gt; between team members. So where do things go wrong? &lt;/p&gt;

&lt;p&gt;Let's define how an ideal (almost utopian) process should look like to get the most out of our meetings without being mentally drained by them.&lt;/p&gt;

&lt;p&gt;This is Part 1 where we talk about the meeting invite. A good meeting invite is the corporate analogy of &lt;em&gt;well begun is half done&lt;/em&gt;.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#the-set-up"&gt;
  &lt;/a&gt;
  The Set Up
&lt;/h2&gt;

&lt;p&gt;The most common way of setting up meetings is to send an email which blocks time in the calendar and informs the invited people about it. We should take care of a few steps to make this email useful.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#the-objective"&gt;
  &lt;/a&gt;
  The objective
&lt;/h3&gt;

&lt;p&gt;The email must have an objective statement. &lt;br&gt;
What are we trying to achieve through this meeting? &lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--355M4tAp--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://c.tenor.com/Xgwu6c8wwwMAAAAM/meeting-yesterdays-meeting.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--355M4tAp--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://c.tenor.com/Xgwu6c8wwwMAAAAM/meeting-yesterdays-meeting.gif" alt="A meeting about yesterday's meeting"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A few examples of meeting objectives:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;An action plan for the day or to solve a problem.&lt;/li&gt;
&lt;li&gt;Brainstorming for the next feature.&lt;/li&gt;
&lt;li&gt;Backlog prioritization or sprint planning.&lt;/li&gt;
&lt;li&gt;Status updates (sucks the most, by unanimous decision).&lt;/li&gt;
&lt;li&gt;Knowledge transfer.&lt;/li&gt;
&lt;li&gt;Clarification calls, like a screen-sharing session.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When you put an objective statement in your email, it sets the tone for the invitees.  &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It makes them understand what you are trying to achieve and they will know what to bring to the table.&lt;/li&gt;
&lt;li&gt;They may point out that there could be other people you may want to include to achieve this objective.&lt;/li&gt;
&lt;li&gt;They may point out that there are people who could have been left out.(it isn't as bad as it sounds, this is the kind of being left out we need)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;
  &lt;a href="#the-agenda"&gt;
  &lt;/a&gt;
  The Agenda
&lt;/h3&gt;

&lt;p&gt;This is the most important part of the invitation. This is the essential detail which makes your meeting look purposeful and important.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--CJBB7B4z--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://media2.giphy.com/media/ekmhEyFhUhcPRLyUs5/giphy.webp%3Fcid%3D6c09b95289b085baeeee5cbf323dcdbf5701f786cecfb4e6%26rid%3Dgiphy.webp%26ct%3Dg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--CJBB7B4z--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://media2.giphy.com/media/ekmhEyFhUhcPRLyUs5/giphy.webp%3Fcid%3D6c09b95289b085baeeee5cbf323dcdbf5701f786cecfb4e6%26rid%3Dgiphy.webp%26ct%3Dg" alt="Agenda joke"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;What should it include?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The items to be discussed - with a one liner detail about them.&lt;/li&gt;
&lt;li&gt;The priority of the items - it may be ok to leave out low priority items at the end of the meeting for the sake of time or readiness. &lt;strong&gt;The agenda may not be met completely&lt;/strong&gt;
&lt;/li&gt;
&lt;li&gt;The type of each item - 

&lt;ul&gt;
&lt;li&gt;Is it an &lt;strong&gt;information&lt;/strong&gt; being handed out? &lt;/li&gt;
&lt;li&gt;Is it something which will require in depth &lt;strong&gt;discussion&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;Are we just going to &lt;strong&gt;define next steps&lt;/strong&gt; and assignees?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The time allotted for each item - this is quite an expert level process. It won't be easy to reach but it should be aimed for. If this is followed well, it means that most meetings cannot run out of time and do not miss out on agenda items.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Sending out an agenda will give the invitees time to think about each item and will set expectations from them. They will not come to the meeting with a blank mind expecting to be surprised. &lt;/p&gt;

&lt;p&gt;If you cannot define an agenda in written, you are not ready to occupy everyone's time yet.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#the-preparation"&gt;
  &lt;/a&gt;
  The preparation
&lt;/h3&gt;

&lt;p&gt;Apart from setting the context and the agenda, it is important to let people know if there are any preparations they need to do to be ready before the meeting. &lt;br&gt;
For e.g.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We are going to discuss "Feature X" which we have been working on and would like to include perspectives of a few more people who were not aware of "Feature X". To get yourself accustomed to the topic, please go through&lt;br&gt;
    - the attached documents&lt;br&gt;
    - the JIRA ticket&lt;br&gt;
    - the designs located at this URI on our cloud storage&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There could be more examples of this but this is the most common use case I go through regularly. This should give you an idea. Try to have as many people up to date with the knowledge required for your meeting.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#the-timing"&gt;
  &lt;/a&gt;
  The Timing
&lt;/h3&gt;

&lt;p&gt;Nobody likes sudden meetings. Everyone is busy with their work and is following a plan. Imagine you are playing your favorite sport right now, would you like to help me with an urgent Math problem? More importantly, it does not make them effective in the meeting if they had no time to go over the agenda and develop some feedback or ideas of their own. &lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--PLq93wT9--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://media0.giphy.com/media/QXPdeQwJYXv6wKXy2G/giphy.webp%3Fcid%3D6c09b95275a17b64135c30040b7ed57aae0d7208140e8df3%26rid%3Dgiphy.webp%26ct%3Dg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--PLq93wT9--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://media0.giphy.com/media/QXPdeQwJYXv6wKXy2G/giphy.webp%3Fcid%3D6c09b95275a17b64135c30040b7ed57aae0d7208140e8df3%26rid%3Dgiphy.webp%26ct%3Dg" alt="Everybody gets a meeting"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The least a team should do is plan one day ahead so that when we join in the morning, we are aware of our meeting schedule for the entire day and are able to plan the rest of our time effectively.&lt;/p&gt;

&lt;p&gt;If the situation requires to create an urgent meeting during the day, make sure you give the invitees enough time to think about the topic before joining the meeting.&lt;/p&gt;

&lt;p&gt;There are some other benefits of planning one day ahead:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;People may suggest a better time as per their availability.&lt;/li&gt;
&lt;li&gt;They get time to go through the preparations.&lt;/li&gt;
&lt;li&gt;They may invite other required people.&lt;/li&gt;
&lt;li&gt;They may suggest a change in agenda.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Remember.&lt;/p&gt;

&lt;p&gt;The participants contribute more in the time they get to prepare for the meeting compared to the duration of the meeting. &lt;/p&gt;

&lt;p&gt;Oh! I almost forgot about the duration. It will be proportional to the list of agenda items and the number of people involved. However, you have to consider the attention span of the people involved. &lt;/p&gt;

&lt;p&gt;It really depends on how interesting the meeting is but usually meetings from 30-60 minutes is what should solve problems effectively without being a burden on people. Cut down the agenda or break down work into smaller teams so that nobody has to sit through long durations. Meeting time (just like all time) is money.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--y_LbZA_1--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://encrypted-tbn0.gstatic.com/images%3Fq%3Dtbn:ANd9GcTo-gaRSkjv9fgWlEOrnUZWe5F1AxmLYDkP3w%26usqp%3DCAU" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--y_LbZA_1--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://encrypted-tbn0.gstatic.com/images%3Fq%3Dtbn:ANd9GcTo-gaRSkjv9fgWlEOrnUZWe5F1AxmLYDkP3w%26usqp%3DCAU" alt="Dangerous meeting"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Keep in mind the above points the next time you set up a meeting. Apart from this, as a meeting leader/organizer, you are expected to have a well thought of reason to have the meeting and are responsible for its success.&lt;/p&gt;

&lt;p&gt;In the next part, things get more interesting when we talk about the meeting process itself. &lt;/p&gt;

&lt;p&gt;Connect with me on &lt;a href="https://twitter.com/abh1navv"&gt;Twitter&lt;/a&gt; and/or subscribe to my weekly newsletter (attached to my twitter profile)&lt;/p&gt;

</description>
      <category>productivity</category>
      <category>leadership</category>
      <category>mentalhealth</category>
      <category>learning</category>
    </item>
    <item>
      <title>What is hooks in React JS?</title>
      <author>Sanajit Jana</author>
      <pubDate>Sun, 22 Aug 2021 13:14:46 +0000</pubDate>
      <link>https://dev.to/sanajitjana/what-is-hooks-in-react-js-41l2</link>
      <guid>https://dev.to/sanajitjana/what-is-hooks-in-react-js-41l2</guid>
      <description>&lt;p&gt;In this article, you will learn what are hooks in React JS? and when to use react hooks? React JS is developed by Facebook in the year 2013. There are many students and the new developers who have confusion between react and hooks in react. Well, it is not different, react is a programming language and hooks is a function that is used in react programming language.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#when-were-hooks-introduce"&gt;
  &lt;/a&gt;
  When were hooks introduce?
&lt;/h2&gt;

&lt;p&gt;React hooks are introduced in version 16.8 which is the recent Update in the react programming language. The concept of hooks has made the programming so easier for the react developers that everyone is now adapting the Hooks concept in their programming.&lt;/p&gt;

&lt;p&gt;React hooks are always utilized in a useState and other react features without writing a class. React hooks are the functions that hook into the react state life-cycle features from the function components.&lt;/p&gt;

&lt;p&gt;Like other features in react hooks does not work inside the classes it needs to be separated from the classes and used in the code. The hooks function must be utilized at the top of the react functions as it makes a clear vision of the program we want to execute in the project. You can use hooks without classes in the react programming and you can create your own hook to reuse the state full behaviour of different components in reactive programming.&lt;/p&gt;

&lt;p&gt;If we want to try any code related to hooks, we need to write it in the functional component itself. If we write out of the functional component it will show us an error (invalid hooks call on the web page) so to avoid that error make sure that you write the hooks in the functional component.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--iA6Y09q6--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/q4xcpnco3la87yil5lmi.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--iA6Y09q6--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/q4xcpnco3la87yil5lmi.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#what-are-the-requirements-to-use-react-hooks"&gt;
  &lt;/a&gt;
  What are the requirements to use react hooks?
&lt;/h2&gt;

&lt;p&gt;As we have mentioned that the react hooks are introduced in the version of 16.8. To make sure that the react hooks works properly. The developer should make sure that he or she is using the NODE version of 6 or above and the NPM version 5.2 or above. If the versions are below this criterion the react hooks will not work exactly the way you wish to see it. It is very important to know when to use react hooks.&lt;/p&gt;

&lt;p&gt;Let’s write a small program of Hooks function using a useState. We will be making a program of increasing numbers on the button click.&lt;/p&gt;

&lt;p&gt;When you add useState you will see that an extension will be added at the top of react. Let’s see react hooks example:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;React&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nx"&gt;useState&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;from&lt;/span&gt; &lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="nx"&gt;react&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nl"&gt;Syntax&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;count&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;setCount&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;useState&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;ButtonClick&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="nx"&gt;setCount&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;count&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Now in the HTML code, you have to write:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;h1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;count&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/h1&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;button&lt;/span&gt; &lt;span class="nx"&gt;onClick&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;ButtonClick&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;Click&lt;/span&gt; &lt;span class="nx"&gt;Me&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/button&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;So this is all about what are hooks in React JS? and react hooks example. If you have any questions you can ask in the comment Info At One always try our best to help you with it…&lt;/p&gt;

</description>
      <category>react</category>
      <category>reacthooks</category>
      <category>reactstate</category>
      <category>reactmap</category>
    </item>
  </channel>
</rss>
