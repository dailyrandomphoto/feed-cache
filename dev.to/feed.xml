<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>DEV Community</title>
    <author>DEV Community</author>
    <description>A constructive and inclusive social network for software developers. With you every step of your journey.</description>
    <link>https://dev.to</link>
    <language>en</language>
    <item>
      <title>Amazing Music Card Using HTML &amp; CSS</title>
      <author>Nikhil Bobade </author>
      <pubDate>Fri, 09 Jul 2021 16:05:29 +0000</pubDate>
      <link>https://dev.to/nikhil27b/amazing-music-card-using-html-css-2o1d</link>
      <guid>https://dev.to/nikhil27b/amazing-music-card-using-html-css-2o1d</guid>
      <description>&lt;p&gt;Hello Guys,&lt;/p&gt;

&lt;p&gt;Today I created a Amazing Music Card Using HTML &amp;amp; CSS. this simple music card using gradient colors and flex also I added font awesome icons to create the controls also added some box-shadow for image. I hope you like this also comments about your thoughts.&lt;/p&gt;


&lt;div class="ltag__link"&gt;
  &lt;a href="/nikhil27b" class="ltag__link__link"&gt;
    &lt;div class="ltag__link__pic"&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--L13Hplw---/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://res.cloudinary.com/practicaldev/image/fetch/s--DVAuMQ0j--/c_fill%2Cf_auto%2Cfl_progressive%2Ch_150%2Cq_auto%2Cw_150/https://dev-to-uploads.s3.amazonaws.com/uploads/user/profile_image/481802/ff897af1-c333-4e04-8f08-d6d1356a1c14.jpeg" alt="nikhil27b"&gt;
    &lt;/div&gt;
  &lt;/a&gt;
  &lt;a href="/nikhil27b/glassmorphism-sign-in-form-using-html-css-3a2a" class="ltag__link__link"&gt;
    &lt;div class="ltag__link__content"&gt;
      &lt;h2&gt;Glassmorphism Sign In Form Using HTML &amp;amp; CSS&lt;/h2&gt;
      &lt;h3&gt;Nikhil Bobade  ・ Jul 6 ・ 1 min read&lt;/h3&gt;
      &lt;div class="ltag__link__taglist"&gt;
        &lt;span class="ltag__link__tag"&gt;#html&lt;/span&gt;
        &lt;span class="ltag__link__tag"&gt;#css&lt;/span&gt;
        &lt;span class="ltag__link__tag"&gt;#webdev&lt;/span&gt;
        &lt;span class="ltag__link__tag"&gt;#beginners&lt;/span&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;p&gt;For more content follow me on Instagram  &lt;a href="https://www.instagram.com/developer_nikhil27/"&gt;@developer_nikhil27&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;

&lt;p&gt;&lt;iframe height="600" src="https://codepen.io/NikhilBobade/embed/WNjxqKQ?height=600&amp;amp;default-tab=result&amp;amp;embed-version=2"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

</description>
      <category>html</category>
      <category>css</category>
      <category>webdev</category>
      <category>beginners</category>
    </item>
    <item>
      <title>Amazon SSM Agent - Risk Of Security</title>
      <author>Vu Dao</author>
      <pubDate>Fri, 09 Jul 2021 15:50:53 +0000</pubDate>
      <link>https://dev.to/awscommunity-asean/amazon-ssm-agent-risk-of-security-4bij</link>
      <guid>https://dev.to/awscommunity-asean/amazon-ssm-agent-risk-of-security-4bij</guid>
      <description>&lt;h2&gt;
  &lt;a href="#table-of-contents"&gt;
  &lt;/a&gt;
  Table Of Contents
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#What-is-AWS-SSM-Agent?"&gt;What is AWS SSM Agent?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#Understand-Amazon-SSM-Agent-In-2-Minutes"&gt;Understand Amazon SSM Agent In 2 Minutes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#How-is-the-security-risk?"&gt;How is the security risk?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#Solution"&gt;Solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#-Conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;




&lt;h2&gt;
  &lt;a href="#what-is-aws-ssm-agent"&gt;
  &lt;/a&gt;
  🚀 &lt;strong&gt;What is AWS SSM Agent?&lt;/strong&gt; &lt;a&gt;&lt;/a&gt;
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html"&gt;AWS Systems Manager Agent&lt;/a&gt; (SSM Agent) is Amazon software that can be installed and configured on an EC2 instance, an on-premises server, or a virtual machine (VM). SSM Agent makes it possible for Systems Manager to update, manage, and configure these resources. The agent processes requests from the Systems Manager service in the AWS Cloud, and then runs them as specified in the request. SSM Agent then sends status and execution information back to the Systems Manager service by using the Amazon Message Delivery Service (service prefix: ec2messages).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#understand-amazon-ssm-agent-in-2-minutes"&gt;
  &lt;/a&gt;
  🚀 &lt;strong&gt;Understand Amazon SSM Agent In 2 Minutes&lt;/strong&gt;
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;In order to provide access to an EC2 through SSM (from console or AWS CLI), we need to install SSM agent on it (as default) and then provide IAM policy to the EC2 so that the SSM Agent service inside the EC2 has permission to get the EC2 information, SSM documents&lt;/li&gt;
&lt;li&gt;Reference to &lt;a href="https://dev.to/vumdao/understand-amazon-ssm-agent-in-2-minutes-1363"&gt;Understand Amazon SSM Agent In 2 Minutes&lt;/a&gt; for more detail&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#how-is-the-security-risk"&gt;
  &lt;/a&gt;
  🚀 &lt;strong&gt;How is the security risk?&lt;/strong&gt; &lt;a&gt;&lt;/a&gt;
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;We often attach following IAM policy to the EC2 to enable SSH access from Session Manager
&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AmazonSSMtoEC2",
            "Effect": "Allow",
            "Action": [
                "ssm:*",
                "ssmmessages:CreateControlChannel",
                "ssmmessages:CreateDataChannel",
                "ssmmessages:OpenControlChannel",
                "ssmmessages:OpenDataChannel",
                "ec2messages:AcknowledgeMessage",
                "ec2messages:DeleteMessage",
                "ec2messages:FailMessage",
                "ec2messages:GetEndpoint",
                "ec2messages:GetMessages",
                "ec2messages:SendReply"
            ],
            "Resource": "*"
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;With this policy, we are providing the SSM agent within the EC2 ability to access any EC2 instances that have SSM agent enabled. Eg.
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# Access the EC2
$ aws ssm start-session --target i-011ce869cbf141225 --region ap-northeast-1

Starting session with SessionId: dev-01fe8e68e8f5c70f5
$ sudo su
root@ec2-instance:/var/snap/amazon-ssm-agent/3553# 

# From this one we can install session manager plugin
root@ec2-instance:/var/snap/amazon-ssm-agent/3553# curl "https://s3.amazonaws.com/session-manager-downloads/plugin/latest/ubuntu_arm64/session-manager-plugin.deb" -o "session-manager-plugin.deb"
root@ec2-instance:/var/snap/amazon-ssm-agent/3553# dpkg -i session-manager-plugin.deb 

# And then access to anywhere
$ aws ssm start-session --target i-0df199f1ba0b1fc11 --region ap-southeast-1

Starting session with SessionId: i-011ce869cbf141225-0b8c635c96e4aa038
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;h2&gt;
  &lt;a href="#solution"&gt;
  &lt;/a&gt;
  🚀 Solution &lt;a&gt;&lt;/a&gt;
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Best practice of provide IAM policy is avoiding wildcard as much as possible
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": [
                "ssm:ListDocuments",
                "ssm:ListCommands",
                "ssm:ListCommandInvocations",
                "ssm:ListDocumentVersions",
                "ssm:DescribeDocument",
                "ssm:GetDocument",
                "ssm:DescribeInstanceInformation",
                "ssm:DescribeDocumentParameters",
                "ssm:DescribeInstanceProperties",
                "ssmmessages:CreateControlChannel",
                "ssmmessages:CreateDataChannel",
                "ssmmessages:OpenControlChannel",
                "ssmmessages:OpenDataChannel",
                "ec2messages:AcknowledgeMessage",
                "ec2messages:DeleteMessage",
                "ec2messages:FailMessage",
                "ec2messages:GetEndpoint",
                "ec2messages:GetMessages",
                "ec2messages:SendReply"
            ],
            "Resource": "*",
            "Effect": "Allow"
        },
        {
            "Sid": "AmazonSSMtoEC2",
            "Effect": "Allow",
            "Action": [
                "ssm:*"
            ],
            "Resource": "arn:aws:ec2:ap-northeast-1:123456789012:instance/i-011ce869cbf141225"
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;With the above policy we now restrict the EC2 to provide SSM itself and not able to acess others through SSM
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;$ aws ssm start-session --target i-0df199f1ba0b1fc11 --region ap-southeast-1

An error occurred (AccessDeniedException) when calling the StartSession operation: User: arn:aws:sts::123456789012:assumed-role/role-ssm/i-011ce869cbf141225 is not authorized to perform: ssm:StartSession on resource: arn:aws:ec2:ap-southeast-1:123456789012:instance/i-0df199f1ba0b1fc11
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;View the log here: /var/log/amazon/ssm/amazon-ssm-agent.log&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;
  &lt;a href="#conclusion"&gt;
  &lt;/a&gt;
  🚀 Conclusion &lt;a&gt;&lt;/a&gt;
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;We use SSM to provide access EC2 instnance or send commands without key pem, so please be careful with setup IAM permission to ensure security.&lt;/li&gt;
&lt;li&gt;More about SSM agent, &lt;a href="https://dev.to/awscommunity-asean/aws-ssm-agent-connection-error-3kn9"&gt;AWS SSM Agent - Connection Error&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;




&lt;div class="ltag__user ltag__user__id__512906"&gt;
  
    .ltag__user__id__512906 .follow-action-button {
      background-color: #000000 !important;
      color: #62df88 !important;
      border-color: #000000 !important;
    }
  
    &lt;a href="/vumdao" class="ltag__user__link profile-image-link"&gt;
      &lt;div class="ltag__user__pic"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--bGwkUMWT--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://res.cloudinary.com/practicaldev/image/fetch/s--ugeYdWM---/c_fill%2Cf_auto%2Cfl_progressive%2Ch_150%2Cq_auto%2Cw_150/https://dev-to-uploads.s3.amazonaws.com/uploads/user/profile_image/512906/2472752c-cfd5-4e02-b3d8-55b790178884.png" alt="vumdao image"&gt;
      &lt;/div&gt;
    &lt;/a&gt;
  &lt;div class="ltag__user__content"&gt;
    &lt;h2&gt;
&lt;a class="ltag__user__link" href="/vumdao"&gt;Vu Dao&lt;/a&gt;Follow
&lt;/h2&gt;
    &lt;div class="ltag__user__summary"&gt;
      &lt;a class="ltag__user__link" href="/vumdao"&gt;Awesome Devops || AWS SA || CloudOpz&lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--i3JOwpme--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-ba8488d21cd8ee1fee097b8410db9deaa41d0ca30b004c0c63de0a479114156f.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/vumdao"&gt;
        vumdao
      &lt;/a&gt; / &lt;a href="https://github.com/vumdao/vumdao"&gt;
        vumdao
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      
    &lt;/h3&gt;
  &lt;/div&gt;
&lt;/div&gt;


</description>
      <category>aws</category>
      <category>ssm</category>
      <category>cloudopz</category>
      <category>iam</category>
    </item>
    <item>
      <title>Vue Academy #2: V-model directive</title>
      <author>CodeOzz</author>
      <pubDate>Fri, 09 Jul 2021 15:25:06 +0000</pubDate>
      <link>https://dev.to/codeozz/vue-academy-2-v-model-directive-36oh</link>
      <guid>https://dev.to/codeozz/vue-academy-2-v-model-directive-36oh</guid>
      <description>&lt;p&gt;Welcome to the second vue academy ! It will be a list of lot of article on vue! I have 2.5 years of experience in this and I can teach a few thing about this !&lt;/p&gt;

&lt;p&gt;For this course we will focus on v-model directive !&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#lets-start"&gt;
  &lt;/a&gt;
  Let's start
&lt;/h3&gt;

&lt;p&gt;First problematic, how do we &lt;strong&gt;manage&lt;/strong&gt; an input value in &lt;code&gt;&amp;lt;input&amp;gt;&lt;/code&gt; ?&lt;/p&gt;

&lt;p&gt;We could do the next :&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;script&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="k"&gt;default&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="na"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;HelloWorld&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="na"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="na"&gt;message&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;''&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;
  &lt;span class="na"&gt;methods&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
     &lt;span class="nx"&gt;updateMessage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;event&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
         &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;message&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;event&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;target&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;
     &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/script&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;div&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;input&lt;/span&gt;
        &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;message&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;
        &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="nd"&gt;input&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;updateMessage&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;
        &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/div&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/template&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;


&lt;p&gt;We need to bind value of input to our current data &lt;code&gt;message&lt;/code&gt; and handle event from this input in order to update our current data &lt;code&gt;message&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;It's not really friendly and we have to do this for every  component...&lt;/p&gt;
&lt;h3&gt;
  &lt;a href="#vmodel"&gt;
  &lt;/a&gt;
  v-model
&lt;/h3&gt;

&lt;p&gt;You can use the v-model directive to create two-way data bindings on form input, textarea, and select elements. It automatically picks the correct way to update the element based on the input type.&lt;/p&gt;

&lt;p&gt;So we can replace the code above by&lt;br&gt;
&lt;/p&gt;
&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;script&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="k"&gt;default&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="na"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;HelloWorld&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="na"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="na"&gt;message&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;''&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/script&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;div&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;input&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;message&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/div&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/template&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;


&lt;p&gt;We can remove the method that update value ! Since v-model will directly update it.&lt;/p&gt;

&lt;p&gt;It's very useful ! &lt;/p&gt;


&lt;div class="ltag_gist-liquid-tag"&gt;
  
&lt;/div&gt;



</description>
      <category>vue</category>
      <category>javascript</category>
      <category>webdev</category>
      <category>typescript</category>
    </item>
    <item>
      <title>JavaScript Bundlers: An in-depth comparative 👍👎 Is Webpack still the best bundler in 2021? 📦</title>
      <author>_CODE</author>
      <pubDate>Fri, 09 Jul 2021 15:15:54 +0000</pubDate>
      <link>https://dev.to/underscorecode/javascript-bundlers-an-in-depth-comparative-is-webpack-still-the-best-bundler-in-2021-59jk</link>
      <guid>https://dev.to/underscorecode/javascript-bundlers-an-in-depth-comparative-is-webpack-still-the-best-bundler-in-2021-59jk</guid>
      <description>&lt;p&gt;Hello, everybody! 🚀&lt;/p&gt;

&lt;p&gt;For the last few days, I've been doing some research on the currently available &lt;strong&gt;JavaScript bundlers&lt;/strong&gt; to try to draw my own conclusions about them and figure out which one would be more appropriate for my projects. And, of course, to find out if it's all about popularity and we developers are overrating some of them and underrating the others 😇&lt;/p&gt;

&lt;p&gt;Since the only bundler I've been working with for the last few years is &lt;strong&gt;Webpack&lt;/strong&gt;, I decided to take a look at &lt;strong&gt;npm trends&lt;/strong&gt; to find out &lt;strong&gt;which the most popular JS bundlers are in 2021&lt;/strong&gt; and give them a try.&lt;/p&gt;

&lt;p&gt;And this is what I got: &lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--l79Te5lD--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9ispe7ysyot47gsnid67.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--l79Te5lD--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9ispe7ysyot47gsnid67.png" alt="Comparative chart from npm-trends showing the top 5 JS bundlers"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So today, we'll be making a comparison between the &lt;strong&gt;5 most popular bundlers&lt;/strong&gt; according to &lt;strong&gt;&lt;em&gt;npm trends&lt;/em&gt;&lt;/strong&gt;: Webpack, Rollup, Browserify, ESbuild and Parcel.&lt;/p&gt;

&lt;p&gt;In this comparative, we will create a &lt;strong&gt;really basic scenario&lt;/strong&gt; for each of them with a couple of the most used resources/tools these days, and we'll be talking about their &lt;strong&gt;pros and cons&lt;/strong&gt; and comparing them all based on &lt;strong&gt;a few parameters&lt;/strong&gt;.&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#first-things-first-what-is-a-bundler"&gt;
  &lt;/a&gt;
  First things first. What is a &lt;strong&gt;&lt;em&gt;bundler&lt;/em&gt;&lt;/strong&gt;? 🤔
&lt;/h1&gt;

&lt;p&gt;A &lt;strong&gt;bundler&lt;/strong&gt; is a tool that &lt;strong&gt;puts together all your JavaScript code and its dependencies&lt;/strong&gt; and throws a &lt;strong&gt;new JavaScript output file&lt;/strong&gt; with everything merged, ready for the web, commonly known as &lt;em&gt;the bundle file&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;These &lt;em&gt;bundlers&lt;/em&gt; can work with &lt;strong&gt;other types of files&lt;/strong&gt; as well apart from JavaScript, but &lt;strong&gt;&lt;em&gt;they need a little help&lt;/em&gt;&lt;/strong&gt; to perform their &lt;em&gt;bundles&lt;/em&gt;. We'll talk about this more in depth in each of the examples below.&lt;/p&gt;

&lt;p&gt;None of them require a &lt;em&gt;config&lt;/em&gt; file, what perfectly works for the most basic bundle. This means you have a &lt;em&gt;.js&lt;/em&gt; file converted into another &lt;em&gt;.js&lt;/em&gt; file with minimal setup. But, once you start having &lt;strong&gt;more and more kinds of files that need to be transpiled&lt;/strong&gt; and, consequently, configured, it's time to add a &lt;strong&gt;&lt;em&gt;config&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;file&lt;/strong&gt; because, otherwise, you'll find yourself immersed in &lt;strong&gt;chaos&lt;/strong&gt; 😰&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#the-scenario-%F0%9F%96%A5"&gt;
  &lt;/a&gt;
  The scenario 🖥
&lt;/h1&gt;

&lt;p&gt;To try out these bundlers, &lt;strong&gt;we don't need a specific complex structure&lt;/strong&gt; for our project, so let's propose a really basic scenario: &lt;strong&gt;an HTML file&lt;/strong&gt;, with &lt;strong&gt;some styles&lt;/strong&gt; (we'll slightly complicate it by using a &lt;strong&gt;preprocessor&lt;/strong&gt; like SASS) and &lt;strong&gt;ready to use ES6&lt;/strong&gt;, which means we will include &lt;em&gt;Babel&lt;/em&gt; even though we're not using React, Vue or any library/framework that rely on it in this comparative. But let's get it setup anyway.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;/dist
   bundle.js
   [styles.css]
/src
   index.js
/styles
   styles.scss
index.html
package.json
[*.config.js]
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;strong&gt;/dist&lt;/strong&gt; will be &lt;strong&gt;the folder created after the bundle process&lt;/strong&gt; and will &lt;strong&gt;contain all the bundled files&lt;/strong&gt;. The bundled file for the styles is &lt;strong&gt;optional&lt;/strong&gt; because we can choose either to inject the styles directly in the HTML or generate a new &lt;em&gt;transpiled&lt;/em&gt; file containing the styles.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;/src&lt;/strong&gt; is the folder containing the &lt;strong&gt;entry point&lt;/strong&gt; from which the bundler will &lt;strong&gt;start the bundle process&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;/styles&lt;/strong&gt; is the folder containing the &lt;strong&gt;original styles file&lt;/strong&gt;, before the bundle.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;index.html&lt;/strong&gt; is the file containing what we'll see in the &lt;strong&gt;browser&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;package.json&lt;/strong&gt; is the file where all the &lt;strong&gt;dependencies&lt;/strong&gt;, &lt;strong&gt;scripts&lt;/strong&gt; and &lt;strong&gt;some configurations&lt;/strong&gt; are stored.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;*.config.js&lt;/strong&gt; is the file where all the &lt;strong&gt;config for the bundler&lt;/strong&gt; is defined. This file is &lt;strong&gt;optional&lt;/strong&gt; for every bundler in this list, &lt;strong&gt;but highly recommended&lt;/strong&gt;. * will be replaced accordingly by the name of the bundler.&lt;/p&gt;




&lt;p&gt;Having said all this, let's see what each of these 5 bundlers can offer us.&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#1-webpack"&gt;
  &lt;/a&gt;
  1. Webpack
&lt;/h1&gt;

&lt;p&gt;Loved by many, hated by some, known to all. And still &lt;strong&gt;the most popular bundler in 2021&lt;/strong&gt;. With &lt;strong&gt;more than 15 million weekly downloads&lt;/strong&gt; (at the time of writing this post), there's no doubt that &lt;strong&gt;Webpack is still the bundler&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;par excellence&lt;/em&gt;&lt;/strong&gt; in 2021. But, is it the easiest to use, configure and understand how it works?&lt;/p&gt;

&lt;p&gt;Let's have a look at how we should configure it to have it ready to work.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#approach-used-by-webpack"&gt;
  &lt;/a&gt;
  Approach used by Webpack
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;em&gt;build&lt;/em&gt; script&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;config&lt;/em&gt; file&lt;/li&gt;
&lt;li&gt;
&lt;em&gt;Loaders&lt;/em&gt; used to &lt;em&gt;transform&lt;/em&gt; files&lt;/li&gt;
&lt;li&gt;
&lt;em&gt;Plugins&lt;/em&gt; for more complex stuff&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;package.json&lt;/code&gt;&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight json"&gt;&lt;code&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="nl"&gt;"scripts"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
   &lt;/span&gt;&lt;span class="nl"&gt;"build"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"rm -rf dist &amp;amp;&amp;amp; webpack --mode development"&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Really really easy. There's no need to do anything else for a basic configuration. Actually, if you don't want to use a different name for your configuration file, you don't even need to specify a configuration in the build script. If you want to use a different one, you should add &lt;em&gt;--config your_config_file.js&lt;/em&gt; to the command.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note that we'll be adding the command &lt;em&gt;rm -rf dist&lt;/em&gt; to every build of every bundler. What this does is removing the &lt;em&gt;dist&lt;/em&gt; folder every time a new &lt;em&gt;build&lt;/em&gt; script is executed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;webpack.config.js&lt;/code&gt;&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;exports&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
   &lt;span class="na"&gt;entry&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;./src/index.js&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
   &lt;span class="na"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="na"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;bundle.js&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="na"&gt;path&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;path&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;resolve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;dist&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="p"&gt;},&lt;/span&gt;
   &lt;span class="na"&gt;module&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="na"&gt;rules&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
         &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="na"&gt;test&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="sr"&gt;/&lt;/span&gt;&lt;span class="se"&gt;\.(&lt;/span&gt;&lt;span class="sr"&gt;js|jsx&lt;/span&gt;&lt;span class="se"&gt;)&lt;/span&gt;&lt;span class="sr"&gt;$/&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="na"&gt;exclude&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;/node-modules/&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="na"&gt;use&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;babel-loader&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;

         &lt;span class="p"&gt;},&lt;/span&gt;
         &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="na"&gt;test&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="sr"&gt;/&lt;/span&gt;&lt;span class="se"&gt;\.&lt;/span&gt;&lt;span class="sr"&gt;html$/&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="na"&gt;use&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;html-loader&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;
         &lt;span class="p"&gt;},&lt;/span&gt;
         &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="na"&gt;test&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="sr"&gt;/&lt;/span&gt;&lt;span class="se"&gt;\.(&lt;/span&gt;&lt;span class="sr"&gt;scss|sass&lt;/span&gt;&lt;span class="se"&gt;)&lt;/span&gt;&lt;span class="sr"&gt;$/&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="na"&gt;use&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;style-loader&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;css-loader&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;sass-loader&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
         &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;]&lt;/span&gt;
   &lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Maybe a little bit more tricky and difficult to understand at first than the other bundlers, but really &lt;strong&gt;easy once you get the sense of how everything works together&lt;/strong&gt;.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#what-is-happening-in-this-config-file"&gt;
  &lt;/a&gt;
  What is happening in this &lt;em&gt;config&lt;/em&gt; file? 🙃
&lt;/h2&gt;

&lt;p&gt;Well, first, we need an &lt;strong&gt;entry point&lt;/strong&gt; for our bundler to start merging everything. That is specified in the &lt;em&gt;entry&lt;/em&gt; attribute and the file will be our file &lt;em&gt;index.js&lt;/em&gt; in the folder &lt;em&gt;src&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Same for the &lt;strong&gt;output file&lt;/strong&gt;, we'll tell Webpack our file will be called &lt;em&gt;bundle.js&lt;/em&gt; and it should be stored in the folder &lt;em&gt;dist&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;And now, what's only left is to &lt;strong&gt;handle the files that are not JavaScript (ES5)&lt;/strong&gt;. Webpack takes care of these files using &lt;strong&gt;loaders&lt;/strong&gt;. To &lt;em&gt;transform&lt;/em&gt; these files, we just need to indicate the file format and which loader(s) will deal with them.&lt;/p&gt;

&lt;p&gt;So that's what we need: a few loaders to take care of our styles, our HTML and our JS (ES6 - remember that we're getting it ready for formats like &lt;em&gt;.jsx&lt;/em&gt;): &lt;code&gt;style-loader&lt;/code&gt;, &lt;code&gt;css-loader&lt;/code&gt; and &lt;code&gt;sass-loader&lt;/code&gt; for the styles, &lt;code&gt;html-loader&lt;/code&gt; for the HTML files and &lt;code&gt;babel-loader&lt;/code&gt; for ES6.&lt;/p&gt;

&lt;p&gt;Notice that we're also &lt;em&gt;transforming&lt;/em&gt; the HTML file (this loader will be useful if we want to add resources that are loaded directly in the HTML file, such as images). This loader is really useful in bigger projects, but not necessary in this case (due to its simple structure), we'll skip this step for the rest of the bundlers.&lt;/p&gt;

&lt;p&gt;And this is it. Everything will be bundled once we run the &lt;em&gt;build&lt;/em&gt; command.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#about-the-bundle"&gt;
  &lt;/a&gt;
  About the bundle
&lt;/h2&gt;

&lt;p&gt;Since we're using &lt;code&gt;style-loader&lt;/code&gt; to bundle the styles, instead of a plugin to minify CSS and generate a new file (&lt;code&gt;MiniCSSExtractPlugin&lt;/code&gt;), the styles are injected into the HTML file inside a &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag, so the only output file is &lt;code&gt;bundle.js&lt;/code&gt;, which needs to be added to &lt;code&gt;index.html&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#what-do-i-think-about-webpack"&gt;
  &lt;/a&gt;
  What do I think about Webpack? 👇
&lt;/h2&gt;

&lt;p&gt;I have to admit that first time I had to face Webpack I thought the configuration would be impossible. It was my first time using a bundler and I was barely able to understand the overall concept. Let alone all the loaders and more complex related stuff because it was a bigger project.&lt;/p&gt;

&lt;p&gt;But after a few from-scratch configurations on my part, I have to say that now &lt;strong&gt;I find it more intuitive and easier to set up&lt;/strong&gt; if I compare it to what it felt like to get to know the rest of them.&lt;/p&gt;

&lt;p&gt;Let's take a look at the others and you'll understand why! &lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#2-rollup"&gt;
  &lt;/a&gt;
  2. Rollup
&lt;/h1&gt;

&lt;p&gt;Let's now turn our attention to &lt;strong&gt;Rollup&lt;/strong&gt;. As well as the rest of the loaders, this has been my first time trying it out, so I'll also provide my first impressions about it 🤓&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#approach-used-by-rollup"&gt;
  &lt;/a&gt;
  Approach used by Rollup
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;em&gt;build&lt;/em&gt; command.&lt;/li&gt;
&lt;li&gt;An &lt;em&gt;optional&lt;/em&gt; &lt;em&gt;config&lt;/em&gt; file.&lt;/li&gt;
&lt;li&gt;
&lt;em&gt;Plugins&lt;/em&gt; used to &lt;em&gt;transform&lt;/em&gt; files&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;package.json&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Basic bundle with no &lt;em&gt;config&lt;/em&gt; file:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight json"&gt;&lt;code&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="nl"&gt;"scripts"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
   &lt;/span&gt;&lt;span class="nl"&gt;"build"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"rm -rf dist &amp;amp;&amp;amp; rollup src/index.js --file dist/bundle.js"&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Using a &lt;em&gt;config&lt;/em&gt; file:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight json"&gt;&lt;code&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="nl"&gt;"scripts"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
   &lt;/span&gt;&lt;span class="nl"&gt;"build"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"rm -rf dist &amp;amp;&amp;amp; rollup -c"&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;A really easy command for building, as well, so nothing else to point out here.&lt;/p&gt;

&lt;p&gt;Let's now check the &lt;em&gt;config&lt;/em&gt; file, that is &lt;strong&gt;optional&lt;/strong&gt; but &lt;strong&gt;recommended&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rollup.config.js&lt;/code&gt;&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;babel&lt;/span&gt; &lt;span class="k"&gt;from&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;@rollup/plugin-babel&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;scss&lt;/span&gt; &lt;span class="k"&gt;from&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;rollup-plugin-scss&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="k"&gt;default&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
   &lt;span class="na"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;./src/index.js&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
   &lt;span class="na"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="na"&gt;file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;./dist/bundle.js&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="na"&gt;format&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;cjs&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
   &lt;span class="p"&gt;},&lt;/span&gt;
   &lt;span class="na"&gt;plugins&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
      &lt;span class="nx"&gt;babel&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt; &lt;span class="na"&gt;exclude&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;node_modules/**&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt; &lt;span class="p"&gt;}),&lt;/span&gt;
      &lt;span class="nx"&gt;scss&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt; &lt;span class="na"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;styles.css&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt; &lt;span class="p"&gt;}),&lt;/span&gt;
   &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;What Webpack defines as &lt;em&gt;loaders&lt;/em&gt;, here in Rollup are called just &lt;em&gt;plugins&lt;/em&gt;. This time we just need a couple of them: the one for transpiling ES6 into ES5 (Babel) and the one for SCSS: &lt;code&gt;@rollup/plugin-babel&lt;/code&gt; and &lt;code&gt;rollup-plugin-scss&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;These plugins have also &lt;strong&gt;their own optional configuration&lt;/strong&gt;. In this case, for Babel, we're excluding the folder &lt;em&gt;node_modules&lt;/em&gt; and for SCSS we're giving the output file a different name. Otherwise, it will remain &lt;em&gt;output.css&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;For configurations with &lt;strong&gt;plain CSS&lt;/strong&gt;, there's a plugin called &lt;code&gt;rollup-plugin-css-only&lt;/code&gt; that works in the exact same way as the plugin we're using for SCSS.&lt;/p&gt;

&lt;p&gt;Note that we need to specify the entry and the output points exactly as we did before with Webpack. &lt;/p&gt;

&lt;p&gt;And that would be it.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#about-the-bundle"&gt;
  &lt;/a&gt;
  About the bundle
&lt;/h2&gt;

&lt;p&gt;The Rollup bundle comprises two files: &lt;code&gt;bundle.js&lt;/code&gt; and &lt;code&gt;styles.css&lt;/code&gt;. It's necessary to import the original styles files in the entry point &lt;code&gt;index.js&lt;/code&gt; for the bundler to be able to find the file (there's no other place where we can reference it).&lt;/p&gt;

&lt;p&gt;Also both &lt;em&gt;bundles&lt;/em&gt; need to be added to the HTML index file.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#my-first-impressions-about-rollup"&gt;
  &lt;/a&gt;
  My first impressions about Rollup 👇
&lt;/h2&gt;

&lt;p&gt;To be honest, I wasn't expecting much of these other &lt;em&gt;easier slash light-weight&lt;/em&gt; bundlers since Webpack has always worked for me, and I have to say that Rollup has surprised me in a good way.&lt;/p&gt;

&lt;p&gt;I find it &lt;strong&gt;pretty similar with Webpack&lt;/strong&gt; (&lt;em&gt;config&lt;/em&gt; file with almost the same structure, &lt;em&gt;plugins&lt;/em&gt; work in the same way as &lt;em&gt;loaders&lt;/em&gt; to translate &lt;em&gt;no-js&lt;/em&gt; files, the easy build command...), which means familiarity, usage recall and, consequently,  ease of use. &lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;only drawback&lt;/strong&gt; I've been able to find so far is &lt;strong&gt;the large number of dependencies it relies on&lt;/strong&gt;, and consequently, the &lt;strong&gt;huge size&lt;/strong&gt; of the project (3x a project bundled with Webpack). We'll be focusing on this more in depth at the end of the post 🔜&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#3-browserify"&gt;
  &lt;/a&gt;
  3. Browserify
&lt;/h1&gt;

&lt;p&gt;Let's now talk about &lt;strong&gt;Browserify&lt;/strong&gt;. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#approach-used-by-browserify"&gt;
  &lt;/a&gt;
  Approach used by Browserify
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;No &lt;em&gt;config&lt;/em&gt; file&lt;/li&gt;
&lt;li&gt;
&lt;em&gt;Transforms&lt;/em&gt; used to &lt;em&gt;transform&lt;/em&gt; files&lt;/li&gt;
&lt;li&gt;Everything you need to configure -&amp;gt; &lt;code&gt;package.json&lt;/code&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The method used by Browserify has nothing to do with the &lt;em&gt;traditional&lt;/em&gt; approach of a &lt;em&gt;build&lt;/em&gt; command and a &lt;em&gt;config&lt;/em&gt; file. With this bundler, &lt;strong&gt;every possible configuration&lt;/strong&gt; is allocated in &lt;code&gt;package.json&lt;/code&gt; and &lt;strong&gt;the build command can get a little bit tedious&lt;/strong&gt; if we don't have the concepts clear.&lt;/p&gt;

&lt;p&gt;It also needs &lt;strong&gt;plugins&lt;/strong&gt; (or &lt;em&gt;transforms&lt;/em&gt;, as they are also called) to &lt;em&gt;transform&lt;/em&gt; everything into something &lt;em&gt;readable&lt;/em&gt; by the browser.&lt;/p&gt;

&lt;p&gt;Let's have a glance at how we can configure it:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;package.json&lt;/code&gt;&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight json"&gt;&lt;code&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="nl"&gt;"scripts"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
   &lt;/span&gt;&lt;span class="nl"&gt;"build"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"rm -rf dist &amp;amp;&amp;amp; browserify -o dist/bundle.js src/index.js"&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;A very basic use of this bundler could be something like the code snippet above. We only have defined the input and output files (no configuration for styles or anything more complex).&lt;/p&gt;

&lt;p&gt;Note the &lt;strong&gt;length of the build command&lt;/strong&gt; having only declared the input source and the output.&lt;/p&gt;

&lt;p&gt;Let me show you how it would look like if we add the suitable plugin for handling plain CSS.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight json"&gt;&lt;code&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="nl"&gt;"scripts"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
   &lt;/span&gt;&lt;span class="nl"&gt;"build"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"rm -rf dist &amp;amp;&amp;amp; browserify -t [browserify-css --output dist/styles.css] -o dist/bundle.js src/index.js"&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Then, if we wanted to add some configuration to the plugin, we would do something like the following down below in the same file:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight json"&gt;&lt;code&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="nl"&gt;"browserify"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
   &lt;/span&gt;&lt;span class="nl"&gt;"browserify-css"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="nl"&gt;"autoInject"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="nl"&gt;"minify"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="nl"&gt;"rootDir"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"."&lt;/span&gt;&lt;span class="w"&gt;
   &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;It starts getting not &lt;em&gt;that maintainable&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;And now, let's complicate it a little bit more by adding plugins for SCSS and Babel. We need a couple of &lt;em&gt;plugins&lt;/em&gt; called &lt;code&gt;Babelify&lt;/code&gt; and &lt;code&gt;scssify&lt;/code&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;h3&gt;
  &lt;a href="#something-to-take-into-account"&gt;
  &lt;/a&gt;
  Something to take into account
&lt;/h3&gt;

&lt;p&gt;I've been trying out this bundler with &lt;strong&gt;the last released version of Node&lt;/strong&gt; (v16.4.2) and the command line throws multiple errors when trying to install any dependencies that rely on &lt;code&gt;node-sass&lt;/code&gt; (&lt;code&gt;scssify&lt;/code&gt; and &lt;code&gt;sassify&lt;/code&gt;, more specifically). A very negative point.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We could do this in two different ways: &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;By saturating the build script with more content 😅&lt;/li&gt;
&lt;li&gt;By adding a &lt;em&gt;transform&lt;/em&gt; property&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#via-the-build-script"&gt;
  &lt;/a&gt;
  Via the build script
&lt;/h3&gt;

&lt;p&gt;For specifying several &lt;em&gt;transforms&lt;/em&gt; in the &lt;em&gt;build&lt;/em&gt; script using Browserify, we should add as many as &lt;em&gt;-t&lt;/em&gt; [ &lt;em&gt;transform options&lt;/em&gt; ] as needed, like this:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight json"&gt;&lt;code&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="nl"&gt;"scripts"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
   &lt;/span&gt;&lt;span class="nl"&gt;"build"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"rm -rf dist &amp;amp;&amp;amp; browserify -t [ scssify --output dist/styles.css ] -t [ babelify --presets [ @babel/preset-env ] ] -o dist/bundle.js src/index.js"&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;

&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;If you're using this method, pay close attention to the &lt;strong&gt;white spaces&lt;/strong&gt; inside the arrays. &lt;strong&gt;They matter&lt;/strong&gt; ✌️&lt;/p&gt;

&lt;p&gt;I find this method &lt;strong&gt;tedious&lt;/strong&gt; and &lt;strong&gt;difficult to understand&lt;/strong&gt;, and above all, &lt;strong&gt;difficult to maintain&lt;/strong&gt;. And we're only using two plugins. All said.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#via-the-transform-property"&gt;
  &lt;/a&gt;
  Via the transform property
&lt;/h3&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight json"&gt;&lt;code&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="nl"&gt;"browserify"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
    &lt;/span&gt;&lt;span class="nl"&gt;"transform"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"babelify"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
            &lt;/span&gt;&lt;span class="nl"&gt;"presets"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="w"&gt;
               &lt;/span&gt;&lt;span class="s2"&gt;"@babel/preset-env"&lt;/span&gt;&lt;span class="w"&gt;
            &lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt;
        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"scssify"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;"autoInject"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt;
    &lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt;
  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;By using this method, the &lt;em&gt;build&lt;/em&gt; script will look like it was originally, when it just performed the simple bundle of the input js file:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight json"&gt;&lt;code&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="nl"&gt;"scripts"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
   &lt;/span&gt;&lt;span class="nl"&gt;"build"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"rm -rf dist &amp;amp;&amp;amp; browserify -o dist/bundle.js src/index.js"&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Much better 😊&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#about-the-bundle"&gt;
  &lt;/a&gt;
  About the bundle
&lt;/h2&gt;

&lt;p&gt;The Browserify bundle consists of the &lt;code&gt;bundle.js&lt;/code&gt; file and, only &lt;strong&gt;if we set an output file for the styles&lt;/strong&gt; in the plugin that takes care of them, &lt;strong&gt;we'll get a&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;styles.css&lt;/em&gt;&lt;/strong&gt; file. &lt;strong&gt;Otherwise&lt;/strong&gt;, the styles will be injected at the bottom of the &lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt; element in the HTML file &lt;strong&gt;inside a &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; element&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Take a look at this two different config examples for &lt;code&gt;browserify-css&lt;/code&gt;:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight json"&gt;&lt;code&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"browserify-css"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="nl"&gt;"autoInject"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="nl"&gt;"minify"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="nl"&gt;"rootDir"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"."&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="nl"&gt;"output"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"dist/styles.css"&lt;/span&gt;&lt;span class="w"&gt;
   &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;This config above will create a separate &lt;em&gt;.css&lt;/em&gt; file.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight json"&gt;&lt;code&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"browserify-css"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="nl"&gt;"autoInject"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="nl"&gt;"minify"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="nl"&gt;"rootDir"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"."&lt;/span&gt;&lt;span class="w"&gt;
   &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;And this other config will inject the code into a &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag in the head of &lt;code&gt;index.html&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#my-first-impressions-about-browserify"&gt;
  &lt;/a&gt;
  My first impressions about Browserify 👇
&lt;/h2&gt;

&lt;p&gt;My less favorite so far. &lt;strong&gt;I don't find it&lt;/strong&gt; as &lt;strong&gt;intuitive&lt;/strong&gt; as the other two, and the &lt;strong&gt;approach&lt;/strong&gt; it uses is totally &lt;strong&gt;different&lt;/strong&gt; from what we are &lt;em&gt;regularly&lt;/em&gt; used to. Also, I think the &lt;strong&gt;configuration is more tedious&lt;/strong&gt; if at first you don't know how and where to handle the required plugins.&lt;/p&gt;

&lt;p&gt;Also, &lt;strong&gt;blank spaces matter&lt;/strong&gt;, and if you don't know that beforehand, you can perfectly spend 2 hours trying to figure out what's wrong with your code 👎&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#4-esbuild"&gt;
  &lt;/a&gt;
  4. ESBuild
&lt;/h1&gt;

&lt;p&gt;Time to talk about &lt;strong&gt;ESBuild&lt;/strong&gt;.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#approach-used-by-esbuild"&gt;
  &lt;/a&gt;
  Approach used by ESBuild
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;em&gt;build&lt;/em&gt; command (encourages the use of the terminal)&lt;/li&gt;
&lt;li&gt;An &lt;em&gt;optional&lt;/em&gt; &lt;em&gt;config&lt;/em&gt; file&lt;/li&gt;
&lt;li&gt;
&lt;em&gt;Plugins&lt;/em&gt; used to transform files&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With ESBuild you can use &lt;strong&gt;either the command line or a&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;config&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;file&lt;/strong&gt; as well as the others, even for more complex configurations. It's totally up to you, but specifying a &lt;em&gt;config&lt;/em&gt; file is always recommended for &lt;strong&gt;maintainability, scalability, readability and productivity&lt;/strong&gt; reasons.&lt;/p&gt;

&lt;p&gt;We're going to create a &lt;em&gt;config&lt;/em&gt; file called &lt;code&gt;esbuild.config.js&lt;/code&gt; and we'll execute it from the &lt;em&gt;build&lt;/em&gt; script by running the command &lt;code&gt;node&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;But first, let's have a look at the simplest way to start bundling our files with ESBuild (no &lt;em&gt;config&lt;/em&gt; file is required this time):&lt;/p&gt;

&lt;p&gt;&lt;code&gt;package.json&lt;/code&gt;&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight json"&gt;&lt;code&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="nl"&gt;"scripts"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
   &lt;/span&gt;&lt;span class="nl"&gt;"build"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"rm -rf dist &amp;amp;&amp;amp; esbuild --bundle src/index.js --outfile=dist/bundle.js"&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;As usual, we declare the entry point and the output file. And that's it. But what happens when we need to &lt;strong&gt;keep bundling more different kind of files&lt;/strong&gt;?&lt;/p&gt;

&lt;p&gt;Let's then take a look at the following example:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight json"&gt;&lt;code&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="nl"&gt;"scripts"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
   &lt;/span&gt;&lt;span class="nl"&gt;"build"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"rm -rf dist &amp;amp;&amp;amp; esbuild --bundle src/index.js --outfile=dist/bundle.js &amp;amp;&amp;amp; esbuild --bundle styles/styles.css --outfile=dist/bundle.css"&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;We are now bundling the styles as well, and adding a little more information to the &lt;em&gt;build&lt;/em&gt; script (mess alert again!) by defining two different bundlers. We could (and definitely will) have more filetypes that would need to get bundled and this could become a total mess.&lt;/p&gt;

&lt;p&gt;So, let's put aside this approach and let's create a &lt;em&gt;config&lt;/em&gt; file.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;esbuild.config.js&lt;/code&gt;&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;esbuild&lt;/span&gt; &lt;span class="k"&gt;from&lt;/span&gt; &lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="s1"&gt;esbuild&lt;/span&gt;&lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nx"&gt;sassPlugin&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;from&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;esbuild-sass-plugin&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;babel&lt;/span&gt; &lt;span class="k"&gt;from&lt;/span&gt; &lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="s1"&gt;esbuild-plugin-babel&lt;/span&gt;&lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="nx"&gt;esbuild&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;build&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
   &lt;span class="na"&gt;entryPoints&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="s1"&gt;src/index.js&lt;/span&gt;&lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
   &lt;span class="na"&gt;bundle&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
   &lt;span class="na"&gt;outfile&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="s1"&gt;dist/bundle.js&lt;/span&gt;&lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
   &lt;span class="na"&gt;plugins&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;sassPlugin&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="nx"&gt;babel&lt;/span&gt;&lt;span class="p"&gt;()],&lt;/span&gt;
&lt;span class="p"&gt;}).&lt;/span&gt;&lt;span class="k"&gt;catch&lt;/span&gt;&lt;span class="p"&gt;(()&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;process&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;exit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;And here comes what I found (let me say) &lt;em&gt;weird&lt;/em&gt; and what took me some time to figure out.&lt;/p&gt;

&lt;p&gt;Maybe because I was expecting to run this &lt;em&gt;config&lt;/em&gt; file in the same way as Webpack and Rollup do (they run their &lt;em&gt;config&lt;/em&gt; file by default if it exists and has the default name), I had some trouble trying to tell ESBuild to take it as an input for configuration.&lt;/p&gt;

&lt;p&gt;Finally, I realized that &lt;strong&gt;it should be called via the node command&lt;/strong&gt; to just run the script 😬&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight json"&gt;&lt;code&gt;&lt;span class="nl"&gt;"scripts"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
   &lt;/span&gt;&lt;span class="nl"&gt;"build"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"rm -rf dist &amp;amp;&amp;amp; node esbuild.config.js"&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;And that was all. &lt;/p&gt;




&lt;p&gt;Something I want to mention here is that, the fact that &lt;strong&gt;there aren't so many plugins from where to pick&lt;/strong&gt; and also &lt;strong&gt;most of them are way outdated&lt;/strong&gt;, doesn't make me particularly happy. And, if you allow me some advice, &lt;strong&gt;try to pick plugins which use either CommonJS&lt;/strong&gt; (which inserts modules through &lt;em&gt;require&lt;/em&gt;) &lt;strong&gt;or ES Modules&lt;/strong&gt; (which does the same using &lt;em&gt;import&lt;/em&gt;),  because if you mix them up... the only things you'll get will be errors and mess everywhere! 😖&lt;/p&gt;

&lt;p&gt;Just make sure you &lt;strong&gt;change the type attribute&lt;/strong&gt; in &lt;code&gt;package.json&lt;/code&gt; &lt;strong&gt;if you're using ES Modules&lt;/strong&gt; (&lt;em&gt;import&lt;/em&gt;) to load your plugins into the &lt;em&gt;config&lt;/em&gt; file:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight json"&gt;&lt;code&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="nl"&gt;"type"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"module"&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Practically all plugins have been created by the community (if not all). For this example, I've chosen &lt;code&gt;esbuild-sass-plugin&lt;/code&gt; for SASS/SCSS and &lt;code&gt;esbuild-plugin-babel&lt;/code&gt; for Babel. Both of them work with &lt;em&gt;import&lt;/em&gt;, so no extra problems.&lt;/p&gt;

&lt;p&gt;A great point to mention: ESBuild is really &lt;strong&gt;fast compared with the others&lt;/strong&gt;. At least in this scenario.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#my-first-impressions-about-esbuild"&gt;
  &lt;/a&gt;
  My first impressions about ESBuild 👇
&lt;/h2&gt;

&lt;p&gt;Mixed emotions. At first, I thought it would be very easy to configure (it is if you only intend to perform a regular bundle) but then I started to &lt;strong&gt;struggle a bit&lt;/strong&gt; with the &lt;em&gt;config&lt;/em&gt; file, &lt;strong&gt;not because of the syntax&lt;/strong&gt; but because of the &lt;strong&gt;multiple errors thrown&lt;/strong&gt; on the terminal &lt;strong&gt;regarding Node&lt;/strong&gt;. &lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#5-parcel"&gt;
  &lt;/a&gt;
  5. Parcel
&lt;/h1&gt;

&lt;p&gt;Let's now have a look at the last bundler in this list: the &lt;em&gt;famous&lt;/em&gt; Parcel. Hi to the huge community of Parcel fans out there 👋&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#approach-used-by-parcel"&gt;
  &lt;/a&gt;
  Approach used by Parcel
&lt;/h2&gt;

&lt;p&gt;The Parcel approach is mainly based on a &lt;strong&gt;&lt;em&gt;zero&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;configuration&lt;/strong&gt; environment 😱 I was reluctant to believe it at first (that's the main reason why I wanted to try it out so bad), but, yes, it's possible to bundle a project like the one we're testing in this post by writing the bare minimum configuration, in a few minutes and without racking your brains 🙌&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#zero-configuration-are-you-sure"&gt;
  &lt;/a&gt;
  Zero configuration? Are you sure? 😪
&lt;/h2&gt;

&lt;p&gt;By zero they mean &lt;strong&gt;very little and precise&lt;/strong&gt;. Let me show you the configuration I used for this basic project:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;package.json&lt;/code&gt;&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight json"&gt;&lt;code&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="nl"&gt;"scripts"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
   &lt;/span&gt;&lt;span class="nl"&gt;"build"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"rm -rf dist &amp;amp;&amp;amp; rm -rf  &amp;amp;&amp;amp; parcel build src/index.js --no-scope-hoist --no-source-maps"&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;The procedure is pretty much the same: we need to indicate where the &lt;strong&gt;entry point&lt;/strong&gt; for our app is located. And I also added the flags &lt;code&gt;--no-scope-hoist&lt;/code&gt; &lt;strong&gt;to avoid odd behaviors&lt;/strong&gt; regarding &lt;code&gt;require&lt;/code&gt; when running &lt;em&gt;js&lt;/em&gt; scripts and &lt;code&gt;--no-source-maps&lt;/code&gt; &lt;strong&gt;to avoid the creation of&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;sourcemaps&lt;/em&gt;&lt;/strong&gt;. Otherwise, Parcel will create one for every bundle file by default.&lt;/p&gt;

&lt;p&gt;Now, if we want &lt;strong&gt;to change the location and the name of the output&lt;/strong&gt; bundle file, we need to change the value of the &lt;em&gt;main&lt;/em&gt; property attribute in &lt;code&gt;package.json&lt;/code&gt;, like this:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight json"&gt;&lt;code&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="nl"&gt;"main"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"dist/bundle.js"&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Otherwise, the bundle will be generated at root level and will be called with the name that is stored in &lt;em&gt;main&lt;/em&gt;, in most cases &lt;em&gt;index.js&lt;/em&gt; (if we didn't change it when running &lt;code&gt;npm init&lt;/code&gt;).&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#now-lets-zeroconfigure-the-styles-and-babel"&gt;
  &lt;/a&gt;
  Now, let's &lt;em&gt;(zero)configure&lt;/em&gt; the styles and Babel
&lt;/h2&gt;

&lt;p&gt;Since we're using SCSS, we need to use SASS as a preprocessor. So, what was my surprise when I read that &lt;strong&gt;SASS is already included with Parcel installation&lt;/strong&gt;. But not only &lt;em&gt;SASS&lt;/em&gt;, also &lt;em&gt;LESS&lt;/em&gt;, &lt;em&gt;Stylus&lt;/em&gt;, and... &lt;em&gt;Babel&lt;/em&gt;! 😧&lt;/p&gt;

&lt;p&gt;So the only step to take here is to create a couple of config files for SASS and Babel.&lt;/p&gt;

&lt;p&gt;Our SASS config file will be named &lt;code&gt;.sassrc&lt;/code&gt; and will contain the following code inside:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;includePaths&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;node_modules&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;And when we run the &lt;em&gt;build&lt;/em&gt; command, Parcel will automatically install the plugin &lt;code&gt;@parcel/transformer-sass&lt;/code&gt; as a dependency and will create a &lt;code&gt;bundle.css&lt;/code&gt; file in the same specified directory for the bundle, and that's all the configuration. Pretty cool, right? &lt;/p&gt;

&lt;p&gt;Now don't forget to link this file to your HTML 🤗  And remember that your &lt;em&gt;.scss&lt;/em&gt; file should has been previously &lt;strong&gt;imported on your entry point&lt;/strong&gt; file in order for the bundler to know what file it has to transform.&lt;/p&gt;

&lt;p&gt;On Babel side, we need to create a &lt;code&gt;.babelrc&lt;/code&gt;config file to specify the needed presets (let's say we want to get it ready for using React in the future):&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="p"&gt;{&lt;/span&gt;
   &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;presets&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;@babel/preset-env&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;@babel/preset-react&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Parcel will automatically call &lt;code&gt;@parcel/transformer-babel&lt;/code&gt; and will do the job for us.&lt;/p&gt;

&lt;p&gt;Don't forget to previously install &lt;code&gt;@babel/preset-env&lt;/code&gt;, &lt;code&gt;@babel/preset-react&lt;/code&gt; and all the dependencies needed by React. &lt;/p&gt;

&lt;p&gt;And that's... it. We're all set and ready to rock 😁 &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#my-first-impressions-about-parcel"&gt;
  &lt;/a&gt;
  My first impressions about Parcel 👇
&lt;/h2&gt;

&lt;p&gt;The first thing I want to point out is that Parcel wasn't that easy for me at the beginning since I had a really (really) hard time trying to get it ready to work, and it seemed like it wouldn't stop throwing errors regarding the OS and creating more trouble out of outdated versions of some dependencies 😥 So, to be honest, Parcel wasn't going to be in this list because I didn't want to talk about it if I couldn't try it out myself. &lt;/p&gt;

&lt;p&gt;But, &lt;em&gt;magically&lt;/em&gt; ✨ (and due to my tireless perseverance 😅), I finally could make it and set everything up to get it ready 🙌&lt;/p&gt;

&lt;p&gt;And after that, it was really easy compared to the rest of the bundlers. So let's draw a veil over the setbacks and let's give it a chance.&lt;/p&gt;

&lt;p&gt;Parcel is also pretty &lt;strong&gt;fast&lt;/strong&gt;, because it uses &lt;strong&gt;&lt;em&gt;cache&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;But... something I don't like at all is the &lt;strong&gt;several security vulnerabilities that appear after installing it&lt;/strong&gt; (around 12, some of them high risk) 😖 That doesn't speak well of you, Parcel. Not to mention the &lt;strong&gt;huge size of the project&lt;/strong&gt;. &lt;strong&gt;The heaviest&lt;/strong&gt; in this comparative.&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#the-comparison"&gt;
  &lt;/a&gt;
  The comparison 📈
&lt;/h1&gt;

&lt;p&gt;Here you have the highlights of this comparative summed up in a table:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--Des71L_W--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/naipfc68z6u3looyvidm.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Des71L_W--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/naipfc68z6u3looyvidm.png" alt="Table that compares different features of the 5 mentioned bundlers"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#what-is-my-final-verdict"&gt;
  &lt;/a&gt;
  What is my final verdict? 😼
&lt;/h1&gt;

&lt;p&gt;Well, I think some of &lt;strong&gt;these other bundlers apart from Webpack can be cool to use for small or side projects&lt;/strong&gt;, but in reality, I personally think that &lt;strong&gt;Webpack&lt;/strong&gt; is still the &lt;strong&gt;best option for robust projects&lt;/strong&gt; (we just have to look at the huge number of downloads per week compared to the rest). &lt;/p&gt;

&lt;p&gt;Also, I find it the &lt;strong&gt;easiest to manage&lt;/strong&gt; since once you get the sense of how it deals with configuration, it's easier to keep adding values to that configuration. But it's not something that obvious. You have to take your time playing with it to get a very basic idea at first.&lt;/p&gt;

&lt;p&gt;Moreover, you have the majority of resources you need (loaders, plugins...) available &lt;strong&gt;from the creators&lt;/strong&gt;, so you make sure you're using &lt;strong&gt;a real source of truth&lt;/strong&gt;. And they are updated really frequently, so you can use it without worry with newer versions of Node and other packages.&lt;/p&gt;

&lt;p&gt;So, yes, &lt;strong&gt;I will keep choosing Webpack&lt;/strong&gt; as my first option over the others.&lt;/p&gt;




&lt;p&gt;My second choice would be &lt;strong&gt;Rollup&lt;/strong&gt; for sure, and I truly think &lt;strong&gt;I will definitely use it in some of my side projects&lt;/strong&gt; since I found it &lt;strong&gt;intuitive to configure&lt;/strong&gt; and it &lt;strong&gt;seems like it works properly on robust projects&lt;/strong&gt; as well. &lt;/p&gt;

&lt;p&gt;And about their &lt;em&gt;plugins&lt;/em&gt;, most of them are also available &lt;strong&gt;from the creators&lt;/strong&gt;, so, again, a real source of truth and many more advantages.&lt;/p&gt;




&lt;p&gt;I also think &lt;strong&gt;Parcel&lt;/strong&gt; is &lt;strong&gt;a very interesting option&lt;/strong&gt; and I'd like to try it with larger projects and check if it really doesn't need further configuration. Definitely a great find.&lt;/p&gt;

&lt;p&gt;And a big plus to the fact that &lt;strong&gt;plugins&lt;/strong&gt; like &lt;em&gt;Babel&lt;/em&gt;, &lt;em&gt;SASS&lt;/em&gt;, &lt;em&gt;LESS&lt;/em&gt; and some more are &lt;strong&gt;built-in and ready to use&lt;/strong&gt; out of the box.&lt;/p&gt;




&lt;p&gt;What about &lt;strong&gt;Browserify&lt;/strong&gt; and &lt;strong&gt;ESBuild&lt;/strong&gt;?&lt;/p&gt;

&lt;p&gt;These two have been &lt;strong&gt;the ones with which I have struggled more&lt;/strong&gt;, especially Browserify. The fact that it doesn't require a &lt;em&gt;config&lt;/em&gt; file and everything should be declared in &lt;em&gt;package.json&lt;/em&gt; kinda forces you to &lt;strong&gt;change the way you think of how bundlers&lt;/strong&gt; are &lt;strong&gt;&lt;em&gt;traditionally&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;configured&lt;/strong&gt;. Not to mention you end up &lt;em&gt;saturating&lt;/em&gt; the file with &lt;strong&gt;way a lot tricky configurations&lt;/strong&gt;, which makes it &lt;strong&gt;difficult to read and maintain&lt;/strong&gt;. &lt;/p&gt;

&lt;p&gt;Also, when it comes to &lt;em&gt;plugins&lt;/em&gt;, &lt;strong&gt;most of them are not developed and maintained by the creators&lt;/strong&gt; (especially the most common) and are &lt;strong&gt;really outdated&lt;/strong&gt; (many of them haven't been updated in the last 4 years) and this fact &lt;strong&gt;leads to problems with newer Node/other packages versions&lt;/strong&gt; and compatibility in general.&lt;/p&gt;




&lt;p&gt;And on &lt;strong&gt;ESBuild&lt;/strong&gt; side, I didn't especially like it either. The first impression was good but then, since the &lt;em&gt;config&lt;/em&gt; file caused me some trouble, it mainly &lt;strong&gt;ended up in confusion about how to manage configuration&lt;/strong&gt; with and without this file. So I found it quite &lt;strong&gt;ambiguous&lt;/strong&gt; and took me a bit to realize &lt;strong&gt;how to set up both scenarios&lt;/strong&gt; in different ways.&lt;/p&gt;

&lt;p&gt;About their &lt;em&gt;plugins&lt;/em&gt;, same as Browserify, &lt;strong&gt;practically all of them has been created by the community&lt;/strong&gt;, not the author, so you have to use them at your own risk. But as a plus point, &lt;strong&gt;they are often updated and maintained&lt;/strong&gt;.&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#now-its-your-turn"&gt;
  &lt;/a&gt;
  Now it's your turn! 🔥
&lt;/h1&gt;

&lt;p&gt;What do you think of this comparative? Do you agree? Which one is your preferred bundler? Do you know some other bundler that is not on the list? Would you like to suggest different bundlers for future comparative posts? Comment below!&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#disclaimer"&gt;
  &lt;/a&gt;
  Disclaimer 👇
&lt;/h1&gt;

&lt;p&gt;Remember this is just a post showing &lt;strong&gt;my impressions about something I tried for the first time&lt;/strong&gt;. I have decided to share the process with you and my opinion about what I experienced. &lt;strong&gt;The opinions expressed in this post don't mean that some bundlers are better over others&lt;/strong&gt;. My advice is to try them all out and draw your own conclusions, like I did. And based on that, use the ones you like best and fit your needs.&lt;/p&gt;




&lt;p&gt;🎉 Don't forget to follow &lt;a class="mentioned-user" href="https://dev.to/underscorecode"&gt;@underscorecode&lt;/a&gt;
 on &lt;a href="https://instagram.com/underscorecode"&gt;Instagram&lt;/a&gt; and &lt;a href="https://twitter.com/underscorecode"&gt;Twitter&lt;/a&gt; for more daily webdev content 🖥🖤&lt;/p&gt;




&lt;h4&gt;
  &lt;a href="#and-last-but-not-least-a-quick-friendly-reminder-before-we-go"&gt;
  &lt;/a&gt;
  And last but not least... A quick friendly reminder before we go 😊
&lt;/h4&gt;

&lt;p&gt;We all know there are million ways to get things done when it comes to programming and development, and we're here to &lt;strong&gt;help and learn&lt;/strong&gt;, so, if you know another possible way to do what others are sharing (&lt;strong&gt;not better, not worse, just different&lt;/strong&gt;), feel free to share it if you feel like it, but, please, &lt;strong&gt;always be kind and respectful&lt;/strong&gt; with the author and the rest of the community. Thank you and happy coding!&lt;/p&gt;

</description>
      <category>javascript</category>
      <category>webpack</category>
      <category>webdev</category>
      <category>productivity</category>
    </item>
    <item>
      <title>5 Handy CLI Tools to Spice up Your Terminal</title>
      <author>Elena Lape</author>
      <pubDate>Fri, 09 Jul 2021 15:09:02 +0000</pubDate>
      <link>https://dev.to/elenalape/5-handy-cli-tools-to-spice-up-your-terminal-31do</link>
      <guid>https://dev.to/elenalape/5-handy-cli-tools-to-spice-up-your-terminal-31do</guid>
      <description>&lt;p&gt;The Command Line.&lt;/p&gt;

&lt;p&gt;A developer's best mate, and also one of the main sources of our frustration (it's &lt;code&gt;esc&lt;/code&gt;, then &lt;code&gt;:q&lt;/code&gt; to quit Vim, by the way).&lt;/p&gt;

&lt;p&gt;Regardless of its social status, there are plenty of great CLI tools that can make quite a difference in the overall terminal experience.&lt;/p&gt;

&lt;p&gt;Here are some of my favourites, in no particular order. Some are tools that I use every day in my work, and others are just fun apps to try if you get bored of a GUI.&lt;/p&gt;

&lt;p&gt;Shall we?&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#1-ohmyzsh"&gt;
  &lt;/a&gt;
  1. &lt;a href="https://ohmyz.sh/"&gt;Oh-My-Zsh&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--A4ap7Ood--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/467bung2smmypgklrfk9.jpg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--A4ap7Ood--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/467bung2smmypgklrfk9.jpg" alt="oh-my-zsh screenshot with wedisagree theme"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://ohmyz.sh/"&gt;Oh-My-Zsh&lt;/a&gt; is a framework for managing your Zsh configuration. It comes bundled with thousands of helpful functions, helpers, plugins, and themes.&lt;/p&gt;

&lt;p&gt;In short — Oh-My-Zsh makes the terminal less intimidating by bringing some colour and autocompletion to tools like &lt;a href="https://git-scm.com/"&gt;Git&lt;/a&gt; and &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt;, as well as plenty of package managers and other popular command line utilities.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# To install
$ sh -c "$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)"

# To configure your plugins, themes, aliases etc.
$ vi ~/.zshrc
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Admittedly, my favourite part is that there are lots of different &lt;a href="https://github.com/ohmyzsh/ohmyzsh/wiki/Themes"&gt;themes&lt;/a&gt; to choose from — my favourite is &lt;a href="https://github.com/ohmyzsh/ohmyzsh/wiki/Themes#wedisagree"&gt;&lt;code&gt;wedisagree&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Note that Oh-My-Zsh is for the &lt;strong&gt;ZSH/Z-Shell&lt;/strong&gt; (not bash or any other), so make sure you’ve got &lt;a href="https://github.com/ohmyzsh/ohmyzsh/wiki/Installing-ZSH"&gt;ZSH&lt;/a&gt; going first.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#2-httpie"&gt;
  &lt;/a&gt;
  2. &lt;a href="https://httpie.io"&gt;HTTPie&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--YvQFrYlX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://raw.githubusercontent.com/httpie/httpie/master/httpie.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--YvQFrYlX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://raw.githubusercontent.com/httpie/httpie/master/httpie.gif" alt="HTTPie in the terminal"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Ever wanted to make a request to an API or a website, without having to pull out an arsenal of devtools?&lt;/p&gt;

&lt;p&gt;Say no more — &lt;a href="https://httpie.io"&gt;HTTPie&lt;/a&gt; is here to save the day.&lt;/p&gt;

&lt;p&gt;HTTPie is a CLI HTTP client that comes with colourised output (that goes fashionably well with Oh-My-Zsh's &lt;code&gt;wedisagree&lt;/code&gt;), really intuitive syntax, and a number of other features to make testing and debugging APIs as simple as it gets.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# To install with brew
$ brew install httpie

# To make a request
$ http httpie.io/hello
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;One feature I find particularly handy is the &lt;a href="https://httpie.io/docs/2.4.0#offline-mode"&gt;&lt;code&gt;--offline&lt;/code&gt; mode&lt;/a&gt;, which lets you build and print out a (colourised and formatted) HTTP request without sending it. That way, you can see exactly the stuff the API in question is going to receive.&lt;/p&gt;

&lt;p&gt;Check out my recent &lt;a href="https://dev.to/elenalape/apis-101-getting-started-with-httpie-2o9g"&gt;Getting started with HTTPie guide&lt;/a&gt; to learn more.&lt;/p&gt;

&lt;p&gt;Full disclosure: I am part of the HTTPie team. However, I have been using it even before I joined!&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#3-wttrin"&gt;
  &lt;/a&gt;
  3. &lt;a href="https://github.com/chubin/wttr.in"&gt;Wttr.in&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--fujA09re--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/te5m2bi8mg9sezcdvs0v.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--fujA09re--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/te5m2bi8mg9sezcdvs0v.png" alt="wttr.in weather forecast preview"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Using a command line just for the sake of using a command line?&lt;/p&gt;

&lt;p&gt;Sign. me. up.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://github.com/chubin/wttr.in"&gt;Wttr.in&lt;/a&gt; is a console-based weather report app. Just add your city to the URL, and send a request like so:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# Using HTTPie
$ http wttr.in/london 

# Or, using cURL
$ curl wttr.in/london 
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;I’ve aliased &lt;code&gt;http wttr.in/London&lt;/code&gt; with &lt;code&gt;weather&lt;/code&gt;. So now, each time I want to see the weather forecast for London, I am able to simply type &lt;code&gt;weather&lt;/code&gt; and save all those precious seconds of having to pick up my phone and navigate to the weather app. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#4-kalk"&gt;
  &lt;/a&gt;
  4. &lt;a href="https://kalk.dev/"&gt;Kalk&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--fayW8_XN--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zwsgl326s2oaf4ohx5wo.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--fayW8_XN--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zwsgl326s2oaf4ohx5wo.gif" alt="Kalk calculator preview"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Computers have evolved so far ahead from their original purpose, which was to calculate things.&lt;/p&gt;

&lt;p&gt;Sure, you can and create some sick beats with your machine, or run Overwatch at one bazillion frames per second. &lt;/p&gt;

&lt;p&gt;But something as simple as &lt;em&gt;calculating&lt;/em&gt; what grade you need to score in an exam to pass the course is more tricky than it should be. Your default OS calculator is an option, but it's very basic. Google kiiind of does the job, but requires internet connection. So do more advanced tools like &lt;a href="https://www.wolframalpha.com/"&gt;Wolfram Alpha&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://kalk.dev/"&gt;Kalk&lt;/a&gt; is... a CLI for a calculator.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# To install using brew
$ brew install kalk

# To launch
$ kalk
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;kalk 0.4.0+3fb73b1 - Copyright (c) 2020-2021 Alexandre Mutel
# Type `help` for more information and at https://github.com/xoofx/kalk

&amp;gt;&amp;gt;&amp;gt; # You can do things such as
&amp;gt;&amp;gt;&amp;gt; x=2; round((54+4)/(4+x))

# x = 2; round((54 + 4) / (4 + x))
x = 2
out = 10
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;It's simply brilliant both as your regular everyday calculator, and a more advanced one that will solve your equations using the same syntax you'd use in a maths class. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#5-taskwarrior"&gt;
  &lt;/a&gt;
  5. &lt;a href="https://taskwarrior.org/"&gt;Taskwarrior&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--qJCwS8_Z--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/yl24k97hju6b6yd5vzze.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--qJCwS8_Z--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/yl24k97hju6b6yd5vzze.png" alt="taskwarrior screenshot from wikipedia.org"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It's an open secret that creating to-do lists is peak procrastination, as a queen of procrastination, I'd like to introduce you to one more way &lt;em&gt;to-do&lt;/em&gt; it.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://taskwarrior.org/"&gt;Taskwarrior&lt;/a&gt; is a nifty tool if you want to keep all your &lt;em&gt;actual&lt;/em&gt; to-dos separate from shopping lists and song lyric ideas that you've got sitting in your note taking app.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# To install with brew
$ brew install taskd

# To create a new task (it will prompt you to create a ~/.taskrc file upon the first run; select yes
$ task add 'Write a dev.to article about CLI tools'

# To view all pending tasks
$ task

# To mark task as complete
$ task &amp;lt;task_id&amp;gt; done
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;You can also annotate, set a priority level or a by-date to individual tasks or their groups.&lt;/p&gt;

&lt;p&gt;Taskwarrior's &lt;a href="https://taskwarrior.org/docs/30second.html"&gt;30 second tutorial&lt;/a&gt; has got everything you need to get started.&lt;/p&gt;




&lt;p&gt;Do you have any favourite CLI tools you'd like to share? Please let me know if you give any of the ones listed above a try! &lt;/p&gt;

&lt;p&gt;I'm going to go finish that burning &lt;code&gt;task&lt;/code&gt; now.&lt;/p&gt;

&lt;p&gt;Unless the &lt;code&gt;weather&lt;/code&gt; is too nice for it.&lt;/p&gt;

</description>
      <category>githunt</category>
      <category>productivity</category>
      <category>tooling</category>
      <category>todayilearned</category>
    </item>
    <item>
      <title>DEPLOY NEXT.JS APP TO VERCEL</title>
      <author>Ha Tuan Em</author>
      <pubDate>Fri, 09 Jul 2021 15:08:18 +0000</pubDate>
      <link>https://dev.to/hte305/deploy-next-js-app-to-vercel-2kj2</link>
      <guid>https://dev.to/hte305/deploy-next-js-app-to-vercel-2kj2</guid>
      <description>&lt;p&gt;Someone want to me make a post for explain how to deploy NEXT.JS application to &lt;a href="https://vercel.com/"&gt;Vercel&lt;/a&gt;. Base on require of them I will make a post. Hope, it will help something to you.&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#i-initial-nextjs-app-or-you-can-clone-my-shopping-cart-repository"&gt;
  &lt;/a&gt;
  I. Initial Next.js app or you can clone my shopping cart &lt;a href="https://github.com/hatuanem199801/next-shopping-example"&gt;repository&lt;/a&gt;
&lt;/h4&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;create-next-app shopping-cart
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h4&gt;
  &lt;a href="#ii-after-creating-your-application-commit-them-to-github"&gt;
  &lt;/a&gt;
  II. After creating your application, commit them to Github.
&lt;/h4&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;git add .
git commit -m "Complete project"
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h4&gt;
  &lt;a href="#iii-create-project-in-vercel"&gt;
  &lt;/a&gt;
  III. Create project in Vercel
&lt;/h4&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--9zpqYTvD--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qdjw54je867grxworj2o.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--9zpqYTvD--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qdjw54je867grxworj2o.png" alt="Create project"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#iv-import-project-or-you-can-search-by-name-of-repository"&gt;
  &lt;/a&gt;
  IV. Import project or you can search by name of repository.
&lt;/h4&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--xoR8GhQJ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ijwzn53vnyqo826aaohd.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--xoR8GhQJ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ijwzn53vnyqo826aaohd.png" alt="import project"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#v-configure-application"&gt;
  &lt;/a&gt;
  V. Configure application
&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;You can add &lt;code&gt;env&lt;/code&gt; as MONGOURI or SERECTKEY, ... in box &lt;code&gt;Environment Variables&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Override command of application in box &lt;code&gt;Build and Output Settings&lt;/code&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--Zxo0bUxC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/coad6982e1f71rpqeqpl.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Zxo0bUxC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/coad6982e1f71rpqeqpl.png" alt="Configure application"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#vi-deploy-and-done"&gt;
  &lt;/a&gt;
  VI. Deploy and done
&lt;/h4&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--TC6nljPg--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/unnkhpj9wo0hvn5ur1ym.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--TC6nljPg--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/unnkhpj9wo0hvn5ur1ym.png" alt="Deploy"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Enjoy your time 🪴&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you for reading.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/hte305"&gt;&lt;br&gt;
&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--eKzEoK4A--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ddtyyk0zuud3h7o0sjpq.png" alt="Buy me a coffee"&gt;&lt;br&gt;
&lt;/a&gt;&lt;/p&gt;

</description>
      <category>nextjs</category>
      <category>vercel</category>
      <category>deploy</category>
      <category>shoppingcart</category>
    </item>
    <item>
      <title>Speeding up the development builds after upgrading to Angular v12</title>
      <author>Brandon Roberts</author>
      <pubDate>Fri, 09 Jul 2021 15:07:52 +0000</pubDate>
      <link>https://dev.to/brandontroberts/speeding-up-the-development-serve-after-upgrading-to-angular-v12-5db5</link>
      <guid>https://dev.to/brandontroberts/speeding-up-the-development-serve-after-upgrading-to-angular-v12-5db5</guid>
      <description>&lt;p&gt;After you've upgraded to Angular v12 from a previous version of Angular, you may notice your &lt;code&gt;ng serve&lt;/code&gt; times have increased, along with missing sourcemaps, and longer rebuild times during development. This post helps you set a default configuration to development to get your application serving the same as previously.&lt;/p&gt;

&lt;p&gt;In Angular version 12, running &lt;code&gt;ng build&lt;/code&gt; now defaults to production mode. This is a welcomed change, as there is less chance of accidentally deploying a development build to production, which is a lot slower and bigger, giving the perception that Angular is slow. This also aligns with other web frameworks that build for production out of the box.&lt;/p&gt;

&lt;p&gt;The way Angular serves the application, it essentially does a build with watch mode. As mentioned before, doing a build is now done by default with production optimizations enabled. This adds more time to the build process.&lt;/p&gt;

&lt;p&gt;There is a migration to add a "development" build configuration.&lt;/p&gt;

&lt;p&gt;To run this migration, run:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;ng update @angular/cli &lt;span class="nt"&gt;--migrate-only&lt;/span&gt; production-by-default
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;


&lt;p&gt;One caveat is that it only supports migrating first-party Angular builders for development mode, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;@angular-devkit/build-angular:dev-server&lt;/li&gt;
&lt;li&gt;@angular-devkit/build-angular:protractor&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To fix this manually, you add the development options as defaults, and a &lt;code&gt;defaultConfiguration&lt;/code&gt; set to an empty string so it doesn't default to &lt;code&gt;production&lt;/code&gt;.&lt;/p&gt;


&lt;div class="ltag_gist-liquid-tag"&gt;
  
&lt;/div&gt;



&lt;p&gt;Now, when running &lt;code&gt;ng serve&lt;/code&gt; you will get a development build, which is faster for local development.&lt;/p&gt;

&lt;p&gt;If you liked this, click the ❤️ so other people will see it. Follow &lt;a href="https://twitter.com/brandontroberts"&gt;me on Twitter&lt;/a&gt; for more tips on Angular, &lt;a href="https://nx.dev"&gt;Nx&lt;/a&gt;, and &lt;a href="https://ngrx.io"&gt;NgRx&lt;/a&gt;!&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#see-also"&gt;
  &lt;/a&gt;
  See Also
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://blog.angular.io/angular-v12-is-now-available-32ed51fbfd49"&gt;Angular v12 blog post&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <category>angular</category>
    </item>
    <item>
      <title>How to Game Dev Metrics w/ Ray Elenteny</title>
      <author>Conor Bronsdon</author>
      <pubDate>Fri, 09 Jul 2021 14:52:28 +0000</pubDate>
      <link>https://dev.to/conorbronsdon/how-to-game-dev-metrics-w-ray-elenteny-5h3l</link>
      <guid>https://dev.to/conorbronsdon/how-to-game-dev-metrics-w-ray-elenteny-5h3l</guid>
      <description>&lt;p&gt;What leads teams to game metrics within their organization?&lt;/p&gt;

&lt;p&gt;On this week’s episode of &lt;a href="https://devinterrupted.com/podcast/how-to-game-dev-metrics/"&gt;Dev Interrupted&lt;/a&gt;, we speak with agile expert Ray Elenteny, Principal Owner at Solutech Consulting, about how people game dev metrics and the underlying issues in culture &amp;amp; leadership that lead to it.&lt;/p&gt;

&lt;p&gt;So whether you're trying to game your own metrics (don't do it!) or solve culture issues that have led to this issue at your organization, give this episode a listen.&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#listen-to-the-full-episode"&gt;
  &lt;/a&gt;
  &lt;strong&gt;Listen to the full episode&lt;/strong&gt;
&lt;/h1&gt;

&lt;p&gt;&lt;iframe width="100%" height="232px" src="https://open.spotify.com/embed/episode/2AJVkiMHT3Zd4vb3pzLEbs"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#episode-highlights-include"&gt;
  &lt;/a&gt;
  &lt;strong&gt;Episode Highlights include:&lt;/strong&gt;
&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Which metrics are easiest to game&lt;/li&gt;
&lt;li&gt;The long-term implications of gaming metrics&lt;/li&gt;
&lt;li&gt;How poor culture and leadership lead engineering teams to game dev metrics&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;
  &lt;a href="#join-the-dev-interrupted-discord-server"&gt;
  &lt;/a&gt;
  &lt;strong&gt;Join the Dev Interrupted Discord Server&lt;/strong&gt;
&lt;/h1&gt;

&lt;p&gt;With over 1200 members, the Dev Interrupted Discord Community is the best place for Engineering Leaders to engage in daily conversation. No sales people allowed. &lt;a href="https://discord.gg/tpkmwM6c3g"&gt;Join the community &amp;gt;&amp;gt;&lt;/a&gt;&lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--wzIBzHH0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/19j3dzgz4r4kzav3w6z8.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--wzIBzHH0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/19j3dzgz4r4kzav3w6z8.png" alt="Join the Dev Interrupted Discord Community!"&gt;&lt;/a&gt;&lt;/p&gt;

</description>
      <category>devops</category>
      <category>agile</category>
      <category>leadership</category>
      <category>culture</category>
    </item>
    <item>
      <title>Learning new tech as a beginner.</title>
      <author>Asim Shrestha</author>
      <pubDate>Fri, 09 Jul 2021 14:47:40 +0000</pubDate>
      <link>https://dev.to/alex1the1great/learning-new-tech-as-a-beginners-2gl7</link>
      <guid>https://dev.to/alex1the1great/learning-new-tech-as-a-beginners-2gl7</guid>
      <description>&lt;h3&gt;
  &lt;a href="#1-code-everyday"&gt;
  &lt;/a&gt;
  1. Code everyday.
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;To become good a coding you have to code every single day. Even if for just 20 minutes a day. You just have to be consistent.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#2-take-your-first-tutorial"&gt;
  &lt;/a&gt;
  2. Take your first tutorial.
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Complete the tutorial. Understand everything, shallow understanding is fine while starting.&lt;/li&gt;
&lt;li&gt;You do not need to understand exactly how the functions work, but you do need to be able to import and use them correctly.&lt;/li&gt;
&lt;li&gt;Don't rush to complete the tutorial. If you feel like skipping any topics from tutorial, take a break and come back later.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#3-add-at-least-2-new-features-to-the-project-after-completing-the-tutorial"&gt;
  &lt;/a&gt;
  3. Add at least 2 new features to the project after completing the tutorial.
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Following tutorial, we are not using our brain. So, we have to use our brain and think ourselves try to add simple features at least. You have understood the tutorial it will not a big deal.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#4-build-a-new-projects-amp-complete-it"&gt;
  &lt;/a&gt;
  4. Build a new projects &amp;amp; complete it.
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ideas don't come out fully formed. They only become clear as you work on them. YOU JUST HAVE TO GET STARTED.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Build a complete new project with the knowledge which you have acquire from the tutorial.&lt;/li&gt;
&lt;li&gt;If you like your new project which you are building then keep on updating(iterating) it.&lt;/li&gt;
&lt;li&gt;Instead of making multiple simple projects, build a big project.&lt;/li&gt;
&lt;li&gt;If you keep on adding new features to a simple project, then it will start to grow to a big project.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#5-habit-of-figuring-out-anything"&gt;
  &lt;/a&gt;
  5. Habit of Figuring out anything.
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Face your challenges.&lt;/li&gt;
&lt;li&gt;How can I add this features to my project?

&lt;ul&gt;
&lt;li&gt;Reading documentation.&lt;/li&gt;
&lt;li&gt;Learning from an article.&lt;/li&gt;
&lt;li&gt;Watching videos.&lt;/li&gt;
&lt;li&gt;Asking for help in stack overflow or any other platform.&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;/ul&gt;

</description>
      <category>programming</category>
      <category>codenewbie</category>
      <category>beginners</category>
      <category>python</category>
    </item>
    <item>
      <title>A Gentle Introduction to Reinforcement Learning </title>
      <author>Satwik Kansal</author>
      <pubDate>Fri, 09 Jul 2021 14:36:20 +0000</pubDate>
      <link>https://dev.to/satwikkansal/a-gentle-introduction-to-reinforcement-learning-75h</link>
      <guid>https://dev.to/satwikkansal/a-gentle-introduction-to-reinforcement-learning-75h</guid>
      <description>&lt;h1&gt;
  &lt;a href="#a-gentle-introduction-to-reinforcement-learning"&gt;
  &lt;/a&gt;
  A gentle introduction to Reinforcement Learning
&lt;/h1&gt;

&lt;p&gt;In 2016, AplhaGo, a program developed for playing the game of &lt;a href="https://en.wikipedia.org/wiki/Go_(game)"&gt;Go&lt;/a&gt;, made headlines when it beat the world champion Go player in a five-game match. It was a remarkable feat because the number of possible legal moves in Go are of the order of 2.1 × 10&lt;sup&gt;170&lt;/sup&gt;.  To put this in context, this number is far, far greater than the number of atoms in the observable universe, which are of the order of 10&lt;sup&gt;80&lt;/sup&gt;. Such a high number of possibilities make it almost impossible to create a program that can play effectively using brute-force or somewhat optimized search algorithms. &lt;/p&gt;

&lt;p&gt;A part of the secret sauce of AlphaGO was the usage of Reinforcement Learning to improve its understanding of the game by playing against itself. Since then, the field of Reinforcement Learning has seen increased interest, and much more efficient programs have been developed to play various games at a pro-human efficiency. Although you would find Reinforcement Learning discussed in the context of Games and Puzzles in most places (including this post), the applications of Reinforcement Learning are much more expansive. The objective of this tutorial is to give you a gentle introduction to the world of Reinforcement Learning. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ℹ️ First things first! This post was written in collaboration with &lt;a href="https://scholar.google.com/citations?user=H4WCOr8AAAAJ"&gt;Alexey Vinel&lt;/a&gt; (Professor, Halmstead University). Some ideas and visuals are borrowed from my previous post on Q-learning written for &lt;a href="https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/"&gt;Learndatasci&lt;/a&gt;. Unlike most posts you'll find on Reinforcement learning, we try to explore Reinforcement Learning here with an angle of multiple agents. So this makes it slightly more complicated and interesting at the same time. While this will be a good resource to develop intuitive understanding of Reinforcement Learning (Reinforcement Q-learning to be specific), it is highly recommended to visit the theoretical parts (some links shared in the appendix), if you're willing to explore Reinforcement Learning beyond this post.&lt;/p&gt;

&lt;p&gt;I had to fork openAIs gym library to implement a custom environment. The code can be found on &lt;a href="https://github.com/satwikkansal/gym-dual-taxi"&gt;this github repository&lt;/a&gt;.  If you'd like to explore an interactive version, you can check &lt;a href="https://colab.research.google.com/github/satwikkansal/gym-dual-taxi/blob/master/draft.ipynb"&gt;out this google colab notebook&lt;/a&gt;. We use Python to implement the algorithms, if you're not familiar with Python you can simply pretend that those snippets don't exist and read through the textual part (including code comments). Alright, time to get started 🚀&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2&gt;
  &lt;a href="#what-is-reinforcement-learning"&gt;
  &lt;/a&gt;
  What is Reinforcement Learning?
&lt;/h2&gt;

&lt;p&gt;Reinforcement learning is a paradigm of Machine Learning where learning happens through the feedback gained by an agent's interaction with its environment. This is also one of the key differentiators of Reinforcement Learning with the other two paradigms of Machine learning (&lt;a href="https://en.wikipedia.org/wiki/Supervised_learning"&gt;Supervised learning&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Unsupervised_learning"&gt;Unsupervised learning&lt;/a&gt;). Supervised learning algorithms require fully labelled-training-data, and Unsupervised learning algorithms need no labels. On the other hand, Reinforcement learning algorithms utilize feedback from the environment they're operating in to get better at the tasks they're being trained to perform. So we can say that Reinforcement Learning lies somewhere in the middle of the spectrum.&lt;/p&gt;

&lt;p&gt;It is inevitable to talk about Reinforcement Learning with clarity without using some technical terms like "agent", "action", "state", "reward", and "environment". So let's try to gain a high-level understanding of Reinforcement Learning and these terms through an analogy,&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#understanding-reinforcement-learning-through-birbing"&gt;
  &lt;/a&gt;
  Understanding Reinforcement learning through Birbing
&lt;/h3&gt;

&lt;p&gt;Let's watch the first few seconds of this video first,&lt;/p&gt;

&lt;p&gt;&lt;iframe width="710" height="399" src="https://www.youtube.com/embed/u7TiRqh7x8s"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;Pretty cool, isn't it?&lt;/p&gt;

&lt;p&gt;And now think about how did someone manage to teach this parrot to reply with certain sounds on certain prompts. And if you carefully observed, part of the answer lies in the food the parrot is given after every cool response. The human asks a question, and the parrot tries to respond in many different ways, and if the parrot's response is the desired one, it is rewarded with food. Now guess what? The next time the parrot is exposed to the same cue, it is likely to answer similarly, expecting more food. This is how we "reinforce" certain behaviours through positive experiences. If I had to explain the above process in terms of Reinforcement learning concepts, it'd be something like,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;"The agent learns to take desired for a given state in the environment", &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;where,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The "agent" is the parrot&lt;/li&gt;
&lt;li&gt;The "state" is questions or cues the parrot is exposed to&lt;/li&gt;
&lt;li&gt;The "actions" are the sounds it is uttering &lt;/li&gt;
&lt;li&gt;The "reward" is the food he gets when he takes the desired action&lt;/li&gt;
&lt;li&gt;And the "environment" is the place where the parrot is living (or, in other words, everything else than the parrot)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The reinforcement can happen through negative experiences too. For example, if a child touches a burning candle out of curiosity, (s)he is unlikely to repeat the same action. So, in this case, instead of a reward, the agent got a penalty, which would disincentivize the agent to repeat the same action in future again.&lt;/p&gt;

&lt;p&gt;&lt;iframe width="710" height="399" src="https://www.youtube.com/embed/hsVEiat444Q"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;If you try to think about it, there are countless similar real-world analogies. This suggests why Reinforcement Learning can be helpful for a wide variety of real-world applications and why it might be a path to create General AI Agents (think of a program that can not just beat a human in the game of Go, but multiple games like Chess, GTA, etc.). It might still take a lot of time to develop agents with general intelligence, but reading about programs like &lt;a href="https://en.wikipedia.org/wiki/MuZero"&gt;MuZero&lt;/a&gt; (one of the many successors of Alpha Go) hints that Reinforcement learning might have a decent role to play in achieving that.&lt;/p&gt;

&lt;p&gt;After reading the analogies, a few questions like below might have come into your mind,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Real-world example is fine, but how do I do this "reinforcement" in the world of programs?&lt;/li&gt;
&lt;li&gt;What are these algorithms, and how do they work?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let's start answering such questions as switch gears and dive into certain technicalities of Reinforcement learning.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#example-problem-statement-selfdriving-taxi"&gt;
  &lt;/a&gt;
  Example problem statement: Self-driving taxi
&lt;/h2&gt;

&lt;p&gt;Wouldn't it be fantastic to train an agent (i.e. create a computer program) to pick up from a location and drop them at their desired location? In the rest of the tutorial, we'll try to solve a simplified version of this problem through reinforcement learning.&lt;/p&gt;

&lt;p&gt;Let's start by specifying typical steps in a Reinforcement learning process,&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Agent observes the environment. The observation is represented in digital form and also called "state".&lt;/li&gt;
&lt;li&gt;The agent utilizes the observation to decide how to act. The strategy agent uses to figure out the action to perform is also referred to as "policy".&lt;/li&gt;
&lt;li&gt;The agent performs the action in the environment.&lt;/li&gt;
&lt;li&gt;The environment, as a result of the action, may move to a new state (i.e. generate different observations) and may return feedback to the agent in the form of rewards/penalties. &lt;/li&gt;
&lt;li&gt;The agent uses the rewards and penalties to refine its policy.&lt;/li&gt;
&lt;li&gt;The process can be repeated until the agent finds an optimal policy.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--1ynz3QjF--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://storage.googleapis.com/lds-media/documents/Reinforcement-Learning-Animation.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--1ynz3QjF--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://storage.googleapis.com/lds-media/documents/Reinforcement-Learning-Animation.gif" alt=""&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now that we're clear about the process, we need to set up the environment. In most cases, what this means is we need to figure out the following details,&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#1-the-statespace"&gt;
  &lt;/a&gt;
  1. The state-space
&lt;/h3&gt;

&lt;p&gt;Typically, a "state" will encode the observable information that the agent can use to learn to act efficiently. For example, in the case of self-driving-taxi, the state information could contain the following information,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The current location of the taxi&lt;/li&gt;
&lt;li&gt;The current location of the passenger&lt;/li&gt;
&lt;li&gt;The destination&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There can be multiple ways to represent such information, and how one ends up doing it depends on the level of sophistication intended. &lt;/p&gt;

&lt;p&gt;The state space is the set of all possible states an environment can be in. For example, if we consider our environment for the self-driving taxi to be a two-dimensional 4x4 grid, there are &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;16 possible locations for the taxi&lt;/li&gt;
&lt;li&gt;16 possible locations for the passenger&lt;/li&gt;
&lt;li&gt;and 16 possible destination&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This means our state-space size becomes 16 x 16 x 16 = 4096, i.e. at any point in time the environment must be in either of these 4096 states. &lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--1UeQ7JwC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/N.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--1UeQ7JwC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/N.gif" alt="https://github.com/satwikkansal/gym-dual-taxi/raw/master/N.gif"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#2-the-action-space"&gt;
  &lt;/a&gt;
  2. The action space
&lt;/h3&gt;

&lt;p&gt;Action space is the set of all possible actions an agent can take in the environment. Taking the same 2D grid-world example, the taxi agent may be allowed to take the following actions,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Move North&lt;/li&gt;
&lt;li&gt;Move South&lt;/li&gt;
&lt;li&gt;Move East&lt;/li&gt;
&lt;li&gt;Move West&lt;/li&gt;
&lt;li&gt;Pickup&lt;/li&gt;
&lt;li&gt;Drop-off&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Again, there can be multiple ways to define the action space, and this is just one of them. The choice also depends on the level of complexity and algorithms you'd want to use later.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#3-the-rewards"&gt;
  &lt;/a&gt;
  3. The rewards
&lt;/h3&gt;

&lt;p&gt;The rewards and penalties are critical for an agent's learning. While deciding the reward structure, we must carefully think about the magnitude, direction (positive or negative), and the reward frequency (every time step / based on specific milestone / etc.). Taking the same grid environment example, some ideas for reward structure can be,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The agent should receive a positive reward when it performs a successful passenger drop-off. The reward should be high in magnitude because this behaviour is highly desired.&lt;/li&gt;
&lt;li&gt;The agent should be penalized if it tries to drop off a passenger in the wrong locations.&lt;/li&gt;
&lt;li&gt;The agent should get a small negative reward for not making it to the destination after every time step. This would incentivize the agent to take faster routes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There can be more ideas for rewards like giving a reward for successful pickup and so on. &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#4-the-transition-rules"&gt;
  &lt;/a&gt;
  4. The transition rules
&lt;/h3&gt;

&lt;p&gt;The transition rules are kind of the brain of the environment. They specify the dynamics of the above discussed components (state, action, and reward). They are often represented in terms of tables (a.k.a state transition tables) which specify that,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For a given state S, if you take an action A, the new state of the environment becomes S', and the reward received is R. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class="table-wrapper-paragraph"&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;State&lt;/th&gt;
&lt;th&gt;Action&lt;/th&gt;
&lt;th&gt;Reward&lt;/th&gt;
&lt;th&gt;Probability&lt;/th&gt;
&lt;th&gt;Next State&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;S&lt;code&gt;p&lt;/code&gt;
&lt;/td&gt;
&lt;td&gt;A&lt;code&gt;q&lt;/code&gt;
&lt;/td&gt;
&lt;td&gt;R&lt;code&gt;pq&lt;/code&gt;
&lt;/td&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;td&gt;S&lt;code&gt;p'&lt;/code&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;An example row could be when the taxi's location is in the middle of grid, the passenger's location in in the bottom-right corner. The agent takes the "Move North" action, it gets a negative reward, and the next state becomes the state that represents the taxi in its new position.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; In the real-world, the state transitions may not be deterministic, i.e. they can be either.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Stochastic; which means the rules operate by probability, i.e. if you take an action, there's an X1% chance you'll end up in state S1, and Xn% chance you'd end up in a state Sn.&lt;/li&gt;
&lt;li&gt;Unknown; which means it is not known in advance what all possible states the agent can get into if it takes action A in a given state S. This might be the case when the agent is operating in the real world.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#implementing-the-environment"&gt;
  &lt;/a&gt;
  Implementing the environment
&lt;/h2&gt;

&lt;p&gt;Implementing a computer program that represents the environment can be a bit of a programming effort. Apart from deciding the specifics like the state space, transition table, reward structure, etc., we need to implement other features like creating a way to input actions into the environment and getting feedback in return. More often than not, there's also a requirement to visualize what's happening under the hood. Since the objective of this tutorial is "Introduction to Reinforcement Learning", we will skip the "how to program a Reinforcement learning environment" part and jump straight to using it. However, if you're interested, you can check the &lt;a href="https://github.com/satwikkansal/gym-dual-taxi"&gt;source code&lt;/a&gt; and follow the comments there.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#specifics-of-the-environment"&gt;
  &lt;/a&gt;
  Specifics of the environment
&lt;/h3&gt;

&lt;p&gt;We'll use a custom environment inspired by OpenAI gym's &lt;a href="https://gym.openai.com/envs/Taxi-v3/"&gt;Taxi-v3 environment&lt;/a&gt;. We have added a twist to the environment. Instead of having a single taxi and a single passenger, we'll be having two taxis and a passenger! The intention behind the mod is to observe interesting dynamics that might arise because of the presence of another taxi. This also means the state space would comprise an additional taxi location, and the action space would comprise of actions of both the taxis now.&lt;/p&gt;

&lt;p&gt;Our environment is built on OpenAI's gym library, making it a bit convenient to implement environments to evaluate Reinforcement learning algorithms. They also include some pre-packaged environment (Taxi-v3 is one of them), and their environments are a popular way to practice Reinforcement Learning and evaluate Reinforcement Learning algorithms. Feel free to check out their docs to know more about them! &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#exploring-the-environment"&gt;
  &lt;/a&gt;
  Exploring the environment
&lt;/h3&gt;

&lt;p&gt;It's time we start diving into some code and explore the specifics of the environment we'll be using for Reinforcement learning in this tutorial.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="c1"&gt;# Let's first install the custom gym module which contains the environment 
&lt;/span&gt;&lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;uninstall&lt;/span&gt; &lt;span class="n"&gt;gym&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;
&lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;git&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;git&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;github&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;com&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;satwikkansal&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;gym&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;dual&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;taxi&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;git&lt;/span&gt;&lt;span class="c1"&gt;#"egg=gym&amp;amp;subdirectory=gym/"
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;gym&lt;/span&gt;
&lt;span class="n"&gt;env&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gym&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'DualTaxi-v1'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;render&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c1"&gt;# PS: If you're using jupyter notebook and get env not registered error; you have to restart your kernel after install the custom gym package in the last step.
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="/Users/satwik/Library/Application%20Support/typora-user-images/image-20210709135319304.png" class="article-body-image-wrapper"&gt;&lt;img src="/Users/satwik/Library/Application%20Support/typora-user-images/image-20210709135319304.png" alt="image-20210709135319304"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In the snippet above, we initialize our custom &lt;code&gt;DualTaxi-v1&lt;/code&gt; environment, and rendered its current state. In the rendered output,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The yellow and red rectangles represents both taxis on the 4x4 grid&lt;/li&gt;
&lt;li&gt;R, G, B, and Y are the 4 possible pick up or drop-off locations for the passenger&lt;/li&gt;
&lt;li&gt;The character “|” represents a wall which the taxis can't cross&lt;/li&gt;
&lt;li&gt;The blue colored letter represents the pick-up location of the passenger&lt;/li&gt;
&lt;li&gt;The purple letter represents the drop-off location.&lt;/li&gt;
&lt;li&gt;Any taxi that gets the passenger aboard, would turn green in color
&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;observation_space&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_space&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;(Discrete(6144), Discrete(36))
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;You might have noticed that the only information that's printed is their discrete nature and the size of the space. The rest of the details are abstracted. This is an important point, and as you'll realize by the end of the post, our RL algorithm won't need any more information. &lt;/p&gt;

&lt;p&gt;However if you're still curious to know how the environment functions, feel free to check out the &lt;a href="https://github.com/satwikkansal/gym-dual-taxi"&gt;enviroment's code&lt;/a&gt; and follow the comments there. Another thing that you can do is peek into the state-transition table (check the code in the appendix if you're curious how to do it)&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#the-objective"&gt;
  &lt;/a&gt;
  The objective
&lt;/h3&gt;

&lt;p&gt;The objective of the environment is pick up the passenger from the blue location and drop to the violet location as fast as possible. An intelligent agent should be able to do this with consistency. Now let's see what information to we have for the environment's state space (a.k.a observation space) and action space. But before we dive into implementing that intelligent agent, let's see how a random agent would perform in this kind of enviromnet,&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;play_random&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    Function to play the episodes.
    """&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;done&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;next_action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_space&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="c1"&gt;# Trying the dumb agent
&lt;/span&gt;&lt;span class="n"&gt;print_frames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;play_random&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# check github for the code for print_frames
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--UZ9DSH9f--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/rl-1.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--UZ9DSH9f--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/rl-1.gif" alt=""&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can see the episode number at the top. In our case, an episode is the timeframe between the steps where the taxis make the first move and the step where they drop a passenger at the desired after picking up. When this happens, the episode is over, and we have to reset the environment to start all over again. &lt;/p&gt;

&lt;p&gt;You can see different actions at the bottom, and how the state keeps changing and the reward the agent gets after every action.&lt;/p&gt;

&lt;p&gt;As you can might have realized, these taxis are taking a while to finish even a single episode. So our random approach is very dumb for sure. Our intelligent agent definitely will have to perform this task better.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#introducing-qlearning"&gt;
  &lt;/a&gt;
  Introducing Q-learning
&lt;/h2&gt;

&lt;p&gt;Q-learning is one among several Reinforcement Learning algorithms. The reason we are picking Q-learning is because it is simple and straightforward to understand. We'll use Q-learning to make our agent somewhat intelligent. &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#intuition-behind-qlearning"&gt;
  &lt;/a&gt;
  Intuition behind Q-learning
&lt;/h3&gt;

&lt;p&gt;The way Q-learning works, is by storing what we call Q-values for every state-action combination. The Q-value represents the "quality" of an action taken from that state. Of course, the initial q-values are just random numbers, but the goal is to iteratively update them in the right direction. After enough iterations, these Q-values can start to converge (i.e. the size of update in upcoming iterations gets so small that it has a negligible impact). Once that is the case, we can safely say that, &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For a given state, the higher the Q-value for the state-action pair, the higher would be the expected long term reward of taking that particular action. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So long story short, the "developing intelligence" part of Q-learning lies in how the Q-values after agent's ineteraction with the environment, which requires discussion of two key concepts,&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#1-the-bellman-equation"&gt;
  &lt;/a&gt;
  1. The bellman equation
&lt;/h3&gt;

&lt;p&gt;Attached below is the bellman equation in the context of updating Q-values, this is the equation we use to update Q-values after agent's interaction with the environment.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://postimg.cc/image/4ghnvcjgn/"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--IFO521r_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://s31.postimg.cc/jp7l94d57/q_learning_equation.png" alt="q_learning_equation.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The Q-value of a state-action pair is the sum of the instant reward and the discounted future reward (of the resulting state). Where,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;st represents the state at time &lt;code&gt;t&lt;/code&gt;
&lt;/li&gt;
&lt;li&gt;at represents action taken at time &lt;code&gt;t&lt;/code&gt; (the agent was in state st at this point in time)&lt;/li&gt;
&lt;li&gt;rt is the reward received by performing the action at in the state st.&lt;/li&gt;
&lt;li&gt;st+1 is the next state that our agent will transition to after performing the action at in the state st.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The discount factor γ(gamma) determines how much importance we want to give to future rewards. A high value for the discount factor (close to &lt;strong&gt;1&lt;/strong&gt;) captures the long-term effective award, whereas, a discount factor of &lt;strong&gt;0&lt;/strong&gt; makes our agent consider only immediate reward, hence making it greedy. &lt;/p&gt;

&lt;p&gt;The $\alpha$ (alpha) is our learning rate. Just like in supervised learning settings, alpha here is representative of the extent to which our Q-values are being updated in every iteration.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#2-epsilon-greedy-method"&gt;
  &lt;/a&gt;
  2. Epsilon greedy method
&lt;/h3&gt;

&lt;p&gt;While we keep updating Q-values every iteration, there's an important choice the agent has to make while taking an action. The choice it faces is whether to "explore" or "exploit"?&lt;/p&gt;

&lt;p&gt;So with time, the Q-values get better at representing the quality of a state-action pair. But to reach that goal, the agent has to try different actions (how can it know if a state-action pair is good if it hasn't tried it?). So it becomes critical for agent to "explore" i.e. take random actions to gather more knowledge about the environment. &lt;/p&gt;

&lt;p&gt;But there's a problem if the agent only explores. Exploration can only get the agent so far. Imagine that the environment agent is in is like a maze. Exploration can put agent on unknown path and give feedback to make q-values more valuable. But if the agent is only taking random actions at every step, it is going to have a hard time reaching the end state of the maze. That's why it is also important to "exploit". The agent should also consider using what it has already learned (i.e. the Q-values) to decided what action to take next.&lt;/p&gt;

&lt;p&gt;That's all to say, the agent needs to balance exploitation and exploration. There are many ways to do this. Once common way to do it with Q-learning is to have a value called "epsilon", which denotes the probability by which the agent will explore. A higher epsilon value results in interactions with more penalties (on average) which is obvious because we are exploring and making random decisions. We can add more sophistication to this method, and its a common practice that people start with a high epsilon value, and keep reducing it as time progresses. This is called epsilon decay. The intution is that as we keep adding more knowledge to Q-values through exploration, the exploitation becomes more trustworthy which in turn means we can explore at a lower rate. &lt;/p&gt;

&lt;p&gt;Note: There's usually some confusion around if epsilon represents probability of "exploration" or "exploitation". You'll find it used both ways on the internet and other resources. I find the first way more comfortable as it fits the terminology "epsilon decay". If you see it other way around, don't get confused, the concept is still the same. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#using-qlearning-for-our-environment"&gt;
  &lt;/a&gt;
  Using Q-learning for our environment
&lt;/h2&gt;

&lt;p&gt;Okay, enough background about Q-learning. Now how do we apply it to our &lt;code&gt;DualTaxi-v1&lt;/code&gt; environment? Because of the fact that we have two taxis in our environment, we can do it in a couple of ways,&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#1-cooperative-approach"&gt;
  &lt;/a&gt;
  1. Cooperative approach
&lt;/h3&gt;

&lt;p&gt;In this approach we can assume that there's a single agent with a single Q-table that controls both the taxis (think of it like a taxi agency). The overall goal of this agent would be to maximize the reward these taxis receive combined.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#2-competitive-approach"&gt;
  &lt;/a&gt;
  2. Competitive approach
&lt;/h3&gt;

&lt;p&gt;In this approach we can train two agents (one for each taxi). Every agent has its own Q-table and gets its own reward. Of course, the next state of the environment still depends on the actions of both the agents. This creates an interesting dynamic where each taxi would be trained to maximize its own individual rewards.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#cooperative-approach-in-action"&gt;
  &lt;/a&gt;
  Cooperative approach in action
&lt;/h2&gt;

&lt;p&gt;Before we see the code, let us specify the steps we'd have to take,&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Initialize the Q-table (size of the Q-table is state_space_size x action_space_size) by all zeros.&lt;/li&gt;
&lt;li&gt;Decide between exploration and exploitation based on the epsilon value.&lt;/li&gt;
&lt;li&gt;Exploration: For each state, select any one among all possible actions for the current state (S).&lt;/li&gt;
&lt;li&gt;Exploitation: For all possible actions from the state (S') select the one with the highest Q-value.&lt;/li&gt;
&lt;li&gt;Travel to the next state (S') as a result of that action (a).&lt;/li&gt;
&lt;li&gt;Update Q-table values using the update equation.&lt;/li&gt;
&lt;li&gt;If the episode is over (i.e. goal state is reached), reset the environment for next iteration.&lt;/li&gt;
&lt;li&gt;Keep repeating steps 2 to 7 until we start seeing decent results in agent's performance.
&lt;/li&gt;
&lt;/ol&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;deque&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt; 


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;bellman_update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    Function to perform q-value update as per bellman equation.
    """&lt;/span&gt;
    &lt;span class="c1"&gt;# Get the old q_value
&lt;/span&gt;    &lt;span class="n"&gt;old_q_value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="c1"&gt;# Find the maximum q_value for the actions in next state
&lt;/span&gt;    &lt;span class="n"&gt;next_max&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="c1"&gt;# Calculate the new q_value as per the equation
&lt;/span&gt;    &lt;span class="n"&gt;new_q_value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;old_q_value&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;gamma&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;next_max&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# Finally, update the q_value
&lt;/span&gt;    &lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new_q_value&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    Selects an action according to epsilon greedy method, performs it, and the calls bellman update
    to update the Q-values.
    """&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_space&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;info&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;bellman_update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;info&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log_every&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;running_metrics_len&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;evaluate_every&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evaluate_trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    This is the training logic. It takes input as a q-table, the environment.
    The training is done for num_episodes episodes. The results are logged preiodcially.

    We also record some useful metrics like average reward in last 50k timesteps, the average
    length of last 50 episodes and so on. These are helpful to gauge how the algorithm is performing
    over time.

    After every few episodes of training. We run evaluation routine, where we just "exploit" i.e. rely on 
    the q-table so far and see how well the agent has learned so far. Over the time, the results should get
    better until the q-table starts converging, after which, there's negligible change in the results.
    """&lt;/span&gt;
    &lt;span class="n"&gt;rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;deque&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;running_metrics_len&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;episode_lengths&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;deque&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;total_timesteps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

    &lt;span class="n"&gt;metrics&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;num_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;done&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;

        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;info&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;total_timesteps&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;total_timesteps&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;log_every&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;rd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;avg_ep_len&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;episode_lengths&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;zeroes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill_percent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calculate_q_table_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'Current Episode: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'Reward distribution: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;rd&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'Last 10 episode lengths (avg: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;avg_ep_len&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;)'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;zeroes&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; Q table zeroes, &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;fill_percent&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; percent filled'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


        &lt;span class="n"&gt;episode_lengths&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;evaluate_every&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'==='&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;"Running evaluation after &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; episodes"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;finish_percent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;avg_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;penalties&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;evaluate_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evaluate_trials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'==='&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;rd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;avg_ep_len&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;episode_lengths&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;zeroes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill_percent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calculate_q_table_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                    &lt;span class="s"&gt;'train_reward_distribution'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;rd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'train_ep_len'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;avg_ep_len&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'fill_percent'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;fill_percent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'test_finish_percent'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;finish_percent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'test_ep_len'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;avg_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'test_penalties'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;penalties&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Training finished."&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;calculate_q_table_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    This function counts what perecentage of cells in the q-table are non-zero.
    Note: There are certain state-action combinations that are illegal, so the table might never 
    be full.
    """&lt;/span&gt;
    &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;
    &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;cell&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;cell&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;fill_percent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;100.0&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill_percent&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    The routine to evaluate an agent. It simply exploits the q-table and records the performance metrics.
    """&lt;/span&gt;
    &lt;span class="n"&gt;total_epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_wins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;wins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;done&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;next_action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;num_penalties&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;wins&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;total_epochs&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;
        &lt;span class="n"&gt;total_penalties&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;num_penalties&lt;/span&gt;
        &lt;span class="n"&gt;total_wins&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;wins&lt;/span&gt;

    &lt;span class="n"&gt;average_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;complete_percent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_evaluation_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_wins&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;print_evaluation_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;average_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;average_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_wins&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;complete_percent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average_penalties&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;print_evaluation_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;average_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_wins&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Evaluation results after {} trials"&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Average time steps taken: {}"&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;average_time&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Average number of penalties incurred: {}"&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;average_penalties&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;"Had &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;total_wins&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; wins in &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; episodes"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_evaluation_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_wins&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;average_time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;total_epochs&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;average_penalties&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;total_penalties&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;complete_percent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;total_wins&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_trials&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;100.0&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;average_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;complete_percent&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# The hyper-parameters of Q-learning
&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt; &lt;span class="c1"&gt;# learning rate
&lt;/span&gt;&lt;span class="n"&gt;gamma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.7&lt;/span&gt; &lt;span class="c1"&gt;# discout factor
&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;
&lt;span class="n"&gt;env&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gym&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'DualTaxi-v1'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;num_episodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;50000&lt;/span&gt;

&lt;span class="c1"&gt;# Initialize a q-table full of zeroes
&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;observation_space&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_space&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# Get back trained q-table and metrics
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;Total encoded states are 6144
==============================
Running evaluation after 0 episodes
Evaluation results after 200 trials
Average time steps taken: 1500.0
Average number of penalties incurred: 1500.0
Had 0 wins in 200 episodes
==============================

----------------------------
Skipping intermediate output
----------------------------


==============================
Running evaluation after 49000 episodes
Evaluation results after 200 trials
Average time steps taken: 210.315
Average number of penalties incurred: 208.585
Had 173 wins in 200 episodes
==============================
Current Episode: 49404
Reward distribution: Counter({-3: 15343, -12: 12055, -4: 11018, -11: 4143, -20: 3906, -30: 1266, -2: 1260, 99: 699, -10: 185, 90: 125})
Last 10 episode lengths (avg: 63.0)
48388 Q table zeroes, 78.12319155092592 percent filled
Training finished.
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;I have skipped the intermediate output on purpose, you can check &lt;a href="https://pastebin.com/XHJLatiX"&gt;this pastebin&lt;/a&gt; if you're interested in full output. &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#competitive-approach"&gt;
  &lt;/a&gt;
  Competitive Approach
&lt;/h3&gt;

&lt;p&gt;The steps for this are similar to the cooperative approach, with the differnce that now we have multiple Q-tables to update.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Initialize the Q-table 1 and 2 for both the agents by all zeros. The size of each Q-table is &lt;code&gt;state_space_size x sqrt(action_space_size)&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Decide between exploration and exploitation based on the epsilon value.&lt;/li&gt;
&lt;li&gt;Exploration: For each state, select any one among all possible actions for the current state (S).&lt;/li&gt;
&lt;li&gt;Exploitation: For all possible actions from the state (S') select the one with the highest Q-value in the Q-tables of respective agents.&lt;/li&gt;
&lt;li&gt;Transition to the next state (S') as a result of that combined action (a1, a2).&lt;/li&gt;
&lt;li&gt;Update Q-table values for both the agents using the update equation and respective rewards &amp;amp; actions.&lt;/li&gt;
&lt;li&gt;If the episode is over (i.e. goal state is reached), reset the environment for next iteration.&lt;/li&gt;
&lt;li&gt;Keep repeating steps 2 to 7 until we start seeing decent results in the performance.
&lt;/li&gt;
&lt;/ol&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update_multi_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    Same as update method discussed in the last section, just modified for two independent q-tables.
    """&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_space&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;action1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode_action&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;action1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;action2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode_action&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;info&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;reward1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;
    &lt;span class="n"&gt;bellman_update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;bellman_update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;info&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train_multi_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log_every&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;running_metrics_len&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;evaluate_every&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evaluate_trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    Same as train method discussed in the last section, just modified for two independent q-tables.
    """&lt;/span&gt;
    &lt;span class="n"&gt;rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;deque&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;running_metrics_len&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;episode_lengths&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;deque&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;total_timesteps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

    &lt;span class="n"&gt;metrics&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;done&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;

        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# Modification here
&lt;/span&gt;            &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;info&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;update_multi_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;total_timesteps&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;total_timesteps&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;log_every&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;rd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;avg_ep_len&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;episode_lengths&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;zeroes1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill_percent1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calculate_q_table_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;zeroes2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill_percent2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calculate_q_table_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'Current Episode: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'Reward distribution: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;rd&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'Last 10 episode lengths (avg: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;avg_ep_len&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;)'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;zeroes1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; Q table 1 zeroes, &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;fill_percent1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; percent filled'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;zeroes2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; Q table 2 zeroes, &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;fill_percent2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; percent filled'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


        &lt;span class="n"&gt;episode_lengths&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;evaluate_every&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'==='&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;"Running evaluation after &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; episodes"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;finish_percent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;avg_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;penalties&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;evaluate_multi_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evaluate_trials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'==='&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;rd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;avg_ep_len&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;episode_lengths&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;zeroes1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill_percent1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calculate_q_table_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;zeroes2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill_percent2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calculate_q_table_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                    &lt;span class="s"&gt;'train_reward_distribution'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;rd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'train_ep_len'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;avg_ep_len&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'fill_percent1'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;fill_percent1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'fill_percent2'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;fill_percent2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'test_finish_percent'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;finish_percent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'test_ep_len'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;avg_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'test_penalties'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;penalties&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Training finished.&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate_multi_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    Same as evaluate method discussed in last section, just modified for two independent q-tables.
    """&lt;/span&gt;
    &lt;span class="n"&gt;total_epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_wins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;wins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;done&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# Modification here
&lt;/span&gt;            &lt;span class="n"&gt;next_action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode_action&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
                &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

            &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;num_penalties&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;wins&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

            &lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;total_epochs&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;
        &lt;span class="n"&gt;total_penalties&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;num_penalties&lt;/span&gt;
        &lt;span class="n"&gt;total_wins&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;wins&lt;/span&gt;

    &lt;span class="n"&gt;average_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;complete_percent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_evaluation_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_wins&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;print_evaluation_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;average_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;average_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_wins&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;complete_percent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average_penalties&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="c1"&gt;# The hyperparameter of Q-learning
&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;
&lt;span class="n"&gt;gamma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.8&lt;/span&gt;
&lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;

&lt;span class="n"&gt;env_c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gym&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'DualTaxi-v1'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;competitive&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;num_episodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;50000&lt;/span&gt;
&lt;span class="n"&gt;q_table1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;env_c&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;observation_space&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;env_c&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_space&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;
&lt;span class="n"&gt;q_table2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;env_c&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;observation_space&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;env_c&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_space&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;
&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;metrics_c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_multi_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env_c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;Total encoded states are 6144
==============================
Running evaluation after 0 episodes
Evaluation results after 200 trials
Average time steps taken: 1500.0
Average number of penalties incurred: 1500.0
Had 0 wins in 200 episodes
==============================

----------------------------
Skipping intermediate output
----------------------------


==============================
Running evaluation after 48000 episodes
Evaluation results after 200 trials
Average time steps taken: 323.39
Average number of penalties incurred: 322.44
Had 158 wins in 200 episodes
==============================
Current Episode: 48445
Reward distribution: Counter({-12: 13993, -3: 12754, -4: 11561, -20: 3995, -11: 3972, -30: 1907, -10: 649, -2: 524, 90: 476, 99: 169})
Last 10 episode lengths (avg: 78.08)
8064 Q table 1 zeroes, 78.125 percent filled
8064 Q table 2 zeroes, 78.125 percent filled
==============================
Running evaluation after 49000 episodes
Evaluation results after 200 trials
Average time steps taken: 434.975
Average number of penalties incurred: 434.115
Had 143 wins in 200 episodes
==============================
Current Episode: 49063
Reward distribution: Counter({-3: 13928, -12: 13605, -4: 10286, -11: 4542, -20: 3917, -30: 1874, -10: 665, -2: 575, 90: 433, 99: 175})
Last 10 episode lengths (avg: 75.1)
8064 Q table 1 zeroes, 78.125 percent filled
8064 Q table 2 zeroes, 78.125 percent filled
Current Episode: 49706
Reward distribution: Counter({-12: 13870, -3: 13169, -4: 11054, -11: 4251, -20: 3985, -30: 1810, -10: 704, -2: 529, 90: 436, 99: 192})
Last 10 episode lengths (avg: 76.12)
8064 Q table 1 zeroes, 78.125 percent filled
8064 Q table 2 zeroes, 78.125 percent filled
Training finished.
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;I have skipped the intermediate output on purpose, you can check &lt;a href="https://pastebin.com/ZPKZtjK1"&gt;this pastebin&lt;/a&gt; if you're interested in full output. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#evaluating-the-performance"&gt;
  &lt;/a&gt;
  Evaluating the performance
&lt;/h2&gt;

&lt;p&gt;If you observed the code carefully, the train functions returned q-tables as well as some metrics. We can use the q-table now for taking agent's actions, and see how intelligent it has become. Also, we'll try to plot these metrics to visualize how the training progressed.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;  
&lt;span class="c1"&gt;# import seaborn as plt
&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    Plotting various metrics over the number of episodes.
    """&lt;/span&gt;
    &lt;span class="n"&gt;ep_nums&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;series&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;ep_num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;metric_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;metric_val&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metric_val&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
                &lt;span class="n"&gt;series&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;metric_name&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metric_val&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;m_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;series&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ep_nums&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'Number of episodes'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;play&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;done&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;next_action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;play_multi&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    Capture frames by playing using the two q-tables.
    """&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;done&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;next_action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode_action&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
                &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
            &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="n"&gt;plot_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--_d7PFUvq--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_train_ep_len.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--_d7PFUvq--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_train_ep_len.png" alt="png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--4vUu2Fql--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_test_ep_len.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--4vUu2Fql--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_test_ep_len.png" alt="png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--L2zrops0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_fill_percent.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--L2zrops0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_fill_percent.png" alt="png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--FTIadJ3I--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_test_finish_percent.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--FTIadJ3I--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_test_finish_percent.png" alt="png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--imYco__r--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_test_penalties.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--imYco__r--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_test_penalties.png" alt="png"&gt;&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="n"&gt;frames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;play&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;print_frames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;frames&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--ismCs_9W--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/rl-2-cooperative.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--ismCs_9W--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/rl-2-cooperative.gif" alt=""&gt;&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="n"&gt;plot_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metrics_c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--Vgf_MIS0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_train_ep_len.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Vgf_MIS0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_train_ep_len.png" alt="png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--lWTBWcnl--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_test_ep_len.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--lWTBWcnl--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_test_ep_len.png" alt="png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--ZK6Cqvu5--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_fill_percent1.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--ZK6Cqvu5--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_fill_percent1.png" alt="png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--X64ubyvv--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_fill_percent2.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--X64ubyvv--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_fill_percent2.png" alt="png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--bAFiAqmn--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_test_finish_percent.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--bAFiAqmn--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_test_finish_percent.png" alt="png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--85gFAzYI--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_test_penalties.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--85gFAzYI--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_test_penalties.png" alt="png"&gt;&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="n"&gt;print_frames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;play_multi&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env_c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--PchCKrpF--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/rl-4-competitive.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--PchCKrpF--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/rl-4-competitive.gif" alt=""&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#some-observations"&gt;
  &lt;/a&gt;
  Some observations
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;While Q-learning agent commits errors initially during exploration but once it has explored enough (seen most of the states), it starts to act wisely.&lt;/li&gt;
&lt;li&gt;Both the approaches did fairly well. However, in relative comparison, the cooperative approach seem to perform better. The plots of competitive approach are more volatile. &lt;/li&gt;
&lt;li&gt;It took around 2000 episodes for agents to explore most of the possible state-action pairs. Note that not state-action pairs are feasible because some states aren't legal (for example, states where both the taxis are at same location aren't possible).&lt;/li&gt;
&lt;li&gt;As the training progressed the number of penalties reduced. They didn't reduce completely because of the epsilon (we're still exploring based on the epsilon value during training). &lt;/li&gt;
&lt;li&gt;The episode length kept decreasing, which means the taxis were able to pickup and drop the passenger faster because of the new learned knowledge in q-tables.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So to summarize, the agent is able to get around the walls, pick the passengers, take less penalties, and reach the destination timely. And the fact that the code where q-learning update happens is merely around 20-30 lines of Python code makes it even more impressive.&lt;/p&gt;

&lt;p&gt;From what we've discussed so far in the post, it's likely that you have a fair bit of intution about how Reinforcement Learning works. Now in the last few sections we will dip our toes in some broader level ideas and concepts that might be relevant to you when exploring Reinforcement Learning further. Let's start with the common challenges of Reinforcement Learning first,&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#common-challenges-while-applying-reinforcement-learning"&gt;
  &lt;/a&gt;
  Common challenges while applying Reinforcement learning
&lt;/h2&gt;

&lt;h3&gt;
  &lt;a href="#finiding-the-right-hyperparameters"&gt;
  &lt;/a&gt;
  Finiding the right Hyperparameters
&lt;/h3&gt;

&lt;p&gt;You might be wondering how did I decide values of alpha, gamma, and epsilon. In the above program, it was mostly based on intuition from my past experience and some "hit and trial". This goes a long way, but there are also some techniques to come up with good values. The process in itself is sometimes referred to as Hyperparamter tuning or Hyperparameter optimization.&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#tuning-the-hyperparameters"&gt;
  &lt;/a&gt;
  Tuning the hyperparameters
&lt;/h4&gt;

&lt;p&gt;A simple way to programmatically come up with the best set of values of the hyperparameter is to create a comprehensive search function that selects the parameters that would result in best agent performance. A more sophisticated way to get the right combination of hyperparameter values would be to use Genetic Algorithms. Also, it is a common practice to make these parameters dynamic instead of fixed values. For example, in our case, all of the three hyperparmeters can be configured to decrease over time because as the agent continues to learn, it builds up more resilient priors.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#choosing-the-right-algorithms"&gt;
  &lt;/a&gt;
  Choosing the right algorithms
&lt;/h3&gt;

&lt;p&gt;Q-learning is just one of the many Reinformcement Learning algorithms out there. There are multiple ways to classify Reinforcement Learning algorithms. The selection depends on various factors including the nature of the environment. For example, if the state space of action space is continuous instead of discrete (imagine that the environment now expects continuous degree values instead of discrete north / east / etc directions as actions, and the state space consists of more precise lat/lng location of taxis instead of grid coordinates), tabular Q-learning can't work. There are hacks to get around continuous spaces (like bucketing their range and making it discrete as a result), but these hacks fail too if the state space and action space gets too large. In those cases, it is preferred to use more generic algorithms, usually the ones that involve approximators like Neural Networks.&lt;/p&gt;

&lt;p&gt;More often than not, in practice, the agent is trained with multiple algorithms initially to decide which algorithm would fit the best.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#reward-structure"&gt;
  &lt;/a&gt;
  Reward Structure
&lt;/h3&gt;

&lt;p&gt;It is important to think strategically about the rewards to be given to the agent. If the rewards are too sparse, the agent might have difficulty in learning. Poorly structured rewards can also lead to cases of non-convergence and situations in which agent gets stuck in local minima. For example, let's say the environment gave +1 reward for successfully picking up passenger, and no penalty for dropping the passenger. So it might happen, that the agent might end up repeatedly picking up and dropping a passenger to maximise it rewards. Similary, if we there was very high negative reward for picking up passenger, agent would eventually learn to not pick a passenger at all, and hence would never finish successfully.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#the-challenges-of-real-world-environments"&gt;
  &lt;/a&gt;
  The challenges of real world environments
&lt;/h3&gt;

&lt;p&gt;Training an agent on an openAI gym environment is realtively easy because you get a lot of things out of the box. The real world, however, is a bit more unorganised. We sensors to ingest environment information and mechanism to translate it into something that can be fed to a Machine Learning algorithm. So such systems involve a lots of techniques overall aside from the learning algorithm. As a simple example, consider a general Reinforcement Learning agent that is being trained to play ATARI games. The information this agent needs to be passed is pixels on the screen. So we might have to use deep learning techniques (like Convolutional Neural Networks) to interpret the pixels on the screen and extract information out of the game (like scores) to enable the agent to interpret the game.&lt;/p&gt;

&lt;p&gt;There's also a challenge of sample efficiency. Since the state spaces and action spaces might be continuous and have big ranges, it becomes critical to achieve a decent sample efficiency that makes Reinforcement Learning feasible. If the algorithm needs high number of episodes (high enough that we cannot make it to produce results in reasonable amount of time), then Reinforcement Learning becomes impractical. &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#respecting-the-theoretical-boundaries"&gt;
  &lt;/a&gt;
  Respecting the theoretical boundaries
&lt;/h3&gt;

&lt;p&gt;It is easy to sometimes get carried away and see Reinforcement Learning to be the solution of most problems. It helps to have a theoretical understanding of how these algorithm works and fundamental concepts like &lt;a href="https://en.wikipedia.org/wiki/Markov_decision_process"&gt;Markov Decision Processes&lt;/a&gt; and awareness of the state of the art algorithms to have a better intution about what can and what can't be solved using present-day Reinforcement Learning algorithms.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#wrapping-up"&gt;
  &lt;/a&gt;
  Wrapping up
&lt;/h2&gt;

&lt;p&gt;In this tutorial, we began with understanding Reinforcement Learning with the help of real-world analogies. Then  we learned about some fundamental conepts like state, action, and rewards. Next, we went over the process of framing a problem such that we can traing an agent through Reinforcement Learning algorithms to solve it.&lt;/p&gt;

&lt;p&gt;We took Self-driving taxi as our reference problem for the rest of the tutorial. We then used OpenAL's gym module in python to provide us with a related environment, where we can develop our agent and evaluate it. Then we observed how terrible our agent was without using any algorithm to play the game, so we went ahead to implement the Q-learning algorithm from scratch. &lt;/p&gt;

&lt;p&gt;We then introduced Q-learning, and went over the steps to use it for our environment. We came up with two approaches (cooperative and competitive). We then evaluated the Q-learning results, and saw how the agent's performance improved significantly after Q-learning.&lt;/p&gt;

&lt;p&gt;As mentioned in beginning, Reinforcement learning is not just limited to openAI gym environments and games. It is also used for managing portfolio and finances, for making humanoid robots, for manufacturing and inventory management, to develop general AI agents (agents that can perform multiple things with a single algorithm, like same agent playing multiple Atari games).&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#appendix"&gt;
  &lt;/a&gt;
  Appendix
&lt;/h2&gt;

&lt;h3&gt;
  &lt;a href="#further-reading"&gt;
  &lt;/a&gt;
  Further reading
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;"Reinforcement Learning: An Introduction" Book by Andrew Barto and Richard S. Sutton. Most popular book about Reinforcement Learning out there. Highly recommended if you're planning to dive deep into the field. &lt;/li&gt;
&lt;li&gt;Lectures by David Silver (also available on &lt;a href="https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;amp;list=PLqYmG7hTraZBiG_XpjnPrSNw-1XQaM_gB&amp;amp;index=3"&gt;YouTube&lt;/a&gt;). Another great resource if you're more into learning from videos than books.&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0"&gt;Tutorial series on medium&lt;/a&gt; on Reinforcement learning using Tensorflow by Arthur Juliani.&lt;/li&gt;
&lt;li&gt;Some interesting topics related to Multi Agent environments,

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.researchgate.net/publication/2933305_Friend-or-Foe_Q-learning_in_General-Sum_Games"&gt;Friend and foe Q-learning in general-sum games&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Game theory concepts like&lt;/li&gt;
&lt;li&gt;Strictly dominant strategies&lt;/li&gt;
&lt;li&gt;Nash equilibrium&lt;/li&gt;
&lt;li&gt;Shapely values for reward distribution&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#visualising-the-transition-table-of-our-dual-taxi-enviroment"&gt;
  &lt;/a&gt;
  Visualising the transition table of our dual taxi enviroment
&lt;/h3&gt;

&lt;p&gt;The following is an attempt to visualize the internal tranistion table of our environment in a human readable way. The source of this information is the &lt;code&gt;env.P&lt;/code&gt; object which contains a mapping of the form&lt;/p&gt;

&lt;p&gt;&lt;code&gt;current_state : action_taken: [(transition_prob, next_state, reward, done)]&lt;/code&gt;, this is all the info we need to simulate the environment and this is what we can use to create the transition table.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt; &lt;span class="c1"&gt;# First let's take a peek at this object
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;{0: {
    0: [(1.0, 0, -30, False)],
  1: [(1.0, 1536, -0.5, True)],
  2: [(1.0, 1560, -0.5, True)],
  3: [(1.0, 1536, -0.5, True)],
  4: [(1.0, 1536, -0.5, True)],
  5: [(1.0, 1536, -0.5, True)],
  6: [(1.0, 96, -0.5, True)],
  7: [(1.0, 0, -30, False)],
  8: [(1.0, 24, -0.5, True)],
  9: [(1.0, 0, -30, False)],
  10: [(1.0, 0, -30, False)],
  11: [(1.0, 0, -30, False)],
  12: [(1.0, 480, -0.5, True)],
  13: [(1.0, 384, -0.5, True)],
  14: [(1.0, 0, -30, False)],
  15: [(1.0, 384, -0.5, True)],
  16: [(1.0, 384, -0.5, True)],
  17: [(1.0, 384, -0.5, True)],
  18: [(1.0, 96, -0.5, True)],
  19: [(1.0, 0, -30, False)],
  20: [(1.0, 24, -0.5, True)],
  21: [(1.0, 0, -30, False)],
  22: [(1.0, 0, -30, False)],
  23: [(1.0, 0, -30, False)],
  24: [(1.0, 96, -0.5, True)],
  25: [(1.0, 0, -30, False)],
  26: [(1.0, 24, -0.5, True)],
  27: [(1.0, 0, -30, False)],
  28: [(1.0, 0, -30, False)],
  29: [(1.0, 0, -30, False)],
  30: [(1.0, 96, -0.5, True)],
  31: [(1.0, 0, -30, False)],
  32: [(1.0, 24, -0.5, True)],
  33: [(1.0, 0, -30, False)],
  34: [(1.0, 0, -30, False)],
  35: [(1.0, 0, -30, False)]},
 1: {0: [(1.0, 1, -30, False)],
  1: [(1.0, 1537, -0.5, True)],
  2: [(1.0, 1561, -0.5, True)],
  3: [(1.0, 1537, -0.5, True)],
  4: [(1.0, 1537, -0.5, True)],
  5: [(1.0, 1537, -0.5, True)],
  6: [(1.0, 97, -0.5, True)],
  7: [(1.0, 1, -30, False)],
  8: [(1.0, 25, -0.5, True)],
  9: [(1.0, 1, -30, False)],
  10: [(1.0, 1, -30, False)],
  11: [(1.0, 1, -30, False)],
  12: [(1.0, 481, -0.5, True)],
  13: [(1.0, 385, -0.5, True)],
  14: [(1.0, 1, -30, False)],
  15: [(1.0, 385, -0.5, True)],
  16: [(1.0, 385, -0.5, True)],
  17: [(1.0, 385, -0.5, True)],
  18: [(1.0, 97, -0.5, True)],
  19: [(1.0, 1, -30, False)],
  20: [(1.0, 25, -0.5, True)],
  21: [(1.0, 1, -30, False)],
  22: [(1.0, 1, -30, False)],
  23: [(1.0, 1, -30, False)],
  24: [(1.0, 97, -0.5, True)],
  25: [(1.0, 1, -30, False)],
  26: [(1.0, 25, -0.5, True)],
  27: [(1.0, 1, -30, False)],
  28: [(1.0, 1, -30, False)],
  29: [(1.0, 1, -30, False)],
  30: [(1.0, 97, -0.5, True)],
  31: [(1.0, 1, -30, False)],
  32: [(1.0, 25, -0.5, True)],
  33: [(1.0, 1, -30, False)],
  34: [(1.0, 1, -30, False)],
  35: [(1.0, 1, -30, False)]},
# omitting the whole output because it's very long! 
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Now, let's put some code together to convert this information in more readable tabular form.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="err"&gt;!&lt;/span&gt; &lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;
&lt;span class="n"&gt;table&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

&lt;span class="n"&gt;env_c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gym&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'DualTaxi-v1'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;competitive&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;state_to_human_readable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;passenger_loc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'R'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'G'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'B'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'Y'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'T1'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'T2'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
    &lt;span class="n"&gt;destination&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'R'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'G'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'B'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'Y'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'Taxi 1: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;, Taxi 2: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;, Pass: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;passenger_loc&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;, Dest: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;destination&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;action_to_human_readable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;actions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'NSEWPD'&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;actions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="n"&gt;actions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;state_num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;transition_info&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;env_c&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;possible_transitions&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;transition_info&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;transition_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;possible_transitions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
            &lt;span class="s"&gt;'State'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;state_to_human_readable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state_num&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
            &lt;span class="s"&gt;'Action'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;action_to_human_readable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode_action&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
            &lt;span class="s"&gt;'Probablity'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;transition_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s"&gt;'Next State'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;state_to_human_readable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
            &lt;span class="s"&gt;'Reward'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s"&gt;'Is over'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;div class="table-wrapper-paragraph"&gt;&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;Action&lt;/th&gt;
      &lt;th&gt;Probablity&lt;/th&gt;
      &lt;th&gt;Next State&lt;/th&gt;
      &lt;th&gt;Reward&lt;/th&gt;
      &lt;th&gt;Is over&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Taxi 1: (0, 0), Taxi 2: (0, 0), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(N, N)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (0, 0), Taxi 2: (0, 0), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(-15, -15)&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Taxi 1: (0, 0), Taxi 2: (0, 0), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(N, S)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (1, 0), Taxi 2: (0, 0), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(-0.5, 0)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Taxi 1: (0, 0), Taxi 2: (0, 0), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(N, E)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (1, 0), Taxi 2: (0, 1), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(-0.5, 0)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Taxi 1: (0, 0), Taxi 2: (0, 0), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(N, W)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (1, 0), Taxi 2: (0, 0), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(-0.5, 0)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Taxi 1: (0, 0), Taxi 2: (0, 0), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(N, P)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (1, 0), Taxi 2: (0, 0), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(-0.5, 0)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;221179&lt;/th&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (3, 3), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(D, S)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (2, 3), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(-0.5, 0)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;221180&lt;/th&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (3, 3), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(D, E)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (3, 3), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(-15, -15)&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;221181&lt;/th&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (3, 3), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(D, W)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (3, 2), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(-0.5, 0)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;221182&lt;/th&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (3, 3), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(D, P)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (3, 3), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(-15, -15)&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;221183&lt;/th&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (3, 3), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(D, D)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (3, 3), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(-15, -15)&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;221184 rows × 6 columns&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#bloopers"&gt;
  &lt;/a&gt;
  Bloopers
&lt;/h3&gt;

&lt;p&gt;In retrospect, the hardest part of writing this post was to get the dual-taxi-environment working. There were so many moments like below,&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--9hQk2Ugx--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/rl-3-blooper.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--9hQk2Ugx--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/rl-3-blooper.gif" alt=""&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It took a lot of trial and errors (tweaking rewards, updating rules for situations like collision, reducing state space) to get to a stage where the solutions for competitive set up were converging. The feeling when the solution converges for the first time is very cool. So if you have some free time, I'd recommend you to hack up an environment yourself (the first time I tried q-learning was with a snake-apple game I developed using pygame), and try to solve it with Reinforcement Learning. Trust me, you'll be humbled and learn lots of interesting things along the way! &lt;/p&gt;

</description>
      <category>python</category>
      <category>machinelearning</category>
      <category>reinforcementlearning</category>
      <category>openai</category>
    </item>
    <item>
      <title>How to Build a Stock Trading Bot with Python</title>
      <author>Saji Wang</author>
      <pubDate>Fri, 09 Jul 2021 14:21:16 +0000</pubDate>
      <link>https://dev.to/codesphere/how-to-build-a-stock-trading-bot-with-python-b1</link>
      <guid>https://dev.to/codesphere/how-to-build-a-stock-trading-bot-with-python-b1</guid>
      <description>&lt;p&gt;Earlier this week, we explored how code has drastically changed financial markets through the use of autonomous trading algorithms. Surprisingly, building your own trading bot is actually not that difficult! &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In this tutorial, we're going to be using Python to build our own trading bot.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Keep in mind that this tutorial is not about how to make billions off of your trading bot. If I had an algorithm that sophisticated I probably wouldn't be giving it away. Rather, I'm going to show you how you can read market data, buy and sell stocks, and program the logic of your trading algorithm, all with some relatively simple Python code.&lt;/p&gt;

&lt;p&gt;And of course:&lt;br&gt;
&lt;em&gt;This article is for information purposes only. It is not intended to be investment advice. Seek a duly licensed professional for investment advice.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;You can open up a quick demo of the project on Codesphere here:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://link.codesphere.com/AX"&gt;https://codesphere.com/#https://github.com/LiorB-D/TradingBot&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;However, you will need an API key before you can actually start trading with our bot - More on that later.&lt;/p&gt;


&lt;h3&gt;
  &lt;a href="#some-helpful%C2%A0terms"&gt;
  &lt;/a&gt;
  Some Helpful Terms
&lt;/h3&gt;

&lt;p&gt;Before we get started, it'll be helpful to define a couple of terms:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Paper Trading: The trading of securities with fake money for educational or testing purposes.&lt;/li&gt;
&lt;li&gt;Backtesting: Testing a trading algorithm against past market data in order to evaluate its effectiveness.&lt;/li&gt;
&lt;li&gt;Moving Average: The average of a certain amount of recent entries in a set of data.&lt;/li&gt;
&lt;li&gt;S&amp;amp;P 500: A stock market index composed of the 500 largest companies listed on US stock exchanges&lt;/li&gt;
&lt;li&gt;Closing Price: The final price of a security during a unit of time&lt;/li&gt;
&lt;li&gt;Good 'Til Cancel (GTC): When you place a trade, it may not be met right away. A broker will continue to try and execute a GTC trade until you cancel it.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;
  &lt;a href="#setup"&gt;
  &lt;/a&gt;
  Setup
&lt;/h3&gt;

&lt;p&gt;The trading API we're going to be using is called Alpaca and is by far one of the most intuitive trading APIs I've found.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://alpaca.markets/"&gt;https://alpaca.markets/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In its free tier, Alpaca includes both Paper and Real Trading and both Historical and Live market data. It also has an incredibly clean user interface and Python library.&lt;/p&gt;

&lt;p&gt;In addition, unless you're willing to leave your python script running on your computer, you're going to need to deploy your trading bot in the cloud. For this, we're going to use Codesphere:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://link.codesphere.com/BA"&gt;https://codesphere.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Since Codesphere's front-end is an IDE, we can develop our bot directly on the platform. If you wish to do the coding on your local machine, however, you can connect your GitHub repo to Codesphere and deploy afterward.&lt;/p&gt;

&lt;p&gt;The only environment setup we really need before we can start coding is to create our pip environment:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pipenv shell&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;And then install the Alpaca API&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pipenv install alpaca_trade_api&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We are also going to need to make a free Alpaca account and then navigate to our Paper Trading Account.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--GeGaprxi--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/84vieolvwgujbvn7gewz.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--GeGaprxi--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/84vieolvwgujbvn7gewz.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Notice your API Key on the right-hand side. When you first open your account, you will be prompted to generate a key and both public and private key will be shown to you. We're going to need those for later.&lt;/p&gt;
&lt;h3&gt;
  &lt;a href="#buying-and-selling%C2%A0stocks"&gt;
  &lt;/a&gt;
  Buying and Selling Stocks
&lt;/h3&gt;

&lt;p&gt;We can then set up our Alpaca Trading library and buy and sell stocks in Python like so:&lt;/p&gt;


&lt;div class="ltag_gist-liquid-tag"&gt;
  
&lt;/div&gt;


&lt;h3&gt;
  &lt;a href="#our-strategy"&gt;
  &lt;/a&gt;
  Our Strategy
&lt;/h3&gt;

&lt;p&gt;The strategy we're going to use is to buy and sell whenever the 5 minute moving average crosses our price. Now, this is FAR from a good trading strategy, but the logic is relatively simple and will allow us to focus on the general structure of a trading bot.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--vZUzumsu--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/uex5g9bbczw0jl1l6vzy.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--vZUzumsu--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/uex5g9bbczw0jl1l6vzy.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In the above example, the red line is the stock price and the blue line is the moving average. When the moving average crosses under our price, we are going to buy a share of our stock. We are then going to hold the stock until the moving average crosses again and goes above the price. When that happens we are going to sell our share, and then wait for the next buying signal.&lt;/p&gt;

&lt;p&gt;In this article, we'll be trading SPY, which is an index that tracks the S&amp;amp;P 500, and we will only be trading one stock at a time.&lt;/p&gt;

&lt;p&gt;Keep in mind that if you were to make these trades with real money, you would have to comply with day trading regulations and brokerage fees, which would likely offset your gains.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#reading-market%C2%A0data"&gt;
  &lt;/a&gt;
  Reading Market Data
&lt;/h3&gt;

&lt;p&gt;Now let's go over how to read market data using the Alpaca API in Python:&lt;/p&gt;


&lt;div class="ltag_gist-liquid-tag"&gt;
  
&lt;/div&gt;


&lt;p&gt;If you're looking for more in-depth information for when you build your strategy, check out Alpaca's documentation:&lt;br&gt;
&lt;a href="https://alpaca.markets/docs/api-documentation/api-v2/market-data/alpaca-data-api-v2/"&gt;https://alpaca.markets/docs/api-documentation/api-v2/market-data/alpaca-data-api-v2/&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;
  &lt;a href="#executing-our%C2%A0strategy"&gt;
  &lt;/a&gt;
  Executing Our Strategy
&lt;/h3&gt;

&lt;p&gt;Now let's finally put all of this together for our complete trading algorithm:&lt;/p&gt;


&lt;div class="ltag_gist-liquid-tag"&gt;
  
&lt;/div&gt;


&lt;p&gt;And there we have it! We just built a trading bot in 54 lines of code! Now if we leave this running on Codesphere throughout the day, we should see our Alpaca dashboard update throughout the day:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--vZUzumsu--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/uex5g9bbczw0jl1l6vzy.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--vZUzumsu--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/uex5g9bbczw0jl1l6vzy.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#backtesting-a%C2%A0strategy"&gt;
  &lt;/a&gt;
  Backtesting a Strategy
&lt;/h3&gt;

&lt;p&gt;Now if you don't want to wait around to see if your algorithm is any good, we can use Alpaca's market data API to backtest our Python algorithm against historical data:&lt;/p&gt;


&lt;div class="ltag_gist-liquid-tag"&gt;
  
&lt;/div&gt;


&lt;h3&gt;
  &lt;a href="#next-steps"&gt;
  &lt;/a&gt;
  Next Steps
&lt;/h3&gt;

&lt;p&gt;So there you have it, we just created a rudimentary trading bot with some fairly simple Python!&lt;/p&gt;

&lt;p&gt;Here is the full repo:&lt;br&gt;
&lt;a href="https://github.com/LiorB-D/TradingBot"&gt;https://github.com/LiorB-D/TradingBot&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;While I highly encourage you guys to play around with the Alpaca API for educational purposes, be extremely careful if you are going to trade real securities. One bug in your code could have disastrous effects on your bank account.&lt;br&gt;
On a lighter note, this is a great opportunity to put those statistics classes you took to work.&lt;/p&gt;




&lt;p&gt;Comment down below if you're going to build your own trading algorithm!&lt;/p&gt;

&lt;p&gt;Happy Coding from your folks at Codesphere, the next generation cloud provider&lt;/p&gt;

</description>
      <category>python</category>
      <category>tutorial</category>
      <category>webdev</category>
      <category>programming</category>
    </item>
    <item>
      <title>New to node.js and struggling with socket.io</title>
      <author>Fletcher Moore</author>
      <pubDate>Fri, 09 Jul 2021 14:02:10 +0000</pubDate>
      <link>https://dev.to/fletch0132/new-to-node-js-and-struggling-with-socket-io-o75</link>
      <guid>https://dev.to/fletch0132/new-to-node-js-and-struggling-with-socket-io-o75</guid>
      <description>&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;Nervous first post but really need some help. I'm working on a Web application for p2p communication (both video and text). &lt;/p&gt;

&lt;p&gt;The video communication works (some teething issues), but my main issue is getting user socket.id. specifically the user just having connected.&lt;/p&gt;

&lt;p&gt;I have tried many things including:&lt;br&gt;
Socket.on("connected", () {&lt;br&gt;
console.log(socket.id);&lt;br&gt;
});&lt;/p&gt;

&lt;p&gt;All I get is "undefined". Yet if I run te same console.log code after the page loads I can get it displayed.&lt;/p&gt;

&lt;p&gt;Not sure how to work with that.&lt;/p&gt;

&lt;p&gt;I want to store the socket.id and username in an object/array &lt;/p&gt;

&lt;p&gt;Thank you &lt;/p&gt;

</description>
      <category>node</category>
      <category>javascript</category>
      <category>webdev</category>
      <category>webrtc</category>
    </item>
  </channel>
</rss>
