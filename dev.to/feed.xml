<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>DEV Community</title>
    <author>DEV Community</author>
    <description>A constructive and inclusive social network for software developers. With you every step of your journey.</description>
    <link>https://dev.to</link>
    <language>en</language>
    <item>
      <title>npm basic commands</title>
      <author>sndp</author>
      <pubDate>Sat, 15 Jan 2022 08:55:22 +0000</pubDate>
      <link>https://dev.to/lizardkinglk/npm-basic-commands-4cja</link>
      <guid>https://dev.to/lizardkinglk/npm-basic-commands-4cja</guid>
      <description>&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--0YnzumZN--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/jjw6w3l8eu2svo2lvyfp.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--0YnzumZN--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/jjw6w3l8eu2svo2lvyfp.png" alt="Photo by Paul Esch-Laurent" width="320" height="205"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#npm"&gt;
  &lt;/a&gt;
  npm
&lt;/h2&gt;

&lt;p&gt;NPM a.k.a. 'Node Package Manager' is a registry of libraries for use in the development of javascript based applications. This is the default and largest package manager in the world.&lt;/p&gt;

&lt;p&gt;The npm was originally invented by Isaac Z. Schlueter 11 years ago. Continued releases with the support of developers made a huge contribution in making npm the best version it is today.&lt;/p&gt;

&lt;p&gt;NPM basically comes with the Node.js runtime environment and as a dependency. Many developers use Node.js with npm effectively to build apps quick and easily. Learning npm would be a big opportunity in the job industry as well.&lt;/p&gt;

&lt;p&gt;npm mainly consists of,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Libraries of packages (registry)&lt;/li&gt;
&lt;li&gt;Package Manager (cli)&lt;/li&gt;
&lt;li&gt;Installer&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#npm-package-registry-gt"&gt;
  &lt;/a&gt;
  npm package registry -&amp;gt;
&lt;/h3&gt;

&lt;p&gt;The npm package registry is a database of javascript packages or libraries. These packages takes the structure of a repository which can be seen as modules, readme files, metadata etc. These libraries are available to use by anyone for their projects. Many libraries are available to open source. We can access the registry by using the npm's cli.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#npm-package-manager-gt"&gt;
  &lt;/a&gt;
  npm package manager -&amp;gt;
&lt;/h3&gt;

&lt;p&gt;The npm has its own multi-platform command line interface and it has many uses for the developer. To get npm cli we have to download and install Node.js. Click &lt;a href="https://nodejs.org/en/download/"&gt;this&lt;/a&gt;.&lt;br&gt;
The npm can be referred to this cli due to the distributed architecture of it. &lt;br&gt;
The cli can be used to,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;login to npm
&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;npm-cli-login
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;ul&gt;
&lt;li&gt;initialize nodejs project 
(create 'package.json' file)
&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;npm init
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;ul&gt;
&lt;li&gt;run custom application based scripts 
(server/index.js/app.js)
&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;npm run server
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;ul&gt;
&lt;li&gt;start application
&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;npm start app
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;ul&gt;
&lt;li&gt;test your code
&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;npm run tests
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h3&gt;
  &lt;a href="#npm-installer-gt"&gt;
  &lt;/a&gt;
  npm installer -&amp;gt;
&lt;/h3&gt;

&lt;p&gt;This part of npm downloads and installs the npm packages to Node.js development project when required.&lt;br&gt;
First we need to initialize our app's package.json file.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;npm init --y&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This command makes a package.json file with default  values engaged with the project's metadata. We can edit this later on ourselves. To get the initial dependencies run the installer by adding dependencies to the package.json file by adding,&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--qwtv1i7W--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/fs8mhz3i470m74q8vgpd.PNG" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--qwtv1i7W--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/fs8mhz3i470m74q8vgpd.PNG" alt="package.json file" width="208" height="132"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can also add the version as the value of package name. &lt;br&gt;
Then we can run the command,&lt;/p&gt;

&lt;p&gt;&lt;code&gt;npm install&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;To automatically (without editing the package.json file first) install and edit the package.json by itself, we can use the command below.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;npm i --save &amp;lt;package_name1&amp;gt;,&amp;lt;package_name2&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Then a folder with packages named as node_modules should be installed to the location the command used in (project root directory).&lt;br&gt;
After that we can use the modules. &lt;/p&gt;

&lt;p&gt;Developers can also use developer dependencies like nodemon, karma etc. by adding &lt;code&gt;-dev&lt;/code&gt; flag.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;npm install -dev &amp;lt;dev_package_name&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We are now ready to code with the aid of newly added dependencies. By using such efficient tool we can reduce the development and reduce time by without using external plugins.&lt;/p&gt;

&lt;p&gt;Learn more about npm with following links.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://docs.npmjs.com/"&gt;https://docs.npmjs.com/&lt;/a&gt;&lt;/p&gt;

</description>
      <category>nodjs</category>
      <category>webdev</category>
      <category>javascript</category>
    </item>
    <item>
      <title>Fantastic Symbols and Where to Find Them - Part 1</title>
      <author>Kemal Akkoyun</author>
      <pubDate>Sat, 15 Jan 2022 08:18:33 +0000</pubDate>
      <link>https://dev.to/kakkoyun/fantastic-symbols-and-where-to-find-them-part-1-1epo</link>
      <guid>https://dev.to/kakkoyun/fantastic-symbols-and-where-to-find-them-part-1-1epo</guid>
      <description>&lt;p&gt;&lt;em&gt;Originally published on &lt;a href="https://www.polarsignals.com/blog/"&gt;polarsignals.com/blog&lt;/a&gt; on 13.01.2022&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Symbolization is a technique that allows you to translate machine memory addresses to human-readable symbol information (symbols).&lt;/p&gt;

&lt;p&gt;Why do we need to read what programs do anyways? We usually do not need to translate everything to a human-readable format when things run smoothly. But when things go south, we need to understand what is going on under the hood.&lt;br&gt;
Symbolization is needed by introspection tools like &lt;a href="https://en.wikipedia.org/wiki/Debugger"&gt;debuggers&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Profiling_(computer_programming)"&gt;profilers&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Core_dump"&gt;core dumps&lt;/a&gt; or any other program that needs to trace the execution of another program.&lt;br&gt;
While a target program is executing on a machine, these types of programs capture the stack traces of the program that is being executed.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A &lt;a href="https://en.wikipedia.org/wiki/Stack_trace"&gt;stack trace&lt;/a&gt; (also called stack backtrace or stack traceback) is a report of the active stack frames at a certain point in time during the execution of a program.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--DLWXtRv8--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://www.polarsignals.com/blog/posts/2022/01/call_stack_layout.svg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--DLWXtRv8--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://www.polarsignals.com/blog/posts/2022/01/call_stack_layout.svg" alt="Call Stack Layout" width="684" height="558"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In raw stack traces, the addresses of the functions that are being called are recorded. The addresses are hexadecimal numbers representing the memory return addresses of the functions. Symbols are needed to translate memory addresses into function and variable names precisely as in the program’s source code to be read by us humans.&lt;br&gt;
Without symbols, all we see are hexadecimal numbers representing the memory addresses that we have captured.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--FQFm-N2v--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://www.polarsignals.com/blog/posts/2022/01/unsymbolized_stack.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--FQFm-N2v--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://www.polarsignals.com/blog/posts/2022/01/unsymbolized_stack.png" alt="Unsymbolized Stack" width="772" height="70"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It sounds simple enough, right? Well, it's not. As with everything else about computers, it's a bit of sorcery. It has its challenges, such as associating them with correct symbols, transforming addresses, and most importantly, actually finding the symbols!&lt;br&gt;
The strategies to get symbol information varies depending on the platform and the programming language implementation that the program is written in.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;For the sake of simplicity, we will be focusing on Linux as the target platform and ignore Windows, macOS and many other platforms. Otherwise, I could end up writing a small size book in here :)&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;
  &lt;a href="#fantastic-symbols-"&gt;
  &lt;/a&gt;
  Fantastic Symbols ...
&lt;/h2&gt;

&lt;p&gt;A symbol (or debug symbol, to be precise) is a special kind of &lt;a href="https://en.wikipedia.org/wiki/Symbol_(programming)"&gt;symbol&lt;/a&gt; that attaches additional information to the symbol table of a program.&lt;br&gt;
This symbol information allows a debugger or a profiler to gain access to information from the program's source code, such as the names of identifiers, including variables and functions.&lt;br&gt;
But where can we find these symbols?&lt;/p&gt;
&lt;h2&gt;
  &lt;a href="#-and-where-to-find-them"&gt;
  &lt;/a&gt;
  ... and Where to Find Them
&lt;/h2&gt;

&lt;p&gt;The actual location of the symbolic information depends on the programming language implementation the program is written in.&lt;br&gt;
We can categorize the programming language implementations into three groups: compiled languages (with or without a runtime), interpreted languages, and &lt;a href="https://en.wikipedia.org/wiki/Just-in-time_compilation"&gt;JIT-compiled&lt;/a&gt; languages.&lt;/p&gt;

&lt;p&gt;If the program is a compiled one, these may be compiled together with the binary file, distributed in a separate file, or discarded during the compilation and/or linking.&lt;br&gt;
Or, if the program is interpreted, these may be stored in the program itself. Let's briefly look at where and how we can find these symbols depending on the programming language implementation.&lt;/p&gt;
&lt;h3&gt;
  &lt;a href="#compiled-language-implementations"&gt;
  &lt;/a&gt;
  Compiled language implementations
&lt;/h3&gt;

&lt;p&gt;Examples of compiled languages include C, C++, Go, Rust and many others.&lt;/p&gt;

&lt;p&gt;The compiled languages usually have a &lt;a href="https://en.wikipedia.org/wiki/Symbol_table"&gt;symbol table&lt;/a&gt; that contains all the symbols used in the program.&lt;br&gt;
The symbol table is usually compiled in the executable binary file. And the binary file is typically in the &lt;a href="https://en.wikipedia.org/wiki/Executable_and_Linkable_Format"&gt;ELF&lt;/a&gt; format (for Linux systems).&lt;br&gt;
Symbol tables are included in the ELF binary file, specifically for mapping the addresses to function names and object names.&lt;br&gt;
In rare cases, it is stored in a separate file, usually with the same name as the binary file, but with a different extension.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--yYLEVYkK--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://www.polarsignals.com/blog/posts/2022/01/elf.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--yYLEVYkK--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://www.polarsignals.com/blog/posts/2022/01/elf.png" alt="ELF" width="880" height="630"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The ELF format is not an easy one to describe in a couple of sentences. For the purpose of this article, we will focus on what we need to know about the ELF format.&lt;br&gt;
Each ELF file is made up of one ELF header, followed by file data. The ELF header is a fixed size and contains information about the data sections.&lt;br&gt;
The relevant part for us is the symbols can live in a special section called &lt;code&gt;.symtab&lt;/code&gt; and &lt;code&gt;.dynsym&lt;/code&gt;.&lt;br&gt;
&lt;code&gt;.dynsym&lt;/code&gt; is the “dynamic symbol table” and it is a smaller version of the &lt;code&gt;.symtab&lt;/code&gt; that only contains global symbols.&lt;/p&gt;

&lt;p&gt;Contents of &lt;code&gt;.dynsym&lt;/code&gt; and &lt;code&gt;.symtab&lt;/code&gt; section using &lt;code&gt;readelf -s /bin/go&lt;/code&gt;:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;Symbol table '.dynsym' contains 38 entries:
   Num: Value Size Type Bind Vis Ndx Name
     0: 0000000000000000 0 NOTYPE LOCAL DEFAULT UND
     1: 00000000006355e0 99 FUNC GLOBAL DEFAULT 1 crosscall2
     2: 00000000006355a0 55 FUNC GLOBAL DEFAULT 1 _cgo_panic
     3: 0000000000465560 25 FUNC GLOBAL DEFAULT 1 _cgo_topofstack
     4: 0000000000000000 0 OBJECT GLOBAL DEFAULT UND [...]@GLIBC_2.2.5 (6)
     5: 0000000000000000 0 OBJECT GLOBAL DEFAULT UND [...]@GLIBC_2.2.5 (4)
     6: 0000000000000000 0 OBJECT GLOBAL DEFAULT UND [...]@GLIBC_2.2.5 (4)
     7: 0000000000000000 0 OBJECT GLOBAL DEFAULT UND [...]@GLIBC_2.2.5 (4)
     8: 0000000000000000 0 OBJECT GLOBAL DEFAULT UND [...]@GLIBC_2.2.5 (4)
     9: 0000000000000000 0 OBJECT GLOBAL DEFAULT UND [...]@GLIBC_2.2.5 (4)
    10: 0000000000000000 0 OBJECT GLOBAL DEFAULT UND [...]@GLIBC_2.2.5 (4)
...
Symbol table '.symtab' contains 13199 entries:
   Num: Value Size Type Bind Vis Ndx Name
     0: 0000000000000000 0 NOTYPE LOCAL DEFAULT UND
     1: 0000000000000000 0 FILE LOCAL DEFAULT ABS go.go
     2: 0000000000401000 0 FUNC LOCAL DEFAULT 1 runtime.text
     3: 0000000000401000 214 FUNC LOCAL DEFAULT 1 net(.text)
     4: 00000000004010e0 214 FUNC LOCAL DEFAULT 1 runtime/cgo(.text)
     5: 00000000004011c0 601 FUNC LOCAL DEFAULT 1 runtime/cgo(.text)
     6: 0000000000401420 480 FUNC LOCAL DEFAULT 1 runtime/cgo(.text)
     7: 0000000000401420 47 FUNC LOCAL HIDDEN 1 threadentry
     8: 0000000000401600 70 FUNC LOCAL DEFAULT 1 runtime/cgo(.text)
     9: 0000000000401646 5 FUNC LOCAL DEFAULT 1 runtime/cgo(.tex[...]
    10: 0000000000401646 5 FUNC LOCAL HIDDEN 1 x_cgo_munmap.cold
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;blockquote&gt;
&lt;p&gt;Go has a unique table (of course). It stores its symbols in a section called &lt;a href="https://pkg.go.dev/debug/gosym#LineTable"&gt;&lt;code&gt;.gopclntab&lt;/code&gt;&lt;/a&gt;. This is a table of functions, line numbers and addresses.&lt;br&gt;
Go does this because it needs to be able to render human-readable stack traces when a panic occurs in runtime;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Note that addresses in the symbol table do not move during execution so that they can be read any time during the execution of the program.&lt;br&gt;
They can easily be loaded into memory independent of the running program and an observer can easily read them.&lt;/p&gt;

&lt;p&gt;We assumed that the binary file is a statically linked executable until this point. However, this might not be the case. The binary file might be dynamically linked to other libraries.&lt;br&gt;
From now on, we will refer to these shared library files and executables (both in ELF format) as &lt;a href="https://en.wikipedia.org/wiki/Object_file"&gt;object files&lt;/a&gt;. Each object file can have its own symbol table.&lt;/p&gt;

&lt;p&gt;We need to note that when we take a snapshot of the stack (a.k.a stack trace), it could include addresses from linked shared libraries and Kernel functions.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Kernel-level software differs as it has its own dynamic symbol table in &lt;code&gt;/proc/kallsyms&lt;/code&gt;, which is a file that contains all the symbols that are used in the kernel. And it can grow as the kernel modules are loaded.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We can read the object files by using binary utilities such as &lt;a href="https://en.wikipedia.org/wiki/Objdump"&gt;objdump&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Readelf"&gt;readelf&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Nm_(Unix)"&gt;nm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To read the &lt;code&gt;.symtab&lt;/code&gt;:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;nm &lt;span class="nv"&gt;$FILE&lt;/span&gt;
&lt;span class="c"&gt;# or&lt;/span&gt;
objdump &lt;span class="nt"&gt;--syms&lt;/span&gt; &lt;span class="nv"&gt;$FILE&lt;/span&gt;
&lt;span class="c"&gt;# or&lt;/span&gt;
readelf &lt;span class="nt"&gt;-a&lt;/span&gt; &lt;span class="nv"&gt;$FILE&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;To read the &lt;code&gt;.dynsym&lt;/code&gt;:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;nm &lt;span class="nt"&gt;-D&lt;/span&gt; &lt;span class="nv"&gt;$FILE&lt;/span&gt;
&lt;span class="c"&gt;# or&lt;/span&gt;
objdump &lt;span class="nt"&gt;--dynamic-syms&lt;/span&gt; &lt;span class="nv"&gt;$FILE&lt;/span&gt;
&lt;span class="c"&gt;# or&lt;/span&gt;
readelf &lt;span class="nt"&gt;-a&lt;/span&gt; &lt;span class="nv"&gt;$FILE&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;For the compiled languages, the symbol table is not the only source of symbols. There are also DWARFs!&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#debuginfo"&gt;
  &lt;/a&gt;
  Debuginfo
&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;ELFs and DWARFs, welcome to fairyland.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Another way to obtain the symbols from an object file is to use the debug information or &lt;code&gt;debuginfo&lt;/code&gt; in short.&lt;br&gt;
Same as the symbol table, this information can be compiled in the binary file, formatted in the &lt;a href="https://en.wikipedia.org/wiki/DWARF"&gt;DWARF(Debugging With Attributed Record Formats)&lt;/a&gt; or in a separate file.&lt;/p&gt;

&lt;p&gt;DWARF is the debug information format most commonly used with ELF. It’s not necessarily tied to ELF, but the two were developed in tandem and work very well together.&lt;br&gt;
This information is split across different ELF sections (&lt;code&gt;.debug_*&lt;/code&gt; and &lt;code&gt;.zdebug_*&lt;/code&gt; for compressed ones), each with its own piece of information to relay.&lt;br&gt;
For our specific needs, we need to use the &lt;code&gt;.debug_info&lt;/code&gt; section to find corresponding functions and &lt;code&gt;.debug_line&lt;/code&gt; section to corresponding line numbers.&lt;/p&gt;

&lt;p&gt;Debuginfo files for software packages are available through package managers in Linux distributions.&lt;br&gt;
Usually for an available package called &lt;code&gt;mypackage&lt;/code&gt; there exists a &lt;code&gt;mypackage-dbgsym&lt;/code&gt;, &lt;code&gt;mypackage-dbg&lt;/code&gt; or &lt;code&gt;mypackage-debuginfo&lt;/code&gt; package.&lt;br&gt;
There are also &lt;a href="https://sourceware.org/elfutils/Debuginfod.html"&gt;public servers&lt;/a&gt; that serve debug information.&lt;/p&gt;
&lt;h4&gt;
  &lt;a href="#one-program-to-bring-them-all-and-in-the-darkness-bind-them-raw-addr2line-endraw-"&gt;
  &lt;/a&gt;
  One Program to bring them all, and in the darkness bind them: &lt;code&gt;addr2line&lt;/code&gt;
&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Wait, what?! Isn't that from another fantasy book?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now that we have the symbol table or debug information, we can use &lt;code&gt;addr2line&lt;/code&gt; (&lt;em&gt;address to line&lt;/em&gt;) to get the source code location of a given address.&lt;br&gt;
&lt;a href="https://linux.die.net/man/1/addr2line"&gt;&lt;code&gt;addr2line&lt;/code&gt;&lt;/a&gt; converts addresses back to function and line numbers.&lt;/p&gt;

&lt;p&gt;Let's see it in action &lt;code&gt;addr2line -a 0x0000000000001154 -e &amp;lt;objectFile&amp;gt;&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;For addr2line &lt;code&gt;&amp;lt;objectFile&amp;gt;&lt;/code&gt; can be any object file compiled with debug information or symbols. It can be an executable, a shared library or output of a strip operation.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Voilà!&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;0x0000000000001154
main
/home/newt/Sandbox/hello-c/hello.c:14
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;I used a simple C executable for this example. And we have got our symbol and attached source information for the corresponding address 🎉&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--0cpAQS----/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://www.polarsignals.com/blog/posts/2022/01/success_kid.jpg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--0cpAQS----/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://www.polarsignals.com/blog/posts/2022/01/success_kid.jpg" alt="Success" width="500" height="500"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I only wish we had compiled programming language implementations out there, then our job here could have been finished. But we are not. We need to keep digging.&lt;br&gt;
But for that, you need to wait for another week. As we hinted at in the title of this post, there will be a part 2! All the best franchises are sequels, right?!&lt;br&gt;
In part 2, we will see how interpreted languages and &lt;a href="https://en.wikipedia.org/wiki/Just-in-time_compilation"&gt;Just-In-Time&lt;/a&gt; compiled languages handle symbols.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Please stay tuned!&lt;/em&gt;&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#dont-worry-we-got-you-covered"&gt;
  &lt;/a&gt;
  Don't worry we got you covered
&lt;/h2&gt;

&lt;p&gt;Even though we simplified things a bit here, if you want to write a program to utilize symbolization, you still have a lot of work to do.&lt;br&gt;
Many open-source tools out there already handle nitty-gritty details of symbolization, like &lt;a href="https://www.brendangregg.com/perf.html"&gt;&lt;code&gt;perf&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The good news is we got you covered. If you are using &lt;a href="https://github.com/parca-dev/parca-agent"&gt;Parca Agent&lt;/a&gt;, we already do &lt;a href="https://www.parca.dev/docs/symbolization"&gt;the heavy lifting&lt;/a&gt; for you to symbolize captured stack traces.&lt;br&gt;
And we keep extending our support for the different languages and runtimes.&lt;/p&gt;

&lt;p&gt;Check &lt;a href="https://www.parca.dev/"&gt;Parca&lt;/a&gt; out and let us know what you think, on &lt;a href="https://discord.gg/ZgUpYgpzXy"&gt;Discord&lt;/a&gt; channel.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#further-reading"&gt;
  &lt;/a&gt;
  Further reading
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Debug_symbol"&gt;https://en.wikipedia.org/wiki/Debug_symbol&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.brendangregg.com/bpf-performance-tools-book.html"&gt;https://www.brendangregg.com/bpf-performance-tools-book.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DataDog/go-profiler-notes/blob/main/stack-traces.md"&gt;https://github.com/DataDog/go-profiler-notes/blob/main/stack-traces.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.brendangregg.com/perf.html"&gt;https://www.brendangregg.com/perf.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jvns.ca/blog/2018/01/09/resolving-symbol-addresses/"&gt;https://jvns.ca/blog/2018/01/09/resolving-symbol-addresses/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#sources"&gt;
  &lt;/a&gt;
  Sources
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/File:Call_stack_layout.svg"&gt;Call Stack Layout&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/corkami/pics/blob/28cb0226093ed57b348723bc473cea0162dad366/binary/elf101/elf101-64.svg"&gt;ELF Executable and Linkable Format diagram by Ange Albertini&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <category>programming</category>
      <category>profiling</category>
      <category>debugging</category>
      <category>observability</category>
    </item>
    <item>
      <title>Encryption Decryption User Details Using Crypto-JS Library to secure application in React</title>
      <author>Deepak</author>
      <pubDate>Sat, 15 Jan 2022 08:05:15 +0000</pubDate>
      <link>https://dev.to/deepakjaiswal/encryption-decryption-user-details-using-crypto-js-library-to-secure-application-in-react-2ge8</link>
      <guid>https://dev.to/deepakjaiswal/encryption-decryption-user-details-using-crypto-js-library-to-secure-application-in-react-2ge8</guid>
      <description>&lt;p&gt;hello developers today we talk about a library named is crypto-js&lt;br&gt;
so why we use this library in our development.&lt;br&gt;
because it secure aur informaion from malicius user.&lt;/p&gt;

&lt;p&gt;It encrypt our information in cipher text. and we decrypt that text in our &lt;br&gt;
servers file to verify on behalf of secret key.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;npm install crypto-js --save&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2&gt;
  &lt;a href="#encrypt-information"&gt;
  &lt;/a&gt;
  encrypt information
&lt;/h2&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;var CryptoJS = require("crypto-js");

// Encrypt
var ciphertext = CryptoJS.AES.encrypt('user info', 'secret key').toString();


&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#decrypt-information"&gt;
  &lt;/a&gt;
  Decrypt information
&lt;/h2&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;var CryptoJS = require("crypto-js");
var bytes  = CryptoJS.AES.decrypt(ciphertext, 'secret key');
var originalText = bytes.toString(CryptoJS.enc.Utf8);
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;The main reason using this library because some use check their history to see their payload on login time user inter into the login form. so we can use crypto library to secure application&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--9LDcPJMy--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kc3sgxa7wkxj80y0i44m.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--9LDcPJMy--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kc3sgxa7wkxj80y0i44m.png" alt="Image description" width="880" height="466"&gt;&lt;/a&gt;&lt;/p&gt;

</description>
      <category>webdev</category>
      <category>web3</category>
      <category>javascript</category>
      <category>react</category>
    </item>
    <item>
      <title>Accelerate your DevOps by introducing Progressive Delivery</title>
      <author>David Herbert💻🚀</author>
      <pubDate>Sat, 15 Jan 2022 08:04:38 +0000</pubDate>
      <link>https://dev.to/daveyhert/accelerate-your-devops-by-introducing-progressive-delivery-15k9</link>
      <guid>https://dev.to/daveyhert/accelerate-your-devops-by-introducing-progressive-delivery-15k9</guid>
      <description>&lt;p&gt;Building a modern application often involves building it as a microservice, which provides developers with more flexibility and agility in terms of deployment options. However, deployment can either be a joy or a nightmare, depending on the strategy used, whether it's deploying microservices, testing new features, updating a business logic, or releasing a new version entirely? With every code change pushed, comes the risk of potential failures, which could be as a result of bad code quality or unexpected bugs, and this can potentially disrupt the user's experience.&lt;/p&gt;

&lt;p&gt;Hence, picking an efficient deployment strategy is key to mitigating these deployment risks and not having to constantly disrupt your user’s experience by going into downtimes or completely having to go offline whenever there is something to be deployed.&lt;/p&gt;

&lt;p&gt;This article aims to highlight the various progressive delivery forms of deployment available, with hopes of providing a clear guide to choosing a suitable deployment strategy that best suits your needs. The following proven deployment techniques make it quite easy to make improvements to our applications, introduce new services and features, test functionality, quickly identify/eliminate vulnerabilities, and safely deliver the right features faster and with confidence.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#what-is-progressive-delivery"&gt;
  &lt;/a&gt;
  What is Progressive Delivery
&lt;/h2&gt;

&lt;p&gt;Progressive delivery is an emerging approach to accelerating DevOps as it combines the principles of Continuous Integration and Continuous Delivery (CI/CD), with modern software development practices which facilitate faster code shipping, reduce risk and improve the user's experience.&lt;/p&gt;

&lt;p&gt;The fundamental idea behind progressive delivery is that developers and organizations can incrementally roll out and deliver new features to users, with safeguards and fine-grained control levers that enable the teams to minimize the risks that come with continuously pushing new features to the production environment, while monitoring the health of the application at each stage of the progressive delivery process. And if anything should go wrong, quickly revert to a stable stage.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#feature-flags"&gt;
  &lt;/a&gt;
  Feature Flags
&lt;/h3&gt;

&lt;p&gt;In progressive delivery, Feature Flags are important in managing the blast radius of these deployed features. They provide feature toggles that enable developers to deploy features to a subset of users based on attributes such as geolocation or other custom profiles. They also enable rolling back these features a toggle away if they don't work as intended. Developers can then gradually expand the radius of their deployments by implementing incremental changes based on the received feedback from the initial release.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#phased-rollouts"&gt;
  &lt;/a&gt;
  Phased Rollouts
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--juO0nCAr--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2jgxh92a3jhmp04e6x1q.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--juO0nCAr--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2jgxh92a3jhmp04e6x1q.png" alt="phased_rollouts" width="689" height="358"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Phased rollouts can be considered a subtle variation of progressive delivery. In the phased rollout deployment methodology, you can manage your product releases more effectively by breaking them up into smaller implementations that can be tested and implemented easily on smaller segments of users. These users are typically the most advanced and active ones whose activity provides the best feedback before being released to wider audiences.&lt;/p&gt;

&lt;p&gt;This approach allows developers to incrementally introduce new functionalities and features to a smaller group of end-users but also minimize the possible impact this can have on the entire user base, by obtaining feedback to help improve the product further. As these phased rollouts progress, the information gathered in the early phases of these phased rollouts can then be used to guide the remainder of the rollouts process, so that there are fewer issues or errors while at the same time, gradually accustoming the users to the new rollouts.&lt;/p&gt;

&lt;p&gt;Twitter is an example of a company known for using the progressive app delivery method of deployment to efficiently roll out new features to its user base, because phased rollouts enable developers to release smaller and more frequent updates for their apps so users can receive new features more quickly, without exposing themselves to bugs. The two core essential characteristics of phased rollouts are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;Experimentation:&lt;/strong&gt; This technique is used to evaluate how an app performs when certain features are implemented in production, for example using A/B testing to learn which version users prefer. A typical use case scenario would be when a team tests the effectiveness of different designs of a sale button to different subsets of users, to see how each design influences conversions and then uses the most successful design as the final rollout.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Feature flagging:&lt;/strong&gt; This technique manages the rollout of a new feature to a limited audience so that its impact can be monitored. It typically involves turning certain features on or off for a subset of users, reducing any risk. Most teams use third-party services such as the &lt;a href="https://configcat.com/"&gt;ConfigCat&lt;/a&gt; feature flag service for their feature flagging.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A safe method for testing new implementations in production.&lt;/li&gt;
&lt;li&gt;It facilitates observability which makes it easier to quickly contain bugs and unexpected issues.&lt;/li&gt;
&lt;li&gt;Since the deployment is broken up into parts, it improves operational management.&lt;/li&gt;
&lt;li&gt;It reduces downtime, limits the blast radius, and lets us learn from the process.&lt;/li&gt;
&lt;li&gt;Through a series of experiments, phased rollouts enable a system to be flexible and powerful through numerous experiments.&lt;/li&gt;
&lt;li&gt;By using Feature Flags, it's easy to mitigate the risk of continuous deployment.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;As deployment occurs over an extended period, teams must concentrate on a single rollout at a time rather than on the full system rollout as a whole.&lt;/li&gt;
&lt;li&gt;It can take longer to implement a feature as it has to be gradually rolled out.&lt;/li&gt;
&lt;li&gt;Issues encountered in a rollout halt further rollouts until resolved.&lt;/li&gt;
&lt;li&gt;The state of continuous change makes system delivery milestones unclear.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#canary-releases"&gt;
  &lt;/a&gt;
  Canary Releases
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--w2ruFhEK--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/3nc6hl264ujb491sunh6.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--w2ruFhEK--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/3nc6hl264ujb491sunh6.png" alt="canary-releases" width="693" height="363"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Long ago, in the coal mining industry, canaries were used to detect the presence of harmful levels of toxic gas as an early warning system (because canaries were very sensitive). Thus, Canary releases were named after the well-known expression, "canary in a coal mine."&lt;/p&gt;

&lt;p&gt;Canary release is a version testing method used for reducing the risk of introducing a new software version into production. This is done by first deploying the new version of the application to a portion of the production platform/infrastructure with no users on it, and then gradually routing a subset of users traffic to this section of the platform which exposes them to the new version.&lt;/p&gt;

&lt;p&gt;The chosen users can be decided based on different strategies: a simple approach is to randomly select users; while some organizations release the new version to internal users and employees before releasing it to external users; another more sophisticated approach is to choose users based on custom profiles such as demography, location, etc.&lt;/p&gt;

&lt;p&gt;As more confidence is gained in the new version and no errors are detected, more servers in the infrastructure can then be made available for it and more users sent to it, but if the new version has any problems or major bugs, the rollback strategy is to reroute these users back to the old version until the problem has been resolved.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#differences-between-canary-releases-and-feature-flags"&gt;
  &lt;/a&gt;
  Differences between Canary Releases and Feature Flags
&lt;/h3&gt;

&lt;p&gt;Canary releases are sometimes confused with feature flags as they also deal with incremental feature releases, but canary releases are different from feature flag releases since feature flag releases are used to expose a specific feature to a small group of users. While Canary releases expose a specific version of the entire application or service to a portion of the platform's infrastructure and then direct some of this user traffic to this new version.&lt;/p&gt;

&lt;p&gt;But though a Canary release and a Feature flag vary, they can both be used together to achieve an efficient release. A typical use-case would be wanting to release the new version as a Canary release but not wanting to have all the features available at first. Using a feature flagging service such as &lt;a href="https://configcat.com/"&gt;ConfigCat&lt;/a&gt; is the best solution for a scenario like this. ConfigCat enables you to implement feature flags or feature toggles in your application, and automatically add emergency kill switches to facilitate the gradual rollout of more features as feature testing is done. I dare say this is the best of both worlds.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Reduces the probability of releasing a new version that negatively impacts a large number of users, hence an ability to test in production.&lt;/li&gt;
&lt;li&gt;Increases the value of the new version since user behavior can be observed and well-instrumented.&lt;/li&gt;
&lt;li&gt;It's relatively easy to revert to the old version if a problem is detected by simply rolling back traffic to the old version.&lt;/li&gt;
&lt;li&gt;Easily restrict access to specific audiences, individual users, or a certain percentage of users&lt;/li&gt;
&lt;li&gt;It is relatively easy to automate the entire release life cycle.&lt;/li&gt;
&lt;li&gt;Easily avoid downtimes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Using canary releases comes with the drawback of having to manage multiple versions of your application simultaneously.&lt;/li&gt;
&lt;li&gt;If the distributed application is installed on the user’s device, there is less control to efficiently use canary releases.&lt;/li&gt;
&lt;li&gt;When releasing canaries, database changes between the old version and the new version would have to be managed.&lt;/li&gt;
&lt;li&gt;Needs resources to maintain the old system and the new system&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#ring-deployment"&gt;
  &lt;/a&gt;
  Ring Deployment
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--cNIFvTxU--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/epyrlese8eqlgqb4apod.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--cNIFvTxU--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/epyrlese8eqlgqb4apod.png" alt="ring-deployment" width="693" height="376"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;With a ring deployment, you manage the deployment risk by providing new features to different groups gradually. These groups are represented by a series of rings, starting from a small ring consisting of a group of few users and then gradually expanding to an ever-growing series of rings until you include all your users.&lt;/p&gt;

&lt;p&gt;There can be multiple production environments in a ring-based deployment. A limited number of users can then be served by each production environment. It is similar to Canary Release but Ring-based deployments, on the other hand, allow you to maintain as many production environments as you want and over time, new releases are deployed to the rings one by one until all rings are updated.&lt;/p&gt;

&lt;p&gt;And just like with most deployments, in-ring deployment, feature flags can help you toggle certain features or functionality you aren't certain about in a ring.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Gradually test your application before it is made available to all users&lt;/li&gt;
&lt;li&gt;It's easy to roll back when something goes wrong with a ring.&lt;/li&gt;
&lt;li&gt;It is easy to gather feedback from each ring.&lt;/li&gt;
&lt;li&gt;It's easy to control the radius of each feature as it is subject to the size of the ring (number of users in a ring)&lt;/li&gt;
&lt;li&gt;Feature flagging can be used for fine-tuned control.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Slow delivery timeline as releases have to be made ring by ring.&lt;/li&gt;
&lt;li&gt;Resources are split across rings.&lt;/li&gt;
&lt;li&gt;Problems occurring in rings can derail the deployment timeline.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#conclusion"&gt;
  &lt;/a&gt;
  Conclusion
&lt;/h2&gt;

&lt;p&gt;With progressive delivery, you can more effectively track, support, and control your deployment and progressively deliver new features and functionalities to your users, as it combines modern software deployment practices such as Feature Flagging, A/B testing, etc to facilitate faster code shipping, reduce risks associated with traditional deployments such as downtimes and greatly improve users experience.&lt;/p&gt;

&lt;p&gt;If you want to move fast and with confidence when deploying, even while breaking things, the different forms of progressive delivery provide better support and control for these incremental deliveries even while in production. By taking a customer-centric approach, progressive delivery increases resilience as well as decreases deployment downtime, and also embraces a smooth transition for end-users in a sense.&lt;/p&gt;

</description>
      <category>devops</category>
      <category>deployment</category>
      <category>webdev</category>
      <category>programming</category>
    </item>
    <item>
      <title>Learning Elm the Wrong Way: A Series (Probably)</title>
      <author>John Pavlick</author>
      <pubDate>Sat, 15 Jan 2022 07:33:19 +0000</pubDate>
      <link>https://dev.to/jmpavlick/learning-elm-the-wrong-way-a-series-probably-2ohp</link>
      <guid>https://dev.to/jmpavlick/learning-elm-the-wrong-way-a-series-probably-2ohp</guid>
      <description>&lt;p&gt;Hi, I'm John, and a few months ago I decided that &lt;em&gt;I've been doing this for too long to be this bad at it&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;I've been an Elm developer since 2018, and the story is really only important to me&lt;sup&gt;1&lt;/sup&gt; (there's enough self-aggrandizing on the Internet&lt;sup&gt;2&lt;/sup&gt; these days) so I'll leave it out - but it's taken me a remarkably long time to feel as if I have any real command of the language and its tooling / ecosystem.&lt;/p&gt;

&lt;p&gt;Some of the Best &amp;amp; Brightest on &lt;a href="http://elmlang.heroku.net/"&gt;the Elm Slack&lt;/a&gt; have been kindly tolerating my ineptitude for some years now, and now that I know about &lt;a href="https://package.elm-lang.org/packages/elm/html/latest/Html-Attributes#classList"&gt;classList&lt;/a&gt; and can hang out in &lt;code&gt;#beginners&lt;/code&gt; and field easy questions, it's time to take the next logical step: start a dev blog.&lt;/p&gt;

&lt;p&gt;Learning by example is well and good, but most of the truly important things I've learned have been by &lt;em&gt;counter-example&lt;/em&gt;. What-to-do is nowhere near as poignant on its own, as it is when contrasted with what-not-to-do. And I'm an expert at what-not-to-do. Unfortunately, I've had to find these poignant counter-examples on my own. I'm here today to 1) confess my sins, and 2) show you how not to be like me. I've got enough sins for which to seek absolution that this will likely be a series, but let's dive in now. If you want to learn Elm the wrong way, start by following the headline-sized point below, don't read anything else, and... I'll see you in &lt;code&gt;#beginners&lt;/code&gt;.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#ignore-fear-the-runtime"&gt;
  &lt;/a&gt;
  Ignore / Fear The Runtime
&lt;/h3&gt;

&lt;p&gt;So you did &lt;a href="https://guide.elm-lang.org/"&gt;https://guide.elm-lang.org/&lt;/a&gt;, and you're aware that "Elm is a functional language that transpiles to Javascript and has its own runtime". Great! Now, if you want to do things the wrong way: &lt;em&gt;never ever think about the runtime, ever again&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To be fair, this isn't wholly advice without merit; the runtime stays out of your way. My background is in data / reporting - I thought, "Oh, good. A runtime. Like when I smash &lt;code&gt;F5&lt;/code&gt; on SQL Server and it runs my query. Like dotnet. Like the JVM."&lt;/p&gt;

&lt;p&gt;But there's a difference that wasn't apparent to me when I got started - since Elm is event-driven, you don't call the runtime; &lt;em&gt;the runtime calls YOU&lt;/em&gt;. I'll say it again:&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#the-runtime-calls-you"&gt;
  &lt;/a&gt;
  The runtime calls you!
&lt;/h4&gt;

&lt;p&gt;Remember when you copied and pasted this block of code into &lt;code&gt;elm reactor&lt;/code&gt; and fiddled with everything around it until your app compiled? I do!&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;main : Program () Model Msg
main =
    Browser.sandbox
        { init = initialModel
        , view = view
        , update = update
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;"Ah. A &lt;code&gt;main&lt;/code&gt; function. Seems right."&lt;/p&gt;

&lt;p&gt;The problem with &lt;code&gt;main&lt;/code&gt; is that by the numbers, it's not a function that you write often; you set it up, and it's good. The Code is Good, and you don't touch the Code when it is Good; and so you proceed to not think very hard about it for a few years.&lt;/p&gt;

&lt;p&gt;At a high level, the difference between &lt;code&gt;public static void main()&lt;/code&gt; and &lt;code&gt;main : Program () Model Msg&lt;/code&gt; is that, in the case of the latter, you are telling a program that already exists which three specific functions it's going to call, to handle very specific parts of your application domain. &lt;/p&gt;

&lt;p&gt;Elm's &lt;code&gt;main&lt;/code&gt; function has more in common with dependency injection - with &lt;code&gt;services.AddSingleton&amp;lt;IEmailProxy, EmailProxy&amp;gt;();&lt;/code&gt;, than it does with &lt;code&gt;public static void main()&lt;/code&gt;. Our old friend &lt;code&gt;psvm()&lt;/code&gt; will execute any old code that we sling in there; in contrast, Elm's runtime only knows that it's going to get messages (from the DOM, from ports, and from HTTP responses); that it's going to render HTML; and that it's going to have to start up your application for the first time. The particulars of what it does in those situations are what the code that you are writing &lt;em&gt;actually does&lt;/em&gt;, and do those things and those things only it must (no side effects, remember?). And this brings me to my next point:&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#msgs-happened-in-the-past-getting-their-names-right-is-important"&gt;
  &lt;/a&gt;
  Msgs happened in the past; getting their names right is important.
&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Hardest two problems, cache invalidation, naming, blah blah blah&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You've already heard the quote, you already &lt;em&gt;know that&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;But you need to realize that when you declare the variants for your &lt;code&gt;Msg&lt;/code&gt; type, that every time you process a &lt;code&gt;Msg&lt;/code&gt; in &lt;code&gt;update&lt;/code&gt;, you are processing the output of an action that &lt;em&gt;already happened&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Since I came from an OO / imperative / procedural background, I brought what I had with me when I started learning this language. If I was writing a Windows Forms application, and I wanted &lt;code&gt;btn_DoSomething&lt;/code&gt; to do something, I'd wire its click event-handler to a function &lt;code&gt;DoSomething&lt;/code&gt;, and something would be done&lt;sup&gt;3&lt;/sup&gt;. So in Elm:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;button [ onClick DoSomething ] [ text "Do Something" ]
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;and&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;type Msg
    = DoSomething
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;and&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;case msg of
    DoSomething -&amp;gt;
        ( model, doSomething )
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;I regret to inform you that this is not the Way.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;DoSomething&lt;/code&gt; means, "perform an action", and tells you nothing about how you got to the point in your program's execution where something was done. So if you choose to &lt;code&gt;DoSomething&lt;/code&gt;, and something was done, what do you name your variant "for when something was done"?&lt;/p&gt;

&lt;p&gt;Probably &lt;code&gt;DoSomethingResult&lt;/code&gt;. But this is a function, in a functional langauge! "DoSomethingResult" is a name, a &lt;em&gt;noun&lt;/em&gt;; and their kind are not welcome here&lt;sup&gt;4&lt;/sup&gt;. Banish from your mind any thoughts that this is a silly hill to die on; we are programmers, we are talking about a programming language, and if you don't spend quite a bit of time thinking about the structure and interpretation&lt;sup&gt;5&lt;/sup&gt; of the language that you speak &lt;em&gt;every day&lt;/em&gt; - you're going to have a bad time. Get interested in linguistics.&lt;/p&gt;

&lt;p&gt;I digress. This is a functional language; custom type variants are functions that are a constructor for their custom type; we need to give them names that are verbs. What's a good verb for receiving a result from a function yclept &lt;code&gt;doSomething&lt;/code&gt;? I don't know, how about, &lt;code&gt;GetDoSomethingResult&lt;/code&gt;? Close, but computer says "no"&lt;sup&gt;6&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;The act of receiving a &lt;code&gt;Msg&lt;/code&gt; is not something that will be done by your program; it is something that &lt;em&gt;was done&lt;/em&gt;, in the &lt;em&gt;past&lt;/em&gt;, by the  runtime. Your &lt;code&gt;Msg&lt;/code&gt; variants are event handlers for the return value of events that come &lt;em&gt;from the runtime&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Who clicked that button? You did! (Who's a good doggy?) But the only way that the event "clicked a button" is any different from any of the other thousand events per second that you emit while using a web browser (scrolling, moving your mouse, clicking on a blank space) is that you told Elm's runtime to listen for that event, and that you could handle whatever result the runtime generated from processing that event.&lt;/p&gt;

&lt;p&gt;So since the event that you want to handle happened in the past, and since it was done by somebody else - what should you call it? In this instance, I would call my variant &lt;code&gt;GotDoSomethingResult&lt;/code&gt;. It's perfect! It tells us that the runtime already received our result from the &lt;code&gt;doSomething&lt;/code&gt; function, and is ready to hand it over to us for further processing. Once you start reading packages, or other open-source projects, you'll see these patterns pop up - lots of &lt;code&gt;Got&lt;/code&gt;s and &lt;code&gt;Clicked&lt;/code&gt;s and &lt;code&gt;Changed&lt;/code&gt;s.&lt;/p&gt;

&lt;p&gt;Let's take what we've learned, and selectively refactor our application that we wrote earlier to use our new knowledge:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;button [ onClick ClickedBtnDoSomething ] [ text "Do Something" ]
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;and&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;type Msg
    = ClickedBtnDoSomething
    | GotDoSomethingResult Result (Http.Error ())
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;and&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;case msg of
    ClickedBtnDoSomething -&amp;gt;
        ...

    GotDoSomethingResult result -&amp;gt;
        ...
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;When you talk about "getting" something, the Thing that you will Get is not yet in your possession, no matter how near the future may be wherein you have that Thing; but when you "Got" something, you can only ever speak of the action of your coming into its possession, in the past-tense. I brought up linguistics because when you start naming your &lt;code&gt;Msg&lt;/code&gt;s (etc) in this way, it allows you to shift your thinking ever-so-slightly in a way that makes it a &lt;em&gt;lot&lt;/em&gt; easier to reason about your application, its state, and why its behavior may or may not be as expected.&lt;/p&gt;

&lt;p&gt;In conclusion, conclusions are hard and I could spill another thousand more keystrokes without actually adding anything else of substance to this post, so I'll stop now. If you're still here: thanks for reading along. If this helped you: there's lots more where this came from. Be sure to tune in next time (probably) for (a lot) more&lt;sup&gt;7&lt;/sup&gt;.&lt;/p&gt;




&lt;ul&gt;
&lt;li&gt;
&lt;sup&gt;1&lt;/sup&gt; The short version is, "I needed to learn Javascript but &lt;em&gt;I really didn't want to do that thing&lt;/em&gt;"&lt;/li&gt;
&lt;li&gt;
&lt;sup&gt;2&lt;/sup&gt; Are you supposed to capitalize "Internet"? I tried it both ways. Both feel weird.&lt;/li&gt;
&lt;li&gt;
&lt;sup&gt;3&lt;/sup&gt; &lt;a href="https://en.wikipedia.org/wiki/Politician%27s_syllogism"&gt;https://en.wikipedia.org/wiki/Politician%27s_syllogism&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;sup&gt;4&lt;/sup&gt; &lt;a href="https://steve-yegge.blogspot.com/2006/03/execution-in-kingdom-of-nouns.html"&gt;https://steve-yegge.blogspot.com/2006/03/execution-in-kingdom-of-nouns.html&lt;/a&gt; (talk about an evergreen blogpost)&lt;/li&gt;
&lt;li&gt;
&lt;sup&gt;5&lt;/sup&gt; The structure and interpretation of computer programs is, at its core, the structure and interpretation of &lt;em&gt;a bunch of thoughts&lt;/em&gt;, both yours and those of others. Language enables thought and gives rise to consciousness; the structure and interpretation of computer programs is the structure and interpretation of &lt;em&gt;thought&lt;/em&gt;, which is the structure and interpretation of &lt;em&gt;language&lt;/em&gt;. Mind that I'm above my pay grade talking about these things, so whether you agree with me or not, go learn about them for yourself; and then it won't matter if I'm right - &lt;em&gt;you&lt;/em&gt; will know.&lt;/li&gt;
&lt;li&gt;
&lt;sup&gt;6&lt;/sup&gt; &lt;a href="https://www.youtube.com/watch?v=zFl6p4D59AA"&gt;https://www.youtube.com/watch?v=zFl6p4D59AA&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;sup&gt;7&lt;/sup&gt; And if this made you mad, @ me in the Elm Slack. Maybe calm down a little first, though.&lt;/li&gt;
&lt;/ul&gt;

</description>
      <category>elm</category>
      <category>webdev</category>
    </item>
    <item>
      <title>Introduction to Apache Spark, SparkQL, and Spark MLib.</title>
      <author>hridyesh bisht</author>
      <pubDate>Sat, 15 Jan 2022 07:08:15 +0000</pubDate>
      <link>https://dev.to/aws-builders/introduction-to-apache-spark-sparkql-and-spark-mlib-da1</link>
      <guid>https://dev.to/aws-builders/introduction-to-apache-spark-sparkql-and-spark-mlib-da1</guid>
      <description>&lt;p&gt;In this blog, i focus on Apache Spark , features and limitations of Apache Spark , architecture of Apache Spark,  architecture of SparkQL, and architecture of Spark MLib .  Let's start by understanding what is Apache Spark,&lt;/p&gt;

&lt;h3 id="q-what-is-apache-spark"&gt;&lt;strong&gt;Q.What is Apache Spark?&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Apache Spark is a unified computing engine and a set of libraries for parallel data processing on computer clusters.&lt;/p&gt;

&lt;p id="why-spark"&gt;&lt;strong&gt;Why  Apache Spark?&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Fast Processing: Spark contains Resilient Distributed datasets (RDD) which saves time taken in reading and writing opertions and hence, hence it runs almost ten to hundred times faster than hadoop.&lt;/li&gt;
&lt;li&gt;In-memory computing: In spark, data is stored in the RAM, so it can access the data quickly and accelerate the speed of analytics.&lt;/li&gt;
&lt;li&gt;Flexible: Spark supports multiple languages and allows the developers to write applications in Java, Scala, R or Python.&lt;/li&gt;
&lt;li&gt;Fault tolerance: Spark contains Resillent Distributed Datasets(RDD) that are designed to handle the failure of any worker node in the cluster. THus, it ensures that the loss of data reduces to zero.&lt;/li&gt;
&lt;li&gt;Better analytics: Spark has a rich set of SQL queries, machine learning algorithms, complex analytics. With all these functionalities can be performed better.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Shortcoming of MapReduce:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Forces your data processing into Map and Reduce&lt;ul&gt;&lt;li&gt;Other workflows missing include join, filter, flatMap, groupByKey, union, intersection, …&lt;/li&gt;&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Based on “Acyclic Data Flow” from Disk to Disk (HDFS)&lt;/li&gt;
&lt;li&gt;Read and write to Disk before and after Map and Reduce (stateless machine)&lt;ul&gt;&lt;li&gt;Not efficient for iterative tasks, i.e. Machine Learning&lt;/li&gt;&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Only Java natively supported&lt;/li&gt;
&lt;li&gt;Only for Batch processing&lt;ul&gt;&lt;li&gt;Interactivity, streaming data&lt;/li&gt;&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;How to does Apache spark solve these shortcomings?&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Capable of leveraging the Hadoop ecosystem, e.g. HDFS, YARN, HBase, S3, …&lt;/li&gt;
&lt;li&gt;Has many other workflows, i.e. join, filter, flatMapdistinct, groupByKey, reduceByKey, sortByKey, collect, count, first…&lt;/li&gt;
&lt;li&gt;In-memory caching of data (for iterative, graph, and machine learning algorithms, etc.)&lt;/li&gt;
&lt;li&gt;Native Scala, Java, Python, and R support&lt;/li&gt;
&lt;li&gt;Supports interactive shells for exploratory data analysis&lt;/li&gt;
&lt;li&gt;Spark API is extremely simple to use&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--GgP51EMO--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.ttgtmedia.com/rms/onlineimages/business_analytics-hadoop_vs_spark-f_mobile.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--GgP51EMO--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.ttgtmedia.com/rms/onlineimages/business_analytics-hadoop_vs_spark-f_mobile.png" alt="" width="560" height="370"&gt;&lt;/a&gt;Credits: &lt;a href="https://cdn.ttgtmedia.com/rms/onlineimages/business_analytics-hadoop_vs_spark-f_mobile.png"&gt;https://cdn.ttgtmedia.com/rms/onlineimages/business_analytics-hadoop_vs_spark-f_mobile.png&lt;/a&gt;&lt;/p&gt;

&lt;h3 id="architecture-of-spark"&gt;Architecture of Spark&lt;/h3&gt;

&lt;p&gt;Spark is accessible, intense, powerful and proficient Big Data tool for handling different enormous information challenges. Apache Spark takes after an ace/slave engineering with two primary Daemons and a Cluster Manager&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Master Daemon – (Master/Driver Process)&lt;/li&gt;
&lt;li&gt;Worker Daemon – (Slave Process)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--JyC2pBfX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://spark.apache.org/docs/latest/img/cluster-overview.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--JyC2pBfX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://spark.apache.org/docs/latest/img/cluster-overview.png" alt="" width="596" height="286"&gt;&lt;/a&gt;Credits:&lt;a href="https://spark.apache.org/docs/latest/img/cluster-overview.png"&gt;https://spark.apache.org/docs/latest/img/cluster-overview.png&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A spark cluster has a solitary Master and many numbers of Slaves/Workers. The driver and the agents run their individual Java procedures and users can execute them on individual machines. Below are the three methods of building Spark with Hadoop Components  -&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Standalone - The arrangement implies Spark possesses the place on the top of HDFS(Hadoop Distributed File System) and space is allotted for HDFS, unequivocally. Here, Spark and MapReduce will run one next to the other to covering all in the form of Cluster.&lt;/li&gt;
&lt;li&gt;Hadoop Yarn - Hadoop Yarn arrangement implies, basically, Spark keeps running on Yarn with no pre-establishment or root get to required. It incorporates Spark into the Hadoop environment or the Hadoop stack. It enables different parts to keep running on the top of the stack having an explicit allocation for HDFS.&lt;/li&gt;
&lt;li&gt;Spark in MapReduce - Spark in MapReduce is utilized to dispatch start work notwithstanding independent arrangement. With SIMR, the client can begin Spark and uses its shell with no regulatory access.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--b9xcYvuP--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://databricks.com/wp-content/uploads/2014/01/SparkHadoop.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--b9xcYvuP--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://databricks.com/wp-content/uploads/2014/01/SparkHadoop.png" alt="" width="472" height="239"&gt;&lt;/a&gt;Credits: &lt;a href="https://databricks.com/wp-content/uploads/2014/01/SparkHadoop.png"&gt;https://databricks.com/wp-content/uploads/2014/01/SparkHadoop.png&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstractions in Spark Architecture&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In this architecture, all the components and layers are loosely coupled. These components are integrated with several extensions as well as libraries. There are mainly two abstractions on which spark architecture is based. They are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Resilient Distributed Datasets (RDD): These are the collection of object which is logically partitioned. &lt;ol&gt;
&lt;li&gt;It supports in-memory computation over spark cluster. Spark RDDs are immutable in nature. &lt;/li&gt;
&lt;li&gt;It supports Hadoop datasets and parallelized collections. &lt;/li&gt;
&lt;li&gt;Hadoop Datasets are created from the files stored on HDFS. Parallelized collections are based on existing scala collections. &lt;/li&gt;
&lt;li&gt;As RDDs are immutable, it offers two operations transformations and actions.&lt;/li&gt;
&lt;/ol&gt;


&lt;/li&gt;
&lt;li&gt;Directed Acyclic Graph (DAG): Directed- Graph which is directly connected from one node to another. This creates a sequence.&lt;ol&gt;
&lt;li&gt;Acyclic – It defines that there is no cycle or loop available.&lt;/li&gt;
&lt;li&gt;Graph – It is a combination of vertices and edges, with all the connections in a sequence&lt;/li&gt;
&lt;li&gt;We can call it a sequence of computations, performed on data. In this graph, edge refers to transformation on top of data. while vertices refer to an RDD partition. This helps to eliminate the Hadoop mapreduce multistage execution model. It also provides efficient performance over Hadoop.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--sopxQOS2--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://hxquangnhat.files.wordpress.com/2015/03/scheduling.jpeg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--sopxQOS2--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://hxquangnhat.files.wordpress.com/2015/03/scheduling.jpeg" alt="" width="880" height="467"&gt;&lt;/a&gt;Credits:&lt;a href="https://hxquangnhat.files.wordpress.com/2015/03/scheduling.jpeg"&gt;https://hxquangnhat.files.wordpress.com/2015/03/scheduling.jpeg&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spark Execution Flow :&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Application Jar: User program and its dependencies except Hadoop &amp;amp; Spark Jars bundled into a Jar file.&lt;/li&gt;
&lt;li&gt;Driver Program: The process to start the execution (main() function)&lt;ol&gt;
&lt;li&gt;Main process co-ordinated by the SparkContext object.&lt;/li&gt;
&lt;li&gt;Allows to configure any spark process wth specific parameters&lt;/li&gt;
&lt;li&gt;Spark actions are executed in the Driver&lt;/li&gt;
&lt;/ol&gt;


&lt;/li&gt;
&lt;li&gt;Cluster Manager: An external service to manage resources on the cluster ( YARN)&lt;ol&gt;
&lt;li&gt;External services for acquring resources on the cluster&lt;/li&gt;
&lt;li&gt;Variety of cluster managers such as Local, Standalone, and Yarn&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Deploy Mode&lt;ol&gt;
&lt;li&gt;Cluster: Driver inside the cluster, framework launches the driver inside of the cluster.&lt;/li&gt;
&lt;li&gt;Client: Driver outside of cluster&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Worker Node: any node that run the application program in cluster.&lt;/li&gt;
&lt;li&gt;Executor: A process launched for an application on a worker node that run tasks and keeps data in memory or disk storage across them. Each application has its own executors.&lt;/li&gt;
&lt;li&gt;Task: a unit of work that will be sent to executor&lt;/li&gt;
&lt;li&gt;Job: a parallel computation consisting of multiple tasks that gets spawned in response to a spark action.&lt;/li&gt;
&lt;li&gt;Stage: Each job is divided into smaller set of tasks called stages that is sequential and depend on each other&lt;/li&gt;
&lt;li&gt;Spark Context: represents the connection to a spark cluster, and can be used to create RDDs accumulator and broadcast variables on that cluster.&lt;ol&gt;
&lt;li&gt;Main entry point for Spark functionality&lt;/li&gt;
&lt;li&gt;Represents the connection to a spark cluster&lt;/li&gt;
&lt;li&gt;Tells spark how &amp;amp; where to access a cluster&lt;/li&gt;
&lt;li&gt;Can be used to create RDDs, accumulators and broadcast variables on that cluster.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id="resillient-distributed-dataset-rdd"&gt;Resillient Distributed dataset  (RDD)&lt;/h3&gt;

&lt;p&gt;Resilient Distributed dataset  (RDD) is a basic abstraction in spark. Immutable, Partitioned collection of elements that can be operated in parallel.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Main characteristics of RDD&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A list of partitions&lt;/li&gt;
&lt;li&gt;A function for computing each split&lt;/li&gt;
&lt;li&gt;A list of dependencies an other RDDs&lt;/li&gt;
&lt;li&gt;Optionally,a partioner for key-value RDDs.&lt;/li&gt;
&lt;li&gt;A list of preferred locations to compute each split on.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Custom RDD can be also implemented (by overriding functions)&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--egMNxszM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn2.hubspot.net/hubfs/5041972/Imported_Blog_Media/fig-1-apache-spark-process-flow-4.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--egMNxszM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn2.hubspot.net/hubfs/5041972/Imported_Blog_Media/fig-1-apache-spark-process-flow-4.png" alt="" width="880" height="582"&gt;&lt;/a&gt;Credits: &lt;a href="https://cdn2.hubspot.net/hubfs/5041972/Imported_Blog_Media/fig-1-apache-spark-process-flow-4.png"&gt;https://cdn2.hubspot.net/hubfs/5041972/Imported_Blog_Media/fig-1-apache-spark-process-flow-4.png&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Transformations: These are functions that accept the existing RDDs as input and outputs one or more RDDs. However, the data in the existing RD in Spark does not change as it is immutable. &lt;ol&gt;&lt;li&gt;These transformation are executed when they are invoked or called. Every time transformation are applied, a new RDD is created. &lt;/li&gt;&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Actions: These are functions that return the end result of RDD computations. It uses a lineage graph to load data onto the RDD in particular order. After all of the transformations are done, actions return the final result to the Spark driver. Actions are Operations that provide non-RDD values. &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt; Features of RDD&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RDD in Apache Spark is an immutable collection of objects which computes on the different node of the cluster. &lt;/li&gt;
&lt;li&gt;Resilient, i.e. fault-tolerant with the help of RDD lineage graph(DAG) and so able to recompute missing or damaged partitions due to node failures.&lt;/li&gt;
&lt;li&gt;Distributed, since Data resides on multiple nodes.&lt;/li&gt;
&lt;li&gt;Dataset represents records of the data you work with. The user can load the data set externally which can be either JSON file, CSV file, text file or database via JDBC with no specific data structure.&lt;/li&gt;
&lt;li&gt;each and every dataset in RDD is logically partitioned across many servers so that they can be computed on different nodes of the cluster. RDDs are fault tolerant i.e. It posses self-recovery in the case of failure.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Dependencies of RDD&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Narrow dependencies:  Each parition of the parent RDD is used by at most one parition of the child RDD. Task can be executed locally and we don't have to shuffle. &lt;ol&gt;
&lt;li&gt;Pipelined execution&lt;/li&gt;
&lt;li&gt;Efficient recovery&lt;/li&gt;
&lt;li&gt;No shuffling&lt;/li&gt;
&lt;/ol&gt;


&lt;/li&gt;
&lt;li&gt;Wide dependencies: multiple child partitions may depend on one partition of the parent RDD. his means we have to shuffle data unless the parents are hash-partitioned.&lt;ol&gt;
&lt;li&gt;Requires shuffling unless parents are hash-partitioned &lt;/li&gt;
&lt;li&gt;Expensive to recover&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--puE6LJhW--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://lh4.googleusercontent.com/proxy/IOyxDouNY6J8KPLjRX11BF_422eWcgWdpu4gHkiZdhBnHAdFqIc8rMjVFoGqKpvJkIHouYanNHFSfuf5PAo4651XzwRvPL2GjObFjebEZXMTfTCOoi-SmwTweQ4%3Dw1200-h630-p-k-no-nu" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--puE6LJhW--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://lh4.googleusercontent.com/proxy/IOyxDouNY6J8KPLjRX11BF_422eWcgWdpu4gHkiZdhBnHAdFqIc8rMjVFoGqKpvJkIHouYanNHFSfuf5PAo4651XzwRvPL2GjObFjebEZXMTfTCOoi-SmwTweQ4%3Dw1200-h630-p-k-no-nu" alt="" width="880" height="462"&gt;&lt;/a&gt;Credits: &lt;a href="https://lh4.googleusercontent.com/proxy/IOyxDouNY6J8KPLjRX11BF_422eWcgWdpu4gHkiZdhBnHAdFqIc8rMjVFoGqKpvJkIHouYanNHFSfuf5PAo4651XzwRvPL2GjObFjebEZXMTfTCOoi-SmwTweQ4=w1200-h630-p-k-no-nu"&gt;https://lh4.googleusercontent.com/proxy/IOyxDouNY6J8KPLjRX11BF_422eWcgWdpu4gHkiZdhBnHAdFqIc8rMjVFoGqKpvJkIHouYanNHFSfuf5PAo4651XzwRvPL2GjObFjebEZXMTfTCOoi-SmwTweQ4=w1200-h630-p-k-no-nu&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How to create a RDD?&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;By loading an external dataset: You can load an external file onto an RDD. The types of you can load are csv, txt  , json, etc. Here is the example of loading a text file onto an RDD. &lt;/li&gt;
&lt;li&gt;By paralleling the collection of objects: When spark's parallelize method is applied to a group of elements, a new distributed dataset is created. This dateset is an RDD. &lt;/li&gt;
&lt;li&gt;By Performing Transformation on the existing RDDs: One or more RDDs can be created by performing transformations on the existing RDDs as mentioned earlier in this tutorial page. &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id="spark-sql"&gt;Spark SQL&lt;/h2&gt;

&lt;p&gt;Spark SQL integrates relational processing with Spark’s functional programming. It provides support for various data sources and makes it possible to weave SQL queries with code transformations thus resulting in a very powerful tool.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spark SQL originated as Apache Hive to run on top of Spark and is now integrated with the Spark stack. Apache Hive had certain limitations Spark SQL was built to overcome these drawbacks and replace Apache Hive.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Spark SQL is faster than Hive when it comes to processing speed. Spark SQL is an Apache Spark module used for structured data processing, which:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;span&gt;Acts as a distributed SQL query engine&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Provides DataFrames for programming abstraction&lt;/li&gt;
&lt;li&gt;Allows to query structured data in Spark programs&lt;/li&gt;
&lt;li&gt;Can be used with platforms such as Scala, Java, R, and Python.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Features of Spark SQL: &lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Integrated − Seamlessly mix SQL queries with Spark programs. Spark SQL lets you query structured data as a distributed dataset (RDD) in Spark, with integrated APIs in Python, Scala and Java. This tight integration makes it easy to run SQL queries alongside complex analytic algorithms.&lt;/li&gt;
&lt;li&gt;Unified Data Access − Load and query data from a variety of sources. Schema-RDDs provide a single interface for efficiently working with structured data, including Apache Hive tables, parquet files and JSON files.&lt;/li&gt;
&lt;li&gt;Hive Compatibility − Run unmodified Hive queries on existing warehouses. Spark SQL reuses the Hive frontend and MetaStore, giving you full compatibility with existing Hive data, queries, and UDFs. Simply install it alongside Hive.&lt;/li&gt;
&lt;li&gt;Standard Connectivity − Connect through JDBC or ODBC. Spark SQL includes a server mode with industry standard JDBC and ODBC connectivity.&lt;/li&gt;
&lt;li&gt;Scalability − Use the same engine for both interactive and long queries. Spark SQL takes advantage of the RDD model to support mid-query fault tolerance, letting it scale to large jobs too. Do not worry about using a different engine for historical data.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Limitations of a Apache Hive:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Hive uses MapReduce which lags in performance with medium and small sized datasets.&lt;/li&gt;
&lt;li&gt;No resume capability&lt;/li&gt;
&lt;li&gt;Hive cannot drop encrypted database&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Spark SQL was built to overcome the limitations of Apache Hive running on top of Spark. Spark SQL uses the metastore services of Hive to query the data stored and manged by Hive.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--pDG_JVZh--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/http://programmerprodigycode.files.wordpress.com/2022/01/9309e-18gptmzkg5riiktdb2zimyg.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--pDG_JVZh--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/http://programmerprodigycode.files.wordpress.com/2022/01/9309e-18gptmzkg5riiktdb2zimyg.png" alt="" width="612" height="512"&gt;&lt;/a&gt;Credits: &lt;a href="http://programmerprodigycode.files.wordpress.com/2022/01/9309e-18gptmzkg5riiktdb2zimyg.png"&gt;http://programmerprodigycode.files.wordpress.com/2022/01/9309e-18gptmzkg5riiktdb2zimyg.png&lt;/a&gt;&lt;/p&gt;

&lt;h3 id="spark-sql-architecture"&gt;Spark SQL Architecture&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Language API: Spark is very compatible as it supports languages like Python, Scala and Java.&lt;/li&gt;
&lt;li&gt;Schema RDD: As Spark SQL works on schema, tables and records you can use Schema RDD or dataframe as a temporary table.&lt;/li&gt;
&lt;li&gt;Data Sources: Spark SQL supports multiple data sources like JSON, Cassandra database, Hive tables. &lt;/li&gt;
&lt;li&gt;Data Source API is used to read and store structure and semi structured data into Spark SQL:&lt;ol&gt;
&lt;li&gt;Structured/Semi-structure data&lt;/li&gt;
&lt;li&gt;Multiple formats&lt;/li&gt;
&lt;li&gt;3 rd party integrations&lt;/li&gt;
&lt;/ol&gt;


&lt;/li&gt;
&lt;li&gt;DataFrame APi converts the data that is read through Data source API into tabular colmns to help perform SQL operations.&lt;ol&gt;
&lt;li&gt;Distributed collection of data organized into named columns&lt;/li&gt;
&lt;li&gt;Equivalent to a relational table in SQL&lt;/li&gt;
&lt;li&gt;Lazily evaluated&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;SQL Interpreter and Optimised handles the functional programming part of Spark SQL. it transforms the Data frames RDDs to get the required results in the required formats.&lt;ol&gt;
&lt;li&gt;Functional programming&lt;/li&gt;
&lt;li&gt;transforming trees&lt;/li&gt;
&lt;li&gt;Faster than rdds&lt;/li&gt;
&lt;li&gt;Processes all size data&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--uN7vRLHE--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://www.edureka.co/blog/wp-content/uploads/2017/04/Spark-SQL-Spark-Tutorial-Edureka.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--uN7vRLHE--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://www.edureka.co/blog/wp-content/uploads/2017/04/Spark-SQL-Spark-Tutorial-Edureka.png" alt="" width="880" height="381"&gt;&lt;/a&gt;Credits: &lt;a href="https://www.edureka.co/blog/wp-content/uploads/2017/04/Spark-SQL-Spark-Tutorial-Edureka.png"&gt;https://www.edureka.co/blog/wp-content/uploads/2017/04/Spark-SQL-Spark-Tutorial-Edureka.png&lt;/a&gt;&lt;/p&gt;

&lt;p id="sql-service-is-the-entry-point-for-working-along-structured-data-in-spark"&gt;&lt;strong&gt;SQL service is the entry point for working along structured data in spark, and is used to fetch the result from the interpreted and optimised data.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id="spark-mlib"&gt;Spark MLib&lt;/h2&gt;

&lt;p&gt;Mlib stands for Machine learnign library in Spark. The goal of this library is to make practical machine learning scalable and easy to implement.&lt;/p&gt;

&lt;p&gt;It contains fast and scalable implementations of standard machine learning algorithms. Through Spark MLlib, data engineers and data scientists have access to different types of statistical analysis, linear algebra and various optimization primitives. Spark Machine Learning library MLlib contains the following applications &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Collaborative Filtering for Recommendations – Alternating Least Squares&lt;/li&gt;
&lt;li&gt;Regression for Predictions – Logistic Regression, Lasso Regression, Ridge Regression, Linear Regression and Support Vector Machines (SVM).&lt;/li&gt;
&lt;li&gt;Clustering – Linear Discriminant Analysis, K-Mean and Gaussian,&lt;/li&gt;
&lt;li&gt;Classification Algorithms – Naïve Bayes, Ensemble Methods, and Decision Trees.&lt;/li&gt;
&lt;li&gt;Dimensionality Reduction –PCA (Principal Component Analysis) and Singular Value Decomposition (SVD)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Benefits of Spark MLib:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Spark MLlib is tightly integrated on top of Spark which eases the development of efficient large-scale machine learning algorithms as are usually iterative in nature.&lt;/li&gt;
&lt;li&gt;MLlib is easy to deploy and does not require any pre-installation, if Hadoop 2 cluster is already installed and running.&lt;/li&gt;
&lt;li&gt;Spark MLlib’s scalability, simplicity, and language compatibility helps data scientists solve iterative data problems faster. &lt;/li&gt;
&lt;li&gt;MLlib provides ultimate performance gains (about 10 to 100 times faster than Hadoop and Apache Mahout). &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--NDUogJ5x--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://spark.apache.org/docs/latest/img/ml-PipelineModel.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--NDUogJ5x--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://spark.apache.org/docs/latest/img/ml-PipelineModel.png" alt="" width="880" height="248"&gt;&lt;/a&gt;Credits: &lt;a href="https://spark.apache.org/docs/latest/img/ml-PipelineModel.png"&gt;https://spark.apache.org/docs/latest/img/ml-PipelineModel.png&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Features of Spark MLlib Library&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;MLlib provides algorithmic optimisations for accurate predictions and efficient distributed learning. &lt;ul&gt;&lt;li&gt;For instance, the alternating least squares machine learning algorithms for making recommendations effectively uses blocking to reduce JVM garbage collection overhead.&lt;/li&gt;&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MLlib benefits from its tight integration with various spark components. &lt;/li&gt;
&lt;li&gt;MLlib provides a package called spark.ml to simplify the development and performance tuning of multi-stage machine learning pipelines. &lt;ul&gt;&lt;li&gt;MLlib provides high-level API’s which help data scientists swap out a standard learning approach instead of using their own specialized machine learning algorithms.&lt;/li&gt;&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MLlib provides fast and distributed implementations of common machine learning algorithms along with a number of low-level primitives and various utilities for statistical analysis, feature extraction, convex optimizations, and distributed linear algebra.&lt;/li&gt;
&lt;li&gt;Spark MLlib library has extensive documentation which describes all the supported utilities and methods with several spark machine learning example codes and the API docs for all the supported languages.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Spark Mlib Tools:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Ml Algorithms: classfication, regrssion, clustering and collaborative filtering&lt;/li&gt;
&lt;li&gt;Featurization: feature extraction, transformation, dimensionality reduction, and selection&lt;/li&gt;
&lt;li&gt;Pipelines: tools for constructing, evaluating and tunnignML pipelines&lt;/li&gt;
&lt;li&gt;Persistence: saving and loading algorithms, models and pipelines&lt;/li&gt;
&lt;li&gt;Utilities: linear algebra, statistics, data handling&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--7RXBCE-e--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://docs.microsoft.com/en-us/sql/big-data-cluster/media/big-data-cluster-overview/ai-ml-spark.png%3Fview%3Dsql-server-ver15" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--7RXBCE-e--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://docs.microsoft.com/en-us/sql/big-data-cluster/media/big-data-cluster-overview/ai-ml-spark.png%3Fview%3Dsql-server-ver15" alt="" width="880" height="247"&gt;&lt;/a&gt;Credits: &lt;a href="https://docs.microsoft.com/en-us/sql/big-data-cluster/media/big-data-cluster-overview/ai-ml-spark.png?view=sql-server-ver15"&gt;https://docs.microsoft.com/en-us/sql/big-data-cluster/media/big-data-cluster-overview/ai-ml-spark.png?view=sql-server-ver15&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Machine learning Pipelines componenets&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Data frame: The ML API uses Dataframe from Spark SQL as a dataset, which can be used to hold a variety of datatypes&lt;/li&gt;
&lt;li&gt;Transformer: This is used to transform one Dataframe to another Dataframe. Examples are  &lt;ol&gt;
&lt;li&gt;Hashing Term Frequency: This calculates how wordoccurs&lt;/li&gt;
&lt;li&gt;Logistic Regression Model: The model which resultsfrom trying logistic regressions on a dataset&lt;/li&gt;
&lt;li&gt;Binarizer: This changes a given threshold value to 1or 0&lt;/li&gt;
&lt;/ol&gt;


&lt;/li&gt;
&lt;li&gt;Estimator: It is an algorithm which can be used on a Dataframe to produce Transformer. Examples are:&lt;ol&gt;
&lt;li&gt;Logistic Regression: It is used to determine the weights for the resulting Logistic Regression Model by processing the dataframe&lt;/li&gt;
&lt;li&gt;Standard Scaler: It is used to calculate the Standard deviation&lt;/li&gt;
&lt;li&gt;Pipeline: Calling fit on a pipeline produces pipeline model, and the pipeline contains only transformers and not the estimators&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Pipeline: A pipeline chains multiple Transformers and Estimators together to specify the ML workflow&lt;/li&gt;
&lt;li&gt;Parameters: To specify the parameters a common API is used by the Transformers and Estimators&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For more information about Hadoop and HDFS,  Hive&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href="https://programmerprodigy.code.blog/2021/06/28/introduction-to-hadoophdfs-and-hbase/"&gt;https://programmerprodigy.code.blog/2021/06/28/introduction-to-hadoophdfs-and-hbase/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://programmerprodigy.code.blog/2022/01/08/hive-sql-layer-above-hadoop/" rel="noreferrer noopener"&gt;https://programmerprodigy.code.blog/2022/01/08/hive-sql-layer-above-hadoop/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
      <category>database</category>
      <category>bigdata</category>
    </item>
    <item>
      <title>AngularJS is dead.</title>
      <author>Manas Mishra</author>
      <pubDate>Sat, 15 Jan 2022 06:52:05 +0000</pubDate>
      <link>https://dev.to/manas_dev/angularjs-is-dead-455n</link>
      <guid>https://dev.to/manas_dev/angularjs-is-dead-455n</guid>
      <description>&lt;p&gt;Yes, you read it right. AngularJS is dead now. The first version of Angular, is not being supported by Angular any more. Angular Developer, published an article over "The Angular Blog", that they are discontinuing the Long Term Support for AngularJS (&lt;a href="https://blog.angular.io/discontinued-long-term-support-for-angularjs-cc066b82e65a"&gt;check here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Angular Team recommend developers and businesses, who are using AngularJS to switch to the successor of Angular JS, which is Angular. Currently, they released Angular v13, but if you are using any angular version, other than the first one, which is actually Angular JS, then you are safe to work with it. But, if your project is still in AngularJS, then &lt;a href="https://blog.angular.io/finding-a-path-forward-with-angularjs-7e186fdd4429"&gt;this article&lt;/a&gt; will help you to migrate it to Angular. &lt;/p&gt;

&lt;p&gt;Here’s what you can expect as a part of this new long term state: &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;CDN links will remain active and &lt;a href="https://angularjs.org/"&gt;AngularJS.org&lt;/a&gt; will remain online&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We’ll provide read-only access to the code, issue, and pull request history on GitHub by archiving the &lt;a href="https://github.com/angular/angular.js?"&gt;angular.js repo&lt;/a&gt; and related repositories (that is, &lt;a href="https://github.com/angular/material"&gt;AngularJS Material&lt;/a&gt;, &lt;a href="https://github.com/angular/bower-material"&gt;bower-material&lt;/a&gt;, etc.)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The AngularJS npm packages will remain on npm and bower, marked as deprecated&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;AngularJS set a solid foundation for Angular and helped define the future of the web, because many of us, who are web developers, learnt Angular as it's first Javascript framework. &lt;/p&gt;

</description>
      <category>angular</category>
      <category>javascript</category>
      <category>webdev</category>
      <category>programming</category>
    </item>
    <item>
      <title>Managing migrations in Prisma (Add/Rename columns)</title>
      <author>Chris Bongers</author>
      <pubDate>Sat, 15 Jan 2022 06:31:34 +0000</pubDate>
      <link>https://dev.to/dailydevtips1/managing-migrations-in-prisma-addrename-columns-57g5</link>
      <guid>https://dev.to/dailydevtips1/managing-migrations-in-prisma-addrename-columns-57g5</guid>
      <description>&lt;p&gt;Migrations are a super powerful way to do database schema migrations.&lt;br&gt;
This will allow you to keep your database in sync with changes you make to your schema while maintaining existing data.&lt;/p&gt;

&lt;p&gt;We already created our &lt;a href="https://daily-dev-tips.com/posts/set-up-a-local-prisma-instance/"&gt;first migration&lt;/a&gt;, which was the initialization of the database.&lt;/p&gt;

&lt;p&gt;Let's go from there and make changes to the schema to see what will happen.&lt;/p&gt;

&lt;p&gt;If you plan to follow along, you can find the &lt;a href="https://github.com/rebelchris/local-prisma/tree/part-1"&gt;GitHub repo here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Open the &lt;code&gt;prisma/prisma.schema&lt;/code&gt; file and make the following changes to the existing schema.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="c1"&gt;// before&lt;/span&gt;
&lt;span class="nx"&gt;model&lt;/span&gt; &lt;span class="nx"&gt;Hobby&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;id&lt;/span&gt;      &lt;span class="nx"&gt;Int&lt;/span&gt;     &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="nd"&gt;id&lt;/span&gt; &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="nd"&gt;default&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;autoincrement&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
  &lt;span class="nx"&gt;title&lt;/span&gt;   &lt;span class="nb"&gt;String&lt;/span&gt;  &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="nd"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;VarChar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nx"&gt;user&lt;/span&gt;    &lt;span class="nx"&gt;User&lt;/span&gt;    &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="nd"&gt;relation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;fields&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;userId&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="nx"&gt;references&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;id&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
  &lt;span class="nx"&gt;userId&lt;/span&gt;  &lt;span class="nx"&gt;Int&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="c1"&gt;// after&lt;/span&gt;
&lt;span class="nx"&gt;model&lt;/span&gt; &lt;span class="nx"&gt;Hobby&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;id&lt;/span&gt;      &lt;span class="nx"&gt;Int&lt;/span&gt;     &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="nd"&gt;id&lt;/span&gt; &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="nd"&gt;default&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;autoincrement&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
  &lt;span class="nx"&gt;name&lt;/span&gt;    &lt;span class="nb"&gt;String&lt;/span&gt;  &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="nd"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;VarChar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nx"&gt;rank&lt;/span&gt;    &lt;span class="nx"&gt;Int&lt;/span&gt;
  &lt;span class="nx"&gt;user&lt;/span&gt;    &lt;span class="nx"&gt;User&lt;/span&gt;    &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="nd"&gt;relation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;fields&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;userId&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="nx"&gt;references&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;id&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
  &lt;span class="nx"&gt;userId&lt;/span&gt;  &lt;span class="nx"&gt;Int&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;As you can see, two things happened here.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The &lt;code&gt;title&lt;/code&gt; column changed to &lt;code&gt;name&lt;/code&gt;
&lt;/li&gt;
&lt;li&gt;We added a &lt;code&gt;rank&lt;/code&gt; column&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then we can create a new migration by running the following command.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;npx prisma migrate dev &lt;span class="nt"&gt;--name&lt;/span&gt; change_hobby_table
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;However, we'll be quickly prompted with a message this is not possible.&lt;/p&gt;

&lt;p&gt;And that is caused because Prisma does not handle renames. This makes sense as they can't identify whether we renamed a column or removed it and added a new one.&lt;/p&gt;

&lt;p&gt;We can run the migration with a &lt;code&gt;-create-only&lt;/code&gt; flag to solve this use case.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;npx prisma migrate dev &lt;span class="nt"&gt;--name&lt;/span&gt; change_hobby_table &lt;span class="nt"&gt;--create-only&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;This will create a new migration file you can find at: &lt;code&gt;prisma/migrations/{time}_change_hobby_table&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you open this file, you can see the SQL that's generated.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight sql"&gt;&lt;code&gt;&lt;span class="c1"&gt;-- AlterTable&lt;/span&gt;
&lt;span class="k"&gt;ALTER&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="nv"&gt;"Hobby"&lt;/span&gt; &lt;span class="k"&gt;DROP&lt;/span&gt; &lt;span class="k"&gt;COLUMN&lt;/span&gt; &lt;span class="nv"&gt;"title"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="k"&gt;ADD&lt;/span&gt; &lt;span class="k"&gt;COLUMN&lt;/span&gt;     &lt;span class="nv"&gt;"name"&lt;/span&gt; &lt;span class="nb"&gt;VARCHAR&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;NOT&lt;/span&gt; &lt;span class="k"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="k"&gt;ADD&lt;/span&gt; &lt;span class="k"&gt;COLUMN&lt;/span&gt;     &lt;span class="nv"&gt;"rank"&lt;/span&gt; &lt;span class="nb"&gt;INTEGER&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;We can manually fix this SQL to fix our current need to rename the title column.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight sql"&gt;&lt;code&gt;&lt;span class="c1"&gt;-- AlterTable&lt;/span&gt;
&lt;span class="k"&gt;ALTER&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="nv"&gt;"Hobby"&lt;/span&gt; &lt;span class="k"&gt;RENAME&lt;/span&gt; &lt;span class="k"&gt;COLUMN&lt;/span&gt; &lt;span class="nv"&gt;"title"&lt;/span&gt; &lt;span class="k"&gt;TO&lt;/span&gt; &lt;span class="nv"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;ALTER&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="nv"&gt;"Hobby"&lt;/span&gt; &lt;span class="k"&gt;ADD&lt;/span&gt; &lt;span class="k"&gt;COLUMN&lt;/span&gt; &lt;span class="nv"&gt;"rank"&lt;/span&gt; &lt;span class="nb"&gt;INTEGER&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;We can execute the migration by running the following command.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;npx prisma migrate dev
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;And once it's done, let's check out our database to see what happened.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--xISfnXPn--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1641364746131/2OV5VasrB.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--xISfnXPn--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1641364746131/2OV5VasrB.png" alt="Database migration Prisma" width="880" height="376"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Perfect, our &lt;code&gt;title&lt;/code&gt; column is now named &lt;code&gt;name&lt;/code&gt;, but it still has all the data.&lt;br&gt;
And we have a new column, &lt;code&gt;rank&lt;/code&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: Prisma has a roadmap item to make this experience better. You can track it here: &lt;a href="https://www.notion.so/Improvement-to-Prisma-Migrate-for-better-handling-of-field-renames-9e46e2553419437684fbe41fe33369bc"&gt;Improvement of Prisma Migrate&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As for today's article, you can find the &lt;a href="https://github.com/rebelchris/local-prisma/tree/part-2"&gt;complete code samples on GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#thank-you-for-reading-and-lets-connect"&gt;
  &lt;/a&gt;
  Thank you for reading, and let's connect!
&lt;/h3&gt;

&lt;p&gt;Thank you for reading my blog. Feel free to subscribe to my email newsletter and connect on &lt;a href="https://www.facebook.com/DailyDevTipsBlog"&gt;Facebook&lt;/a&gt; or &lt;a href="https://twitter.com/DailyDevTips1"&gt;Twitter&lt;/a&gt;&lt;/p&gt;

</description>
      <category>database</category>
      <category>node</category>
    </item>
    <item>
      <title>Introduction to Amazon Kinesis</title>
      <author>Adit Modi</author>
      <pubDate>Sat, 15 Jan 2022 04:35:02 +0000</pubDate>
      <link>https://dev.to/aws-builders/introduction-to-amazon-kinesis-18fh</link>
      <guid>https://dev.to/aws-builders/introduction-to-amazon-kinesis-18fh</guid>
      <description>&lt;h1&gt;
  &lt;a href="#introduction"&gt;
  &lt;/a&gt;
  Introduction
&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Amazon Kinesis&lt;/strong&gt; is a platform for streaming data on AWS that makes it easy to load and analyze streaming data. Amazon Kinesis also enables you to build custom streaming data applications for specialized needs. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With Kinesis, you can ingest real-time data such as application logs, website clickstreams, Internet of Things (IoT) telemetry data, and more into your databases, data lakes, and data warehouses, or build your own real-time applications using this data. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Amazon Kinesis enables you to process and analyze data as it arrives and respond in real-time instead of having to wait until all your data is collected before the processing can begin.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Currently there are four pieces of the Kinesis platform that can be utilized based on your use case:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;Amazon Kinesis Data Streams&lt;/strong&gt; enables you to build custom applications that process or analyze streaming data.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Amazon Kinesis Video Streams&lt;/strong&gt; enables you to build custom applications that process or analyze streaming video.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Amazon Kinesis Data Firehose&lt;/strong&gt; enables you to deliver real-time streaming data to AWS destinations such as such as Amazon S3, Amazon Redshift, OpenSearch Service, and Splunk.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Amazon Kinesis Data Analytics&lt;/strong&gt; enables you to process and analyze streaming data with standard SQL or with Java (managed Apache Flink).&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--zIEisSf4--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/687ydk73s4r9gv8vkpjt.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--zIEisSf4--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/687ydk73s4r9gv8vkpjt.png" alt="Image description" width="880" height="495"&gt;&lt;/a&gt; &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Big Data Analytics Options on AWS&lt;/strong&gt; is a Series containing different articles that provides a basic introduction to different Big Data Analytics Options on AWS. Each article covers the detailed guide on how each service is used for collecting, processing, storing, and analyzing big data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Kinesis Data Streams and Kinesis Video Streams enable you to build custom applications that process or analyze streaming data in real time. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Kinesis Data Streams can continuously capture and store terabytes of data per hour from hundreds of thousands of sources, such as website clickstreams, financial transactions, social media feeds, IT logs, and location-tracking events. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Kinesis Video Streams can continuously capture video data from smartphones, security cameras, drones, satellites, dashcams, and other edge devices.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With the Amazon Kinesis Client Library (KCL), you can build Amazon Kinesis applications and use streaming data to power real-time dashboards, generate alerts, and implement dynamic pricing and advertising. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can also emit data from Kinesis Data Streams and Kinesis Video Streams to other AWS services such as Amazon S3, Amazon Redshift, Amazon EMR, and AWS Lambda.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Provision the level of input and output required for your data stream, in blocks of one megabyte per second (MB/sec), using the AWS Management Console, API, or SDKs. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The size of your stream can be adjusted up or down at any time without restarting the stream and without any impact on the data sources pushing data to the stream. Within seconds, data put into a stream is available for analysis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With Amazon Kinesis Data Firehose, you don't need to write applications or manage resources. You configure your data producers to send data to Kinesis Data Firehose and it automatically delivers the data to the AWS destination or third party (Splunk) that you specified. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can also configure Kinesis Data Firehose to transform your data before data delivery. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Amazon Kinesis Data Analytics is the easiest way to process and analyze real-time, streaming data. With Kinesis Data Analytics, you just use standard SQL or Java (Flink) to process your data streams, so you don’t have to learn any new programming languages.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Simply point Kinesis Data Analytics at an incoming data stream, write your SQL queries, and specify where you want to load the results. Kinesis Data Analytics takes care of running your SQL queries continuously on data while it’s in transit and sending the results to the destinations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For complex data processing applications, Amazon Kinesis Data Analytics provides an option use open-source libraries such as Apache Flink, Apache Beam, AWS SDK, and AWS service integrations. It includes more than ten connectors from Apache Flink, and gives you the ability to build custom integrations. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It’s also compatible with the AWS Glue Schema Registry, a serverless feature of AWS Glue that enables you to validate and control the evolution of streaming data using registered Apache Avro schemas.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can use Apache Flink in Amazon Kinesis Data Analytics to build applications whose processed records affect the results exactly once, referred to as exactly once processing. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This means that even in the case of an application disruption, like internal service maintenance or user-initiated application update, the service will ensure that all data is processed and there is no duplicate data. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The service stores previous and in-progress computations, or state, in running application storage. This enables you to compare real-time and past results over any time period, and provides fast recovery during application disruptions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The subsequent sections focus primarily on &lt;strong&gt;Amazon Kinesis Data Streams&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#ideal-usage-patterns"&gt;
  &lt;/a&gt;
  Ideal usage patterns
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Amazon Kinesis Data Steams is useful wherever there is a need to move data rapidly off producers (data sources) and continuously process it. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;That processing can be to transform the data before emitting into another data store, drive real-time metrics and analytics, or derive and aggregate multiple streams into more complex streams, or downstream processing. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The following are typical scenarios for using Kinesis Data Streams for analytics:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Real-time data analytics&lt;/strong&gt; – Kinesis Data Streams enables real-time data analytics on streaming data, such as analyzing website clickstream data and customer engagement analytics.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Log and data feed intake and processing&lt;/strong&gt; – With Kinesis Data Streams, you can have producers push data directly into an Amazon Kinesis stream. For example, you can submit system and application logs to Kinesis Data Streams and access the stream for processing within seconds. This prevents the log data from being lost if the front-end or application server fails, and reduces local log storage on the source. Kinesis Data Streams provides accelerated data intake because you are not batching up the data on the servers before you submit it for intake.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Real-time metrics and reporting&lt;/strong&gt; – You can use data ingested into Kinesis Data Streams for extracting metrics and generating KPIs to power reports and dashboards at real-time speeds. This enables data-processing application logic to work on data as it is streaming in continuously, rather than waiting for data batches to arrive.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#cost-model"&gt;
  &lt;/a&gt;
  Cost model
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Amazon Kinesis Data Streams has simple pay-as-you-go pricing, with no upfront costs or minimum fees, and you pay only for the resources you consume. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An Amazon Kinesis stream is made up of one or more shards. Each shard gives you a capacity of five read transactions per second, up to a maximum total of 2 MB of data read per second. Each shard can support up to 1,000 write transactions per second, and up to a maximum total of 1 MB data written per second.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The data capacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the capacity of each shard. There are two components to pricing:&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Primary pricing includes an hourly charge per shard and a charge for each one million PUT transactions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pricing for optional components for extended retention and enhanced fan-out.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For more information, see Amazon Kinesis Data Streams Pricing. Applications that run on Amazon EC2 and process Amazon Kinesis streams also incur standard Amazon EC2 costs.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#performance"&gt;
  &lt;/a&gt;
  Performance
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Amazon Kinesis Data Streams enables you to choose the throughput capacity you require in terms of shards. With each shard in an Amazon Kinesis stream, you can capture up to 1 megabyte per second of data at 1,000 write transactions per second. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Your Amazon Kinesis applications can read data from each shard at up to 2 megabytes per second. You can provision as many shards as you need to get the throughput capacity you want; for example, a one gigabyte per second data stream would require 1024 shards.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Additionally, there is a new feature. Enhanced fan-out enables developers to scale up the number of stream consumers (applications reading data from a stream in real-time) by offering each stream consumer their own read throughput. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Developers can register stream consumers to use enhanced fan-out and receive their own 2MB/sec pipe of read throughput per shard. This throughput automatically scales with the number of shards in a stream.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#durability-and-availability"&gt;
  &lt;/a&gt;
  Durability and availability
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Amazon Kinesis Data Streams synchronously replicates data across three Availability Zones in an AWS Region, providing high availability and data durability.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Additionally, you can store a cursor in Amazon DynamoDB to durably track what has been read from an Amazon Kinesis stream. In the event that your application fails in the middle of reading data from the stream, you can restart your application and use the cursor to pick up from the exact spot where the failed application left off.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#scalability-and-elasticity"&gt;
  &lt;/a&gt;
  Scalability and elasticity
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You can increase or decrease the capacity of the stream at any time according to your business or operational needs, without any interruption to ongoing stream processing. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;By using API calls or development tools, you can automate scaling of your Amazon Kinesis Data Streams environment to meet demand and ensure you only pay for what you need.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#interfaces"&gt;
  &lt;/a&gt;
  Interfaces
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;There are two interfaces to Kinesis Data Streams:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;Input&lt;/strong&gt; which is used by data producers to put data into Kinesis Data Streams&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Output&lt;/strong&gt; to process and analyze data that comes in&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Producers can write data using the Amazon Kinesis PUT API, an AWS Software Development Kit (SDK) or toolkit abstraction, the Amazon Kinesis Producer Library (KPL), or the Amazon Kinesis Agent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For processing data that has already been put into an Amazon Kinesis stream, there are client libraries provided to build and operate real-time streaming data processing applications. The KCL acts as an intermediary between Amazon Kinesis Data Streams and your business applications which contain the specific processing logic. There is also integration to read from an Amazon Kinesis stream into Apache Spark Streaming running on Amazon EMR.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#antipatterns"&gt;
  &lt;/a&gt;
  Anti-patterns
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Amazon Kinesis Data Streams has the following anti-patterns:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;Small scale consistent throughput&lt;/strong&gt; – Even though Kinesis Data Streams works for streaming data at 200 KB per second or less, it is designed and optimized for larger data throughputs.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Long-term data storage and analytics&lt;/strong&gt; – Kinesis Data Streams is not suited for long-term data storage. By default, data is retained for 24 hours, and you can extend the retention period by up to 365 days.&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;/ul&gt;




&lt;p&gt;Hope this guide gives you an Introduction to Amazon Kinesis.&lt;/p&gt;

&lt;p&gt;Let me know your thoughts in the comment section 👇&lt;br&gt;
And if you haven't yet, make sure to follow me on below handles:&lt;/p&gt;

&lt;p&gt;👋 &lt;strong&gt;connect with me on &lt;a href="https://www.linkedin.com/in/adit-modi-2a4362191/"&gt;LinkedIn&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
🤓 &lt;strong&gt;connect with me on &lt;a href="https://twitter.com/adi_12_modi"&gt;Twitter&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
🐱‍💻 &lt;strong&gt;follow me on &lt;a href="https://github.com/AditModi"&gt;github&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
✍️ &lt;strong&gt;Do Checkout &lt;a href="https://aditmodi.hashnode.dev"&gt;my blogs&lt;/a&gt;&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;Like, share and follow me 🚀 for more content.&lt;/p&gt;


&lt;div class="ltag__user ltag__user__id__497987"&gt;
  
    .ltag__user__id__497987 .follow-action-button {
      background-color: #000000 !important;
      color: #fa6c00 !important;
      border-color: #000000 !important;
    }
  
    &lt;a href="/aditmodi" class="ltag__user__link profile-image-link"&gt;
      &lt;div class="ltag__user__pic"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--FYJrtrA_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://res.cloudinary.com/practicaldev/image/fetch/s--mUzXbt1U--/c_fill%2Cf_auto%2Cfl_progressive%2Ch_150%2Cq_auto%2Cw_150/https://dev-to-uploads.s3.amazonaws.com/uploads/user/profile_image/497987/edde8513-7308-4a4d-9592-2be160d074d2.png" alt="aditmodi image"&gt;
      &lt;/div&gt;
    &lt;/a&gt;
  &lt;div class="ltag__user__content"&gt;
    &lt;h2&gt;
&lt;a class="ltag__user__link" href="/aditmodi"&gt;Adit Modi&lt;/a&gt;Follow
&lt;/h2&gt;
    &lt;div class="ltag__user__summary"&gt;
      &lt;a class="ltag__user__link" href="/aditmodi"&gt;Cloud Engineer | AWS Community Builder | 8x AWS Certified | 3x Azure Certified | Author of Cloud Tech , DailyDevOps &amp;amp; BigDataJournal | DEV moderator&lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;





&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/whitepapers/latest/big-data-analytics-options/amazon-kinesis.html"&gt;Reference Notes&lt;/a&gt;&lt;/p&gt;

</description>
      <category>aws</category>
      <category>bigdata</category>
      <category>analytics</category>
      <category>cloud</category>
    </item>
    <item>
      <title>How to deploy a static website in Netlify for free</title>
      <author>Archana Gandhi</author>
      <pubDate>Sat, 15 Jan 2022 03:28:14 +0000</pubDate>
      <link>https://dev.to/archanagandhi/how-to-deploy-a-static-website-in-netlify-for-free-3dg9</link>
      <guid>https://dev.to/archanagandhi/how-to-deploy-a-static-website-in-netlify-for-free-3dg9</guid>
      <description>&lt;p&gt;The question we all have after building our first website is "how to host" and "where to host" that too for FREE. I've the same question on my mind after creating my first ever website. &lt;/p&gt;

&lt;p&gt;Here is a solution of you, In this article I'm going to show you the step-by-step process on how to host a static website in &lt;a href="https://www.netlify.com/"&gt;netlify&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#what-and-why-netlify"&gt;
  &lt;/a&gt;
  What and Why Netlify?
&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Netlify is a cloud computing company that offers hosting and serverless backend services for web applications and static websites.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Netlife is free and easy to use. You can deploy your project in seconds from github repo. It also provides free SSL certificate.&lt;/p&gt;

&lt;p&gt;If you haven't signed up yet, Go ahead and &lt;a href="https://app.netlify.com/signup"&gt;Signup&lt;/a&gt; for free.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#step-1-add-your-website"&gt;
  &lt;/a&gt;
  Step 1: Add your website
&lt;/h3&gt;

&lt;p&gt;Once you've done with the signup process, You'll be headed to Overview page.There are two ways to add your site in netlify.You can either add from Github, Gitlab or bitbucket (or) You can simply drag and drop your website folder.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--PRI-cXeD--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ycyfenyqawqw9x99d90n.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--PRI-cXeD--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ycyfenyqawqw9x99d90n.png" alt="Overview page" width="836" height="459"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#first-method-git"&gt;
  &lt;/a&gt;
  First Method (Git)
&lt;/h3&gt;

&lt;p&gt;Once you've clicked "New site from Git", This page will appears.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--PKzUDRzB--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/u9896f63r7n7u1viitiq.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--PKzUDRzB--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/u9896f63r7n7u1viitiq.png" alt="git" width="880" height="499"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You need to connect to your github account and authorize it.&lt;br&gt;
Now just choose the repository you want to deploy and click deploy site.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--lFyoKEea--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kisyqp8d9u0udh8a3p5d.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--lFyoKEea--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kisyqp8d9u0udh8a3p5d.png" alt="repos" width="880" height="544"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once the build completes, your site is deployed.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--8mjoTpcF--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dqinb5muz247vqgxbt7m.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--8mjoTpcF--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dqinb5muz247vqgxbt7m.png" alt="deploy" width="880" height="386"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#second-method-drag-n-drop"&gt;
  &lt;/a&gt;
  Second Method (drag N drop)
&lt;/h3&gt;

&lt;p&gt;This method is pretty simple, You just need to drag and drop the project folder and you are done with the deployment.&lt;/p&gt;

&lt;p&gt;So you can deploy in any of the methods. After deploying, Netlify will automatically create name for your site. You can change it in "site settings -&amp;gt; change site name"&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--ND0rdYPZ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hjfwklw1dl1hakxczvdo.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--ND0rdYPZ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/hjfwklw1dl1hakxczvdo.png" alt="site" width="810" height="428"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can also set up custom domain and push new changes to the site.&lt;/p&gt;

&lt;p&gt;Anndddd, It's all DONE. You site is now live!!!!&lt;/p&gt;

&lt;p&gt;If you like this post, Please like and follow me! &lt;/p&gt;

</description>
      <category>webdev</category>
      <category>netlify</category>
      <category>beginners</category>
      <category>javascript</category>
    </item>
    <item>
      <title>GraphQL API recon with mitmproxy</title>
      <author>fx2301</author>
      <pubDate>Sat, 15 Jan 2022 02:03:08 +0000</pubDate>
      <link>https://dev.to/fx2301/graphql-api-recon-with-mitmproxy-2f5d</link>
      <guid>https://dev.to/fx2301/graphql-api-recon-with-mitmproxy-2f5d</guid>
      <description>&lt;h2&gt;
  &lt;a href="#why"&gt;
  &lt;/a&gt;
  Why?
&lt;/h2&gt;

&lt;p&gt;Capturing live examples of GraphQL queries and responses all in one place vastly simplifies recon.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#when"&gt;
  &lt;/a&gt;
  When?
&lt;/h2&gt;

&lt;p&gt;You most want to do this when introspection is disabled. Otherwise when you need examples to help make sense of the API's semantics, or to develop a better intuition for where the weaknesses may be.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#how"&gt;
  &lt;/a&gt;
  How?
&lt;/h2&gt;

&lt;p&gt;This script works out-of-the-box for the majority scenario: POST requests to &lt;code&gt;/graphql&lt;/code&gt; that use the &lt;code&gt;operationName&lt;/code&gt; parameter.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;mitmdump &lt;span class="nt"&gt;-s&lt;/span&gt; capture.py
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;capture.py:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;re&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;mitmproxy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;http&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;response&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flow&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;http&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HTTPFlow&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;flow&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;endswith&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'/graphql'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;payload&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flow&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'utf-8'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;filename&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;r&lt;/span&gt;&lt;span class="s"&gt;'[^a-zA-Z0-9]'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'_'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;payload&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'operationName'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;'.example.txt'&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'w'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;payload&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fp&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;indent&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n\n&lt;/span&gt;&lt;span class="s"&gt;// ==== REQUEST ====&lt;/span&gt;&lt;span class="se"&gt;\n\n&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;payload&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'query'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n\n&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"// ==== RESPONSE ====&lt;/span&gt;&lt;span class="se"&gt;\n\n&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flow&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;fp&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;indent&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



</description>
      <category>ethicalhacking</category>
      <category>cybersecurity</category>
      <category>recon</category>
      <category>graphql</category>
    </item>
    <item>
      <title>How to Declare Variables in Android String Resources?</title>
      <author>Vincent Tsen</author>
      <pubDate>Sat, 15 Jan 2022 02:00:39 +0000</pubDate>
      <link>https://dev.to/vtsen/how-to-declare-variables-in-android-string-resources-1he8</link>
      <guid>https://dev.to/vtsen/how-to-declare-variables-in-android-string-resources-1he8</guid>
      <description>&lt;p&gt;&lt;strong&gt;To be referenced by another string resources to avoid duplicated hard-coded strings in different places.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This article was originally published at &lt;a href="https://vtsen.hashnode.dev/how-to-declare-variables-in-android-string-resources"&gt;vtsen.hashnode.dev&lt;/a&gt; on Dec 25, 2021.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Suppose you have 2 string resources &lt;code&gt;app_name&lt;/code&gt; and &lt;code&gt;about_text&lt;/code&gt; below, you have 2 duplicated hard-coded strings.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;&amp;lt;resources&amp;gt;
    &amp;lt;string name="app_name"&amp;gt;My App Name&amp;lt;/string&amp;gt;
    &amp;lt;string name="about_text"&amp;gt;My App Name&amp;lt;/string&amp;gt;
&amp;lt;/resources&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#reference-another-string-resources"&gt;
  &lt;/a&gt;
  Reference Another String Resources
&lt;/h2&gt;

&lt;p&gt;To get rid of duplicated hard-coded strings, what you can do is reference the &lt;code&gt;app_name&lt;/code&gt; from the &lt;code&gt;about_text&lt;/code&gt;.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;&amp;lt;resources&amp;gt;
    &amp;lt;string name="app_name"&amp;gt;My App Name&amp;lt;/string&amp;gt;
    &amp;lt;string name="about_text"&amp;gt;@string/app_name&amp;lt;/string&amp;gt;
&amp;lt;/resources&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;But what if you have more complicated &lt;code&gt;about_text&lt;/code&gt; like below?&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;&amp;lt;resources&amp;gt;
    &amp;lt;string name="app_name"&amp;gt;My App Name&amp;lt;/string&amp;gt;
    &amp;lt;string name="about_text"&amp;gt;My App Name is an awesome app!&amp;lt;/string&amp;gt;
&amp;lt;/resources&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#use-string-format"&gt;
  &lt;/a&gt;
  Use String Format
&lt;/h2&gt;

&lt;p&gt;You can change the &lt;code&gt;about_text&lt;/code&gt; to string format to allow the string to be constructed at run time.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;&amp;lt;resources&amp;gt;
    &amp;lt;string name="app_name"&amp;gt;My App Name&amp;lt;/string&amp;gt;
    &amp;lt;string name="about_text"&amp;gt;%s is an awesome app!&amp;lt;/string&amp;gt;
&amp;lt;/resources&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;In code (in your activity class for example), you pass in the &lt;code&gt;app_name&lt;/code&gt; string.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;val aboutText = resources.getString(
    R.string.about_text,
    resources.getString(R.string.app_name))
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Well, there is another better solution to use &lt;strong&gt;DOCTYPE&lt;/strong&gt;! You don't need to pass in the string variable from code.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#use-doctype-resources-entity"&gt;
  &lt;/a&gt;
  Use DOCTYPE resources ENTITY
&lt;/h2&gt;

&lt;p&gt;The DOCTYPE resources ENTITY declaration like a variable.  You can reference it from another string resources using &lt;code&gt;&amp;amp;&amp;lt;entity_name&amp;gt;;&lt;/code&gt; syntax.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;&amp;lt;!DOCTYPE resources [
    &amp;lt;!ENTITY app_name "My App Name"&amp;gt;
    &amp;lt;!ENTITY another_name "Test 1 2 3"&amp;gt;
    ]&amp;gt;

&amp;lt;resources&amp;gt;
    &amp;lt;string name="app_name"&amp;gt;&amp;amp;app_name;&amp;lt;/string&amp;gt;
    &amp;lt;string name="about_text"&amp;gt;&amp;amp;app_name; is an awesome app!&amp;lt;/string&amp;gt;
&amp;lt;/resources&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;em&gt;P/S: If there is another way better than using DOCTYPE, please let me know.&lt;/em&gt;&lt;/p&gt;

</description>
      <category>android</category>
      <category>kotlin</category>
      <category>beginners</category>
    </item>
  </channel>
</rss>
