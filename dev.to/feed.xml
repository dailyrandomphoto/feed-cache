<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>DEV Community</title>
    <author>DEV Community</author>
    <description>A constructive and inclusive social network for software developers. With you every step of your journey.</description>
    <link>https://dev.to</link>
    <language>en</language>
    <item>
      <title>Vue Academy #2: V-model directive</title>
      <author>CodeOzz</author>
      <pubDate>Fri, 09 Jul 2021 15:25:06 +0000</pubDate>
      <link>https://dev.to/codeozz/vue-academy-2-v-model-directive-36oh</link>
      <guid>https://dev.to/codeozz/vue-academy-2-v-model-directive-36oh</guid>
      <description>&lt;p&gt;Welcome to the second vue academy ! It will be a list of lot of article on vue! I have 2.5 years of experience in this and I can teach a few thing about this !&lt;/p&gt;

&lt;p&gt;For this course we will focus on v-model directive !&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#lets-start"&gt;
  &lt;/a&gt;
  Let's start
&lt;/h3&gt;

&lt;p&gt;First problematic, how do we &lt;strong&gt;manage&lt;/strong&gt; an input value in &lt;code&gt;&amp;lt;input&amp;gt;&lt;/code&gt; ?&lt;/p&gt;

&lt;p&gt;We could do the next :&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;script&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="k"&gt;default&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="na"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;HelloWorld&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="na"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="na"&gt;message&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;''&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;
  &lt;span class="na"&gt;methods&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
     &lt;span class="nx"&gt;updateMessage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;event&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
         &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;message&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;event&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;target&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;
     &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/script&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;div&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;input&lt;/span&gt;
        &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;message&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;
        &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="nd"&gt;input&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;updateMessage&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;
        &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/div&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/template&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;


&lt;p&gt;We need to bind value of input to our current data &lt;code&gt;message&lt;/code&gt; and handle event from this input in order to update our current data &lt;code&gt;message&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;It's not really friendly and we have to do this for every  component...&lt;/p&gt;
&lt;h3&gt;
  &lt;a href="#vmodel"&gt;
  &lt;/a&gt;
  v-model
&lt;/h3&gt;

&lt;p&gt;You can use the v-model directive to create two-way data bindings on form input, textarea, and select elements. It automatically picks the correct way to update the element based on the input type.&lt;/p&gt;

&lt;p&gt;So we can replace the code above by&lt;br&gt;
&lt;/p&gt;
&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;script&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="k"&gt;default&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="na"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;HelloWorld&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="na"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="na"&gt;message&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;''&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/script&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;div&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;input&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;message&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/div&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/template&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;


&lt;p&gt;We can remove the method that update value ! Since v-model will directly update it.&lt;/p&gt;

&lt;p&gt;It's very useful ! &lt;/p&gt;


&lt;div class="ltag_gist-liquid-tag"&gt;
  
&lt;/div&gt;



</description>
      <category>vue</category>
      <category>javascript</category>
      <category>webdev</category>
      <category>typescript</category>
    </item>
    <item>
      <title>5 Handy CLI Tools to Spice up Your Terminal</title>
      <author>Elena Lape</author>
      <pubDate>Fri, 09 Jul 2021 15:09:02 +0000</pubDate>
      <link>https://dev.to/elenalape/5-handy-cli-tools-to-spice-up-your-terminal-31do</link>
      <guid>https://dev.to/elenalape/5-handy-cli-tools-to-spice-up-your-terminal-31do</guid>
      <description>&lt;p&gt;The Command Line.&lt;/p&gt;

&lt;p&gt;A developer's best mate, and also one of the main sources of our frustration (it's &lt;code&gt;esc&lt;/code&gt;, then &lt;code&gt;:q&lt;/code&gt; to quit Vim, by the way).&lt;/p&gt;

&lt;p&gt;Regardless of its social status, there are plenty of great CLI tools that can make quite a difference in the overall terminal experience.&lt;/p&gt;

&lt;p&gt;Here are some of my favourites, in no particular order. Some are tools that I use every day in my work, and others are just fun apps to try if you get bored of a GUI.&lt;/p&gt;

&lt;p&gt;Shall we?&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#1-ohmyzsh"&gt;
  &lt;/a&gt;
  1. &lt;a href="https://ohmyz.sh/"&gt;Oh-My-Zsh&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--A4ap7Ood--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/467bung2smmypgklrfk9.jpg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--A4ap7Ood--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/467bung2smmypgklrfk9.jpg" alt="oh-my-zsh screenshot with wedisagree theme"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://ohmyz.sh/"&gt;Oh-My-Zsh&lt;/a&gt; is a framework for managing your Zsh configuration. It comes bundled with thousands of helpful functions, helpers, plugins, and themes.&lt;/p&gt;

&lt;p&gt;In short ‚Äî Oh-My-Zsh makes the terminal less intimidating by bringing some colour and autocompletion to tools like &lt;a href="https://git-scm.com/"&gt;Git&lt;/a&gt; and &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt;, as well as plenty of package managers and other popular command line utilities.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# To install
$ sh -c "$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)"

# To configure your plugins, themes, aliases etc.
$ vi ~/.zshrc
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Admittedly, my favourite part is that there are lots of different &lt;a href="https://github.com/ohmyzsh/ohmyzsh/wiki/Themes"&gt;themes&lt;/a&gt; to choose from ‚Äî my favourite is &lt;a href="https://github.com/ohmyzsh/ohmyzsh/wiki/Themes#wedisagree"&gt;&lt;code&gt;wedisagree&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Note that Oh-My-Zsh is for the &lt;strong&gt;ZSH/Z-Shell&lt;/strong&gt; (not bash or any other), so make sure you‚Äôve got &lt;a href="https://github.com/ohmyzsh/ohmyzsh/wiki/Installing-ZSH"&gt;ZSH&lt;/a&gt; going first.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#2-httpie"&gt;
  &lt;/a&gt;
  2. &lt;a href="https://httpie.io"&gt;HTTPie&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--YvQFrYlX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://raw.githubusercontent.com/httpie/httpie/master/httpie.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--YvQFrYlX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://raw.githubusercontent.com/httpie/httpie/master/httpie.gif" alt="HTTPie in the terminal"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Ever wanted to make a request to an API or a website, without having to pull out an arsenal of devtools?&lt;/p&gt;

&lt;p&gt;Say no more ‚Äî &lt;a href="https://httpie.io"&gt;HTTPie&lt;/a&gt; is here to save the day.&lt;/p&gt;

&lt;p&gt;HTTPie is a CLI HTTP client that comes with colourised output (that goes fashionably well with Oh-My-Zsh's &lt;code&gt;wedisagree&lt;/code&gt;), really intuitive syntax, and a number of other features to make testing and debugging APIs as simple as it gets.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# To install with brew
$ brew install httpie

# To make a request
$ http httpie.io/hello
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;One feature I find particularly handy is the &lt;a href="https://httpie.io/docs/2.4.0#offline-mode"&gt;&lt;code&gt;--offline&lt;/code&gt; mode&lt;/a&gt;, which lets you build and print out a (colourised and formatted) HTTP request without sending it. That way, you can see exactly the stuff the API in question is going to receive.&lt;/p&gt;

&lt;p&gt;Check out my recent &lt;a href="https://dev.to/elenalape/apis-101-getting-started-with-httpie-2o9g"&gt;Getting started with HTTPie guide&lt;/a&gt; to learn more.&lt;/p&gt;

&lt;p&gt;Full disclosure: I am part of the HTTPie team. However, I have been using it even before I joined!&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#3-wttrin"&gt;
  &lt;/a&gt;
  3. &lt;a href="https://github.com/chubin/wttr.in"&gt;Wttr.in&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--fujA09re--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/te5m2bi8mg9sezcdvs0v.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--fujA09re--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/te5m2bi8mg9sezcdvs0v.png" alt="wttr.in weather forecast preview"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Using a command line just for the sake of using a command line?&lt;/p&gt;

&lt;p&gt;Sign. me. up.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://github.com/chubin/wttr.in"&gt;Wttr.in&lt;/a&gt; is a console-based weather report app. Just add your city to the URL, and send a request like so:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# Using HTTPie
$ http wttr.in/london 

# Or, using cURL
$ curl wttr.in/london 
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;I‚Äôve aliased &lt;code&gt;http wttr.in/London&lt;/code&gt; with &lt;code&gt;weather&lt;/code&gt;. So now, each time I want to see the weather forecast for London, I am able to simply type &lt;code&gt;weather&lt;/code&gt; and save all those precious seconds of having to pick up my phone and navigate to the weather app. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#4-kalk"&gt;
  &lt;/a&gt;
  4. &lt;a href="https://kalk.dev/"&gt;Kalk&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--fayW8_XN--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zwsgl326s2oaf4ohx5wo.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--fayW8_XN--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zwsgl326s2oaf4ohx5wo.gif" alt="Kalk calculator preview"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Computers have evolved so far ahead from their original purpose, which was to calculate things.&lt;/p&gt;

&lt;p&gt;Sure, you can and create some sick beats with your machine, or run Overwatch at one bazillion frames per second. &lt;/p&gt;

&lt;p&gt;But something as simple as &lt;em&gt;calculating&lt;/em&gt; what grade you need to score in an exam to pass the course is more tricky than it should be. Your default OS calculator is an option, but it's very basic. Google kiiind of does the job, but requires internet connection. So do more advanced tools like &lt;a href="https://www.wolframalpha.com/"&gt;Wolfram Alpha&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://kalk.dev/"&gt;Kalk&lt;/a&gt; is... a CLI for a calculator.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# To install using brew
$ brew install kalk

# To launch
$ kalk
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;kalk 0.4.0+3fb73b1 - Copyright (c) 2020-2021 Alexandre Mutel
# Type `help` for more information and at https://github.com/xoofx/kalk

&amp;gt;&amp;gt;&amp;gt; # You can do things such as
&amp;gt;&amp;gt;&amp;gt; x=2; round((54+4)/(4+x))

# x = 2; round((54 + 4) / (4 + x))
x = 2
out = 10
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;It's simply brilliant both as your regular everyday calculator, and a more advanced one that will solve your equations using the same syntax you'd use in a maths class. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#5-taskwarrior"&gt;
  &lt;/a&gt;
  5. &lt;a href="https://taskwarrior.org/"&gt;Taskwarrior&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--qJCwS8_Z--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/yl24k97hju6b6yd5vzze.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--qJCwS8_Z--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/yl24k97hju6b6yd5vzze.png" alt="taskwarrior screenshot from wikipedia.org"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It's an open secret that creating to-do lists is peak procrastination, as a queen of procrastination, I'd like to introduce you to one more way &lt;em&gt;to-do&lt;/em&gt; it.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://taskwarrior.org/"&gt;Taskwarrior&lt;/a&gt; is a nifty tool if you want to keep all your &lt;em&gt;actual&lt;/em&gt; to-dos separate from shopping lists and song lyric ideas that you've got sitting in your note taking app.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# To install with brew
$ brew install kalk

# To create a new task (it will prompt you to create a ~/.taskrc file upon the first run; select yes
$ task add 'Write a dev.to article about CLI tools'

# To view all pending tasks
$ task

# To mark task as complete
$ task &amp;lt;task_id&amp;gt; done
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;You can also annotate, set a priority level or a by-date to individual tasks or their groups.&lt;/p&gt;

&lt;p&gt;Taskwarrior's &lt;a href="https://taskwarrior.org/docs/30second.html"&gt;30 second tutorial&lt;/a&gt; has got everything you need to get started.&lt;/p&gt;




&lt;p&gt;Do you have any favourite CLI tools you'd like to share? Please let me know if you give any of the ones listed above a try! &lt;/p&gt;

&lt;p&gt;I'm going to go finish that burning &lt;code&gt;task&lt;/code&gt; now.&lt;/p&gt;

&lt;p&gt;Unless the &lt;code&gt;weather&lt;/code&gt; is too nice for it.&lt;/p&gt;

</description>
      <category>githunt</category>
      <category>productivity</category>
      <category>tooling</category>
      <category>todayilearned</category>
    </item>
    <item>
      <title>DEPLOY NEXT.JS APP TO VERCEL</title>
      <author>Ha Tuan Em</author>
      <pubDate>Fri, 09 Jul 2021 15:08:18 +0000</pubDate>
      <link>https://dev.to/hte305/deploy-next-js-app-to-vercel-2kj2</link>
      <guid>https://dev.to/hte305/deploy-next-js-app-to-vercel-2kj2</guid>
      <description>&lt;p&gt;Someone want to me make a post for explain how to deploy NEXT.JS application to &lt;a href="https://vercel.com/"&gt;Vercel&lt;/a&gt;. Base on require of them I will make a post. Hope, it will help something to you.&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#i-initial-nextjs-app-or-you-can-clone-my-shopping-cart-repository"&gt;
  &lt;/a&gt;
  I. Initial Next.js app or you can clone my shopping cart &lt;a href="https://github.com/hatuanem199801/next-shopping-example"&gt;repository&lt;/a&gt;
&lt;/h4&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;create-next-app shopping-cart
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h4&gt;
  &lt;a href="#ii-after-creating-your-application-commit-them-to-github"&gt;
  &lt;/a&gt;
  II. After creating your application, commit them to Github.
&lt;/h4&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;git add .
git commit -m "Complete project"
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h4&gt;
  &lt;a href="#iii-create-project-in-vercel"&gt;
  &lt;/a&gt;
  III. Create project in Vercel
&lt;/h4&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--9zpqYTvD--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qdjw54je867grxworj2o.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--9zpqYTvD--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qdjw54je867grxworj2o.png" alt="Create project"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#iv-import-project-or-you-can-search-by-name-of-repository"&gt;
  &lt;/a&gt;
  IV. Import project or you can search by name of repository.
&lt;/h4&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--xoR8GhQJ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ijwzn53vnyqo826aaohd.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--xoR8GhQJ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ijwzn53vnyqo826aaohd.png" alt="import project"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#v-configure-application"&gt;
  &lt;/a&gt;
  V. Configure application
&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;You can add &lt;code&gt;env&lt;/code&gt; as MONGOURI or SERECTKEY, ... in box &lt;code&gt;Environment Variables&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Override command of application in box &lt;code&gt;Build and Output Settings&lt;/code&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--Zxo0bUxC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/coad6982e1f71rpqeqpl.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Zxo0bUxC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/coad6982e1f71rpqeqpl.png" alt="Configure application"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#vi-deploy-and-done"&gt;
  &lt;/a&gt;
  VI. Deploy and done
&lt;/h4&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--TC6nljPg--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/unnkhpj9wo0hvn5ur1ym.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--TC6nljPg--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/unnkhpj9wo0hvn5ur1ym.png" alt="Deploy"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Enjoy your time ü™¥&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you for reading.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/hte305"&gt;&lt;br&gt;
&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--eKzEoK4A--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ddtyyk0zuud3h7o0sjpq.png" alt="Buy me a coffee"&gt;&lt;br&gt;
&lt;/a&gt;&lt;/p&gt;

</description>
      <category>nextjs</category>
      <category>vercel</category>
      <category>deploy</category>
      <category>shoppingcart</category>
    </item>
    <item>
      <title>How to Game Dev Metrics w/ Ray Elenteny</title>
      <author>Conor Bronsdon</author>
      <pubDate>Fri, 09 Jul 2021 14:52:28 +0000</pubDate>
      <link>https://dev.to/conorbronsdon/how-to-game-dev-metrics-w-ray-elenteny-5h3l</link>
      <guid>https://dev.to/conorbronsdon/how-to-game-dev-metrics-w-ray-elenteny-5h3l</guid>
      <description>&lt;p&gt;What leads teams to game metrics within their organization?&lt;/p&gt;

&lt;p&gt;On this week‚Äôs episode of &lt;a href="https://devinterrupted.com/podcast/how-to-game-dev-metrics/"&gt;Dev Interrupted&lt;/a&gt;, we speak with agile expert Ray Elenteny, Principal Owner at Solutech Consulting, about how people game dev metrics and the underlying issues in culture &amp;amp; leadership that lead to it.&lt;/p&gt;

&lt;p&gt;So whether you're trying to game your own metrics (don't do it!) or solve culture issues that have led to this issue at your organization, give this episode a listen.&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#listen-to-the-full-episode"&gt;
  &lt;/a&gt;
  &lt;strong&gt;Listen to the full episode&lt;/strong&gt;
&lt;/h1&gt;

&lt;p&gt;&lt;iframe width="100%" height="232px" src="https://open.spotify.com/embed/episode/2AJVkiMHT3Zd4vb3pzLEbs"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#episode-highlights-include"&gt;
  &lt;/a&gt;
  &lt;strong&gt;Episode Highlights include:&lt;/strong&gt;
&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Which metrics are easiest to game&lt;/li&gt;
&lt;li&gt;The long-term implications of gaming metrics&lt;/li&gt;
&lt;li&gt;How poor culture and leadership lead engineering teams to game dev metrics&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;
  &lt;a href="#join-the-dev-interrupted-discord-server"&gt;
  &lt;/a&gt;
  &lt;strong&gt;Join the Dev Interrupted Discord Server&lt;/strong&gt;
&lt;/h1&gt;

&lt;p&gt;With over 1200 members, the Dev Interrupted Discord Community is the best place for Engineering Leaders to engage in daily conversation. No sales people allowed. &lt;a href="https://discord.gg/tpkmwM6c3g"&gt;Join the community &amp;gt;&amp;gt;&lt;/a&gt;&lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--wzIBzHH0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/19j3dzgz4r4kzav3w6z8.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--wzIBzHH0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/19j3dzgz4r4kzav3w6z8.png" alt="Join the Dev Interrupted Discord Community!"&gt;&lt;/a&gt;&lt;/p&gt;

</description>
      <category>devops</category>
      <category>agile</category>
      <category>leadership</category>
      <category>culture</category>
    </item>
    <item>
      <title>Learning new tech as a beginner.</title>
      <author>Asim Shrestha</author>
      <pubDate>Fri, 09 Jul 2021 14:47:40 +0000</pubDate>
      <link>https://dev.to/alex1the1great/learning-new-tech-as-a-beginners-2gl7</link>
      <guid>https://dev.to/alex1the1great/learning-new-tech-as-a-beginners-2gl7</guid>
      <description>&lt;h3&gt;
  &lt;a href="#1-code-everyday"&gt;
  &lt;/a&gt;
  1. Code everyday.
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;To become good a coding you have to code every single day. Even if for just 20 minutes a day. You just have to be consistent.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#2-take-your-first-tutorial"&gt;
  &lt;/a&gt;
  2. Take your first tutorial.
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Complete the tutorial. Understand everything, shallow understanding is fine while starting.&lt;/li&gt;
&lt;li&gt;You do not need to understand exactly how the functions work, but you do need to be able to import and use them correctly.&lt;/li&gt;
&lt;li&gt;Don't rush to complete the tutorial. If you feel like skipping any topics from tutorial, take a break and come back later.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#3-add-at-least-2-new-features-to-the-project-after-completing-the-tutorial"&gt;
  &lt;/a&gt;
  3. Add at least 2 new features to the project after completing the tutorial.
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Following tutorial, we are not using our brain. So, we have to use our brain and think ourselves try to add simple features at least. You have understood the tutorial it will not a big deal.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#4-build-a-new-projects-amp-complete-it"&gt;
  &lt;/a&gt;
  4. Build a new projects &amp;amp; complete it.
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ideas don't come out fully formed. They only become clear as you work on them. YOU JUST HAVE TO GET STARTED.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Build a complete new project with the knowledge which you have acquire from the tutorial.&lt;/li&gt;
&lt;li&gt;If you like your new project which you are building then keep on updating(iterating) it.&lt;/li&gt;
&lt;li&gt;Instead of making multiple simple projects, build a big project.&lt;/li&gt;
&lt;li&gt;If you keep on adding new features to a simple project, then it will start to grow to a big project.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#5-habit-of-figuring-out-anything"&gt;
  &lt;/a&gt;
  5. Habit of Figuring out anything.
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Face your challenges.&lt;/li&gt;
&lt;li&gt;How can I add this features to my project?

&lt;ul&gt;
&lt;li&gt;Reading documentation.&lt;/li&gt;
&lt;li&gt;Learning from an article.&lt;/li&gt;
&lt;li&gt;Watching videos.&lt;/li&gt;
&lt;li&gt;Asking for help in stack overflow or any other platform.&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;/ul&gt;

</description>
      <category>programming</category>
      <category>codenewbie</category>
      <category>beginners</category>
      <category>python</category>
    </item>
    <item>
      <title>A Gentle Introduction to Reinforcement Learning </title>
      <author>Satwik Kansal</author>
      <pubDate>Fri, 09 Jul 2021 14:36:20 +0000</pubDate>
      <link>https://dev.to/satwikkansal/a-gentle-introduction-to-reinforcement-learning-75h</link>
      <guid>https://dev.to/satwikkansal/a-gentle-introduction-to-reinforcement-learning-75h</guid>
      <description>&lt;h1&gt;
  &lt;a href="#a-gentle-introduction-to-reinforcement-learning"&gt;
  &lt;/a&gt;
  A gentle introduction to Reinforcement Learning
&lt;/h1&gt;

&lt;p&gt;In 2016, AplhaGo, a program developed for playing the game of &lt;a href="https://en.wikipedia.org/wiki/Go_(game)"&gt;Go&lt;/a&gt;, made headlines when it beat the world champion Go player in a five-game match. It was a remarkable feat because the number of possible legal moves in Go are of the order of 2.1 √ó 10&lt;sup&gt;170&lt;/sup&gt;.  To put this in context, this number is far, far greater than the number of atoms in the observable universe, which are of the order of 10&lt;sup&gt;80&lt;/sup&gt;. Such a high number of possibilities make it almost impossible to create a program that can play effectively using brute-force or somewhat optimized search algorithms. &lt;/p&gt;

&lt;p&gt;A part of the secret sauce of AlphaGO was the usage of Reinforcement Learning to improve its understanding of the game by playing against itself. Since then, the field of Reinforcement Learning has seen increased interest, and much more efficient programs have been developed to play various games at a pro-human efficiency. Although you would find Reinforcement Learning discussed in the context of Games and Puzzles in most places (including this post), the applications of Reinforcement Learning are much more expansive. The objective of this tutorial is to give you a gentle introduction to the world of Reinforcement Learning. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;‚ÑπÔ∏è First things first! This post was written in collaboration with &lt;a href="https://scholar.google.com/citations?user=H4WCOr8AAAAJ"&gt;Alexey Vinel&lt;/a&gt; (Professor, Halmstead University). Some ideas and visuals are borrowed from my previous post on Q-learning written for &lt;a href="https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/"&gt;Learndatasci&lt;/a&gt;. Unlike most posts you'll find on Reinforcement learning, we try to explore Reinforcement Learning here with an angle of multiple agents. So this makes it slightly more complicated and interesting at the same time. While this will be a good resource to develop intuitive understanding of Reinforcement Learning (Reinforcement Q-learning to be specific), it is highly recommended to visit the theoretical parts (some links shared in the appendix), if you're willing to explore Reinforcement Learning beyond this post.&lt;/p&gt;

&lt;p&gt;I had to fork openAIs gym library to implement a custom environment. The code can be found on &lt;a href="https://github.com/satwikkansal/gym-dual-taxi"&gt;this github repository&lt;/a&gt;.  If you'd like to explore an interactive version, you can check &lt;a href="https://colab.research.google.com/github/satwikkansal/gym-dual-taxi/blob/master/draft.ipynb"&gt;out this google colab notebook&lt;/a&gt;. We use Python to implement the algorithms, if you're not familiar with Python you can simply pretend that those snippets don't exist and read through the textual part (including code comments). Alright, time to get started üöÄ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2&gt;
  &lt;a href="#what-is-reinforcement-learning"&gt;
  &lt;/a&gt;
  What is Reinforcement Learning?
&lt;/h2&gt;

&lt;p&gt;Reinforcement learning is a paradigm of Machine Learning where learning happens through the feedback gained by an agent's interaction with its environment. This is also one of the key differentiators of Reinforcement Learning with the other two paradigms of Machine learning (&lt;a href="https://en.wikipedia.org/wiki/Supervised_learning"&gt;Supervised learning&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Unsupervised_learning"&gt;Unsupervised learning&lt;/a&gt;). Supervised learning algorithms require fully labelled-training-data, and Unsupervised learning algorithms need no labels. On the other hand, Reinforcement learning algorithms utilize feedback from the environment they're operating in to get better at the tasks they're being trained to perform. So we can say that Reinforcement Learning lies somewhere in the middle of the spectrum.&lt;/p&gt;

&lt;p&gt;It is inevitable to talk about Reinforcement Learning with clarity without using some technical terms like "agent", "action", "state", "reward", and "environment". So let's try to gain a high-level understanding of Reinforcement Learning and these terms through an analogy,&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#understanding-reinforcement-learning-through-birbing"&gt;
  &lt;/a&gt;
  Understanding Reinforcement learning through Birbing
&lt;/h3&gt;

&lt;p&gt;Let's watch the first few seconds of this video first,&lt;/p&gt;

&lt;p&gt;&lt;iframe width="710" height="399" src="https://www.youtube.com/embed/u7TiRqh7x8s"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;Pretty cool, isn't it?&lt;/p&gt;

&lt;p&gt;And now think about how did someone manage to teach this parrot to reply with certain sounds on certain prompts. And if you carefully observed, part of the answer lies in the food the parrot is given after every cool response. The human asks a question, and the parrot tries to respond in many different ways, and if the parrot's response is the desired one, it is rewarded with food. Now guess what? The next time the parrot is exposed to the same cue, it is likely to answer similarly, expecting more food. This is how we "reinforce" certain behaviours through positive experiences. If I had to explain the above process in terms of Reinforcement learning concepts, it'd be something like,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;"The agent learns to take desired for a given state in the environment", &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;where,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The "agent" is the parrot&lt;/li&gt;
&lt;li&gt;The "state" is questions or cues the parrot is exposed to&lt;/li&gt;
&lt;li&gt;The "actions" are the sounds it is uttering &lt;/li&gt;
&lt;li&gt;The "reward" is the food he gets when he takes the desired action&lt;/li&gt;
&lt;li&gt;And the "environment" is the place where the parrot is living (or, in other words, everything else than the parrot)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The reinforcement can happen through negative experiences too. For example, if a child touches a burning candle out of curiosity, (s)he is unlikely to repeat the same action. So, in this case, instead of a reward, the agent got a penalty, which would disincentivize the agent to repeat the same action in future again.&lt;/p&gt;

&lt;p&gt;&lt;iframe width="710" height="399" src="https://www.youtube.com/embed/hsVEiat444Q"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;If you try to think about it, there are countless similar real-world analogies. This suggests why Reinforcement Learning can be helpful for a wide variety of real-world applications and why it might be a path to create General AI Agents (think of a program that can not just beat a human in the game of Go, but multiple games like Chess, GTA, etc.). It might still take a lot of time to develop agents with general intelligence, but reading about programs like &lt;a href="https://en.wikipedia.org/wiki/MuZero"&gt;MuZero&lt;/a&gt; (one of the many successors of Alpha Go) hints that Reinforcement learning might have a decent role to play in achieving that.&lt;/p&gt;

&lt;p&gt;After reading the analogies, a few questions like below might have come into your mind,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Real-world example is fine, but how do I do this "reinforcement" in the world of programs?&lt;/li&gt;
&lt;li&gt;What are these algorithms, and how do they work?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let's start answering such questions as switch gears and dive into certain technicalities of Reinforcement learning.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#example-problem-statement-selfdriving-taxi"&gt;
  &lt;/a&gt;
  Example problem statement: Self-driving taxi
&lt;/h2&gt;

&lt;p&gt;Wouldn't it be fantastic to train an agent (i.e. create a computer program) to pick up from a location and drop them at their desired location? In the rest of the tutorial, we'll try to solve a simplified version of this problem through reinforcement learning.&lt;/p&gt;

&lt;p&gt;Let's start by specifying typical steps in a Reinforcement learning process,&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Agent observes the environment. The observation is represented in digital form and also called "state".&lt;/li&gt;
&lt;li&gt;The agent utilizes the observation to decide how to act. The strategy agent uses to figure out the action to perform is also referred to as "policy".&lt;/li&gt;
&lt;li&gt;The agent performs the action in the environment.&lt;/li&gt;
&lt;li&gt;The environment, as a result of the action, may move to a new state (i.e. generate different observations) and may return feedback to the agent in the form of rewards/penalties. &lt;/li&gt;
&lt;li&gt;The agent uses the rewards and penalties to refine its policy.&lt;/li&gt;
&lt;li&gt;The process can be repeated until the agent finds an optimal policy.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--1ynz3QjF--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://storage.googleapis.com/lds-media/documents/Reinforcement-Learning-Animation.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--1ynz3QjF--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://storage.googleapis.com/lds-media/documents/Reinforcement-Learning-Animation.gif" alt=""&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now that we're clear about the process, we need to set up the environment. In most cases, what this means is we need to figure out the following details,&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#1-the-statespace"&gt;
  &lt;/a&gt;
  1. The state-space
&lt;/h3&gt;

&lt;p&gt;Typically, a "state" will encode the observable information that the agent can use to learn to act efficiently. For example, in the case of self-driving-taxi, the state information could contain the following information,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The current location of the taxi&lt;/li&gt;
&lt;li&gt;The current location of the passenger&lt;/li&gt;
&lt;li&gt;The destination&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There can be multiple ways to represent such information, and how one ends up doing it depends on the level of sophistication intended. &lt;/p&gt;

&lt;p&gt;The state space is the set of all possible states an environment can be in. For example, if we consider our environment for the self-driving taxi to be a two-dimensional 4x4 grid, there are &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;16 possible locations for the taxi&lt;/li&gt;
&lt;li&gt;16 possible locations for the passenger&lt;/li&gt;
&lt;li&gt;and 16 possible destination&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This means our state-space size becomes 16 x 16 x 16 = 4096, i.e. at any point in time the environment must be in either of these 4096 states. &lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--1UeQ7JwC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/N.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--1UeQ7JwC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/N.gif" alt="https://github.com/satwikkansal/gym-dual-taxi/raw/master/N.gif"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#2-the-action-space"&gt;
  &lt;/a&gt;
  2. The action space
&lt;/h3&gt;

&lt;p&gt;Action space is the set of all possible actions an agent can take in the environment. Taking the same 2D grid-world example, the taxi agent may be allowed to take the following actions,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Move North&lt;/li&gt;
&lt;li&gt;Move South&lt;/li&gt;
&lt;li&gt;Move East&lt;/li&gt;
&lt;li&gt;Move West&lt;/li&gt;
&lt;li&gt;Pickup&lt;/li&gt;
&lt;li&gt;Drop-off&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Again, there can be multiple ways to define the action space, and this is just one of them. The choice also depends on the level of complexity and algorithms you'd want to use later.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#3-the-rewards"&gt;
  &lt;/a&gt;
  3. The rewards
&lt;/h3&gt;

&lt;p&gt;The rewards and penalties are critical for an agent's learning. While deciding the reward structure, we must carefully think about the magnitude, direction (positive or negative), and the reward frequency (every time step / based on specific milestone / etc.). Taking the same grid environment example, some ideas for reward structure can be,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The agent should receive a positive reward when it performs a successful passenger drop-off. The reward should be high in magnitude because this behaviour is highly desired.&lt;/li&gt;
&lt;li&gt;The agent should be penalized if it tries to drop off a passenger in the wrong locations.&lt;/li&gt;
&lt;li&gt;The agent should get a small negative reward for not making it to the destination after every time step. This would incentivize the agent to take faster routes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There can be more ideas for rewards like giving a reward for successful pickup and so on. &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#4-the-transition-rules"&gt;
  &lt;/a&gt;
  4. The transition rules
&lt;/h3&gt;

&lt;p&gt;The transition rules are kind of the brain of the environment. They specify the dynamics of the above discussed components (state, action, and reward). They are often represented in terms of tables (a.k.a state transition tables) which specify that,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For a given state S, if you take an action A, the new state of the environment becomes S', and the reward received is R. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class="table-wrapper-paragraph"&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;State&lt;/th&gt;
&lt;th&gt;Action&lt;/th&gt;
&lt;th&gt;Reward&lt;/th&gt;
&lt;th&gt;Probability&lt;/th&gt;
&lt;th&gt;Next State&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;S&lt;code&gt;p&lt;/code&gt;
&lt;/td&gt;
&lt;td&gt;A&lt;code&gt;q&lt;/code&gt;
&lt;/td&gt;
&lt;td&gt;R&lt;code&gt;pq&lt;/code&gt;
&lt;/td&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;td&gt;S&lt;code&gt;p'&lt;/code&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;An example row could be when the taxi's location is in the middle of grid, the passenger's location in in the bottom-right corner. The agent takes the "Move North" action, it gets a negative reward, and the next state becomes the state that represents the taxi in its new position.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; In the real-world, the state transitions may not be deterministic, i.e. they can be either.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Stochastic; which means the rules operate by probability, i.e. if you take an action, there's an X1% chance you'll end up in state S1, and Xn% chance you'd end up in a state Sn.&lt;/li&gt;
&lt;li&gt;Unknown; which means it is not known in advance what all possible states the agent can get into if it takes action A in a given state S. This might be the case when the agent is operating in the real world.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#implementing-the-environment"&gt;
  &lt;/a&gt;
  Implementing the environment
&lt;/h2&gt;

&lt;p&gt;Implementing a computer program that represents the environment can be a bit of a programming effort. Apart from deciding the specifics like the state space, transition table, reward structure, etc., we need to implement other features like creating a way to input actions into the environment and getting feedback in return. More often than not, there's also a requirement to visualize what's happening under the hood. Since the objective of this tutorial is "Introduction to Reinforcement Learning", we will skip the "how to program a Reinforcement learning environment" part and jump straight to using it. However, if you're interested, you can check the &lt;a href="https://github.com/satwikkansal/gym-dual-taxi"&gt;source code&lt;/a&gt; and follow the comments there.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#specifics-of-the-environment"&gt;
  &lt;/a&gt;
  Specifics of the environment
&lt;/h3&gt;

&lt;p&gt;We'll use a custom environment inspired by OpenAI gym's &lt;a href="https://gym.openai.com/envs/Taxi-v3/"&gt;Taxi-v3 environment&lt;/a&gt;. We have added a twist to the environment. Instead of having a single taxi and a single passenger, we'll be having two taxis and a passenger! The intention behind the mod is to observe interesting dynamics that might arise because of the presence of another taxi. This also means the state space would comprise an additional taxi location, and the action space would comprise of actions of both the taxis now.&lt;/p&gt;

&lt;p&gt;Our environment is built on OpenAI's gym library, making it a bit convenient to implement environments to evaluate Reinforcement learning algorithms. They also include some pre-packaged environment (Taxi-v3 is one of them), and their environments are a popular way to practice Reinforcement Learning and evaluate Reinforcement Learning algorithms. Feel free to check out their docs to know more about them! &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#exploring-the-environment"&gt;
  &lt;/a&gt;
  Exploring the environment
&lt;/h3&gt;

&lt;p&gt;It's time we start diving into some code and explore the specifics of the environment we'll be using for Reinforcement learning in this tutorial.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="c1"&gt;# Let's first install the custom gym module which contains the environment 
&lt;/span&gt;&lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;uninstall&lt;/span&gt; &lt;span class="n"&gt;gym&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;
&lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;git&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;git&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;github&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;com&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;satwikkansal&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;gym&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;dual&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;taxi&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;git&lt;/span&gt;&lt;span class="c1"&gt;#"egg=gym&amp;amp;subdirectory=gym/"
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;gym&lt;/span&gt;
&lt;span class="n"&gt;env&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gym&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'DualTaxi-v1'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;render&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c1"&gt;# PS: If you're using jupyter notebook and get env not registered error; you have to restart your kernel after install the custom gym package in the last step.
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="/Users/satwik/Library/Application%20Support/typora-user-images/image-20210709135319304.png" class="article-body-image-wrapper"&gt;&lt;img src="/Users/satwik/Library/Application%20Support/typora-user-images/image-20210709135319304.png" alt="image-20210709135319304"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In the snippet above, we initialize our custom &lt;code&gt;DualTaxi-v1&lt;/code&gt; environment, and rendered its current state. In the rendered output,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The yellow and red rectangles represents both taxis on the 4x4 grid&lt;/li&gt;
&lt;li&gt;R, G, B, and Y are the 4 possible pick up or drop-off locations for the passenger&lt;/li&gt;
&lt;li&gt;The character ‚Äú|‚Äù represents a wall which the taxis can't cross&lt;/li&gt;
&lt;li&gt;The blue colored letter represents the pick-up location of the passenger&lt;/li&gt;
&lt;li&gt;The purple letter represents the drop-off location.&lt;/li&gt;
&lt;li&gt;Any taxi that gets the passenger aboard, would turn green in color
&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;observation_space&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_space&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;(Discrete(6144), Discrete(36))
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;You might have noticed that the only information that's printed is their discrete nature and the size of the space. The rest of the details are abstracted. This is an important point, and as you'll realize by the end of the post, our RL algorithm won't need any more information. &lt;/p&gt;

&lt;p&gt;However if you're still curious to know how the environment functions, feel free to check out the &lt;a href="https://github.com/satwikkansal/gym-dual-taxi"&gt;enviroment's code&lt;/a&gt; and follow the comments there. Another thing that you can do is peek into the state-transition table (check the code in the appendix if you're curious how to do it)&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#the-objective"&gt;
  &lt;/a&gt;
  The objective
&lt;/h3&gt;

&lt;p&gt;The objective of the environment is pick up the passenger from the blue location and drop to the violet location as fast as possible. An intelligent agent should be able to do this with consistency. Now let's see what information to we have for the environment's state space (a.k.a observation space) and action space. But before we dive into implementing that intelligent agent, let's see how a random agent would perform in this kind of enviromnet,&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;play_random&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    Function to play the episodes.
    """&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;done&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;next_action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_space&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="c1"&gt;# Trying the dumb agent
&lt;/span&gt;&lt;span class="n"&gt;print_frames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;play_random&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# check github for the code for print_frames
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--UZ9DSH9f--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/rl-1.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--UZ9DSH9f--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/rl-1.gif" alt=""&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can see the episode number at the top. In our case, an episode is the timeframe between the steps where the taxis make the first move and the step where they drop a passenger at the desired after picking up. When this happens, the episode is over, and we have to reset the environment to start all over again. &lt;/p&gt;

&lt;p&gt;You can see different actions at the bottom, and how the state keeps changing and the reward the agent gets after every action.&lt;/p&gt;

&lt;p&gt;As you can might have realized, these taxis are taking a while to finish even a single episode. So our random approach is very dumb for sure. Our intelligent agent definitely will have to perform this task better.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#introducing-qlearning"&gt;
  &lt;/a&gt;
  Introducing Q-learning
&lt;/h2&gt;

&lt;p&gt;Q-learning is one among several Reinforcement Learning algorithms. The reason we are picking Q-learning is because it is simple and straightforward to understand. We'll use Q-learning to make our agent somewhat intelligent. &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#intuition-behind-qlearning"&gt;
  &lt;/a&gt;
  Intuition behind Q-learning
&lt;/h3&gt;

&lt;p&gt;The way Q-learning works, is by storing what we call Q-values for every state-action combination. The Q-value represents the "quality" of an action taken from that state. Of course, the initial q-values are just random numbers, but the goal is to iteratively update them in the right direction. After enough iterations, these Q-values can start to converge (i.e. the size of update in upcoming iterations gets so small that it has a negligible impact). Once that is the case, we can safely say that, &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For a given state, the higher the Q-value for the state-action pair, the higher would be the expected long term reward of taking that particular action. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So long story short, the "developing intelligence" part of Q-learning lies in how the Q-values after agent's ineteraction with the environment, which requires discussion of two key concepts,&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#1-the-bellman-equation"&gt;
  &lt;/a&gt;
  1. The bellman equation
&lt;/h3&gt;

&lt;p&gt;Attached below is the bellman equation in the context of updating Q-values, this is the equation we use to update Q-values after agent's interaction with the environment.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://postimg.cc/image/4ghnvcjgn/"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--IFO521r_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://s31.postimg.cc/jp7l94d57/q_learning_equation.png" alt="q_learning_equation.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The Q-value of a state-action pair is the sum of the instant reward and the discounted future reward (of the resulting state). Where,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;st represents the state at time &lt;code&gt;t&lt;/code&gt;
&lt;/li&gt;
&lt;li&gt;at represents action taken at time &lt;code&gt;t&lt;/code&gt; (the agent was in state st at this point in time)&lt;/li&gt;
&lt;li&gt;rt is the reward received by performing the action at in the state st.&lt;/li&gt;
&lt;li&gt;st+1 is the next state that our agent will transition to after performing the action at in the state st.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The discount factor Œ≥(gamma) determines how much importance we want to give to future rewards. A high value for the discount factor (close to &lt;strong&gt;1&lt;/strong&gt;) captures the long-term effective award, whereas, a discount factor of &lt;strong&gt;0&lt;/strong&gt; makes our agent consider only immediate reward, hence making it greedy. &lt;/p&gt;

&lt;p&gt;The $\alpha$ (alpha) is our learning rate. Just like in supervised learning settings, alpha here is representative of the extent to which our Q-values are being updated in every iteration.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#2-epsilon-greedy-method"&gt;
  &lt;/a&gt;
  2. Epsilon greedy method
&lt;/h3&gt;

&lt;p&gt;While we keep updating Q-values every iteration, there's an important choice the agent has to make while taking an action. The choice it faces is whether to "explore" or "exploit"?&lt;/p&gt;

&lt;p&gt;So with time, the Q-values get better at representing the quality of a state-action pair. But to reach that goal, the agent has to try different actions (how can it know if a state-action pair is good if it hasn't tried it?). So it becomes critical for agent to "explore" i.e. take random actions to gather more knowledge about the environment. &lt;/p&gt;

&lt;p&gt;But there's a problem if the agent only explores. Exploration can only get the agent so far. Imagine that the environment agent is in is like a maze. Exploration can put agent on unknown path and give feedback to make q-values more valuable. But if the agent is only taking random actions at every step, it is going to have a hard time reaching the end state of the maze. That's why it is also important to "exploit". The agent should also consider using what it has already learned (i.e. the Q-values) to decided what action to take next.&lt;/p&gt;

&lt;p&gt;That's all to say, the agent needs to balance exploitation and exploration. There are many ways to do this. Once common way to do it with Q-learning is to have a value called "epsilon", which denotes the probability by which the agent will explore. A higher epsilon value results in interactions with more penalties (on average) which is obvious because we are exploring and making random decisions. We can add more sophistication to this method, and its a common practice that people start with a high epsilon value, and keep reducing it as time progresses. This is called epsilon decay. The intution is that as we keep adding more knowledge to Q-values through exploration, the exploitation becomes more trustworthy which in turn means we can explore at a lower rate. &lt;/p&gt;

&lt;p&gt;Note: There's usually some confusion around if epsilon represents probability of "exploration" or "exploitation". You'll find it used both ways on the internet and other resources. I find the first way more comfortable as it fits the terminology "epsilon decay". If you see it other way around, don't get confused, the concept is still the same. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#using-qlearning-for-our-environment"&gt;
  &lt;/a&gt;
  Using Q-learning for our environment
&lt;/h2&gt;

&lt;p&gt;Okay, enough background about Q-learning. Now how do we apply it to our &lt;code&gt;DualTaxi-v1&lt;/code&gt; environment? Because of the fact that we have two taxis in our environment, we can do it in a couple of ways,&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#1-cooperative-approach"&gt;
  &lt;/a&gt;
  1. Cooperative approach
&lt;/h3&gt;

&lt;p&gt;In this approach we can assume that there's a single agent with a single Q-table that controls both the taxis (think of it like a taxi agency). The overall goal of this agent would be to maximize the reward these taxis receive combined.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#2-competitive-approach"&gt;
  &lt;/a&gt;
  2. Competitive approach
&lt;/h3&gt;

&lt;p&gt;In this approach we can train two agents (one for each taxi). Every agent has its own Q-table and gets its own reward. Of course, the next state of the environment still depends on the actions of both the agents. This creates an interesting dynamic where each taxi would be trained to maximize its own individual rewards.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#cooperative-approach-in-action"&gt;
  &lt;/a&gt;
  Cooperative approach in action
&lt;/h2&gt;

&lt;p&gt;Before we see the code, let us specify the steps we'd have to take,&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Initialize the Q-table (size of the Q-table is state_space_size x action_space_size) by all zeros.&lt;/li&gt;
&lt;li&gt;Decide between exploration and exploitation based on the epsilon value.&lt;/li&gt;
&lt;li&gt;Exploration: For each state, select any one among all possible actions for the current state (S).&lt;/li&gt;
&lt;li&gt;Exploitation: For all possible actions from the state (S') select the one with the highest Q-value.&lt;/li&gt;
&lt;li&gt;Travel to the next state (S') as a result of that action (a).&lt;/li&gt;
&lt;li&gt;Update Q-table values using the update equation.&lt;/li&gt;
&lt;li&gt;If the episode is over (i.e. goal state is reached), reset the environment for next iteration.&lt;/li&gt;
&lt;li&gt;Keep repeating steps 2 to 7 until we start seeing decent results in agent's performance.
&lt;/li&gt;
&lt;/ol&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;deque&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt; 


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;bellman_update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    Function to perform q-value update as per bellman equation.
    """&lt;/span&gt;
    &lt;span class="c1"&gt;# Get the old q_value
&lt;/span&gt;    &lt;span class="n"&gt;old_q_value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="c1"&gt;# Find the maximum q_value for the actions in next state
&lt;/span&gt;    &lt;span class="n"&gt;next_max&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="c1"&gt;# Calculate the new q_value as per the equation
&lt;/span&gt;    &lt;span class="n"&gt;new_q_value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;old_q_value&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;gamma&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;next_max&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# Finally, update the q_value
&lt;/span&gt;    &lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new_q_value&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    Selects an action according to epsilon greedy method, performs it, and the calls bellman update
    to update the Q-values.
    """&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_space&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;info&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;bellman_update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;info&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log_every&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;running_metrics_len&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;evaluate_every&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evaluate_trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    This is the training logic. It takes input as a q-table, the environment.
    The training is done for num_episodes episodes. The results are logged preiodcially.

    We also record some useful metrics like average reward in last 50k timesteps, the average
    length of last 50 episodes and so on. These are helpful to gauge how the algorithm is performing
    over time.

    After every few episodes of training. We run evaluation routine, where we just "exploit" i.e. rely on 
    the q-table so far and see how well the agent has learned so far. Over the time, the results should get
    better until the q-table starts converging, after which, there's negligible change in the results.
    """&lt;/span&gt;
    &lt;span class="n"&gt;rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;deque&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;running_metrics_len&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;episode_lengths&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;deque&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;total_timesteps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

    &lt;span class="n"&gt;metrics&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;num_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;done&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;

        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;info&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;total_timesteps&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;total_timesteps&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;log_every&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;rd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;avg_ep_len&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;episode_lengths&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;zeroes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill_percent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calculate_q_table_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'Current Episode: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'Reward distribution: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;rd&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'Last 10 episode lengths (avg: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;avg_ep_len&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;)'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;zeroes&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; Q table zeroes, &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;fill_percent&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; percent filled'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


        &lt;span class="n"&gt;episode_lengths&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;evaluate_every&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'==='&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;"Running evaluation after &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; episodes"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;finish_percent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;avg_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;penalties&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;evaluate_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evaluate_trials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'==='&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;rd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;avg_ep_len&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;episode_lengths&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;zeroes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill_percent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calculate_q_table_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                    &lt;span class="s"&gt;'train_reward_distribution'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;rd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'train_ep_len'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;avg_ep_len&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'fill_percent'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;fill_percent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'test_finish_percent'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;finish_percent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'test_ep_len'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;avg_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'test_penalties'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;penalties&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Training finished."&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;calculate_q_table_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    This function counts what perecentage of cells in the q-table are non-zero.
    Note: There are certain state-action combinations that are illegal, so the table might never 
    be full.
    """&lt;/span&gt;
    &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;
    &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;cell&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;cell&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;fill_percent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;100.0&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill_percent&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    The routine to evaluate an agent. It simply exploits the q-table and records the performance metrics.
    """&lt;/span&gt;
    &lt;span class="n"&gt;total_epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_wins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;wins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;done&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;next_action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;num_penalties&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;wins&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;total_epochs&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;
        &lt;span class="n"&gt;total_penalties&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;num_penalties&lt;/span&gt;
        &lt;span class="n"&gt;total_wins&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;wins&lt;/span&gt;

    &lt;span class="n"&gt;average_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;complete_percent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_evaluation_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_wins&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;print_evaluation_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;average_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;average_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_wins&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;complete_percent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average_penalties&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;print_evaluation_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;average_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_wins&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Evaluation results after {} trials"&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Average time steps taken: {}"&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;average_time&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Average number of penalties incurred: {}"&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;average_penalties&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;"Had &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;total_wins&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; wins in &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; episodes"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_evaluation_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_wins&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;average_time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;total_epochs&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;average_penalties&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;total_penalties&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;complete_percent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;total_wins&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_trials&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;100.0&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;average_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;complete_percent&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# The hyper-parameters of Q-learning
&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt; &lt;span class="c1"&gt;# learning rate
&lt;/span&gt;&lt;span class="n"&gt;gamma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.7&lt;/span&gt; &lt;span class="c1"&gt;# discout factor
&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;
&lt;span class="n"&gt;env&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gym&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'DualTaxi-v1'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;num_episodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;50000&lt;/span&gt;

&lt;span class="c1"&gt;# Initialize a q-table full of zeroes
&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;observation_space&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_space&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# Get back trained q-table and metrics
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;Total encoded states are 6144
==============================
Running evaluation after 0 episodes
Evaluation results after 200 trials
Average time steps taken: 1500.0
Average number of penalties incurred: 1500.0
Had 0 wins in 200 episodes
==============================

----------------------------
Skipping intermediate output
----------------------------


==============================
Running evaluation after 49000 episodes
Evaluation results after 200 trials
Average time steps taken: 210.315
Average number of penalties incurred: 208.585
Had 173 wins in 200 episodes
==============================
Current Episode: 49404
Reward distribution: Counter({-3: 15343, -12: 12055, -4: 11018, -11: 4143, -20: 3906, -30: 1266, -2: 1260, 99: 699, -10: 185, 90: 125})
Last 10 episode lengths (avg: 63.0)
48388 Q table zeroes, 78.12319155092592 percent filled
Training finished.
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;I have skipped the intermediate output on purpose, you can check &lt;a href="https://pastebin.com/XHJLatiX"&gt;this pastebin&lt;/a&gt; if you're interested in full output. &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#competitive-approach"&gt;
  &lt;/a&gt;
  Competitive Approach
&lt;/h3&gt;

&lt;p&gt;The steps for this are similar to the cooperative approach, with the differnce that now we have multiple Q-tables to update.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Initialize the Q-table 1 and 2 for both the agents by all zeros. The size of each Q-table is &lt;code&gt;state_space_size x sqrt(action_space_size)&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Decide between exploration and exploitation based on the epsilon value.&lt;/li&gt;
&lt;li&gt;Exploration: For each state, select any one among all possible actions for the current state (S).&lt;/li&gt;
&lt;li&gt;Exploitation: For all possible actions from the state (S') select the one with the highest Q-value in the Q-tables of respective agents.&lt;/li&gt;
&lt;li&gt;Transition to the next state (S') as a result of that combined action (a1, a2).&lt;/li&gt;
&lt;li&gt;Update Q-table values for both the agents using the update equation and respective rewards &amp;amp; actions.&lt;/li&gt;
&lt;li&gt;If the episode is over (i.e. goal state is reached), reset the environment for next iteration.&lt;/li&gt;
&lt;li&gt;Keep repeating steps 2 to 7 until we start seeing decent results in the performance.
&lt;/li&gt;
&lt;/ol&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update_multi_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    Same as update method discussed in the last section, just modified for two independent q-tables.
    """&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_space&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;action1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode_action&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;action1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;action2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode_action&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;info&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;reward1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;
    &lt;span class="n"&gt;bellman_update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;bellman_update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;info&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train_multi_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log_every&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;running_metrics_len&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;evaluate_every&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evaluate_trials&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    Same as train method discussed in the last section, just modified for two independent q-tables.
    """&lt;/span&gt;
    &lt;span class="n"&gt;rewards&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;deque&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;running_metrics_len&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;episode_lengths&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;deque&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;total_timesteps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

    &lt;span class="n"&gt;metrics&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;done&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;

        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# Modification here
&lt;/span&gt;            &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;info&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;update_multi_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;total_timesteps&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;total_timesteps&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;log_every&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;rd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;avg_ep_len&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;episode_lengths&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;zeroes1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill_percent1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calculate_q_table_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;zeroes2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill_percent2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calculate_q_table_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'Current Episode: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'Reward distribution: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;rd&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'Last 10 episode lengths (avg: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;avg_ep_len&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;)'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;zeroes1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; Q table 1 zeroes, &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;fill_percent1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; percent filled'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;zeroes2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; Q table 2 zeroes, &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;fill_percent2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; percent filled'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


        &lt;span class="n"&gt;episode_lengths&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;evaluate_every&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'==='&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;"Running evaluation after &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; episodes"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;finish_percent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;avg_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;penalties&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;evaluate_multi_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evaluate_trials&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'==='&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;rd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rewards&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;avg_ep_len&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;episode_lengths&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;zeroes1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill_percent1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calculate_q_table_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;zeroes2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill_percent2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calculate_q_table_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                    &lt;span class="s"&gt;'train_reward_distribution'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;rd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'train_ep_len'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;avg_ep_len&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'fill_percent1'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;fill_percent1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'fill_percent2'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;fill_percent2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'test_finish_percent'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;finish_percent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'test_ep_len'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;avg_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;'test_penalties'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;penalties&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Training finished.&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate_multi_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    Same as evaluate method discussed in last section, just modified for two independent q-tables.
    """&lt;/span&gt;
    &lt;span class="n"&gt;total_epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_wins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;wins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;done&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# Modification here
&lt;/span&gt;            &lt;span class="n"&gt;next_action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode_action&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
                &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

            &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;num_penalties&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;wins&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

            &lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;total_epochs&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;
        &lt;span class="n"&gt;total_penalties&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;num_penalties&lt;/span&gt;
        &lt;span class="n"&gt;total_wins&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;wins&lt;/span&gt;

    &lt;span class="n"&gt;average_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;complete_percent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_evaluation_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_wins&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;print_evaluation_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;average_penalties&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;average_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;num_trials&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;total_wins&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;complete_percent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average_penalties&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="c1"&gt;# The hyperparameter of Q-learning
&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;
&lt;span class="n"&gt;gamma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.8&lt;/span&gt;
&lt;span class="n"&gt;epsilon&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;

&lt;span class="n"&gt;env_c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gym&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'DualTaxi-v1'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;competitive&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;num_episodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;50000&lt;/span&gt;
&lt;span class="n"&gt;q_table1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;env_c&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;observation_space&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;env_c&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_space&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;
&lt;span class="n"&gt;q_table2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;env_c&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;observation_space&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;env_c&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_space&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt;
&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;metrics_c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_multi_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env_c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;Total encoded states are 6144
==============================
Running evaluation after 0 episodes
Evaluation results after 200 trials
Average time steps taken: 1500.0
Average number of penalties incurred: 1500.0
Had 0 wins in 200 episodes
==============================

----------------------------
Skipping intermediate output
----------------------------


==============================
Running evaluation after 48000 episodes
Evaluation results after 200 trials
Average time steps taken: 323.39
Average number of penalties incurred: 322.44
Had 158 wins in 200 episodes
==============================
Current Episode: 48445
Reward distribution: Counter({-12: 13993, -3: 12754, -4: 11561, -20: 3995, -11: 3972, -30: 1907, -10: 649, -2: 524, 90: 476, 99: 169})
Last 10 episode lengths (avg: 78.08)
8064 Q table 1 zeroes, 78.125 percent filled
8064 Q table 2 zeroes, 78.125 percent filled
==============================
Running evaluation after 49000 episodes
Evaluation results after 200 trials
Average time steps taken: 434.975
Average number of penalties incurred: 434.115
Had 143 wins in 200 episodes
==============================
Current Episode: 49063
Reward distribution: Counter({-3: 13928, -12: 13605, -4: 10286, -11: 4542, -20: 3917, -30: 1874, -10: 665, -2: 575, 90: 433, 99: 175})
Last 10 episode lengths (avg: 75.1)
8064 Q table 1 zeroes, 78.125 percent filled
8064 Q table 2 zeroes, 78.125 percent filled
Current Episode: 49706
Reward distribution: Counter({-12: 13870, -3: 13169, -4: 11054, -11: 4251, -20: 3985, -30: 1810, -10: 704, -2: 529, 90: 436, 99: 192})
Last 10 episode lengths (avg: 76.12)
8064 Q table 1 zeroes, 78.125 percent filled
8064 Q table 2 zeroes, 78.125 percent filled
Training finished.
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;I have skipped the intermediate output on purpose, you can check &lt;a href="https://pastebin.com/ZPKZtjK1"&gt;this pastebin&lt;/a&gt; if you're interested in full output. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#evaluating-the-performance"&gt;
  &lt;/a&gt;
  Evaluating the performance
&lt;/h2&gt;

&lt;p&gt;If you observed the code carefully, the train functions returned q-tables as well as some metrics. We can use the q-table now for taking agent's actions, and see how intelligent it has become. Also, we'll try to plot these metrics to visualize how the training progressed.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;  
&lt;span class="c1"&gt;# import seaborn as plt
&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    Plotting various metrics over the number of episodes.
    """&lt;/span&gt;
    &lt;span class="n"&gt;ep_nums&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;series&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;ep_num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;metric_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;metric_val&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metric_val&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
                &lt;span class="n"&gt;series&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;metric_name&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metric_val&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;m_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;series&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ep_nums&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'Number of episodes'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;play&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;done&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;next_action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;play_multi&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="s"&gt;"""
    Capture frames by playing using the two q-tables.
    """&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_episodes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;done&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;next_action&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode_action&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
                &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
            &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_action&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="n"&gt;plot_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--_d7PFUvq--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_train_ep_len.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--_d7PFUvq--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_train_ep_len.png" alt="png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--4vUu2Fql--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_test_ep_len.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--4vUu2Fql--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_test_ep_len.png" alt="png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--L2zrops0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_fill_percent.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--L2zrops0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_fill_percent.png" alt="png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--FTIadJ3I--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_test_finish_percent.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--FTIadJ3I--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_test_finish_percent.png" alt="png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--imYco__r--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_test_penalties.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--imYco__r--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/coop_test_penalties.png" alt="png"&gt;&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="n"&gt;frames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;play&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;print_frames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;frames&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--ismCs_9W--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/rl-2-cooperative.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--ismCs_9W--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/rl-2-cooperative.gif" alt=""&gt;&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="n"&gt;plot_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metrics_c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--Vgf_MIS0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_train_ep_len.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Vgf_MIS0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_train_ep_len.png" alt="png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--lWTBWcnl--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_test_ep_len.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--lWTBWcnl--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_test_ep_len.png" alt="png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--ZK6Cqvu5--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_fill_percent1.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--ZK6Cqvu5--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_fill_percent1.png" alt="png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--X64ubyvv--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_fill_percent2.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--X64ubyvv--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_fill_percent2.png" alt="png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--bAFiAqmn--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_test_finish_percent.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--bAFiAqmn--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_test_finish_percent.png" alt="png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--85gFAzYI--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_test_penalties.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--85gFAzYI--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/comp_test_penalties.png" alt="png"&gt;&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="n"&gt;print_frames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;play_multi&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q_table1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q_table2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env_c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--PchCKrpF--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/rl-4-competitive.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--PchCKrpF--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/rl-4-competitive.gif" alt=""&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#some-observations"&gt;
  &lt;/a&gt;
  Some observations
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;While Q-learning agent commits errors initially during exploration but once it has explored enough (seen most of the states), it starts to act wisely.&lt;/li&gt;
&lt;li&gt;Both the approaches did fairly well. However, in relative comparison, the cooperative approach seem to perform better. The plots of competitive approach are more volatile. &lt;/li&gt;
&lt;li&gt;It took around 2000 episodes for agents to explore most of the possible state-action pairs. Note that not state-action pairs are feasible because some states aren't legal (for example, states where both the taxis are at same location aren't possible).&lt;/li&gt;
&lt;li&gt;As the training progressed the number of penalties reduced. They didn't reduce completely because of the epsilon (we're still exploring based on the epsilon value during training). &lt;/li&gt;
&lt;li&gt;The episode length kept decreasing, which means the taxis were able to pickup and drop the passenger faster because of the new learned knowledge in q-tables.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So to summarize, the agent is able to get around the walls, pick the passengers, take less penalties, and reach the destination timely. And the fact that the code where q-learning update happens is merely around 20-30 lines of Python code makes it even more impressive.&lt;/p&gt;

&lt;p&gt;From what we've discussed so far in the post, it's likely that you have a fair bit of intution about how Reinforcement Learning works. Now in the last few sections we will dip our toes in some broader level ideas and concepts that might be relevant to you when exploring Reinforcement Learning further. Let's start with the common challenges of Reinforcement Learning first,&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#common-challenges-while-applying-reinforcement-learning"&gt;
  &lt;/a&gt;
  Common challenges while applying Reinforcement learning
&lt;/h2&gt;

&lt;h3&gt;
  &lt;a href="#finiding-the-right-hyperparameters"&gt;
  &lt;/a&gt;
  Finiding the right Hyperparameters
&lt;/h3&gt;

&lt;p&gt;You might be wondering how did I decide values of alpha, gamma, and epsilon. In the above program, it was mostly based on intuition from my past experience and some "hit and trial". This goes a long way, but there are also some techniques to come up with good values. The process in itself is sometimes referred to as Hyperparamter tuning or Hyperparameter optimization.&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#tuning-the-hyperparameters"&gt;
  &lt;/a&gt;
  Tuning the hyperparameters
&lt;/h4&gt;

&lt;p&gt;A simple way to programmatically come up with the best set of values of the hyperparameter is to create a comprehensive search function that selects the parameters that would result in best agent performance. A more sophisticated way to get the right combination of hyperparameter values would be to use Genetic Algorithms. Also, it is a common practice to make these parameters dynamic instead of fixed values. For example, in our case, all of the three hyperparmeters can be configured to decrease over time because as the agent continues to learn, it builds up more resilient priors.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#choosing-the-right-algorithms"&gt;
  &lt;/a&gt;
  Choosing the right algorithms
&lt;/h3&gt;

&lt;p&gt;Q-learning is just one of the many Reinformcement Learning algorithms out there. There are multiple ways to classify Reinforcement Learning algorithms. The selection depends on various factors including the nature of the environment. For example, if the state space of action space is continuous instead of discrete (imagine that the environment now expects continuous degree values instead of discrete north / east / etc directions as actions, and the state space consists of more precise lat/lng location of taxis instead of grid coordinates), tabular Q-learning can't work. There are hacks to get around continuous spaces (like bucketing their range and making it discrete as a result), but these hacks fail too if the state space and action space gets too large. In those cases, it is preferred to use more generic algorithms, usually the ones that involve approximators like Neural Networks.&lt;/p&gt;

&lt;p&gt;More often than not, in practice, the agent is trained with multiple algorithms initially to decide which algorithm would fit the best.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#reward-structure"&gt;
  &lt;/a&gt;
  Reward Structure
&lt;/h3&gt;

&lt;p&gt;It is important to think strategically about the rewards to be given to the agent. If the rewards are too sparse, the agent might have difficulty in learning. Poorly structured rewards can also lead to cases of non-convergence and situations in which agent gets stuck in local minima. For example, let's say the environment gave +1 reward for successfully picking up passenger, and no penalty for dropping the passenger. So it might happen, that the agent might end up repeatedly picking up and dropping a passenger to maximise it rewards. Similary, if we there was very high negative reward for picking up passenger, agent would eventually learn to not pick a passenger at all, and hence would never finish successfully.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#the-challenges-of-real-world-environments"&gt;
  &lt;/a&gt;
  The challenges of real world environments
&lt;/h3&gt;

&lt;p&gt;Training an agent on an openAI gym environment is realtively easy because you get a lot of things out of the box. The real world, however, is a bit more unorganised. We sensors to ingest environment information and mechanism to translate it into something that can be fed to a Machine Learning algorithm. So such systems involve a lots of techniques overall aside from the learning algorithm. As a simple example, consider a general Reinforcement Learning agent that is being trained to play ATARI games. The information this agent needs to be passed is pixels on the screen. So we might have to use deep learning techniques (like Convolutional Neural Networks) to interpret the pixels on the screen and extract information out of the game (like scores) to enable the agent to interpret the game.&lt;/p&gt;

&lt;p&gt;There's also a challenge of sample efficiency. Since the state spaces and action spaces might be continuous and have big ranges, it becomes critical to achieve a decent sample efficiency that makes Reinforcement Learning feasible. If the algorithm needs high number of episodes (high enough that we cannot make it to produce results in reasonable amount of time), then Reinforcement Learning becomes impractical. &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#respecting-the-theoretical-boundaries"&gt;
  &lt;/a&gt;
  Respecting the theoretical boundaries
&lt;/h3&gt;

&lt;p&gt;It is easy to sometimes get carried away and see Reinforcement Learning to be the solution of most problems. It helps to have a theoretical understanding of how these algorithm works and fundamental concepts like &lt;a href="https://en.wikipedia.org/wiki/Markov_decision_process"&gt;Markov Decision Processes&lt;/a&gt; and awareness of the state of the art algorithms to have a better intution about what can and what can't be solved using present-day Reinforcement Learning algorithms.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#wrapping-up"&gt;
  &lt;/a&gt;
  Wrapping up
&lt;/h2&gt;

&lt;p&gt;In this tutorial, we began with understanding Reinforcement Learning with the help of real-world analogies. Then  we learned about some fundamental conepts like state, action, and rewards. Next, we went over the process of framing a problem such that we can traing an agent through Reinforcement Learning algorithms to solve it.&lt;/p&gt;

&lt;p&gt;We took Self-driving taxi as our reference problem for the rest of the tutorial. We then used OpenAL's gym module in python to provide us with a related environment, where we can develop our agent and evaluate it. Then we observed how terrible our agent was without using any algorithm to play the game, so we went ahead to implement the Q-learning algorithm from scratch. &lt;/p&gt;

&lt;p&gt;We then introduced Q-learning, and went over the steps to use it for our environment. We came up with two approaches (cooperative and competitive). We then evaluated the Q-learning results, and saw how the agent's performance improved significantly after Q-learning.&lt;/p&gt;

&lt;p&gt;As mentioned in beginning, Reinforcement learning is not just limited to openAI gym environments and games. It is also used for managing portfolio and finances, for making humanoid robots, for manufacturing and inventory management, to develop general AI agents (agents that can perform multiple things with a single algorithm, like same agent playing multiple Atari games).&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#appendix"&gt;
  &lt;/a&gt;
  Appendix
&lt;/h2&gt;

&lt;h3&gt;
  &lt;a href="#further-reading"&gt;
  &lt;/a&gt;
  Further reading
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;"Reinforcement Learning: An Introduction" Book by Andrew Barto and Richard S. Sutton. Most popular book about Reinforcement Learning out there. Highly recommended if you're planning to dive deep into the field. &lt;/li&gt;
&lt;li&gt;Lectures by David Silver (also available on &lt;a href="https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;amp;list=PLqYmG7hTraZBiG_XpjnPrSNw-1XQaM_gB&amp;amp;index=3"&gt;YouTube&lt;/a&gt;). Another great resource if you're more into learning from videos than books.&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0"&gt;Tutorial series on medium&lt;/a&gt; on Reinforcement learning using Tensorflow by Arthur Juliani.&lt;/li&gt;
&lt;li&gt;Some interesting topics related to Multi Agent environments,

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.researchgate.net/publication/2933305_Friend-or-Foe_Q-learning_in_General-Sum_Games"&gt;Friend and foe Q-learning in general-sum games&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Game theory concepts like&lt;/li&gt;
&lt;li&gt;Strictly dominant strategies&lt;/li&gt;
&lt;li&gt;Nash equilibrium&lt;/li&gt;
&lt;li&gt;Shapely values for reward distribution&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#visualising-the-transition-table-of-our-dual-taxi-enviroment"&gt;
  &lt;/a&gt;
  Visualising the transition table of our dual taxi enviroment
&lt;/h3&gt;

&lt;p&gt;The following is an attempt to visualize the internal tranistion table of our environment in a human readable way. The source of this information is the &lt;code&gt;env.P&lt;/code&gt; object which contains a mapping of the form&lt;/p&gt;

&lt;p&gt;&lt;code&gt;current_state : action_taken: [(transition_prob, next_state, reward, done)]&lt;/code&gt;, this is all the info we need to simulate the environment and this is what we can use to create the transition table.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt; &lt;span class="c1"&gt;# First let's take a peek at this object
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;{0: {
    0: [(1.0, 0, -30, False)],
  1: [(1.0, 1536, -0.5, True)],
  2: [(1.0, 1560, -0.5, True)],
  3: [(1.0, 1536, -0.5, True)],
  4: [(1.0, 1536, -0.5, True)],
  5: [(1.0, 1536, -0.5, True)],
  6: [(1.0, 96, -0.5, True)],
  7: [(1.0, 0, -30, False)],
  8: [(1.0, 24, -0.5, True)],
  9: [(1.0, 0, -30, False)],
  10: [(1.0, 0, -30, False)],
  11: [(1.0, 0, -30, False)],
  12: [(1.0, 480, -0.5, True)],
  13: [(1.0, 384, -0.5, True)],
  14: [(1.0, 0, -30, False)],
  15: [(1.0, 384, -0.5, True)],
  16: [(1.0, 384, -0.5, True)],
  17: [(1.0, 384, -0.5, True)],
  18: [(1.0, 96, -0.5, True)],
  19: [(1.0, 0, -30, False)],
  20: [(1.0, 24, -0.5, True)],
  21: [(1.0, 0, -30, False)],
  22: [(1.0, 0, -30, False)],
  23: [(1.0, 0, -30, False)],
  24: [(1.0, 96, -0.5, True)],
  25: [(1.0, 0, -30, False)],
  26: [(1.0, 24, -0.5, True)],
  27: [(1.0, 0, -30, False)],
  28: [(1.0, 0, -30, False)],
  29: [(1.0, 0, -30, False)],
  30: [(1.0, 96, -0.5, True)],
  31: [(1.0, 0, -30, False)],
  32: [(1.0, 24, -0.5, True)],
  33: [(1.0, 0, -30, False)],
  34: [(1.0, 0, -30, False)],
  35: [(1.0, 0, -30, False)]},
 1: {0: [(1.0, 1, -30, False)],
  1: [(1.0, 1537, -0.5, True)],
  2: [(1.0, 1561, -0.5, True)],
  3: [(1.0, 1537, -0.5, True)],
  4: [(1.0, 1537, -0.5, True)],
  5: [(1.0, 1537, -0.5, True)],
  6: [(1.0, 97, -0.5, True)],
  7: [(1.0, 1, -30, False)],
  8: [(1.0, 25, -0.5, True)],
  9: [(1.0, 1, -30, False)],
  10: [(1.0, 1, -30, False)],
  11: [(1.0, 1, -30, False)],
  12: [(1.0, 481, -0.5, True)],
  13: [(1.0, 385, -0.5, True)],
  14: [(1.0, 1, -30, False)],
  15: [(1.0, 385, -0.5, True)],
  16: [(1.0, 385, -0.5, True)],
  17: [(1.0, 385, -0.5, True)],
  18: [(1.0, 97, -0.5, True)],
  19: [(1.0, 1, -30, False)],
  20: [(1.0, 25, -0.5, True)],
  21: [(1.0, 1, -30, False)],
  22: [(1.0, 1, -30, False)],
  23: [(1.0, 1, -30, False)],
  24: [(1.0, 97, -0.5, True)],
  25: [(1.0, 1, -30, False)],
  26: [(1.0, 25, -0.5, True)],
  27: [(1.0, 1, -30, False)],
  28: [(1.0, 1, -30, False)],
  29: [(1.0, 1, -30, False)],
  30: [(1.0, 97, -0.5, True)],
  31: [(1.0, 1, -30, False)],
  32: [(1.0, 25, -0.5, True)],
  33: [(1.0, 1, -30, False)],
  34: [(1.0, 1, -30, False)],
  35: [(1.0, 1, -30, False)]},
# omitting the whole output because it's very long! 
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Now, let's put some code together to convert this information in more readable tabular form.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="err"&gt;!&lt;/span&gt; &lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;
&lt;span class="n"&gt;table&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

&lt;span class="n"&gt;env_c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gym&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;make&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'DualTaxi-v1'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;competitive&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;state_to_human_readable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;passenger_loc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'R'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'G'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'B'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'Y'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'T1'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'T2'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
    &lt;span class="n"&gt;destination&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'R'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'G'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'B'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'Y'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'Taxi 1: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;, Taxi 2: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;, Pass: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;passenger_loc&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;, Dest: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;destination&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;action_to_human_readable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;actions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'NSEWPD'&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;actions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="n"&gt;actions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;state_num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;transition_info&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;env_c&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;possible_transitions&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;transition_info&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;transition_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;possible_transitions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
            &lt;span class="s"&gt;'State'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;state_to_human_readable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state_num&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
            &lt;span class="s"&gt;'Action'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;action_to_human_readable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode_action&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
            &lt;span class="s"&gt;'Probablity'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;transition_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s"&gt;'Next State'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;state_to_human_readable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_state&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
            &lt;span class="s"&gt;'Reward'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;reward&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s"&gt;'Is over'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;div class="table-wrapper-paragraph"&gt;&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;Action&lt;/th&gt;
      &lt;th&gt;Probablity&lt;/th&gt;
      &lt;th&gt;Next State&lt;/th&gt;
      &lt;th&gt;Reward&lt;/th&gt;
      &lt;th&gt;Is over&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Taxi 1: (0, 0), Taxi 2: (0, 0), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(N, N)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (0, 0), Taxi 2: (0, 0), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(-15, -15)&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Taxi 1: (0, 0), Taxi 2: (0, 0), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(N, S)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (1, 0), Taxi 2: (0, 0), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(-0.5, 0)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Taxi 1: (0, 0), Taxi 2: (0, 0), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(N, E)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (1, 0), Taxi 2: (0, 1), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(-0.5, 0)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Taxi 1: (0, 0), Taxi 2: (0, 0), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(N, W)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (1, 0), Taxi 2: (0, 0), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(-0.5, 0)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Taxi 1: (0, 0), Taxi 2: (0, 0), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(N, P)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (1, 0), Taxi 2: (0, 0), Pass: R, Dest: R&lt;/td&gt;
      &lt;td&gt;(-0.5, 0)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;221179&lt;/th&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (3, 3), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(D, S)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (2, 3), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(-0.5, 0)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;221180&lt;/th&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (3, 3), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(D, E)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (3, 3), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(-15, -15)&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;221181&lt;/th&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (3, 3), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(D, W)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (3, 2), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(-0.5, 0)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;221182&lt;/th&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (3, 3), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(D, P)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (3, 3), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(-15, -15)&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;221183&lt;/th&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (3, 3), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(D, D)&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;Taxi 1: (3, 3), Taxi 2: (3, 3), Pass: T2, Dest: Y&lt;/td&gt;
      &lt;td&gt;(-15, -15)&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;221184 rows √ó 6 columns&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#bloopers"&gt;
  &lt;/a&gt;
  Bloopers
&lt;/h3&gt;

&lt;p&gt;In retrospect, the hardest part of writing this post was to get the dual-taxi-environment working. There were so many moments like below,&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--9hQk2Ugx--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/rl-3-blooper.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--9hQk2Ugx--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://github.com/satwikkansal/gym-dual-taxi/raw/master/static/rl-3-blooper.gif" alt=""&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It took a lot of trial and errors (tweaking rewards, updating rules for situations like collision, reducing state space) to get to a stage where the solutions for competitive set up were converging. The feeling when the solution converges for the first time is very cool. So if you have some free time, I'd recommend you to hack up an environment yourself (the first time I tried q-learning was with a snake-apple game I developed using pygame), and try to solve it with Reinforcement Learning. Trust me, you'll be humbled and learn lots of interesting things along the way! &lt;/p&gt;

</description>
      <category>python</category>
      <category>machinelearning</category>
      <category>reinforcementlearning</category>
      <category>openai</category>
    </item>
    <item>
      <title>How to Build a Stock Trading Bot with Python</title>
      <author>Saji Wang</author>
      <pubDate>Fri, 09 Jul 2021 14:21:16 +0000</pubDate>
      <link>https://dev.to/codesphere/how-to-build-a-stock-trading-bot-with-python-b1</link>
      <guid>https://dev.to/codesphere/how-to-build-a-stock-trading-bot-with-python-b1</guid>
      <description>&lt;p&gt;Earlier this week, we explored how code has drastically changed financial markets through the use of autonomous trading algorithms. Surprisingly, building your own trading bot is actually not that difficult!¬†&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In this tutorial, we're going to be using Python to build our own trading bot.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Keep in mind that this tutorial is not about how to make billions off of your trading bot. If I had an algorithm that sophisticated I probably wouldn't be giving it away. Rather, I'm going to show you how you can read market data, buy and sell stocks, and program the logic of your trading algorithm, all with some relatively simple Python code.&lt;/p&gt;

&lt;p&gt;And of course:&lt;br&gt;
&lt;em&gt;This article is for information purposes only. It is not intended to be investment advice. Seek a duly licensed professional for investment advice.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;You can open up a quick demo of the project on Codesphere here:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://link.codesphere.com/AX"&gt;https://codesphere.com/#https://github.com/LiorB-D/TradingBot&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;However, you will need an API key before you can actually start trading with our bot‚Ää-‚ÄäMore on that later.&lt;/p&gt;


&lt;h3&gt;
  &lt;a href="#some-helpful%C2%A0terms"&gt;
  &lt;/a&gt;
  Some Helpful¬†Terms
&lt;/h3&gt;

&lt;p&gt;Before we get started, it'll be helpful to define a couple of terms:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Paper Trading: The trading of securities with fake money for educational or testing purposes.&lt;/li&gt;
&lt;li&gt;Backtesting: Testing a trading algorithm against past market data in order to evaluate its effectiveness.&lt;/li&gt;
&lt;li&gt;Moving Average: The average of a certain amount of recent entries in a set of data.&lt;/li&gt;
&lt;li&gt;S&amp;amp;P 500: A stock market index composed of the 500 largest companies listed on US stock exchanges&lt;/li&gt;
&lt;li&gt;Closing Price: The final price of a security during a unit of time&lt;/li&gt;
&lt;li&gt;Good 'Til Cancel (GTC): When you place a trade, it may not be met right away. A broker will continue to try and execute a GTC trade until you cancel it.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;
  &lt;a href="#setup"&gt;
  &lt;/a&gt;
  Setup
&lt;/h3&gt;

&lt;p&gt;The trading API we're going to be using is called Alpaca and is by far one of the most intuitive trading APIs I've found.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://alpaca.markets/"&gt;https://alpaca.markets/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In its free tier, Alpaca includes both Paper and Real Trading and both Historical and Live market data. It also has an incredibly clean user interface and Python library.&lt;/p&gt;

&lt;p&gt;In addition, unless you're willing to leave your python script running on your computer, you're going to need to deploy your trading bot in the cloud. For this, we're going to use Codesphere:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://link.codesphere.com/BA"&gt;https://codesphere.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Since Codesphere's front-end is an IDE, we can develop our bot directly on the platform. If you wish to do the coding on your local machine, however, you can connect your GitHub repo to Codesphere and deploy afterward.&lt;/p&gt;

&lt;p&gt;The only environment setup we really need before we can start coding is to create our pip environment:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pipenv shell&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;And then install the Alpaca API&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pipenv install alpaca_trade_api&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We are also going to need to make a free Alpaca account and then navigate to our Paper Trading Account.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--GeGaprxi--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/84vieolvwgujbvn7gewz.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--GeGaprxi--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/84vieolvwgujbvn7gewz.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Notice your API Key on the right-hand side. When you first open your account, you will be prompted to generate a key and both public and private key will be shown to you. We're going to need those for later.&lt;/p&gt;
&lt;h3&gt;
  &lt;a href="#buying-and-selling%C2%A0stocks"&gt;
  &lt;/a&gt;
  Buying and Selling¬†Stocks
&lt;/h3&gt;

&lt;p&gt;We can then set up our Alpaca Trading library and buy and sell stocks in Python like so:&lt;/p&gt;


&lt;div class="ltag_gist-liquid-tag"&gt;
  
&lt;/div&gt;


&lt;h3&gt;
  &lt;a href="#our-strategy"&gt;
  &lt;/a&gt;
  Our Strategy
&lt;/h3&gt;

&lt;p&gt;The strategy we're going to use is to buy and sell whenever the 5 minute moving average crosses our price. Now, this is FAR from a good trading strategy, but the logic is relatively simple and will allow us to focus on the general structure of a trading bot.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--vZUzumsu--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/uex5g9bbczw0jl1l6vzy.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--vZUzumsu--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/uex5g9bbczw0jl1l6vzy.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In the above example, the red line is the stock price and the blue line is the moving average. When the moving average crosses under our price, we are going to buy a share of our stock. We are then going to hold the stock until the moving average crosses again and goes above the price. When that happens we are going to sell our share, and then wait for the next buying signal.&lt;/p&gt;

&lt;p&gt;In this article, we'll be trading SPY, which is an index that tracks the S&amp;amp;P 500, and we will only be trading one stock at a time.&lt;/p&gt;

&lt;p&gt;Keep in mind that if you were to make these trades with real money, you would have to comply with day trading regulations and brokerage fees, which would likely offset your gains.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#reading-market%C2%A0data"&gt;
  &lt;/a&gt;
  Reading Market¬†Data
&lt;/h3&gt;

&lt;p&gt;Now let's go over how to read market data using the Alpaca API in Python:&lt;/p&gt;


&lt;div class="ltag_gist-liquid-tag"&gt;
  
&lt;/div&gt;


&lt;p&gt;If you're looking for more in-depth information for when you build your strategy, check out Alpaca's documentation:&lt;br&gt;
&lt;a href="https://alpaca.markets/docs/api-documentation/api-v2/market-data/alpaca-data-api-v2/"&gt;https://alpaca.markets/docs/api-documentation/api-v2/market-data/alpaca-data-api-v2/&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;
  &lt;a href="#executing-our%C2%A0strategy"&gt;
  &lt;/a&gt;
  Executing Our¬†Strategy
&lt;/h3&gt;

&lt;p&gt;Now let's finally put all of this together for our complete trading algorithm:&lt;/p&gt;


&lt;div class="ltag_gist-liquid-tag"&gt;
  
&lt;/div&gt;


&lt;p&gt;And there we have it! We just built a trading bot in 54 lines of code! Now if we leave this running on Codesphere throughout the day, we should see our Alpaca dashboard update throughout the day:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--vZUzumsu--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/uex5g9bbczw0jl1l6vzy.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--vZUzumsu--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/uex5g9bbczw0jl1l6vzy.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#backtesting-a%C2%A0strategy"&gt;
  &lt;/a&gt;
  Backtesting a¬†Strategy
&lt;/h3&gt;

&lt;p&gt;Now if you don't want to wait around to see if your algorithm is any good, we can use Alpaca's market data API to backtest our Python algorithm against historical data:&lt;/p&gt;


&lt;div class="ltag_gist-liquid-tag"&gt;
  
&lt;/div&gt;


&lt;h3&gt;
  &lt;a href="#next-steps"&gt;
  &lt;/a&gt;
  Next Steps
&lt;/h3&gt;

&lt;p&gt;So there you have it, we just created a rudimentary trading bot with some fairly simple Python!&lt;/p&gt;

&lt;p&gt;Here is the full repo:&lt;br&gt;
&lt;a href="https://github.com/LiorB-D/TradingBot"&gt;https://github.com/LiorB-D/TradingBot&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;While I highly encourage you guys to play around with the Alpaca API for educational purposes, be extremely careful if you are going to trade real securities. One bug in your code could have disastrous effects on your bank account.&lt;br&gt;
On a lighter note, this is a great opportunity to put those statistics classes you took to work.&lt;/p&gt;




&lt;p&gt;Comment down below if you're going to build your own trading algorithm!&lt;/p&gt;

&lt;p&gt;Happy Coding from your folks at Codesphere, the next generation cloud provider&lt;/p&gt;

</description>
      <category>python</category>
      <category>tutorial</category>
      <category>webdev</category>
      <category>programming</category>
    </item>
    <item>
      <title>New to node.js and struggling with socket.io</title>
      <author>Fletcher Moore</author>
      <pubDate>Fri, 09 Jul 2021 14:02:10 +0000</pubDate>
      <link>https://dev.to/fletch0132/new-to-node-js-and-struggling-with-socket-io-o75</link>
      <guid>https://dev.to/fletch0132/new-to-node-js-and-struggling-with-socket-io-o75</guid>
      <description>&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;Nervous first post but really need some help. I'm working on a Web application for p2p communication (both video and text). &lt;/p&gt;

&lt;p&gt;The video communication works (some teething issues), but my main issue is getting user socket.id. specifically the user just having connected.&lt;/p&gt;

&lt;p&gt;I have tried many things including:&lt;br&gt;
Socket.on("connected", () {&lt;br&gt;
console.log(socket.id);&lt;br&gt;
});&lt;/p&gt;

&lt;p&gt;All I get is "undefined". Yet if I run te same console.log code after the page loads I can get it displayed.&lt;/p&gt;

&lt;p&gt;Not sure how to work with that.&lt;/p&gt;

&lt;p&gt;I want to store the socket.id and username in an object/array &lt;/p&gt;

&lt;p&gt;Thank you &lt;/p&gt;

</description>
      <category>node</category>
      <category>javascript</category>
      <category>webdev</category>
      <category>webrtc</category>
    </item>
    <item>
      <title>Mastering JS' 5 Best forEach Tutorials</title>
      <author>Mastering JS</author>
      <pubDate>Fri, 09 Jul 2021 13:51:42 +0000</pubDate>
      <link>https://dev.to/masteringjs/mastering-js-5-best-foreach-tutorials-117a</link>
      <guid>https://dev.to/masteringjs/mastering-js-5-best-foreach-tutorials-117a</guid>
      <description>&lt;p&gt;At Mastering JS, we typically recommend using &lt;code&gt;for/of&lt;/code&gt; loops to &lt;a href="https://masteringjs.io/tutorials/fundamentals/array-iterate"&gt;iterate through an array in JavaScript&lt;/a&gt;. However, &lt;code&gt;forEach()&lt;/code&gt; is still very common, and it is sometimes more convenient for one-liners.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--9d1aW-k3--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/apasfs6amoie7ms5kuz1.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--9d1aW-k3--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/apasfs6amoie7ms5kuz1.png" alt="loop comparison"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here's some of our best tutorials for &lt;a href="https://masteringjs.io/tutorials/fundamentals/foreach"&gt;JavaScript &lt;code&gt;forEach()&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;1) &lt;a href="https://masteringjs.io/tutorials/fundamentals/foreach-object"&gt;Iterating Through an Object with &lt;code&gt;forEach()&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;JavaScript arrays have a &lt;code&gt;forEach()&lt;/code&gt; function, but general objects do not. This tutorial explains 3 ways to iterate through an object using &lt;code&gt;forEach()&lt;/code&gt;: using &lt;code&gt;Object.keys()&lt;/code&gt;, &lt;code&gt;Object.values()&lt;/code&gt;, and &lt;code&gt;Object.entries()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;2) &lt;a href="https://masteringjs.io/tutorials/fundamentals/foreach-index"&gt;How to Break Out of a JavaScript forEach() Loop&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can't use the &lt;code&gt;break&lt;/code&gt; statement with &lt;code&gt;forEach()&lt;/code&gt;, because &lt;code&gt;forEach()&lt;/code&gt; is a function, &lt;strong&gt;not&lt;/strong&gt; a loop. If you want to break out of a &lt;code&gt;forEach()&lt;/code&gt; loop, the easiest way is to use &lt;code&gt;.every()&lt;/code&gt; instead of &lt;code&gt;.forEach()&lt;/code&gt;. This tutorial presents 3 ways to simulate &lt;code&gt;break&lt;/code&gt; in a &lt;code&gt;forEach()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;3) &lt;a href="https://masteringjs.io/tutorials/fundamentals/foreach-index"&gt;Get The Current Array Index in JavaScript forEach()&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;forEach()&lt;/code&gt; function calls your &lt;a href="https://masteringjs.io/tutorials/fundamentals/callbacks"&gt;callback&lt;/a&gt; with 3 parameters. Most &lt;code&gt;forEach()&lt;/code&gt; loops only use the first parameter, the &lt;em&gt;value&lt;/em&gt;. The 2nd parameter is the current array &lt;em&gt;index&lt;/em&gt;. This tutorial shows you how to get the array index.&lt;/p&gt;

&lt;p&gt;4) &lt;a href="https://masteringjs.io/tutorials/fundamentals/foreach-continue"&gt;Using Continue in JavaScript forEach()&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Because &lt;code&gt;forEach()&lt;/code&gt; is a function, &lt;strong&gt;not&lt;/strong&gt; a loop, you can't use &lt;code&gt;continue&lt;/code&gt;. However, with &lt;code&gt;forEach()&lt;/code&gt;, &lt;code&gt;return&lt;/code&gt; behaves like &lt;code&gt;continue&lt;/code&gt;. This tutorial presents 2 alternatives for simulating &lt;code&gt;continue&lt;/code&gt; with &lt;code&gt;forEach()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;5) &lt;a href="https://masteringjs.io/tutorials/fundamentals/async-foreach"&gt;How to Use forEach in an Async Function&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;forEach()&lt;/code&gt; function doesn't work well with &lt;a href="https://masteringjs.io/tutorials/fundamentals/async-await"&gt;async await&lt;/a&gt;. This is one of the major reasons why we recommend &lt;code&gt;for/of&lt;/code&gt; over &lt;code&gt;forEach()&lt;/code&gt;. This tutorial presents 2 alternative patterns that simulate async &lt;code&gt;forEach()&lt;/code&gt;.&lt;/p&gt;

</description>
      <category>javascript</category>
      <category>node</category>
      <category>vue</category>
      <category>codenewbie</category>
    </item>
    <item>
      <title>üöÄ10 Trending projects on GitHub for web developers - 9th July 2021</title>
      <author>Iain Freestone</author>
      <pubDate>Fri, 09 Jul 2021 13:49:33 +0000</pubDate>
      <link>https://dev.to/iainfreestone/10-trending-projects-on-github-for-web-developers-9th-july-2021-5gl6</link>
      <guid>https://dev.to/iainfreestone/10-trending-projects-on-github-for-web-developers-9th-july-2021-5gl6</guid>
      <description>&lt;p&gt;Trending Projects is available as a weekly newsletter please sign up at &lt;a href="https://www.iainfreestone.com"&gt;www.iainfreestone.com&lt;/a&gt; to ensure you never miss an issue.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#1-machine-learning-for-beginners"&gt;
  &lt;/a&gt;
  1. Machine Learning for Beginners
&lt;/h3&gt;

&lt;p&gt;12 weeks, 24 lessons, classic Machine Learning for all.&lt;/p&gt;


&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--i3JOwpme--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-ba8488d21cd8ee1fee097b8410db9deaa41d0ca30b004c0c63de0a479114156f.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/microsoft"&gt;
        microsoft
      &lt;/a&gt; / &lt;a href="https://github.com/microsoft/ML-For-Beginners"&gt;
        ML-For-Beginners
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      12 weeks, 24 lessons, classic Machine Learning for all
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;p&gt;&lt;a href="https://github.com/microsoft/ML-For-Beginners/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/5de80cbb57075704e04fe747ad3ad191aa6f34c131df08e56c0d64fd87abcfe8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6963726f736f66742f4d4c2d466f722d426567696e6e6572732e737667" alt="GitHub license"&gt;&lt;/a&gt;
&lt;a href="https://GitHub.com/microsoft/ML-For-Beginners/graphs/contributors/"&gt;&lt;img src="https://camo.githubusercontent.com/5928183d1e2b214910584f0a1a33cef45a70531548904b2257343e04d0b94249/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f6d6963726f736f66742f4d4c2d466f722d426567696e6e6572732e737667" alt="GitHub contributors"&gt;&lt;/a&gt;
&lt;a href="https://GitHub.com/microsoft/ML-For-Beginners/issues/"&gt;&lt;img src="https://camo.githubusercontent.com/aab9f79d64b8b1bf143c160c806b5c79f6cee50240592502c57f312113f7383f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6d6963726f736f66742f4d4c2d466f722d426567696e6e6572732e737667" alt="GitHub issues"&gt;&lt;/a&gt;
&lt;a href="https://GitHub.com/microsoft/ML-For-Beginners/pulls/"&gt;&lt;img src="https://camo.githubusercontent.com/0a604cca2c6363c6af0501543ef3a6565b7b82fd2ff894be8e417cf8db7d27c3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f6d6963726f736f66742f4d4c2d466f722d426567696e6e6572732e737667" alt="GitHub pull-requests"&gt;&lt;/a&gt;
&lt;a href="http://makeapullrequest.com" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0ff11ed110cfa69f703ef0dcca3cee6141c0a8ef465e8237221ae245de3deb3d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5052732d77656c636f6d652d627269676874677265656e2e7376673f7374796c653d666c61742d737175617265" alt="PRs Welcome"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://GitHub.com/microsoft/ML-For-Beginners/watchers/"&gt;&lt;img src="https://camo.githubusercontent.com/de9e0a6feb4334207b106185b7867b42c3d9fc189bc4adaf0b5733c93535353d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f77617463686572732f6d6963726f736f66742f4d4c2d466f722d426567696e6e6572732e7376673f7374796c653d736f6369616c266c6162656c3d5761746368" alt="GitHub watchers"&gt;&lt;/a&gt;
&lt;a href="https://GitHub.com/microsoft/ML-For-Beginners/network/"&gt;&lt;img src="https://camo.githubusercontent.com/fd710b83bb546a380eb15d0d154fcf63736a675d1736a3cd8825e638eb882502/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6d6963726f736f66742f4d4c2d466f722d426567696e6e6572732e7376673f7374796c653d736f6369616c266c6162656c3d466f726b" alt="GitHub forks"&gt;&lt;/a&gt;
&lt;a href="https://GitHub.com/microsoft/ML-For-Beginners/stargazers/"&gt;&lt;img src="https://camo.githubusercontent.com/cfd66469d198134664186871e4dbf0d45d4c3904e9ad51f553d55080d55b5af2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6963726f736f66742f4d4c2d466f722d426567696e6e6572732e7376673f7374796c653d736f6369616c266c6162656c3d53746172" alt="GitHub stars"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;
Machine Learning for Beginners - A Curriculum&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;üåç Travel around the world as we explore Machine Learning by means of world cultures üåç&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 24-lesson curriculum all about &lt;strong&gt;Machine Learning&lt;/strong&gt;. In this curriculum, you will learn about what is sometimes called &lt;strong&gt;classic machine learning&lt;/strong&gt;, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our forthcoming 'AI for Beginners' curriculum. Pair these lessons with our forthcoming 'Data Science for Beginners' curriculum, as well!&lt;/p&gt;
&lt;p&gt;Travel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to 'stick'.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;‚úçÔ∏è Hearty thanks to our&lt;/strong&gt;‚Ä¶&lt;/p&gt;
&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/microsoft/ML-For-Beginners"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;





&lt;h3&gt;
  &lt;a href="#2-petitevue"&gt;
  &lt;/a&gt;
  2. petite-vue
&lt;/h3&gt;

&lt;p&gt;petite-vue is an 5kb subset alternative distribution of Vue optimized for progressive enhancement. It provides the same template syntax and reactivity mental model with standard Vue. However, it is specifically optimized for "sprinkling" small amount of interactions on an existing HTML page rendered by a server framework&lt;/p&gt;


&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--i3JOwpme--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-ba8488d21cd8ee1fee097b8410db9deaa41d0ca30b004c0c63de0a479114156f.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/vuejs"&gt;
        vuejs
      &lt;/a&gt; / &lt;a href="https://github.com/vuejs/petite-vue"&gt;
        petite-vue
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      5kb subset of Vue optimized for progressive enhancement
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;h1&gt;
petite-vue&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;petite-vue&lt;/code&gt; is an alternative distribution of Vue optimized for progressive enhancement. It provides the same template syntax and reactivity mental model with standard Vue. However, it is specifically optimized for "sprinkling" small amount of interactions on an existing HTML page rendered by a server framework. See more details in &lt;a href="https://raw.githubusercontent.com/vuejs/petite-vue/main/#comparison-with-standard-vue"&gt;how it differs from standard Vue&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Only ~5.8kb&lt;/li&gt;
&lt;li&gt;Vue-compatible template syntax&lt;/li&gt;
&lt;li&gt;DOM-based, mutates in place&lt;/li&gt;
&lt;li&gt;Driven by &lt;code&gt;@vue/reactivity&lt;/code&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;
Status&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This is pretty new. There are probably bugs and there might still be API changes, so &lt;strong&gt;use at your own risk.&lt;/strong&gt; Is it usable though? Very much. Check out the &lt;a href="https://github.com/vuejs/petite-vue/tree/main/examples"&gt;examples&lt;/a&gt; to see what it's capable of.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The issue list is intentionally disabled because I have higher priority things to focus on for now and don't want to be distracted. If you found a bug, you'll have to either workaround it or submit a PR to fix it yourself. That‚Ä¶&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/vuejs/petite-vue"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;





&lt;h3&gt;
  &lt;a href="#3-milkdown"&gt;
  &lt;/a&gt;
  3. Milkdown
&lt;/h3&gt;

&lt;p&gt;Plugin driven WYSIWYG markdown editor.&lt;/p&gt;


&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--i3JOwpme--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-ba8488d21cd8ee1fee097b8410db9deaa41d0ca30b004c0c63de0a479114156f.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/Saul-Mirone"&gt;
        Saul-Mirone
      &lt;/a&gt; / &lt;a href="https://github.com/Saul-Mirone/milkdown"&gt;
        milkdown
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      üçº Plugin driven WYSIWYG  markdown editor.
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;h1&gt;
&lt;a rel="noopener noreferrer" href="https://raw.githubusercontent.com/Saul-Mirone/milkdown/main//gh-pages/public/milkdown-mini.svg"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--y7TdPbuw--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://raw.githubusercontent.com/Saul-Mirone/milkdown/main/gh-pages/public/milkdown-mini.svg" height="30px"&gt;&lt;/a&gt; Milkdown&lt;/h1&gt;
&lt;div&gt;
    &lt;a rel="noopener noreferrer" href="https://raw.githubusercontent.com/Saul-Mirone/milkdown/main//gh-pages/public/milkdown-homepage.svg"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--AJlT59_Z--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://raw.githubusercontent.com/Saul-Mirone/milkdown/main/gh-pages/public/milkdown-homepage.svg"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;A plugin-driven WYSIWYG markdown Editor, inspired by &lt;a href="https://typora.io/" rel="nofollow"&gt;Typora&lt;/a&gt;, built on top of &lt;a href="https://prosemirror.net/" rel="nofollow"&gt;prosemirror&lt;/a&gt; and &lt;a href="https://github.com/remarkjs/remark"&gt;remark&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Designed by Meo.&lt;/p&gt;
&lt;h1&gt;
Documentation&lt;/h1&gt;
&lt;p&gt;For more information, please check our &lt;a href="https://saul-mirone.github.io/milkdown/" rel="nofollow"&gt;official documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;
What's Next&lt;/h1&gt;
&lt;p&gt;You can check our &lt;a href="https://github.com/Saul-Mirone/milkdown/projects/1"&gt;Milkdown TODO&lt;/a&gt; project page to know what's on the plan.&lt;/p&gt;
&lt;h1&gt;
Contributor&lt;/h1&gt;
&lt;p&gt;&lt;a title="Saul-Mirone" href="https://github.com/Saul-Mirone"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--6040WrNP--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://avatars.githubusercontent.com/u/10047788%3Fv%3D4" width="100" alt="profile picture of Saul Mirone"&gt;&lt;/a&gt;
&lt;a title="xia" href="https://github.com/xiadd"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--aw8-Kbnw--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://avatars.githubusercontent.com/u/8351437%3Fv%3D4" width="100" alt="profile picture of xiadd"&gt;&lt;/a&gt;
&lt;a title="kitty" href="https://github.com/Kitty0730"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--arMHWykr--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://avatars.githubusercontent.com/u/14139395%3Fv%3D4" width="100" alt="profile picture of kitty"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;
Sponsors&lt;/h1&gt;
&lt;p&gt;If you like this project, please consider fund me to help the maintenance.&lt;/p&gt;
&lt;p&gt;&lt;a title="Johno Scott" href="https://github.com/johnoscott"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--WsqwT1u1--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://avatars.githubusercontent.com/u/291958%3Fv%3D4" width="100" alt="profile picture of Johno Scott"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;
License&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Saul-Mirone/milkdown/main//LICENSE"&gt;MIT&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;



&lt;/div&gt;
&lt;br&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/Saul-Mirone/milkdown"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;br&gt;
&lt;/div&gt;
&lt;br&gt;





&lt;h3&gt;
  &lt;a href="#4-fronts"&gt;
  &lt;/a&gt;
  4. Fronts
&lt;/h3&gt;

&lt;p&gt;Fronts is a progressive micro frontends framework for building Web applications, and it's based on the module federation of Webpack.&lt;/p&gt;


&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--i3JOwpme--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-ba8488d21cd8ee1fee097b8410db9deaa41d0ca30b004c0c63de0a479114156f.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/unadlib"&gt;
        unadlib
      &lt;/a&gt; / &lt;a href="https://github.com/unadlib/fronts"&gt;
        fronts
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      A progressive  micro frontends framework for building Web applications
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;p&gt;&lt;a href="https://fronts.js.org/" rel="nofollow"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--ohJ9MZKX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://raw.githubusercontent.com/unadlib/fronts/master/website/static/img/logo.svg" height="96" alt="Fronts Logo"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a rel="noopener noreferrer" href="https://github.com/unadlib/fronts/workflows/Node%20CI/badge.svg"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--ahL3qJG8--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/unadlib/fronts/workflows/Node%2520CI/badge.svg" alt="Node CI"&gt;&lt;/a&gt;
&lt;a href="http://badge.fury.io/js/fronts" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/12fac87a971be32fb31ebcbeb51849a611ba78e1ee539727bf25fbb09af69739/68747470733a2f2f62616467652e667572792e696f2f6a732f66726f6e74732e737667" alt="npm version"&gt;&lt;/a&gt;
&lt;a rel="noopener noreferrer" href="https://camo.githubusercontent.com/f20185893bd782736cd6c8449278ebb92616ad7d6f263c6daca900f7368309d3/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f6c2f66726f6e7473"&gt;&lt;img src="https://camo.githubusercontent.com/f20185893bd782736cd6c8449278ebb92616ad7d6f263c6daca900f7368309d3/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f6c2f66726f6e7473" alt="license"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fronts is a progressive micro frontends framework for building Web applications, and it's based on the &lt;a href="https://webpack.js.org/concepts/module-federation/" rel="nofollow"&gt;module federation&lt;/a&gt; of Webpack.&lt;/p&gt;
&lt;h2&gt;
Motivation&lt;/h2&gt;
&lt;p&gt;Among the many micro frontends solutions, &lt;a href="https://github.com/single-spa/single-spa"&gt;single-spa&lt;/a&gt; and &lt;a href="https://webpack.js.org/concepts/module-federation/" rel="nofollow"&gt;Module Federation&lt;/a&gt; are the best of them.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/single-spa/single-spa"&gt;single-spa&lt;/a&gt; is a micro frontends framework based on router configuration. The centralization of configuration brings some limitations, such as it is difficult to granulate nestable micro frontends, module granularity control, module sharing, and so on.&lt;/p&gt;
&lt;p&gt;In 2019, Zack Jackson proposed and implemented Module Federation. Module Federation is a completely different concept from single-spa, and allows a JavaScript application to dynamically load code from another application. It completely solves the problem of code dependency sharing and runtime modularity. The idea is true - &lt;a href="https://medium.com/swlh/webpack-5-module-federation-a-game-changer-to-javascript-architecture-bcdd30e02669" rel="nofollow"&gt;A game-changer in JavaScript architecture&lt;/a&gt; as mentioned in Zack Jackson's article. And it's currently supported by Webpack, Next.js, and Rollup.&lt;/p&gt;
&lt;p&gt;Although the Module Federation concept is so amazing, it has not yet‚Ä¶&lt;/p&gt;
&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/unadlib/fronts"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;





&lt;h3&gt;
  &lt;a href="#5-vscodethemegenerator"&gt;
  &lt;/a&gt;
  5. vscode-theme-generator
&lt;/h3&gt;

&lt;p&gt;Easily generate themes for VS Code with only a few colors.&lt;/p&gt;


&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--i3JOwpme--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-ba8488d21cd8ee1fee097b8410db9deaa41d0ca30b004c0c63de0a479114156f.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/Tyriar"&gt;
        Tyriar
      &lt;/a&gt; / &lt;a href="https://github.com/Tyriar/vscode-theme-generator"&gt;
        vscode-theme-generator
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      Easily generate themes for VS Code with only a few colors
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;h1&gt;
vscode-theme-generator&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/Tyriar/vscode-theme-generator" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/5ce59df1a4ef346bc5ba5293484f79e802d9a2fbd4998822a4381e08f58ceb64/68747470733a2f2f7472617669732d63692e6f72672f5479726961722f7673636f64652d7468656d652d67656e657261746f722e7376673f6272616e63683d6d6173746572" alt="Build Status"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a preview that leverages the new VS Code theming options in v1.12.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Problem&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;New themes are typically forked from other themes, carrying the bugs with them&lt;/li&gt;
&lt;li&gt;.tmThemes are overly verbose and difficult to maintain&lt;/li&gt;
&lt;li&gt;Themes are difficult to write from scratch&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;The Solution&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What if all you needed to do to generate a theme was specify a few colors and everything else was handled for you? Well that's what this module aims to accomplish. All you need to do is specify a set of "base colors" (background, foreground and 4 accent colors) and you have a reasonably good looking theme.&lt;/p&gt;
&lt;p&gt;All other VS Code theme colors are then derived from those base colors, with the option to tweak each underlying color as well.&lt;/p&gt;
&lt;h2&gt;
Example&lt;/h2&gt;
&lt;p&gt;This is all that's needed to generate a great looking theme:&lt;/p&gt;
&lt;div class="highlight highlight-source-ts position-relative js-code-highlight"&gt;
&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-kos"&gt;{&lt;/span&gt; &lt;span class="pl-s1"&gt;generateTheme&lt;/span&gt;&lt;span class="pl-kos"&gt;,&lt;/span&gt; &lt;span class="pl-smi"&gt;IColorSet&lt;/span&gt; &lt;span class="pl-kos"&gt;}&lt;/span&gt; &lt;span class="pl-k"&gt;from&lt;/span&gt; &lt;span class="pl-s"&gt;'vscode-theme-generator'&lt;/span&gt;&lt;span class="pl-kos"&gt;;&lt;/span&gt;
&lt;span class="pl-k"&gt;const&lt;/span&gt; &lt;span class="pl-s1"&gt;colorSet&lt;/span&gt;&lt;/pre&gt;‚Ä¶
&lt;/div&gt;
&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/Tyriar/vscode-theme-generator"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;





&lt;h3&gt;
  &lt;a href="#6-qwik"&gt;
  &lt;/a&gt;
  6. Qwik
&lt;/h3&gt;

&lt;p&gt;An Open-Source framework designed for best possible time to interactive, by focusing on resumability of server-side-rendering of HTML, and fine-grained lazy-loading of code.&lt;/p&gt;


&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--i3JOwpme--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-ba8488d21cd8ee1fee097b8410db9deaa41d0ca30b004c0c63de0a479114156f.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/BuilderIO"&gt;
        BuilderIO
      &lt;/a&gt; / &lt;a href="https://github.com/BuilderIO/qwik"&gt;
        qwik
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      An Open-Source framework designed for best possible time to interactive, by focusing on resumability of server-side-rendering of HTML, and fine-grained lazy-loading of code.
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;br&gt;
&lt;p&gt;
  &lt;a rel="noopener noreferrer" href="https://camo.githubusercontent.com/3518364b161ab1351455c0f3774d01973e25602a4b63a3e9129c21deddb2f223/68747470733a2f2f63646e2e6275696c6465722e696f2f6170692f76312f696d6167652f617373657473253246594a494762346930316a7677305352644c3542742532463636376162366332323833643463346438373866623930383361616363313066"&gt;&lt;img alt="Qwik Logo" width="400" src="https://camo.githubusercontent.com/3518364b161ab1351455c0f3774d01973e25602a4b63a3e9129c21deddb2f223/68747470733a2f2f63646e2e6275696c6465722e696f2f6170692f76312f696d6167652f617373657473253246594a494762346930316a7677305352644c3542742532463636376162366332323833643463346438373866623930383361616363313066"&gt;&lt;/a&gt;
&lt;/p&gt;



&lt;h1&gt;
&lt;code&gt;Qwik&lt;/code&gt; DOM-Centric, Resumable Web-App Framework&lt;/h1&gt;

&lt;p&gt;An Open-Source framework designed for best possible &lt;a href="https://web.dev/interactive/" rel="nofollow"&gt;time to interactive&lt;/a&gt;, by focusing on &lt;a href="https://github.com/BuilderIO/qwik/blob/main/docs/RESUMABLE.md"&gt;resumability&lt;/a&gt; of server-side-rendering of HTML, and &lt;a href="https://github.com/BuilderIO/qwik/blob/main/docs/LAZY_LOADING.md"&gt;fine-grained lazy-loading&lt;/a&gt; of code.&lt;/p&gt;

&lt;h2&gt;
Getting Started&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Visit &lt;a href="https://stackblitz.com/edit/qwik-todo-demo" rel="nofollow"&gt;StackBlitz&lt;/a&gt; for a simple demo you can play with.&lt;/li&gt;
&lt;li&gt;Visit &lt;a href="https://raw.githubusercontent.com/BuilderIO/qwik/main/./integration"&gt;integration&lt;/a&gt; folder for guided tours of Qwik to learn how it works.&lt;/li&gt;
&lt;li&gt;Understand the difference between &lt;a href="https://github.com/BuilderIO/qwik/blob/main/docs/RESUMABLE.md"&gt;resumable and replayable&lt;/a&gt; applications.&lt;/li&gt;
&lt;li&gt;Learn about Qwik's high level &lt;a href="https://github.com/BuilderIO/qwik/blob/main/docs/LAZY_LOADING.md"&gt;mental model&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
Blog Posts&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://dev.to/mhevery/a-first-look-at-qwik-the-html-first-framework-af" rel="nofollow"&gt;A first look at Qwik - the HTML first framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://dev.to/mhevery/death-by-closure-and-how-qwik-solves-it-44jj" rel="nofollow"&gt;Death by Closure (and how Qwik solves it)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
Community&lt;/h2&gt;

&lt;p&gt;Join our &lt;a href="https://discord.gg/JHVpZmqSs4" rel="nofollow"&gt;discord&lt;/a&gt; community.&lt;/p&gt;




&lt;p&gt;
  Made with ‚ù§Ô∏è by &lt;a href="https://www.builder.io/" rel="nofollow"&gt;Builder.io&lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
  &lt;/div&gt;
&lt;br&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/BuilderIO/qwik"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;br&gt;
&lt;/div&gt;
&lt;br&gt;





&lt;h3&gt;
  &lt;a href="#7-captain-stack"&gt;
  &lt;/a&gt;
  7. Captain Stack
&lt;/h3&gt;

&lt;p&gt;This feature is somewhat similar to Github Copilot's code suggestion. But instead of using AI, it sends your search query to Google, then retrieves StackOverflow answers and autocompletes them for you.&lt;/p&gt;


&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--i3JOwpme--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-ba8488d21cd8ee1fee097b8410db9deaa41d0ca30b004c0c63de0a479114156f.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/hieunc229"&gt;
        hieunc229
      &lt;/a&gt; / &lt;a href="https://github.com/hieunc229/copilot-clone"&gt;
        copilot-clone
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      VSCode extension for code suggestion
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;p&gt;&lt;a rel="noopener noreferrer" href="https://raw.githubusercontent.com/hieunc229/copilot-clone/master/./logo.svg"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--HUvSv-NM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://raw.githubusercontent.com/hieunc229/copilot-clone/master/./logo.svg" alt="Captain Stack"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;
Captain Stack ‚Äî Code suggestion for VSCode&lt;/h1&gt;
&lt;p&gt;This feature is somewhat similar to &lt;a href="https://copilot.github.com/"&gt;Github Copilot&lt;/a&gt;'s code suggestion. But instead of using AI, it sends your search query to Google, then retrieves StackOverflow answers and autocompletes them for you.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Captain Stack is launched on Product Hunt and would appricate your support&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.producthunt.com/posts/captain-stack?utm_source=badge-featured&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-captain-stack" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/19a33bf154dd0ecfd1b2890acd3bc7e7c3735df20f53cc94673344c6b7c510bd/68747470733a2f2f6170692e70726f6475637468756e742e636f6d2f776964676574732f656d6265642d696d6167652f76312f66656174757265642e7376673f706f73745f69643d333032343337267468656d653d6c69676874" alt="Captain Stack - An open source alternative to GitHub Copilot | Product Hunt" width="250" height="54"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a rel="noopener noreferrer" href="https://raw.githubusercontent.com/hieunc229/copilot-clone/master/./demo.gif"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Kwl3AtOf--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://raw.githubusercontent.com/hieunc229/copilot-clone/master/./demo.gif" alt="Demo Video"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;
Table of contents:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hieunc229/copilot-clone/master/#1-installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hieunc229/copilot-clone/master/#2-play-with-captain-stack"&gt;Play with Captain Stack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hieunc229/copilot-clone/master/#3-notes"&gt;Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hieunc229/copilot-clone/master/#4-changelog"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Note: ‚ö†Ô∏è This extension uses a proposed API (inline-completion) and can only be used for extension development in &lt;a href="https://code.visualstudio.com/insiders/" rel="nofollow"&gt;VSCode Insider release&lt;/a&gt;. It's not yet available on VSCode&lt;/em&gt;&lt;/p&gt;

&lt;h2&gt;
1. Installation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Check out the installation video: &lt;a href="https://youtu.be/MD-kzsF0Scg" rel="nofollow"&gt;https://youtu.be/MD-kzsF0Scg&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before installation, make sure you have &lt;a href="https://code.visualstudio.com/insiders/" rel="nofollow"&gt;VSCode Insider&lt;/a&gt;. You'll be using this version. To install and starting Captain Stack:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download this repository to your local machine. Unzip and open it on VSCode Insider (make sure the root directory is the same as &lt;code&gt;package.json&lt;/code&gt; file)&lt;/li&gt;
&lt;li&gt;(optional) Run &lt;code&gt;npm install&lt;/code&gt; in the terminal to‚Ä¶&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/hieunc229/copilot-clone"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;





&lt;h3&gt;
  &lt;a href="#8-vscodevim"&gt;
  &lt;/a&gt;
  8. VSCodeVim
&lt;/h3&gt;

&lt;p&gt;VSCodeVim is a Vim emulator for Visual Studio Code. &lt;/p&gt;


&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--i3JOwpme--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-ba8488d21cd8ee1fee097b8410db9deaa41d0ca30b004c0c63de0a479114156f.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/VSCodeVim"&gt;
        VSCodeVim
      &lt;/a&gt; / &lt;a href="https://github.com/VSCodeVim/Vim"&gt;
        Vim
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      ‚≠ê Vim for Visual Studio Code
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;h2&gt;
&lt;a rel="noopener noreferrer" href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/images/icon.png"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--PaE0MnD7--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://raw.githubusercontent.com/VSCodeVim/Vim/master/images/icon.png" height="128"&gt;&lt;/a&gt;&lt;br&gt;VSCodeVim&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Vim emulation for Visual Studio Code&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://aka.ms/vscodevim" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c6562f5c7aa19673be9e5809040f90557bd451e1796d517b3c57cf754e695eff/68747470733a2f2f76736d61726b6574706c61636562616467652e61707068622e636f6d2f76657273696f6e2f7673636f646576696d2e76696d2e737667" alt=""&gt;&lt;/a&gt;
&lt;a href="https://marketplace.visualstudio.com/items?itemName=vscodevim.vim" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2e60af83f0bde09498f3852bedd56bcb33cdac123235d8b608e3149c1500e5f0/68747470733a2f2f76736d61726b6574706c61636562616467652e61707068622e636f6d2f696e7374616c6c732d73686f72742f7673636f646576696d2e76696d2e737667" alt=""&gt;&lt;/a&gt;
&lt;a href="https://github.com/VSCodeVim/Vim/actions?query=workflow%3Abuild+branch%3Amaster"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--e1XiA1c6--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/VSCodeVim/Vim/workflows/build/badge.svg%3Fbranch%3Dmaster" alt=""&gt;&lt;/a&gt;
&lt;a href="https://vscodevim.herokuapp.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/7666ac5c7d5b95e1f9672be8dce0621b1bbad16cfc00639bff69df625c9d8fa6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7673636f646576696d2d736c61636b2d626c75652e7376673f6c6f676f3d736c61636b" alt=""&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;VSCodeVim is a Vim emulator for &lt;a href="https://code.visualstudio.com/" rel="nofollow"&gt;Visual Studio Code&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
üöö For a full list of supported Vim features, please refer to our &lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/ROADMAP.md"&gt;roadmap&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;
üìÉ Our &lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/CHANGELOG.md"&gt;change log&lt;/a&gt; outlines the breaking/major/minor updates between releases.&lt;/li&gt;
&lt;li&gt;
‚ùì If you need to ask any questions, join us on &lt;a href="https://vscodevim.herokuapp.com/" rel="nofollow"&gt;Slack&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Report missing features/bugs on &lt;a href="https://github.com/VSCodeVim/Vim/issues"&gt;GitHub&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

 &lt;strong&gt;Table of Contents&lt;/strong&gt; (click to expand)
&lt;ul&gt;
&lt;li&gt;
&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#-installation"&gt;Installation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#mac"&gt;Mac setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#windows"&gt;Windows setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#linux-setup"&gt;Linux setup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#%EF%B8%8F-settings"&gt;Settings&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#vscodevim-settings"&gt;VSCodeVim settings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#neovim-integration"&gt;Neovim Integration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#key-remapping"&gt;Key remapping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#vim-settings"&gt;Vim settings&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#%EF%B8%8F-multi-cursor-mode"&gt;Multi-Cursor mode&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#-emulated-plugins"&gt;Emulated plugins&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#vim-airline"&gt;vim-airline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#vim-easymotion"&gt;vim-easymotion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#vim-surround"&gt;vim-surround&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#vim-commentary"&gt;vim-commentary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#vim-indent-object"&gt;vim-indent-object&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#vim-sneak"&gt;vim-sneak&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#camelcasemotion"&gt;CamelCaseMotion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#input-method"&gt;Input Method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#replacewithregister"&gt;ReplaceWithRegister&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#vim-textobj-entire"&gt;vim-textobj-entire&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#-vscodevim-tricks"&gt;VSCodeVim tricks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#-faq"&gt;F.A.Q / Troubleshooting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/VSCodeVim/Vim/master/#%EF%B8%8F-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
üíæ Installation&lt;/h2&gt;
&lt;p&gt;VSCodeVim is automatically enabled following &lt;a href="https://marketplace.visualstudio.com/items?itemName=vscodevim.vim" rel="nofollow"&gt;installation&lt;/a&gt; and reloading of VS Code.&lt;/p&gt;
&lt;h3&gt;
Mac&lt;/h3&gt;
&lt;p&gt;To enable key-repeating execute the following in your Terminal and restart VS Code:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell position-relative js-code-highlight"&gt;
&lt;pre&gt;$ defaults write com.microsoft.VSCode ApplePressAndHoldEnabled -bool &lt;span class="pl-c1"&gt;false&lt;/span&gt;         &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; For VS Code&lt;/span&gt;
$ defaults write com.microsoft.VSCodeInsiders ApplePressAndHoldEnabled -bool &lt;span class="pl-c1"&gt;false&lt;/span&gt; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; For&lt;/span&gt;&lt;/pre&gt;‚Ä¶
&lt;/div&gt;
&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/VSCodeVim/Vim"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;





&lt;h3&gt;
  &lt;a href="#9-didact"&gt;
  &lt;/a&gt;
  9. Didact
&lt;/h3&gt;

&lt;p&gt;A DIY guide to build your own React&lt;/p&gt;


&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--i3JOwpme--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-ba8488d21cd8ee1fee097b8410db9deaa41d0ca30b004c0c63de0a479114156f.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/pomber"&gt;
        pomber
      &lt;/a&gt; / &lt;a href="https://github.com/pomber/didact"&gt;
        didact
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      A DIY guide to build your own React
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;p&gt;&lt;a rel="noopener noreferrer" href="https://cloud.githubusercontent.com/assets/1911623/26426031/5176c348-40ad-11e7-9f1a-1e2f8840b562.jpeg"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--4D4Ma4q---/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cloud.githubusercontent.com/assets/1911623/26426031/5176c348-40ad-11e7-9f1a-1e2f8840b562.jpeg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;
Didact&lt;/h1&gt;
&lt;h4&gt;
A DIY guide to build your own React&lt;/h4&gt;
&lt;p&gt;This repository goes together with a &lt;a href="https://engineering.hexacta.com/didact-learning-how-react-works-by-building-it-from-scratch-51007984e5c5" rel="nofollow"&gt;series of posts&lt;/a&gt; that explains how to build React from scratch step by step. &lt;strong&gt;You can jump straight to &lt;a href="https://pomb.us/build-your-own-react" rel="nofollow"&gt;the last post&lt;/a&gt; which is self-contained and includes everything.&lt;/strong&gt;&lt;/p&gt;
&lt;div class="table-wrapper-paragraph"&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Blog Post&lt;/th&gt;
&lt;th&gt;Code sample&lt;/th&gt;
&lt;th&gt;Commits&lt;/th&gt;
&lt;th&gt;Other languages&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://engineering.hexacta.com/didact-learning-how-react-works-by-building-it-from-scratch-51007984e5c5" rel="nofollow"&gt;Introduction&lt;/a&gt;&lt;/td&gt;



&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://engineering.hexacta.com/didact-rendering-dom-elements-91c9aa08323b" rel="nofollow"&gt;Rendering DOM elements&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://codepen.io/pomber/pen/eWbwBq?editors=0010" rel="nofollow"&gt;codepen&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/hexacta/didact/commit/fc4d360d91a1e68f0442d39dbce5b9cca5a08f24"&gt;diff&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/chinanf-boy/didact-explain#1-%E6%B8%B2%E6%9F%93dom%E5%85%83%E7%B4%A0"&gt;‰∏≠Êñá&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://engineering.hexacta.com/didact-element-creation-and-jsx-d05171c55c56" rel="nofollow"&gt;Element creation and JSX&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://codepen.io/pomber/pen/xdmoWE?editors=0010" rel="nofollow"&gt;codepen&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/hexacta/didact/commit/15010f8e7b8b54841d1e2dd9eacf7b3c06b1a24b"&gt;diff&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/chinanf-boy/didact-explain#2-%E5%85%83%E7%B4%A0%E5%88%9B%E5%BB%BA%E5%92%8Cjsx"&gt;‰∏≠Êñá&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://engineering.hexacta.com/didact-instances-reconciliation-and-virtual-dom-9316d650f1d0" rel="nofollow"&gt;Virtual DOM and reconciliation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://codepen.io/pomber/pen/WjLqYW?editors=0010" rel="nofollow"&gt;codepen&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href="https://github.com/hexacta/didact/commit/8eb7ffd6f5e210526fb4c274c4f60d609fe2f810"&gt;diff&lt;/a&gt; &lt;a href="https://github.com/hexacta/didact/commit/6f5fdb7331ed77ba497fa5917d920eafe1f4c8dc"&gt;diff&lt;/a&gt; &lt;a href="https://github.com/hexacta/didact/commit/35619a039d48171a6e6c53bd433ed049f2d718cb"&gt;diff&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/chinanf-boy/didact-explain#3-%E5%AE%9E%E4%BE%8B-%E5%AF%B9%E6%AF%94%E5%92%8C%E8%99%9A%E6%8B%9Fdom"&gt;‰∏≠Êñá&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://engineering.hexacta.com/didact-components-and-state-53ab4c900e37" rel="nofollow"&gt;Components and State&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://codepen.io/pomber/pen/RVqBrx" rel="nofollow"&gt;codepen&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/hexacta/didact/commit/2e290ff5c486b8a3f361abcbc6e36e2c21db30b8"&gt;diff&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/chinanf-boy/didact-explain#4-%E7%BB%84%E4%BB%B6%E5%92%8C%E7%8A%B6%E6%80%81"&gt;‰∏≠Êñá&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href="https://engineering.hexacta.com/didact-fiber-incremental-reconciliation-b2fe028dcaec" rel="nofollow"&gt;Fiber: Incremental reconciliation&lt;/a&gt; (self-contained post)&lt;/td&gt;
&lt;td&gt;&lt;a href="https://codepen.io/pomber/pen/veVOdd" rel="nofollow"&gt;codepen&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href="https://github.com/hexacta/didact/commit/6174a2289e69895acd8fc85abdc3aaff1ded9011"&gt;diff&lt;/a&gt; &lt;a href="https://github.com/hexacta/didact/commit/accafb81e116a0569f8b7d70e5b233e14af999ad"&gt;diff&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/chinanf-boy/didact-explain#5-fibre-%E9%80%92%E5%A2%9E%E5%AF%B9%E6%AF%94"&gt;‰∏≠Êñá&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href="https://pomb.us/build-your-own-react" rel="nofollow"&gt;The one with Hooks&lt;/a&gt; (self-contained post)&lt;/td&gt;
&lt;td&gt;&lt;a href="https://codesandbox.io/s/didact-8-21ost" rel="nofollow"&gt;codesandbox&lt;/a&gt;&lt;/td&gt;

&lt;td&gt;&lt;a href="https://www.tangdingblog.cn/blog/react/buildyourownreact-2020-09-22/" rel="nofollow"&gt;‰∏≠Êñá&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Follow &lt;a href="https://twitter.com/pomber" rel="nofollow"&gt;@pomber&lt;/a&gt; on twitter for updates.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;
License&lt;/h2&gt;
&lt;p&gt;The MIT License (MIT)&lt;/p&gt;
&lt;/div&gt;



&lt;/div&gt;
&lt;br&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/pomber/didact"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;br&gt;
&lt;/div&gt;
&lt;br&gt;





&lt;h3&gt;
  &lt;a href="#10-uikit"&gt;
  &lt;/a&gt;
  10. UIkit
&lt;/h3&gt;

&lt;p&gt;A lightweight and modular front-end framework for developing fast and powerful web interfaces&lt;/p&gt;


&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--i3JOwpme--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-ba8488d21cd8ee1fee097b8410db9deaa41d0ca30b004c0c63de0a479114156f.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/uikit"&gt;
        uikit
      &lt;/a&gt; / &lt;a href="https://github.com/uikit/uikit"&gt;
        uikit
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      A lightweight and modular front-end framework for developing fast and powerful web interfaces
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;p&gt;&lt;a href="https://getuikit.com/" rel="nofollow"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--5GBWXrv1--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cloud.githubusercontent.com/assets/321047/21769911/474d7d9e-d681-11e6-9fe0-d95f8ccfd3a9.jpg" alt="uikit banner"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;
UIkit&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://discord.gg/NEt4Pv7" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a1da00c3cbdb499d574eb6658ac513245d5ad10d87f37ed1ac112ad026d5a5f3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d6f6e253230646973636f72642d3732383964612e737667" alt="Discord"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;UIkit is a lightweight and modular front-end framework for developing fast and powerful web interfaces.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href="https://getuikit.com" rel="nofollow"&gt;Homepage&lt;/a&gt; - Learn more about UIkit&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://twitter.com/getuikit" rel="nofollow"&gt;@getuikit&lt;/a&gt; - Get the latest buzz on Twitter&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://discord.gg/NEt4Pv7" rel="nofollow"&gt;Discord Chat&lt;/a&gt; - Join our developer chat on Discord.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
  &lt;b&gt;UIkit is an Open Source project developed by YOOtheme.&lt;/b&gt;
  &lt;br&gt;&lt;br&gt;
  &lt;a href="https://yootheme.com" rel="nofollow"&gt;
      &lt;img width="134" height="30" src="https://camo.githubusercontent.com/91105a50618c98176a6a52fee730351255ac9cc059b4a64072aa34a02e9e334a/687474703a2f2f796f6f7468656d652e636f6d2f736974652f696d616765732f796f6f7468656d652d6c6f676f2e737667"&gt;
  &lt;/a&gt;
&lt;/p&gt;




&lt;h2&gt;
Getting started&lt;/h2&gt;

&lt;p&gt;You have the following options to get UIkit:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Download the &lt;a href="https://github.com/uikit/uikit/releases/latest"&gt;latest release&lt;/a&gt; with pre-built CSS and JS.&lt;/li&gt;
&lt;li&gt;Install with &lt;a href="https://npmjs.com" rel="nofollow"&gt;npm&lt;/a&gt; to get all source files as they are available on Github: &lt;code&gt;npm install uikit&lt;/code&gt;
&lt;/li&gt;
&lt;li&gt;Install with &lt;a href="https://yarnpkg.com/" rel="nofollow"&gt;yarn&lt;/a&gt; to get all source files as they are available on Github: &lt;code&gt;yarn add uikit&lt;/code&gt;
&lt;/li&gt;
&lt;li&gt;Directly load UIkit from &lt;a href="https://www.jsdelivr.com" rel="nofollow"&gt;jsDelivr&lt;/a&gt;: &lt;a href="https://www.jsdelivr.com/package/npm/uikit" rel="nofollow"&gt;https://www.jsdelivr.com/package/npm/uikit&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Clone the repo to get all source files including build scripts: &lt;code&gt;git clone git://github.com/uikit/uikit.git&lt;/code&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
Developers&lt;/h2&gt;

&lt;p&gt;To always have the latest development version of UIkit, even before a release, you may want to use npm or yarn with the &lt;code&gt;dev&lt;/code&gt; tag.&lt;/p&gt;


&lt;ul&gt;
&lt;li&gt;Using npm‚Ä¶&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;br&gt;
  &lt;/div&gt;
&lt;br&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/uikit/uikit"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;br&gt;
&lt;/div&gt;
&lt;br&gt;





&lt;h3&gt;
  &lt;a href="#stargazing"&gt;
  &lt;/a&gt;
  Stargazing üìà
&lt;/h3&gt;

&lt;h4&gt;
  &lt;a href="#top-risers-over-last-7-days"&gt;
  &lt;/a&gt;
  Top risers over last 7 days
&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;a href="https://github.com/public-apis/public-apis"&gt;Public APIs&lt;/a&gt; +3,575 stars&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/solidjs/solid"&gt;Solid&lt;/a&gt; +1,631 stars&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/bradtraversy/50projects50days"&gt;50 Projects in 50 Days&lt;/a&gt; +1,602 stars&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/ossf/scorecard"&gt;Security Scorecards&lt;/a&gt; +727 stars&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/kamranahmedse/developer-roadmap"&gt;Web Developer Roadmap&lt;/a&gt; +642 stars&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;
  &lt;a href="#top-growth-over-last-7-days"&gt;
  &lt;/a&gt;
  Top growth(%) over last 7 days
&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;a href="https://github.com/ossf/scorecard"&gt;Security Scorecards&lt;/a&gt; +97%&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/netlify/framework-info"&gt;Framework Info&lt;/a&gt; +75%&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/bradtraversy/50projects50days"&gt;50 Projects in 50 Days&lt;/a&gt; +38%&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/aidenybai/million"&gt;million&lt;/a&gt; +26%&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/solidjs/solid"&gt;Solid&lt;/a&gt; +23%&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;
  &lt;a href="#top-risers-over-last-30-days"&gt;
  &lt;/a&gt;
  Top risers over last 30 days
&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;a href="https://github.com/jwasham/coding-interview-university"&gt;Coding Interview University&lt;/a&gt; +7,706 stars&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/public-apis/public-apis"&gt;Public APIs&lt;/a&gt; +6,905 stars&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/trimstray/the-book-of-secret-knowledge"&gt;The Book Of Secret Knowledge&lt;/a&gt; +5,288 stars&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/microsoft/Web-Dev-For-Beginners"&gt;Web Development for Beginners&lt;/a&gt; +3,554 stars&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/maaslalani/slides"&gt;Slides&lt;/a&gt; +3,268 stars&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;
  &lt;a href="#top-growth-over-last-30-days"&gt;
  &lt;/a&gt;
  Top growth(%) over last 30 days
&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;a href="https://github.com/wellyshen/react-cool-virtual"&gt;React Virtual Cool&lt;/a&gt; +638%&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/maaslalani/slides"&gt;Slides&lt;/a&gt; +305%&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/aidenybai/million"&gt;million&lt;/a&gt; +160%&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/ossf/scorecard"&gt;Security Scorecards&lt;/a&gt; +146%&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/SigNoz/signoz"&gt;SigNoz&lt;/a&gt; +37%&lt;/li&gt;
&lt;/ol&gt;




&lt;p&gt;Trending Projects is available as a weekly newsletter please sign up at &lt;a href="https://www.iainfreestone.com"&gt;www.iainfreestone.com&lt;/a&gt; to ensure you never miss an issue.&lt;/p&gt;

&lt;p&gt;If you enjoyed this article you can &lt;a href="https://twitter.com/iain_freestone"&gt;follow me&lt;/a&gt; on Twitter where I regularly post bite size tips relating to HTML, CSS and JavaScript.&lt;/p&gt;

</description>
      <category>react</category>
      <category>javascript</category>
      <category>webdev</category>
      <category>productivity</category>
    </item>
    <item>
      <title>Building multiple themes with CSS..</title>
      <author>sakethk</author>
      <pubDate>Fri, 09 Jul 2021 13:33:26 +0000</pubDate>
      <link>https://dev.to/sakethkowtha/building-multiple-themes-with-css-mf3</link>
      <guid>https://dev.to/sakethkowtha/building-multiple-themes-with-css-mf3</guid>
      <description>&lt;p&gt;This article is about building a site with multiple color schemes. It will change the theme without reloading the document, without adding theme class names dynamically and without duplicating the code.&lt;/p&gt;

&lt;p&gt;I am using CSS variables in this example. You can try this even if you are using SASS or LESS.&lt;/p&gt;

&lt;p&gt;In this example we will create a document that has background color, Dropdown (to pick scheme) , card, heading and paragraph. Here I am creating six color schemes and have given random names to them. Below are the names&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Light&lt;/li&gt;
&lt;li&gt;Dark&lt;/li&gt;
&lt;li&gt;Sunset&lt;/li&gt;
&lt;li&gt;Moon light&lt;/li&gt;
&lt;li&gt;Cartoon&lt;/li&gt;
&lt;li&gt;bluecrystal&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In dropdown you will find the system option i.e., your OS theme. If your are using Dark mode in your OS, it will be Dark. Else, it will be &lt;code&gt;Light&lt;/code&gt;. &lt;/p&gt;

&lt;p&gt;For each scheme i defined four colors &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;--main-bg-color (Body backgound color)&lt;/li&gt;
&lt;li&gt;--card-shadow-color (Card shadow)&lt;/li&gt;
&lt;li&gt;--primary-text-color (Heading text color)&lt;/li&gt;
&lt;li&gt;--secondry-text-color (Paragraph text color)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;
  &lt;a href="#important"&gt;
  &lt;/a&gt;
  Important
&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;We are getting system theme with the help of &lt;code&gt;prefers-color-scheme&lt;/code&gt; media query.&lt;/li&gt;
&lt;li&gt;I am using &lt;code&gt;data-theme&lt;/code&gt; attribute to set color scheme. Whenever scheme changes just we need to update &lt;code&gt;data-theme&lt;/code&gt; attribute value.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is the Codepen. You can find color schemes and some other styles for Card, Heading and Paragraph in CSS section.&lt;/p&gt;

&lt;p&gt;&lt;iframe height="600" src="https://codepen.io/saketh-kowtha/embed/OJmXGXN?height=600&amp;amp;default-tab=result&amp;amp;embed-version=2"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

</description>
      <category>html</category>
      <category>css</category>
      <category>beginners</category>
      <category>webdev</category>
    </item>
    <item>
      <title>Scrape Google News with Python</title>
      <author>Dimitry Zub</author>
      <pubDate>Fri, 09 Jul 2021 13:15:26 +0000</pubDate>
      <link>https://dev.to/dimitryzub/scrape-google-news-with-python-4o14</link>
      <guid>https://dev.to/dimitryzub/scrape-google-news-with-python-4o14</guid>
      <description>&lt;p&gt;Contents: intro, imports, what will be scraped, process, code, code with pagination, links, outro.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#intro"&gt;
  &lt;/a&gt;
  Intro
&lt;/h3&gt;

&lt;p&gt;This blog post is a continuation of Google's web scraping series. Here you'll see how to scrape Google News Results using Python with &lt;code&gt;beautifulsoup&lt;/code&gt;, &lt;code&gt;requests&lt;/code&gt;, &lt;code&gt;lxml&lt;/code&gt; libraries. An alternative API solution will be shown.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#imports"&gt;
  &lt;/a&gt;
  Imports
&lt;/h3&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;requests&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lxml&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bs4&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;serpapi&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GoogleSearch&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h3&gt;
  &lt;a href="#what-will-be-scraped"&gt;
  &lt;/a&gt;
  What will be scraped
&lt;/h3&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--FdD8n_rG--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vr0a02qjk31qv6pdfmgb.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--FdD8n_rG--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vr0a02qjk31qv6pdfmgb.png" alt="image"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#process"&gt;
  &lt;/a&gt;
  Process
&lt;/h3&gt;

&lt;p&gt;Selecting &lt;strong&gt;container, title, link, source, snippet, published time&lt;/strong&gt;.&lt;br&gt;
&lt;a href="https://i.giphy.com/media/WwAOMVPxAvFhvG3MWB/giphy.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://i.giphy.com/media/WwAOMVPxAvFhvG3MWB/giphy.gif"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;
  &lt;a href="#code"&gt;
  &lt;/a&gt;
  Code
&lt;/h3&gt;


&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;requests&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lxml&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bs4&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;

&lt;span class="n"&gt;headers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s"&gt;"User-Agent"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582"&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s"&gt;"q"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"gta san andreas"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;"hl"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"en"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;"tbm"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"nws"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"https://www.google.com/search"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;soup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'lxml'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'.dbsr'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_one&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'.nDgy9d'&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;
    &lt;span class="n"&gt;link&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'href'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;source&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_one&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'.WF4CUc'&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;
    &lt;span class="n"&gt;snippet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_one&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'.Y3v8qd'&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;
    &lt;span class="n"&gt;date_published&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_one&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'.WG9SHc span'&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;link&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;snippet&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;date_published&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;source&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;-------------&lt;/span&gt;
&lt;span class="s"&gt;'''
San Andreas: Cesar &amp;amp; Kendl Is Grand Theft Auto's Best Relationship
https://screenrant.com/gta-san-andreas-cesar-kendl-best-relationship-why/
Many Grand Theft Auto relationships are negative or purely transactional. 
That makes Cesar and Kendl's genuine love for each other stand out.
4 hours ago
Screen Rant
'''&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;h3&gt;
  &lt;a href="#code-with-pagination"&gt;
  &lt;/a&gt;
  Code with pagination
&lt;/h3&gt;


&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bs4&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;requests&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;urllib&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lxml&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;paginate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;previous_url&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# Break from infinite recursion
&lt;/span&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;previous_url&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt;

    &lt;span class="n"&gt;headers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s"&gt;"User-Agent"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)"&lt;/span&gt;
                      &lt;span class="s"&gt;"Chrome/72.0.3538.102 Safari/537.36 Edge/18.19582"&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;
    &lt;span class="n"&gt;soup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'lxml'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# First page
&lt;/span&gt;    &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;

    &lt;span class="n"&gt;next_page_node&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_one&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'a#pnnext'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Stop when there is no next page
&lt;/span&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;next_page_node&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt;

    &lt;span class="n"&gt;next_page_url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urllib&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;urljoin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'https://www.google.com/'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;next_page_node&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'href'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="c1"&gt;# Pages after the first one
&lt;/span&gt;    &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="k"&gt;from&lt;/span&gt; &lt;span class="n"&gt;paginate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next_page_url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;scrape&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;pages&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;paginate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"https://www.google.com/search?hl=en-US&amp;amp;q=gta san andreas&amp;amp;tbm=nws"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;pages&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'Current page: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_one&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;".YyVfkd"&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'.dbsr'&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_one&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'.nDgy9d'&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;
            &lt;span class="n"&gt;link&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'href'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;source&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_one&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'.WF4CUc'&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;
            &lt;span class="n"&gt;snippet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_one&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'.Y3v8qd'&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;
            &lt;span class="n"&gt;date_published&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_one&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'.WG9SHc span'&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;link&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;snippet&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;date_published&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;source&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;scrape&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="o"&gt;-------------------&lt;/span&gt;
&lt;span class="s"&gt;'''
Current page: 1

San Andreas: Cesar &amp;amp; Kendl Is Grand Theft Auto's Best Relationship
https://screenrant.com/gta-san-andreas-cesar-kendl-best-relationship-why/
Many Grand Theft Auto relationships are negative or purely transactional. 
That makes Cesar and Kendl's genuine love for each other stand out.
4 hours ago
Screen Rant

...

Current page: 8

Il recr√©√© des covers d'album sur "GTA : San Andreas" et c'est ...
https://intrld.com/gtpmagazine-le-magazine-parodique-du-jeu-gta/
On a trouv√© LE compte parodique √† suivre : Grand Theft Parody, le magazine 
qui revisite des pochettes mythiques sur GTA : San Andreas.
2 weeks ago
Interlude
'''&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;h3&gt;
  &lt;a href="#using-google-news-result-api"&gt;
  &lt;/a&gt;
  Using &lt;a href="https://serpapi.com/news-results"&gt;Google News Result API&lt;/a&gt;
&lt;/h3&gt;

&lt;p&gt;SerpApi is a paid API with a free trial that soon will be replaced with a free plan.&lt;/p&gt;

&lt;p&gt;The main differences as I usually write in this blog post is that it's much faster and straightforward process rather than tinkering &lt;code&gt;CSS&lt;/code&gt;, &lt;code&gt;XPath&lt;/code&gt; selectors or dealing with Javascript-driven websites e.g. Google Maps which SerpApi scrapes like a charm. &lt;/p&gt;

&lt;p&gt;If you want to get things quickly and write code faster, and don't want to maintain the parser then, I believe that API solution is a way to go.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;serpapi&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GoogleSearch&lt;/span&gt;

&lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="s"&gt;"api_key"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"YOUR_API_KEY"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;"engine"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"google"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;"q"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"gta san andreas"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;"gl"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"us"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;"tbm"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"nws"&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;search&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GoogleSearch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'news_results'&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;-----------------------&lt;/span&gt;
&lt;span class="s"&gt;'''
{'position': 1, 'link': 'https://www.sportskeeda.com/gta/5-strange-gta-san-andreas-glitches', 'title': '5 strange GTA San Andreas glitches', 'source': 'Sportskeeda', 'date': '9 hours ago', 'snippet': 'GTA San Andreas has a wide assortment of interesting and strange glitches.', 'thumbnail': 'https://serpapi.com/searches/60e71e1f8b7ed2dfbde7629b/images/1394ee64917c752bdbe711e1e56e90b20906b4761045c01a2cefb327f91d40bb.jpeg'}
'''&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h3&gt;
  &lt;a href="#google-news-results-api-with-pagination"&gt;
  &lt;/a&gt;
  Google News Results API with Pagination
&lt;/h3&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="c1"&gt;# https://github.com/serpapi/google-search-results-python
&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;serpapi&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GoogleSearch&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;scrape&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s"&gt;"engine"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"google"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s"&gt;"q"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"coca cola"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s"&gt;"tbm"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"nws"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s"&gt;"api_key"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"YOUR_API_KEY"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="n"&gt;search&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GoogleSearch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pages&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pagination&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;pages&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;"Current page: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'serpapi_pagination'&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;'current'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;news_result&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;"news_results"&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;"Title: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;news_result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'title'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;Link: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;news_result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'link'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="n"&gt;scrape&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;


&lt;span class="o"&gt;------------------------&lt;/span&gt;
&lt;span class="s"&gt;'''
Current page: 1
Title: 5 strange GTA San Andreas glitches
Link: https://www.sportskeeda.com/gta/5-strange-gta-san-andreas-glitches

...

Current page: 14
Title: Ambitious Grand Theft Auto: San Andreas Mod Turns It Into A Spider-Man Game
Link: https://gamerant.com/grand-theft-auto-san-andreas-spider-man-game-mod/
...
'''&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h3&gt;
  &lt;a href="#links"&gt;
  &lt;/a&gt;
  Links
&lt;/h3&gt;

&lt;p&gt;&lt;a href="https://replit.com/@DimitryZub1/Scrape-Google-News-with-Pagination-python-serpapi#main.py"&gt;Code in the online IDE&lt;/a&gt; ‚Ä¢ &lt;a href="https://serpapi.com/news-results"&gt;Google News Result API&lt;/a&gt; &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#outro"&gt;
  &lt;/a&gt;
  Outro
&lt;/h3&gt;

&lt;p&gt;If you want to see how to scrape something using Python/Ruby that I didn't write about yet or you want to see some project made with SerpApi, please write me a message.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Yours, D&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href="https://i.giphy.com/media/8vIFoKU8s4m4CBqCao/giphy.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://i.giphy.com/media/8vIFoKU8s4m4CBqCao/giphy.gif"&gt;&lt;/a&gt;&lt;/p&gt;

</description>
      <category>python</category>
      <category>tutorial</category>
      <category>datascience</category>
      <category>webscraping</category>
    </item>
  </channel>
</rss>
