<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>DEV Community</title>
    <author>DEV Community</author>
    <description>A constructive and inclusive social network for software developers. With you every step of your journey.</description>
    <link>https://dev.to</link>
    <language>en</language>
    <item>
      <title>Weekly Digest 33/2021</title>
      <author>Marco Biedermann</author>
      <pubDate>Sun, 22 Aug 2021 16:48:31 +0000</pubDate>
      <link>https://dev.to/marcobiedermann/weekly-digest-33-2021-a7k</link>
      <guid>https://dev.to/marcobiedermann/weekly-digest-33-2021-a7k</guid>
      <description>&lt;p&gt;Welcome to my Weekly Digest #33 of this year.&lt;/p&gt;

&lt;p&gt;This weekly digest contains a lot of interesting and inspiring articles, videos, tweets, podcasts, and designs I consumed during this week.&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#interesting-articles-to-read"&gt;
  &lt;/a&gt;
  Interesting articles to read
&lt;/h2&gt;

&lt;h3&gt;
  &lt;a href="#how-to-calculate-rems-from-pixels"&gt;
  &lt;/a&gt;
  How to calculate REMs from pixels
&lt;/h3&gt;

&lt;p&gt;Sizing things in CSS can be tricky. But REMs are probably easier than you think.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://typeofnan.dev/how-to-calculate-rems-from-pixels/"&gt;How to calculate REMs from pixels&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#creating-victory-charts-in-react-native"&gt;
  &lt;/a&gt;
  Creating Victory charts in React Native
&lt;/h3&gt;

&lt;p&gt;Engage users with effective data visuals that are easy to read and understand. Here's how to apply Victory charts in your React Native app.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://blog.logrocket.com/creating-victory-charts-react-native/"&gt;Creating Victory charts in React Native - LogRocket Blog&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#accessibility-from-the-ground-up"&gt;
  &lt;/a&gt;
  Accessibility from the Ground Up
&lt;/h3&gt;

&lt;p&gt;So you are starting or restarting that website or web app from scratch, and you want to do things well. You know about accessibility, you know it‚Äôs important, and no one up the food chain (all them bosses) is challenging this (or invested enough to care). That‚Äôs great news! Let‚Äôs make that project A C C E S S I B L E. ‚ú®&lt;/p&gt;

&lt;p&gt;&lt;a href="https://kittygiraudel.com/2021/08/20/accessibility-from-the-ground-up/"&gt;Accessibility from the Ground Up&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#deno-a-breath-of-fresh-air-for-the-serverside-javascript"&gt;
  &lt;/a&gt;
  Deno, a breath of fresh air for the server-side JavaScript
&lt;/h3&gt;

&lt;p&gt;Deno is made by the original creator of Node.js, Ryan Dahl. It is his successor project that aims to fix all regrettable things from the Node.js design. Let me share with you why I like it so much!&lt;/p&gt;

&lt;p&gt;&lt;a href="https://pawelgrzybek.com/deno-a-breath-of-fresh-air-for-the-server-side-javascript/"&gt;Deno, a breath of fresh air for the server-side JavaScript | pawelgrzybek.com&lt;/a&gt;&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#some-great-videos-i-watched-this-week"&gt;
  &lt;/a&gt;
  Some great videos I watched this week
&lt;/h2&gt;

&lt;h3&gt;
  &lt;a href="#create-a-paid-subscription-newsletter"&gt;
  &lt;/a&gt;
  Create a Paid Subscription Newsletter
&lt;/h3&gt;

&lt;p&gt;With MailPoet and WooCommerce, both plugins for WordPress, we can build a paid subscription newsletter. Our only costs are transaction fees, whatever WordPress hosting we might need, and a yearly fee for WooCommerce Subscriptions, making it a very cost-friendly choice.&lt;/p&gt;

&lt;p&gt;&lt;iframe width="710" height="399" src="https://www.youtube.com/embed/UA4ec7YWokY"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;by &lt;a href="https://twitter.com/chriscoyier"&gt;Chris Coyier&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#build-a-curvaceous-homepage"&gt;
  &lt;/a&gt;
  Build a Curvaceous Homepage
&lt;/h3&gt;

&lt;p&gt;Learn how to design a website with curved or wavy backgrounds using HTML and CSS. Then take things to the next level by adding a morphing SVG animation with JavaScript&lt;/p&gt;

&lt;p&gt;&lt;iframe width="710" height="399" src="https://www.youtube.com/embed/lPJVi797Uy0"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;by &lt;a href="https://twitter.com/fireship_dev"&gt;Fireship&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#how-functional-programming-can-make-you-a-better-developer"&gt;
  &lt;/a&gt;
  How Functional Programming Can Make You A Better Developer
&lt;/h3&gt;

&lt;p&gt;We've all heard of functional programming, but how can you apply it to your existing code?&lt;/p&gt;

&lt;p&gt;&lt;iframe width="710" height="399" src="https://www.youtube.com/embed/EqO4TcNLjl0"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;by &lt;a href="https://twitter.com/ericnormand"&gt;Eric Normand&lt;/a&gt;&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#useful-github-repositories"&gt;
  &lt;/a&gt;
  Useful GitHub repositories
&lt;/h2&gt;

&lt;h3&gt;
  &lt;a href="#alpinejs"&gt;
  &lt;/a&gt;
  Alpine.js
&lt;/h3&gt;

&lt;p&gt;A rugged, minimal framework for composing JavaScript behavior in your markup.&lt;/p&gt;


&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--i3JOwpme--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-ba8488d21cd8ee1fee097b8410db9deaa41d0ca30b004c0c63de0a479114156f.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/alpinejs"&gt;
        alpinejs
      &lt;/a&gt; / &lt;a href="https://github.com/alpinejs/alpine"&gt;
        alpine
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      A rugged, minimal framework for composing JavaScript behavior in your markup. 
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;h1&gt;
Alpine.js&lt;/h1&gt;
&lt;p&gt;Go to the Alpine docs for most things: &lt;a href="https://alpinejs.dev" rel="nofollow"&gt;Alpine Docs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Stay here for contribution-related information.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Looking for V2 docs? &lt;a href="https://github.com/alpinejs/alpine/tree/v2.8.2"&gt;here they are&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;
Contribution Guide:&lt;/h2&gt;
&lt;h3&gt;
Quickstart&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;clone this repo locally&lt;/li&gt;
&lt;li&gt;run &lt;code&gt;npm install&lt;/code&gt; &amp;amp; &lt;code&gt;npm run build&lt;/code&gt;
&lt;/li&gt;
&lt;li&gt;Include the &lt;code&gt;/packages/alpinejs/dist/cdn.js&lt;/code&gt; file from a &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag on a webpage and you're good to go!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;
Brief Tour&lt;/h3&gt;
&lt;p&gt;You can get everything installed with: &lt;code&gt;npm install&lt;/code&gt; in the root directory of this repo after cloning it locally.&lt;/p&gt;
&lt;p&gt;This repo is a "mono-repo" using npm workspaces for managing the packages. Each package has its own folder in the &lt;code&gt;/packages&lt;/code&gt; directory.&lt;/p&gt;
&lt;p&gt;Rather than having to run separate builds for each package, all package bundles are handled with the same command: &lt;code&gt;npm run build&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Here's a brief look at each package in this repo:&lt;/p&gt;
&lt;div class="table-wrapper-paragraph"&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Package&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/alpinejs/alpinepackages/alpinejs"&gt;alpinejs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The main Alpine repo with all of Alpine's core&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/alpinejs/alpinepackages/csp"&gt;csp&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A repo to provide a "CSP safe" build of&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/div&gt;‚Ä¶&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/alpinejs/alpine"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;


&lt;h3&gt;
  &lt;a href="#payloads-all-the-things"&gt;
  &lt;/a&gt;
  Payloads All The Things
&lt;/h3&gt;

&lt;p&gt;A list of useful payloads and bypass for Web Application Security and Pentest/CTF&lt;/p&gt;


&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--i3JOwpme--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-ba8488d21cd8ee1fee097b8410db9deaa41d0ca30b004c0c63de0a479114156f.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/swisskyrepo"&gt;
        swisskyrepo
      &lt;/a&gt; / &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings"&gt;
        PayloadsAllTheThings
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      A list of useful payloads and bypass for Web Application Security and Pentest/CTF
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;h1&gt;
Payloads All The Things &lt;a href="https://twitter.com/intent/tweet?text=Payloads%20All%20The%20Things,%20a%20list%20of%20useful%20payloads%20and%20bypasses%20for%20Web%20Application%20Security%20-%20by%20@pentest_swissky&amp;amp;url=https://github.com/swisskyrepo/PayloadsAllTheThings/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/90bc908826728c0e4261acfff5619fd732c7be2b2a00624fce6363c9a3623c90/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f687474702f736869656c64732e696f2e7376673f7374796c653d736f6369616c" alt="Tweet"&gt;&lt;/a&gt;
&lt;/h1&gt;
&lt;p&gt;A list of useful payloads and bypasses for Web Application Security
Feel free to improve with your payloads and techniques
I ‚ù§Ô∏è pull requests :)&lt;/p&gt;
&lt;p&gt;You can also contribute with a üçª IRL, or using the sponsor button.&lt;/p&gt;
&lt;p&gt;
  &lt;a rel="noopener noreferrer" href="https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.github/banner.png"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--x7NHBE3Q--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.github/banner.png"&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Every section contains the following files, you can use the &lt;code&gt;_template_vuln&lt;/code&gt; folder to create a new chapter:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;README.md - vulnerability description and how to exploit it, including several payloads&lt;/li&gt;
&lt;li&gt;Intruder - a set of files to give to Burp Intruder&lt;/li&gt;
&lt;li&gt;Images - pictures for the README.md&lt;/li&gt;
&lt;li&gt;Files - some files referenced in the README.md&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You might also like the &lt;code&gt;Methodology and Resources&lt;/code&gt; folder :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/"&gt;Methodology and Resources&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Active%20Directory%20Attack.md"&gt;Active Directory Attack.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Cloud%20-%20AWS%20Pentest.md"&gt;Cloud - AWS Pentest.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Cloud%20-%20Azure%20Pentest.md"&gt;Cloud - Azure Pentest.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Cobalt%20Strike%20-%20Cheatsheet.md"&gt;Cobalt Strike - Cheatsheet.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Linux%20-%20Persistence.md"&gt;Linux - Persistence.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Linux%20-%20Privilege%20Escalation.md"&gt;Linux - Privilege Escalation.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Metasploit%20-%20Cheatsheet.md"&gt;Metasploit - Cheatsheet.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Methodology%20and%20enumeration.md"&gt;Methodology and enumeration.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Network%20Pivoting%20Techniques.md"&gt;Network Pivoting Techniques.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Network%20Discovery.md"&gt;Network Discovery.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Reverse%20Shell%20Cheatsheet.md"&gt;Reverse Shell Cheatsheet.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Subdomains%20Enumeration.md"&gt;Subdomains Enumeration.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Download%20and%20Execute.md"&gt;Windows -&lt;/a&gt;‚Ä¶&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;br&gt;
  &lt;/div&gt;
&lt;br&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/swisskyrepo/PayloadsAllTheThings"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;br&gt;
&lt;/div&gt;
&lt;br&gt;


&lt;h3&gt;
  &lt;a href="#manim"&gt;
  &lt;/a&gt;
  Manim
&lt;/h3&gt;

&lt;p&gt;A community-maintained Python framework for creating mathematical animations.&lt;/p&gt;


&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--i3JOwpme--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-ba8488d21cd8ee1fee097b8410db9deaa41d0ca30b004c0c63de0a479114156f.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/ManimCommunity"&gt;
        ManimCommunity
      &lt;/a&gt; / &lt;a href="https://github.com/ManimCommunity/manim"&gt;
        manim
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      A community-maintained Python framework for creating mathematical animations. 
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;p&gt;
    &lt;a href="https://www.manim.community/" rel="nofollow"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s---EDE34ka--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://raw.githubusercontent.com/ManimCommunity/manim/main/logo/cropped.png"&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;br&gt;
    &lt;a href="https://pypi.org/project/manim/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/cca5335efa233fb816f007abd9f409155d708b3cf6c3aa719b17d2bf3f8060cb/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6d616e696d2e7376673f7374796c653d666c6174266c6f676f3d70797069" alt="PyPI Latest Release"&gt;&lt;/a&gt;
    &lt;a href="https://hub.docker.com/r/manimcommunity/manim" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6c58eb13621b314d779cd763a02532756c75b1b96c552621229db4563622e231/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f762f6d616e696d636f6d6d756e6974792f6d616e696d3f636f6c6f723d253233303939636563266c6162656c3d646f636b6572253230696d616765266c6f676f3d646f636b6572" alt="Docker image"&gt; &lt;/a&gt;
    &lt;a href="https://mybinder.org/v2/gh/ManimCommunity/jupyter_examples/HEAD?filepath=basic_example_scenes.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667"&gt;&lt;/a&gt;
    &lt;a href="http://choosealicense.com/licenses/mit/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4aa186e2d2a345cf9afad1e0cd31303d171c7106ace7fceb184eeffd178fbeff/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d7265642e7376673f7374796c653d666c6174" alt="MIT License"&gt;&lt;/a&gt;
    &lt;a href="https://www.reddit.com/r/manim/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8e4df3f838f74d5da5ecddb5ad9d60c9068e9d067634d4cb2a8901d447322e8f/68747470733a2f2f696d672e736869656c64732e696f2f7265646469742f7375627265646469742d73756273637269626572732f6d616e696d2e7376673f636f6c6f723d6f72616e6765266c6162656c3d726564646974266c6f676f3d726564646974" alt="Reddit"&gt;&lt;/a&gt;
    &lt;a href="https://twitter.com/manim_community/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/25c518c2120e9f7ff86c90b6e66f34a78b5e5f1a20cf1e8fc3db40236c17d074/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f68747470732f747769747465722e636f6d2f636c6f7564706f7373652e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f772532302534306d616e696d5f636f6d6d756e697479" alt="Twitter"&gt;
    &lt;/a&gt;&lt;a href="https://www.manim.community/discord/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/33de7bc0381e53a7f9141a86f0c8e73fa7f219550a3519c8b19463d8f4dd54c5/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3538313733383733313933343035363434392e7376673f6c6162656c3d646973636f726426636f6c6f723d79656c6c6f77266c6f676f3d646973636f7264" alt="Discord"&gt;&lt;/a&gt;
    &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://camo.githubusercontent.com/d91ed7ac7abbd5a6102cbe988dd8e9ac21bde0a73d97be7603b891ad08ce3479/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" alt="Code style: black"&gt;
    &lt;/a&gt;&lt;a href="https://docs.manim.community/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4da0511407388503513185e0b77fe9d7f95d664c7f174ea8568b036feade813e/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6d616e696d63652f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status"&gt;&lt;/a&gt;
    &lt;a href="https://pepy.tech/project/manim" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c3f8474f1deec687c91e898514aeaecca8b15d28a22d348ab66135cd75ce34c4/68747470733a2f2f706570792e746563682f62616467652f6d616e696d2f6d6f6e74683f" alt="Downloads"&gt; &lt;/a&gt;
    &lt;a rel="noopener noreferrer" href="https://github.com/ManimCommunity/manim/workflows/CI/badge.svg"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--QiLJil7z--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/ManimCommunity/manim/workflows/CI/badge.svg" alt="CI"&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;br&gt;
    &lt;i&gt;An animation engine for explanatory math videos&lt;/i&gt;
&lt;/p&gt;




&lt;p&gt;Manim is an animation engine for explanatory math videos. It's used to create precise animations programmatically, as demonstrated in the videos of &lt;a href="https://www.3blue1brown.com/" rel="nofollow"&gt;3Blue1Brown&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;NOTE: This repository is maintained by the Manim Community and is not associated with Grant Sanderson or 3Blue1Brown in any way (although we are definitely indebted to him for providing his work to the world). If you would like to study how Grant makes his videos, head over to his repository (&lt;a href="https://github.com/3b1b/manim"&gt;3b1b/manim&lt;/a&gt;). This fork is updated more frequently than his, and it's recommended to use this fork if you'd like to use Manim for your own projects.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2&gt;
Table of Contents:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/ManimCommunity/manim#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ManimCommunity/manim#usage"&gt;Usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ManimCommunity/manim#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ManimCommunity/manim#docker"&gt;Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ManimCommunity/manim#help-with-manim"&gt;Help with Manim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ManimCommunity/manim#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ManimCommunity/manim#license"&gt;License&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;
Installation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;WARNING:&lt;/strong&gt; These instructions are for the community version &lt;em&gt;only&lt;/em&gt;. Trying to use these instructions to install &lt;a href="https://github.com/3b1b/manim"&gt;3b1b/manim&lt;/a&gt; or instructions there to install this version will cause‚Ä¶&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/ManimCommunity/manim"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#dribbble-shots"&gt;
  &lt;/a&gt;
  dribbble shots
&lt;/h2&gt;

&lt;h3&gt;
  &lt;a href="#fitness-app"&gt;
  &lt;/a&gt;
  Fitness App
&lt;/h3&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--fsJpa8LO--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.dribbble.com/users/257709/screenshots/16290615/media/d56b8456ba0f807b023bbf5916014a91.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--fsJpa8LO--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.dribbble.com/users/257709/screenshots/16290615/media/d56b8456ba0f807b023bbf5916014a91.png" alt="https://cdn.dribbble.com/users/257709/screenshots/16290615/media/d56b8456ba0f807b023bbf5916014a91.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;by &lt;a href="https://dribbble.com/shots/16290615-Fitness-App"&gt;uixNinja&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#3d-glass-card-illustration"&gt;
  &lt;/a&gt;
  3D Glass Card Illustration
&lt;/h3&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--E0HZqBoM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.dribbble.com/users/2424774/screenshots/16289478/media/9f74e74364ca09bce0004bcc17ba1bd5.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--E0HZqBoM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.dribbble.com/users/2424774/screenshots/16289478/media/9f74e74364ca09bce0004bcc17ba1bd5.png" alt="https://cdn.dribbble.com/users/2424774/screenshots/16289478/media/9f74e74364ca09bce0004bcc17ba1bd5.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;by &lt;a href="https://dribbble.com/shots/16289478-3D-Glass-Card-Illustration"&gt;M Wildan Cahya Syarief&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#saas-app"&gt;
  &lt;/a&gt;
  Saas app
&lt;/h3&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--ru53UZiV--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.dribbble.com/users/941761/screenshots/16288330/media/8d74212c4435187c5b96da6dbdcebd7c.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--ru53UZiV--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.dribbble.com/users/941761/screenshots/16288330/media/8d74212c4435187c5b96da6dbdcebd7c.png" alt="https://cdn.dribbble.com/users/941761/screenshots/16288330/media/8d74212c4435187c5b96da6dbdcebd7c.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;by &lt;a href="https://dribbble.com/shots/16288330-Saas-app"&gt;DŒûNYS SŒûRGUSHKIN&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#online-course-app-ui-concept"&gt;
  &lt;/a&gt;
  Online Course App UI Concept
&lt;/h3&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--ryNy86b1--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.dribbble.com/users/5665869/screenshots/16287572/media/8bdc3efb4ec106a11fe9519239a8a20d.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--ryNy86b1--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.dribbble.com/users/5665869/screenshots/16287572/media/8bdc3efb4ec106a11fe9519239a8a20d.png" alt="https://cdn.dribbble.com/users/5665869/screenshots/16287572/media/8bdc3efb4ec106a11fe9519239a8a20d.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;by &lt;a href="https://dribbble.com/shots/16287572-Online-Course-App-UI-Concept"&gt;Mushfiqur Rahman&lt;/a&gt;&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#tweets"&gt;
  &lt;/a&gt;
  Tweets
&lt;/h2&gt;


&lt;blockquote class="ltag__twitter-tweet"&gt;
      &lt;div class="ltag__twitter-tweet__media"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--AqyOXVrd--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://pbs.twimg.com/media/E88QE6KVEBQuJOI.jpg" alt="unknown tweet media content"&gt;
      &lt;/div&gt;

  &lt;div class="ltag__twitter-tweet__main"&gt;
    &lt;div class="ltag__twitter-tweet__header"&gt;
      &lt;img class="ltag__twitter-tweet__profile-image" src="https://res.cloudinary.com/practicaldev/image/fetch/s--v36Wumvw--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://pbs.twimg.com/profile_images/1408463200266297346/R4gPT3TA_normal.jpg" alt="Chance profile image"&gt;
      &lt;div class="ltag__twitter-tweet__full-name"&gt;
        Chance
      &lt;/div&gt;
      &lt;div class="ltag__twitter-tweet__username"&gt;
        @chancethedev
      &lt;/div&gt;
      &lt;div class="ltag__twitter-tweet__twitter-logo"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--ir1kO05j--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/twitter-f95605061196010f91e64806688390eb1a4dbc9e913682e043eb8b1e06ca484f.svg" alt="twitter logo"&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class="ltag__twitter-tweet__body"&gt;
      IE, dang 
    &lt;/div&gt;
    &lt;div class="ltag__twitter-tweet__date"&gt;
      22:02 PM - 16 Aug 2021
    &lt;/div&gt;


    &lt;div class="ltag__twitter-tweet__actions"&gt;
      &lt;a href="https://twitter.com/intent/tweet?in_reply_to=1427390286284525596" class="ltag__twitter-tweet__actions__button"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--fFnoeFxk--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/twitter-reply-action-238fe0a37991706a6880ed13941c3efd6b371e4aefe288fe8e0db85250708bc4.svg" alt="Twitter reply action"&gt;
      &lt;/a&gt;
      &lt;a href="https://twitter.com/intent/retweet?tweet_id=1427390286284525596" class="ltag__twitter-tweet__actions__button"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--k6dcrOn8--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/twitter-retweet-action-632c83532a4e7de573c5c08dbb090ee18b348b13e2793175fea914827bc42046.svg" alt="Twitter retweet action"&gt;
      &lt;/a&gt;
      &lt;a href="https://twitter.com/intent/like?tweet_id=1427390286284525596" class="ltag__twitter-tweet__actions__button"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--SRQc9lOp--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/twitter-like-action-1ea89f4b87c7d37465b0eb78d51fcb7fe6c03a089805d7ea014ba71365be5171.svg" alt="Twitter like action"&gt;
      &lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;



&lt;blockquote class="ltag__twitter-tweet"&gt;
      &lt;div class="ltag__twitter-tweet__media"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--EY7Gxn1B--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://pbs.twimg.com/media/E84Dy_wUYAMh0Oj.jpg" alt="unknown tweet media content"&gt;
      &lt;/div&gt;

  &lt;div class="ltag__twitter-tweet__main"&gt;
    &lt;div class="ltag__twitter-tweet__header"&gt;
      &lt;img class="ltag__twitter-tweet__profile-image" src="https://res.cloudinary.com/practicaldev/image/fetch/s--N-mxNDvs--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://pbs.twimg.com/profile_images/1417282972273713160/MTMjeZEb_normal.png" alt="Michael Chan profile image"&gt;
      &lt;div class="ltag__twitter-tweet__full-name"&gt;
        Michael Chan
      &lt;/div&gt;
      &lt;div class="ltag__twitter-tweet__username"&gt;
        &lt;a class="mentioned-user" href="https://dev.to/chantastic"&gt;@chantastic&lt;/a&gt;

      &lt;/div&gt;
      &lt;div class="ltag__twitter-tweet__twitter-logo"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--ir1kO05j--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/twitter-f95605061196010f91e64806688390eb1a4dbc9e913682e043eb8b1e06ca484f.svg" alt="twitter logo"&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class="ltag__twitter-tweet__body"&gt;
      I'm impressed that you can get to paginated async data in 25 lines of &lt;a href="https://twitter.com/sveltejs"&gt;@sveltejs&lt;/a&gt; ‚Äî including (minimal) loading and error states&lt;br&gt;&lt;br&gt;Really impressive amount of boilerplate reduction&lt;br&gt;&lt;br&gt;&lt;a href="https://t.co/9a8AD9S4HW"&gt;codesandbox.io/s/mutable-curr‚Ä¶&lt;/a&gt; 
    &lt;/div&gt;
    &lt;div class="ltag__twitter-tweet__date"&gt;
      01:42 AM - 16 Aug 2021
    &lt;/div&gt;


    &lt;div class="ltag__twitter-tweet__actions"&gt;
      &lt;a href="https://twitter.com/intent/tweet?in_reply_to=1427083338339160070" class="ltag__twitter-tweet__actions__button"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--fFnoeFxk--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/twitter-reply-action-238fe0a37991706a6880ed13941c3efd6b371e4aefe288fe8e0db85250708bc4.svg" alt="Twitter reply action"&gt;
      &lt;/a&gt;
      &lt;a href="https://twitter.com/intent/retweet?tweet_id=1427083338339160070" class="ltag__twitter-tweet__actions__button"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--k6dcrOn8--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/twitter-retweet-action-632c83532a4e7de573c5c08dbb090ee18b348b13e2793175fea914827bc42046.svg" alt="Twitter retweet action"&gt;
      &lt;/a&gt;
      &lt;a href="https://twitter.com/intent/like?tweet_id=1427083338339160070" class="ltag__twitter-tweet__actions__button"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--SRQc9lOp--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/twitter-like-action-1ea89f4b87c7d37465b0eb78d51fcb7fe6c03a089805d7ea014ba71365be5171.svg" alt="Twitter like action"&gt;
      &lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;



&lt;blockquote class="ltag__twitter-tweet"&gt;
      &lt;div class="ltag__twitter-tweet__media"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--PRHPpL2N--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://pbs.twimg.com/media/E9PomUQXIAUzeSB.jpg" alt="unknown tweet media content"&gt;
      &lt;/div&gt;

  &lt;div class="ltag__twitter-tweet__main"&gt;
    &lt;div class="ltag__twitter-tweet__header"&gt;
      &lt;img class="ltag__twitter-tweet__profile-image" src="https://res.cloudinary.com/practicaldev/image/fetch/s--MgaQKNor--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://pbs.twimg.com/profile_images/1413331583218634752/HnrLTzso_normal.jpg" alt="Tania profile image"&gt;
      &lt;div class="ltag__twitter-tweet__full-name"&gt;
        Tania
      &lt;/div&gt;
      &lt;div class="ltag__twitter-tweet__username"&gt;
        &lt;a class="mentioned-user" href="https://dev.to/taniarascia"&gt;@taniarascia&lt;/a&gt;

      &lt;/div&gt;
      &lt;div class="ltag__twitter-tweet__twitter-logo"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--ir1kO05j--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/twitter-f95605061196010f91e64806688390eb1a4dbc9e913682e043eb8b1e06ca484f.svg" alt="twitter logo"&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class="ltag__twitter-tweet__body"&gt;
      Would you rather fight 1 horse-sized duck or 100 duck-sized horses? 
    &lt;/div&gt;
    &lt;div class="ltag__twitter-tweet__date"&gt;
      15:30 PM - 20 Aug 2021
    &lt;/div&gt;


    &lt;div class="ltag__twitter-tweet__actions"&gt;
      &lt;a href="https://twitter.com/intent/tweet?in_reply_to=1428741311008583681" class="ltag__twitter-tweet__actions__button"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--fFnoeFxk--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/twitter-reply-action-238fe0a37991706a6880ed13941c3efd6b371e4aefe288fe8e0db85250708bc4.svg" alt="Twitter reply action"&gt;
      &lt;/a&gt;
      &lt;a href="https://twitter.com/intent/retweet?tweet_id=1428741311008583681" class="ltag__twitter-tweet__actions__button"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--k6dcrOn8--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/twitter-retweet-action-632c83532a4e7de573c5c08dbb090ee18b348b13e2793175fea914827bc42046.svg" alt="Twitter retweet action"&gt;
      &lt;/a&gt;
      &lt;a href="https://twitter.com/intent/like?tweet_id=1428741311008583681" class="ltag__twitter-tweet__actions__button"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--SRQc9lOp--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/twitter-like-action-1ea89f4b87c7d37465b0eb78d51fcb7fe6c03a089805d7ea014ba71365be5171.svg" alt="Twitter like action"&gt;
      &lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;



&lt;blockquote class="ltag__twitter-tweet"&gt;
    &lt;div class="ltag__twitter-tweet__media ltag__twitter-tweet__media__two-pics"&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--k4mTyrD6--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://pbs.twimg.com/media/E9P36LSVEAIiL8o.jpg" alt="unknown tweet media content"&gt;
    &lt;/div&gt;

  &lt;div class="ltag__twitter-tweet__main"&gt;
    &lt;div class="ltag__twitter-tweet__header"&gt;
      &lt;img class="ltag__twitter-tweet__profile-image" src="https://res.cloudinary.com/practicaldev/image/fetch/s--TTZOvLCd--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://pbs.twimg.com/profile_images/1332018227942019073/Jh666gw3_normal.jpg" alt="Adam Argyle profile image"&gt;
      &lt;div class="ltag__twitter-tweet__full-name"&gt;
        Adam Argyle
      &lt;/div&gt;
      &lt;div class="ltag__twitter-tweet__username"&gt;
        &lt;a class="mentioned-user" href="https://dev.to/argyleink"&gt;@argyleink&lt;/a&gt;

      &lt;/div&gt;
      &lt;div class="ltag__twitter-tweet__twitter-logo"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--ir1kO05j--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/twitter-f95605061196010f91e64806688390eb1a4dbc9e913682e043eb8b1e06ca484f.svg" alt="twitter logo"&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class="ltag__twitter-tweet__body"&gt;
      CSS Gradient Shadow üòé&lt;br&gt;&lt;br&gt;A pseudo-element, sized and positioned within the container, offset + scaled with transforms, then blurred with filters&lt;br&gt;&lt;br&gt;demo &lt;a href="https://t.co/A1QMxyMap4"&gt;codepen.io/argyleink/pen/‚Ä¶&lt;/a&gt; 
    &lt;/div&gt;
    &lt;div class="ltag__twitter-tweet__date"&gt;
      13:38 PM - 21 Aug 2021
    &lt;/div&gt;


    &lt;div class="ltag__twitter-tweet__actions"&gt;
      &lt;a href="https://twitter.com/intent/tweet?in_reply_to=1429075313158496261" class="ltag__twitter-tweet__actions__button"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--fFnoeFxk--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/twitter-reply-action-238fe0a37991706a6880ed13941c3efd6b371e4aefe288fe8e0db85250708bc4.svg" alt="Twitter reply action"&gt;
      &lt;/a&gt;
      &lt;a href="https://twitter.com/intent/retweet?tweet_id=1429075313158496261" class="ltag__twitter-tweet__actions__button"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--k6dcrOn8--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/twitter-retweet-action-632c83532a4e7de573c5c08dbb090ee18b348b13e2793175fea914827bc42046.svg" alt="Twitter retweet action"&gt;
      &lt;/a&gt;
      &lt;a href="https://twitter.com/intent/like?tweet_id=1429075313158496261" class="ltag__twitter-tweet__actions__button"&gt;
        &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--SRQc9lOp--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/twitter-like-action-1ea89f4b87c7d37465b0eb78d51fcb7fe6c03a089805d7ea014ba71365be5171.svg" alt="Twitter like action"&gt;
      &lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;





&lt;h2&gt;
  &lt;a href="#picked-pens"&gt;
  &lt;/a&gt;
  Picked Pens
&lt;/h2&gt;

&lt;h3&gt;
  &lt;a href="#responsive-css-food-truck"&gt;
  &lt;/a&gt;
  Responsive CSS Food Truck
&lt;/h3&gt;

&lt;p&gt;&lt;iframe height="600" src="https://codepen.io/cobra_winfrey/embed/oNWRbBG?height=600&amp;amp;default-tab=result&amp;amp;embed-version=2"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;by &lt;a href="https://twitter.com/cobra_winfrey"&gt;Adam Kuhn&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#gradient-shadow"&gt;
  &lt;/a&gt;
  Gradient shadow
&lt;/h3&gt;

&lt;p&gt;&lt;iframe height="600" src="https://codepen.io/argyleink/embed/WNxeBKa?height=600&amp;amp;default-tab=result&amp;amp;embed-version=2"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;by &lt;a href="https://twitter.com/argyleink"&gt;Adam Argyle&lt;/a&gt;&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#podcasts-worth-listening"&gt;
  &lt;/a&gt;
  Podcasts worth listening
&lt;/h2&gt;

&lt;h3&gt;
  &lt;a href="#syntax-the-weird-and-wonderful-link-tag"&gt;
  &lt;/a&gt;
  Syntax ‚Äì The Weird and Wonderful Link Tag
&lt;/h3&gt;

&lt;p&gt;In this Hasty Treat, Scott and Wes talk about the tag ‚Äî why it‚Äôs weird and wonderful, and what you can do with it!&lt;/p&gt;

&lt;p&gt;&lt;iframe width="100%" height="232px" src="https://open.spotify.com/embed/episode/3T1TlF3vsPlczVTGk65lUi"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#ladybug-getting-started-with-typescript"&gt;
  &lt;/a&gt;
  Ladybug ‚Äì Getting Started With TypeScript
&lt;/h3&gt;

&lt;p&gt;TypeScript is an open-source programming language built on JavaScript that provides static type definitions. It has taken the front-end development community by storm over the past few years and today we‚Äôre going to give you a beginner-level rundown on the language.&lt;/p&gt;

&lt;p&gt;&lt;iframe width="100%" height="232px" src="https://open.spotify.com/embed/episode/1PCnhBpT02fTkXdSKBbAjc"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#codepen-radio-gathering-data"&gt;
  &lt;/a&gt;
  CodePen Radio ‚Äì Gathering Data
&lt;/h3&gt;

&lt;p&gt;Marie and Chris talk about all the sources of data we have, think about, and use to help us. We do have one main database on CodePen, and truth be told, it‚Äôs got a bunch of data in it.&lt;/p&gt;

&lt;p&gt;&lt;iframe width="100%" height="232px" src="https://open.spotify.com/embed/episode/4hRXo267tsV9PAvixL3bji"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#syntax-moist-code-%C3%97-memoization-%C3%97-ready-for-fulltime-%C3%97-deadlines-%C3%97-design-ethics-%C3%97-react-components-%C3%97-video-hosting-%C3%97-local-fonts-%C3%97-more"&gt;
  &lt;/a&gt;
  Syntax ‚Äì - Moist code √ó Memoization √ó Ready for full-time? √ó Deadlines √ó Design ethics √ó React components √ó Video hosting √ó Local fonts √ó More!
&lt;/h3&gt;

&lt;p&gt;It‚Äôs another Potluck! In this episode, Scott and Wes answer your questions about memoization, how to know when you‚Äôre ready for a full-time dev job, what to do when you underestimate projects, design ethics, local fonts, and more!&lt;/p&gt;

&lt;p&gt;&lt;iframe width="100%" height="232px" src="https://open.spotify.com/embed/episode/09Mk0cdSrc46gkILKPhkEk"&gt;
&lt;/iframe&gt;
&lt;/p&gt;




&lt;p&gt;Thank you for reading, talk to you next week, and stay safe! üëã&lt;/p&gt;

</description>
      <category>css</category>
      <category>javascript</category>
      <category>react</category>
      <category>webdev</category>
    </item>
    <item>
      <title>Animated Skills Bar using HTML and CSS</title>
      <author>Shantanu Jana</author>
      <pubDate>Sun, 22 Aug 2021 16:45:18 +0000</pubDate>
      <link>https://dev.to/shantanu_jana/animated-skills-bar-html-and-css-1fbl</link>
      <guid>https://dev.to/shantanu_jana/animated-skills-bar-html-and-css-1fbl</guid>
      <description>&lt;p&gt;In this article I am going to show you how to create &lt;a href="https://www.foolishdeveloper.com/2021/08/animated-skills-bar-html-css.html"&gt;Animated Skills Bar using only HTML and CSS&lt;/a&gt; code. I have designed many more types of progress bars before. But in that case I used JavaScript or JQuery. I have created this Skills Bar only with the help of HTML and CSS code.&lt;/p&gt;

&lt;p&gt;First I made a small box on a web page. Then I added a title to that box and used four progress bars. Each progressbar is given a specific value. When you load this page, this animation will reach your pre-determined meaning from zero. It will take you two seconds to reach your predefined value so we can see a kind of animation here.&lt;/p&gt;

&lt;p&gt;Here I have created this animation using @keyframes of CSS code.&lt;/p&gt;

&lt;p&gt;Below I show you the complete step by step how I created this Animated Skills Bar using HTML and CSS code. You can also &lt;a href="https://www.foolishdeveloper.com/2021/08/animated-skills-bar-html-css.html"&gt;download the source code&lt;/a&gt; to create it.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#step-1-design-the-web-page"&gt;
  &lt;/a&gt;
  Step 1: Design the web page
&lt;/h3&gt;

&lt;p&gt;First I designed the web page using some CSS code below. Here I have used the background color blue of the webpage.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight css"&gt;&lt;code&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nl"&gt;padding&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;margin&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;box-sizing&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;border-box&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nt"&gt;body&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nl"&gt;height&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="m"&gt;100vh&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;background-color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;#0a7aca&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--FuJH2Vxm--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/epyjxu3wh7u1h3fcim5e.jpg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--FuJH2Vxm--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/epyjxu3wh7u1h3fcim5e.jpg" alt="Design the web page"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#step-2-create-a-box-on-the-webpage"&gt;
  &lt;/a&gt;
  Step 2: Create a box on the webpage
&lt;/h3&gt;

&lt;p&gt;Now I have created a box using the HTML and CSS code below. As I said before there is a box on the web page in which all the progress bars are made. &lt;/p&gt;

&lt;p&gt;I used &lt;code&gt;box-shadow: 0 20px 30px rgba (0,0,0,0.2)&lt;/code&gt; here to create a color shadow around that box. I used &lt;code&gt;border-radius: 10px&lt;/code&gt; to make it a bit round.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight html"&gt;&lt;code&gt;&lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;class=&lt;/span&gt;&lt;span class="s"&gt;"wrapper"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;class=&lt;/span&gt;&lt;span class="s"&gt;"container"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight css"&gt;&lt;code&gt;&lt;span class="nc"&gt;.wrapper&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nl"&gt;width&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;40%&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;min-width&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;590px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;position&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;absolute&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;translate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;-50%&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;-50%&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="nl"&gt;left&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;50%&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;top&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;50%&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="nc"&gt;.container&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nl"&gt;width&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;100%&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;padding&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;30px&lt;/span&gt; &lt;span class="m"&gt;30px&lt;/span&gt; &lt;span class="m"&gt;50px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;background-color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="no"&gt;white&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;border-radius&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;10px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;box-shadow&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;20px&lt;/span&gt; &lt;span class="m"&gt;30px&lt;/span&gt; &lt;span class="n"&gt;rgba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt; 

&lt;span class="nc"&gt;.container&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nl"&gt;font-family&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;"Poppins"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;sans-serif&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="no"&gt;black&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;font-weight&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;500&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--MefV0eq0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qkm2fusz9ofg33kdlcbl.jpg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--MefV0eq0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/qkm2fusz9ofg33kdlcbl.jpg" alt="Create a box on the webpage"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#step-3-add-the-title-to-the-box"&gt;
  &lt;/a&gt;
  Step 3: Add the title to the box
&lt;/h3&gt;

&lt;p&gt;Now I have created a heading using the code below. A heading has been used here as you can see in the picture above. The &lt;code&gt;font-size: 33px&lt;/code&gt; of this heading and &lt;code&gt;text-align: center&lt;/code&gt; has been used to place it in the middle.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight html"&gt;&lt;code&gt;  &lt;span class="nt"&gt;&amp;lt;h2&amp;gt;&lt;/span&gt;Animated Skills &lt;span class="nt"&gt;&amp;lt;/h2&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight css"&gt;&lt;code&gt;&lt;span class="nt"&gt;h2&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nl"&gt;margin-bottom&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;50px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;letter-spacing&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;2px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;text-align&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;center&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;font-size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;33px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;font-weight&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;bold&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--lyiXIGXn--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/iuypas2ogqtasiptievz.jpg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--lyiXIGXn--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/iuypas2ogqtasiptievz.jpg" alt="Add the title to the box"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#step-4-add-all-the-information-in-the-skills-bar"&gt;
  &lt;/a&gt;
  Step 4: Add all the information in the Skills bar
&lt;/h3&gt;

&lt;p&gt;Now I have added all the basic information of this Animated Skills Bar using HTML code. First, I have added information from one of the four bars in this progress bar. And its possible result I have shown in the picture below.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight html"&gt;&lt;code&gt; &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;class=&lt;/span&gt;&lt;span class="s"&gt;"skills"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
   &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;class=&lt;/span&gt;&lt;span class="s"&gt;"details"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
     &lt;span class="nt"&gt;&amp;lt;span&amp;gt;&lt;/span&gt;HTML&lt;span class="nt"&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
     &lt;span class="nt"&gt;&amp;lt;span&amp;gt;&lt;/span&gt;90%&lt;span class="nt"&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
   &lt;span class="nt"&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
   &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;class=&lt;/span&gt;&lt;span class="s"&gt;"bar"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;id=&lt;/span&gt;&lt;span class="s"&gt;"html-bar"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
   &lt;span class="nt"&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
 &lt;span class="nt"&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--yjGW-lhf--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/klan0248c8e2plezoyb4.jpg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--yjGW-lhf--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/klan0248c8e2plezoyb4.jpg" alt="Add all the information in the Skills bar"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Above we have added a Skills bar information. Now I have added the information of the other three progress bars.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight html"&gt;&lt;code&gt; &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;class=&lt;/span&gt;&lt;span class="s"&gt;"skills"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
   &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;class=&lt;/span&gt;&lt;span class="s"&gt;"details"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
     &lt;span class="nt"&gt;&amp;lt;span&amp;gt;&lt;/span&gt;CSS&lt;span class="nt"&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
     &lt;span class="nt"&gt;&amp;lt;span&amp;gt;&lt;/span&gt;75%&lt;span class="nt"&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
   &lt;span class="nt"&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
   &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;class=&lt;/span&gt;&lt;span class="s"&gt;"bar"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;id=&lt;/span&gt;&lt;span class="s"&gt;"css-bar"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
   &lt;span class="nt"&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
 &lt;span class="nt"&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight html"&gt;&lt;code&gt; &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;class=&lt;/span&gt;&lt;span class="s"&gt;"skills"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
   &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;class=&lt;/span&gt;&lt;span class="s"&gt;"details"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
     &lt;span class="nt"&gt;&amp;lt;span&amp;gt;&lt;/span&gt;Javascript&lt;span class="nt"&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
     &lt;span class="nt"&gt;&amp;lt;span&amp;gt;&lt;/span&gt;72%&lt;span class="nt"&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
   &lt;span class="nt"&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
   &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;class=&lt;/span&gt;&lt;span class="s"&gt;"bar"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;id=&lt;/span&gt;&lt;span class="s"&gt;"javascript-bar"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
   &lt;span class="nt"&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
 &lt;span class="nt"&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight html"&gt;&lt;code&gt; &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;class=&lt;/span&gt;&lt;span class="s"&gt;"skills"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
   &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;class=&lt;/span&gt;&lt;span class="s"&gt;"details"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
     &lt;span class="nt"&gt;&amp;lt;span&amp;gt;&lt;/span&gt;jQuery&lt;span class="nt"&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
     &lt;span class="nt"&gt;&amp;lt;span&amp;gt;&lt;/span&gt;68%&lt;span class="nt"&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
   &lt;span class="nt"&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
   &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;class=&lt;/span&gt;&lt;span class="s"&gt;"bar"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;lt;div&lt;/span&gt; &lt;span class="na"&gt;id=&lt;/span&gt;&lt;span class="s"&gt;"jQuery-bar"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
   &lt;span class="nt"&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
 &lt;span class="nt"&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--A1E8Yxgm--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/tzct95bb8zsm7h90bw0x.jpg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--A1E8Yxgm--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/tzct95bb8zsm7h90bw0x.jpg" alt="Add all the information in the Skills bar"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#step-6-design-the-skills-bar-with-css-code"&gt;
  &lt;/a&gt;
  Step 6: Design the skills bar with css code
&lt;/h3&gt;

&lt;p&gt;Now I have designed the above added information with the help of CSS code. Here a border is used around the animation line for which I have used &lt;code&gt;border: 2px solid # 0d96e0&lt;/code&gt;. Here I have used the height of the animation line: 9px. &lt;/p&gt;

&lt;p&gt;I used &lt;code&gt;border-radius: 10px&lt;/code&gt; to make the two ends of the line even and round. I have used the &lt;code&gt;width: 0&lt;/code&gt; of this progress bar animation line, which means that under normal circumstances no skills animation will be seen here. Later I gave different values ‚Äã‚Äãfor each using @keyframes.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight css"&gt;&lt;code&gt;
&lt;span class="nc"&gt;.details&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nl"&gt;width&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;100%&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;display&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;flex&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;  
    &lt;span class="nl"&gt;justify-content&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;space-between&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;margin-bottom&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;10px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="nc"&gt;.bar&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nl"&gt;position&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;relative&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="nl"&gt;border&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;2px&lt;/span&gt; &lt;span class="nb"&gt;solid&lt;/span&gt; &lt;span class="m"&gt;#0d96e0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="nl"&gt;border-radius&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;20px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="nc"&gt;.bar&lt;/span&gt; &lt;span class="nt"&gt;div&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nl"&gt;position&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;relative&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;width&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;height&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;9px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;border-radius&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;10px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;background-color&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;#0d96e0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;


&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Using the CSS code below I have created a distance between each of the skills bars. For this &lt;code&gt;margin-bottom: 30px&lt;/code&gt; is used which will create a distance of 30px between each bar.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight css"&gt;&lt;code&gt;&lt;span class="nc"&gt;.skills&lt;/span&gt;&lt;span class="nd"&gt;:not&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nd"&gt;:last-child&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nl"&gt;margin-bottom&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;30px&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--rtpaLmcI--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kjo25c8dm6v2ei5bslor.jpg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--rtpaLmcI--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kjo25c8dm6v2ei5bslor.jpg" alt="Design the skills bar with css code"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#step-7-set-a-specific-value-for-the-animation-of-each-skills-bar"&gt;
  &lt;/a&gt;
  Step 7: Set a specific value for the animation of each Skills bar
&lt;/h3&gt;

&lt;p&gt;As I said above, I used the &lt;code&gt;width: 0&lt;/code&gt; of the progress bar animation line here, which means that the animation line cannot be seen under normal conditions. Now I have given different values ‚Äã‚Äãfor each.&lt;/p&gt;

&lt;p&gt;I have used &lt;code&gt;width: 90%&lt;/code&gt; for the first line here, which means that this colorful line will stop at 90% when it is loaded. I set a time of two seconds to do this animation with it.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight css"&gt;&lt;code&gt;&lt;span class="nf"&gt;#html-bar&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nl"&gt;animation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;html-fill&lt;/span&gt; &lt;span class="m"&gt;2s&lt;/span&gt; &lt;span class="n"&gt;forwards&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;@keyframes&lt;/span&gt; &lt;span class="n"&gt;html-fill&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="err"&gt;100&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nl"&gt;width&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;90%&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--irsRNbEU--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/n67plsh1kbbcmk3gvlwr.jpg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--irsRNbEU--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/n67plsh1kbbcmk3gvlwr.jpg" alt="et a specific value for the animation of each Skills bar"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In the same way I have added specific values ‚Äã‚Äãfor three more CSS progress bars. The more you change the value of width, the more the value of the colorful line will change.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight css"&gt;&lt;code&gt;&lt;span class="nf"&gt;#css-bar&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nl"&gt;animation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;css-fill&lt;/span&gt; &lt;span class="m"&gt;2s&lt;/span&gt; &lt;span class="n"&gt;forwards&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;@keyframes&lt;/span&gt; &lt;span class="n"&gt;css-fill&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="err"&gt;100&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nl"&gt;width&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;75%&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight css"&gt;&lt;code&gt;&lt;span class="nf"&gt;#js-bar&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nl"&gt;animation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;js-fill&lt;/span&gt; &lt;span class="m"&gt;2s&lt;/span&gt; &lt;span class="n"&gt;forwards&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;@keyframes&lt;/span&gt; &lt;span class="n"&gt;js-fill&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="err"&gt;100&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nl"&gt;width&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;72%&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight css"&gt;&lt;code&gt;&lt;span class="nf"&gt;#jQuery-bar&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nl"&gt;animation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;jQuery-fill&lt;/span&gt; &lt;span class="m"&gt;2s&lt;/span&gt; &lt;span class="n"&gt;forwards&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;@keyframes&lt;/span&gt; &lt;span class="n"&gt;jQuery-fill&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="err"&gt;100&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nl"&gt;width&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="m"&gt;58%&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--jiMtUMDt--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vk63w9reh3bet93dr68i.jpg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--jiMtUMDt--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vk63w9reh3bet93dr68i.jpg" alt="Animated Skills Bar HTML and CSS"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Hopefully from this tutorial above you have fully learned how I created this Animated Skills Bar using only HTML and CSS code. In the meantime I have created many more designs using HTML and CSS code. You can see those designs if you want. You will let us know how you like this skills bar by commenting.&lt;/p&gt;

&lt;p&gt;You can visit my blog for more tutorials like this.&lt;br&gt;
&lt;a href="https://www.foolishdeveloper.com/"&gt;https://www.foolishdeveloper.com/&lt;/a&gt;&lt;/p&gt;

</description>
      <category>html</category>
      <category>css</category>
      <category>beginners</category>
      <category>webdev</category>
    </item>
    <item>
      <title>How to implement Infinite Scroll in React</title>
      <author>K Ramakrishna Vaidya</author>
      <pubDate>Sun, 22 Aug 2021 16:19:52 +0000</pubDate>
      <link>https://dev.to/kgrvaidya/how-to-implement-infinite-scroll-in-react-5d17</link>
      <guid>https://dev.to/kgrvaidya/how-to-implement-infinite-scroll-in-react-5d17</guid>
      <description>&lt;p&gt;Infinite scrolling becoming more and more popular and we can see it in most of the applications, like LinkedIn, Facebook, Instagram, Youtube etc to name a few. So what exactly is "Infinite scrolling"? how to make an infinite scrolling view in react? Let's see. &lt;/p&gt;

&lt;p&gt;I'm Ramakrishna and I'm a full stack developer. I love to know "how" part of the solutions rather than just building them.&lt;/p&gt;

&lt;p&gt;I was taking a front end challenge and that challenge was about to build a simple react application, similar to Netflix (Without that fancy UI). But the catch was to include Lazy loading on page content. So as I scroll horizontally / vertically the content should be lazily loaded.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#why"&gt;
  &lt;/a&gt;
  Why?
&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Lazy loading helps in application performance. User will be able to interact with the application much faster, as it loads only essential at the first render and will render other things as the user proceeds.&lt;/li&gt;
&lt;li&gt;Browser load reduces. As the browser tries to load application in small small peices, it can render quickly and make the UX better. 
Coming back to the previous issue, so how to get the inifnite scrolling?&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;
  &lt;a href="#how-infinite-scroll-works"&gt;
  &lt;/a&gt;
  How Infinite Scroll works?
&lt;/h3&gt;

&lt;p&gt;To implement something, we need to understand how it works under the hood. &lt;/p&gt;

&lt;p&gt;So, as for the infinite scrolling, let's take FB as an example. An user might follow 1000s of friends and pages and might have millions of posts to watch. But loading that much posts will effect the performance. So for simplicity's sake, FB will load 10 posts on initial load. As user reaches the end of 10th post, it makes an async call to fetch next 10 posts. So as long as user scrolls, it fetches more and more posts. &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#react-implementation"&gt;
  &lt;/a&gt;
  React Implementation.
&lt;/h3&gt;

&lt;p&gt;This implementation is done using a custom hook and IntersectionObserver. Let's dive into code. &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Custom Hook to fetch posts as user scrolls.
&lt;/li&gt;
&lt;/ol&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nx"&gt;useState&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;useEffect&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;useCallback&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;from&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;react&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;axios&lt;/span&gt; &lt;span class="k"&gt;from&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;axios&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;useFetch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;page&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="dl"&gt;''&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;loading&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;setLoading&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;useState&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;error&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;setError&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;useState&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;setList&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;useState&lt;/span&gt;&lt;span class="p"&gt;([]);&lt;/span&gt;
  &lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;formattedList&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;setFormattedList&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;useState&lt;/span&gt;&lt;span class="p"&gt;([]);&lt;/span&gt;

  &lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;getPosts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;useCallback&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;async&lt;/span&gt; &lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="k"&gt;await&lt;/span&gt; &lt;span class="nx"&gt;setLoading&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
      &lt;span class="k"&gt;await&lt;/span&gt; &lt;span class="nx"&gt;setError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
      &lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
      &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;list&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nx"&gt;list&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;length&lt;/span&gt; &lt;span class="o"&gt;===&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nx"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;await&lt;/span&gt; &lt;span class="nx"&gt;axios&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="kd"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;url&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
      &lt;span class="k"&gt;await&lt;/span&gt; &lt;span class="nx"&gt;setList&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="k"&gt;await&lt;/span&gt; &lt;span class="nx"&gt;setFormattedList&lt;/span&gt;&lt;span class="p"&gt;([...&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;slice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
     &lt;span class="p"&gt;}&lt;/span&gt;
     &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;list&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;length&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;formattedList&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;length&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;await&lt;/span&gt; &lt;span class="nx"&gt;setFormattedList&lt;/span&gt;&lt;span class="p"&gt;([...&lt;/span&gt;&lt;span class="nx"&gt;formattedList&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt;&lt;span class="nx"&gt;list&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;slice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;formattedList&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;formattedList&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;length&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
     &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="nx"&gt;setLoading&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;catch&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nx"&gt;setError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;page&lt;/span&gt;&lt;span class="p"&gt;]);&lt;/span&gt;

  &lt;span class="nx"&gt;useEffect&lt;/span&gt;&lt;span class="p"&gt;(()&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;getPosts&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="nx"&gt;getPosts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;page&lt;/span&gt;&lt;span class="p"&gt;]);&lt;/span&gt;

  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nx"&gt;loading&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;error&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;formattedList&lt;/span&gt; &lt;span class="p"&gt;};&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="k"&gt;default&lt;/span&gt; &lt;span class="nx"&gt;useFetch&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Above function is a custom hook to fetch data as per the scroll. Main thing to note here is, it's taking the url dynamically and makes a call only first time. And the url used here DOESN'T HAVE PAGINATION. So the hook is built in a way to handle splitting of result in progressive way. Find out more on useCallback &lt;a href="https://dmitripavlutin.com/dont-overuse-react-usecallback/"&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now, let's use the custom hook in our component. I have a custom component, which list's albums on each row, and each album will have multiple songs.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;App.js component which uses custom hook for infinite scroll.
&lt;/li&gt;
&lt;/ol&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="s1"&gt;./App.css&lt;/span&gt;&lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;React&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nx"&gt;Suspense&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;useState&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;useRef&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;useEffect&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;useCallback&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;from&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;react&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;useFetch&lt;/span&gt; &lt;span class="k"&gt;from&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;./utils/customFetch&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nx"&gt;AlbumList&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;from&lt;/span&gt; &lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="s1"&gt;./components&lt;/span&gt;&lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;App&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;

&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;page&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;setPage&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;useState&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nx"&gt;loading&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;error&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;formattedList&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;useFetch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;page&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="s1"&gt;https://jsonplaceholder.typicode.com/albums&lt;/span&gt;&lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;row&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;useRef&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;handleObserver&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;useCallback&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nx"&gt;entries&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;entries&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;target&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;isIntersecting&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;setPage&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nx"&gt;prev&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;prev&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="p"&gt;[]);&lt;/span&gt;

&lt;span class="nx"&gt;useEffect&lt;/span&gt;&lt;span class="p"&gt;(()&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;option&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="na"&gt;root&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;rootMargin&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;20px&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;threshold&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
  &lt;span class="p"&gt;};&lt;/span&gt;
  &lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;observer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nx"&gt;IntersectionObserver&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;handleObserver&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;option&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;row&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;current&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;observer&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;observe&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;row&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;current&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;handleObserver&lt;/span&gt;&lt;span class="p"&gt;]);&lt;/span&gt;

  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;div&lt;/span&gt; &lt;span class="nx"&gt;className&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;App&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;Suspense&lt;/span&gt; &lt;span class="nx"&gt;fallback&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;div&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="nx"&gt;Loading&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/div&amp;gt;}&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;        &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;AlbumList&lt;/span&gt; &lt;span class="nx"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;Component 1&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt; &lt;span class="nx"&gt;albums&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;formattedList&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="sr"&gt;/&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;      &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/Suspense&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;      &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;loading&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;p&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="nx"&gt;Loading&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/p&amp;gt;&lt;/span&gt;&lt;span class="err"&gt;}
&lt;/span&gt;      &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;error&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;p&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="nb"&gt;Error&lt;/span&gt;&lt;span class="o"&gt;!&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/p&amp;gt;&lt;/span&gt;&lt;span class="err"&gt;}
&lt;/span&gt;      &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;div&lt;/span&gt; &lt;span class="nx"&gt;ref&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;row&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="sr"&gt;/&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;    &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="sr"&gt;/div&lt;/span&gt;&lt;span class="err"&gt;&amp;gt;
&lt;/span&gt;  &lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="k"&gt;default&lt;/span&gt; &lt;span class="nx"&gt;App&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Here, I'm loading an AlbumList custom component which will fetch 10 albums on initial load. an empty div is put after AlbumList (To mark the end of current view / page). The ref is used inside IntersectionObserver to listen to when the scroll position reached this div. If the Observer detects intersection, it calls the customHook to fetch next 10 albums. &lt;br&gt;
Like this, this custom hook can be used everywhere we need to make an infinite scrolling. Read more about IntersectionObserver &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/Intersection_Observer_API"&gt;here&lt;/a&gt; and &lt;a href="https://dev.to/producthackers/intersection-observer-using-react-49ko"&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;My github repo link is &lt;a href="https://github.com/kgrvaidya/k-bee-react.git"&gt;here&lt;/a&gt;, which has the complete running version of the infinite scrolling.&lt;br&gt;&lt;br&gt;
Feel free to put feedback :)&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#references"&gt;
  &lt;/a&gt;
  References
&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href="https://medium.com/suyeonme/react-how-to-implement-an-infinite-scroll-749003e9896a"&gt;Infinite Scroll using React&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.google.com/url?sa=i&amp;amp;url=https%3A%2F%2Fblog.logrocket.com%2Finfinite-scroll-techniques-in-react-adcfd7ff32bd%2F&amp;amp;psig=AOvVaw2NvspI9OhF0rbwVls-BIG_&amp;amp;ust=1629735387266000&amp;amp;source=images&amp;amp;cd=vfe&amp;amp;ved=0CAsQjRxqFwoTCJjVicuDxfICFQAAAAAdAAAAABAD"&gt;Infinite Scroll Image&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
      <category>react</category>
      <category>javascript</category>
      <category>hooks</category>
      <category>intersectionobserver</category>
    </item>
    <item>
      <title>How to Install Pardus GNOME 21</title>
      <author>Ali Orhun Akkirman</author>
      <pubDate>Sun, 22 Aug 2021 16:09:00 +0000</pubDate>
      <link>https://dev.to/aciklab/how-to-install-pardus-gnome-21-15gm</link>
      <guid>https://dev.to/aciklab/how-to-install-pardus-gnome-21-15gm</guid>
      <description>&lt;h1&gt;
  &lt;a href="#pardus-21x"&gt;
  &lt;/a&gt;
  Pardus 21.x
&lt;/h1&gt;

&lt;p&gt;Pardus 21 was released on August 21, 2021! Pardus 21 was released in 3 different environments as XFCE, GNOME and Server. Version 21.x will receive 5 minor updates over approximately 2 years and will be "Supported" until May 1, 2025&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#downloading-pardus-21-gnome-iso"&gt;
  &lt;/a&gt;
  Downloading Pardus 21 GNOME ISO
&lt;/h1&gt;

&lt;p&gt;You can &lt;a href="https://www.pardus.org.tr/surumler/"&gt;download&lt;/a&gt; the GNOME desktop environment ISO disk image of the Pardus 21.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--jutic66o--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/puvuegfzb1qfbaqowxe7.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--jutic66o--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/puvuegfzb1qfbaqowxe7.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#installing-pardus-21"&gt;
  &lt;/a&gt;
  Installing Pardus 21
&lt;/h1&gt;

&lt;p&gt;You can reach the following screen by starting your Pardus 21 ISO in various virtual or physical environments.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--EpumblCS--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/e76z33ji4ksllqgu4vn0.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--EpumblCS--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/e76z33ji4ksllqgu4vn0.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After choosing the language setting on the relevant screen, it would be appropriate to select "Pardus Live" to open an environment that you can use to test your own computer with the Pardus operating system. Of course, if this media is running on a USB stick or similar environment, it will obviously be slower in speed, so it will be more efficient after the actual installation.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--uTrjVN-_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/wmpnf624a07nbxmvjpu2.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--uTrjVN-_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/wmpnf624a07nbxmvjpu2.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;When "Pardus Live" is opened, an empty GNOME like the one below will welcome you. Then you can switch to the Installation Wizard with the "Install Pardus" icon on the desktop.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--Iu2DiY91--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/357w2ov9ycwfiwmoqq2w.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Iu2DiY91--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/357w2ov9ycwfiwmoqq2w.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In the following steps, you can go through the Language, Time Zone and Keyboard selection steps according to your wishes.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--pyB1mDZW--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rwafu7u8grmbyix09hup.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--pyB1mDZW--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rwafu7u8grmbyix09hup.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--dbvmCyEz--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/an4r8wzgu59b8mykkk53.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--dbvmCyEz--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/an4r8wzgu59b8mykkk53.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--6DpPDif_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/gzrx78l67wrfo1xrotxm.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--6DpPDif_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/gzrx78l67wrfo1xrotxm.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After this step, "name", "username" and "password" must be entered for the local administrator account with "sudo" on your system. On the same screen, you are also asked to type the "computer name".&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--3I87IJFm--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/p6io7ly307rjaeupnnx2.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--3I87IJFm--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/p6io7ly307rjaeupnnx2.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Then comes the most important step, how to do the disk partitioning. If you are using another operating system on your disk or if you want to partition manually, you have to do it manually, but in this article we will apply the situation on a blank disk. Therefore, you need to select the disk you want to install in the "Manual Partitioning" option.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--TvgkClCa--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/eawfyl8rfrfsuwjuycdk.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--TvgkClCa--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/eawfyl8rfrfsuwjuycdk.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It warns you again that all data on the disk will be deleted when you make the selection and proceed. Indeed, this is the most important step. If you have data to lose, you need to be careful. If you're using a virtual machine, it won't delete anything from your real physical machine. After this step, if you haven't made any adjustments to your disk, it will ask one more question and just say "Yes".&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--d5Ykfz5x--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8i1qkfuaim3lu00150d4.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--d5Ykfz5x--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8i1qkfuaim3lu00150d4.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--XdzoFxha--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/c3b8pfttzkpqi9i5hawb.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--XdzoFxha--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/c3b8pfttzkpqi9i5hawb.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Then the summary part of the installation will be shared with you. Up until this step, nothing has been done about the installation except erasing the disk.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--XpczEG7W--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/bzdhful8ff5i9ampitub.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--XpczEG7W--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/bzdhful8ff5i9ampitub.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As soon as you click Next, the installation will begin.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s---YfS40Cp--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/7iyiga3c2y49g09i3vh6.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s---YfS40Cp--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/7iyiga3c2y49g09i3vh6.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This step, which will vary depending on your SSD or HDD disk performance, will take between 2-10 minutes. Then the reboot screen will appear.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--5AvueNYN--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vf5kmysv95ydor8ewqp8.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--5AvueNYN--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vf5kmysv95ydor8ewqp8.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In the new version of Pardus, the GRUB login screen has also become a successful design.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--1Phc1JVh--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2porhd5vtyhg3wki0l3o.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--1Phc1JVh--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2porhd5vtyhg3wki0l3o.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Then, when you see the GDM screen open, your installation has been completed successfully. You can start your session with the "your password" you entered during installation.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--bHStZjij--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1tcset89cqe5og534rq7.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--bHStZjij--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1tcset89cqe5og534rq7.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After starting the session, the "Welcome application" appears and you can organize your desktop according to your own taste.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--kEvSkbSj--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/brv7vkr61qoav65qnili.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--kEvSkbSj--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/brv7vkr61qoav65qnili.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Have a good time...&lt;/p&gt;

</description>
      <category>pardus</category>
      <category>gnome</category>
      <category>yirmibir</category>
      <category>21</category>
    </item>
    <item>
      <title>Using Axios Interceptors In Javascript and Typescript</title>
      <author>Mike From CodeSpectre</author>
      <pubDate>Sun, 22 Aug 2021 15:54:52 +0000</pubDate>
      <link>https://dev.to/codespectremike/using-axios-interceptors-in-javascript-and-typescript-4g93</link>
      <guid>https://dev.to/codespectremike/using-axios-interceptors-in-javascript-and-typescript-4g93</guid>
      <description>&lt;p&gt;&lt;strong&gt;Axios&lt;/strong&gt; is a robust, easy to use, promise-based http client for javascript and node.js. Axios provides developers with tools to handle all http-related functions. Axios interceptors are one of the essential tools Axios provides us for dealing with HTTP requests and responses. &lt;/p&gt;

&lt;p&gt;Axios is a great way to handle any sort of HTTP requests in javascript or typescript, and I use it in all of my projects that require accessing an API. It provides all the necessary functions for passing data to and from APIs and then accessing it on the frontend. &lt;/p&gt;

&lt;p&gt;If you've never used Axios, I suggest checking out the documentation here: &lt;a href="https://axios-http.com/docs/intro"&gt;https://axios-http.com/docs/intro&lt;/a&gt; &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#interceptors"&gt;
  &lt;/a&gt;
  Interceptors
&lt;/h2&gt;

&lt;p&gt;Interceptors are exactly what they sound like. They "intercept" requests and responses. &lt;/p&gt;

&lt;p&gt;This can be useful if you need to perform some validation on the data before sending a request or when retrieving a response. &lt;/p&gt;

&lt;p&gt;Here's an example of intercepting a request:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight tsx"&gt;&lt;code&gt;&lt;span class="nx"&gt;axios&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;interceptors&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;request&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;use&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;

&lt;span class="c1"&gt;// if name not in request config then reject&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;Promise&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;reject&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="s2"&gt;Error: Please Include a name&lt;/span&gt;&lt;span class="dl"&gt;"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;Promise&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;reject&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;You can also use it on responses like this:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight tsx"&gt;&lt;code&gt;&lt;span class="nx"&gt;axios&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;interceptors&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;use&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
&lt;span class="c1"&gt;// filter out null or undefined values using filter()&lt;/span&gt;
    &lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;filtered_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;res&lt;/span&gt;&lt;span class="p"&gt;?.&lt;/span&gt;&lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;?.&lt;/span&gt;&lt;span class="nx"&gt;names&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nx"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;filtered_names&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;Promise&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;reject&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="p"&gt;});&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;In the response example, we're filtering an array of names that was passed back to make sure no null or undefined values are included. &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#custom-axios-instances"&gt;
  &lt;/a&gt;
  Custom Axios Instances
&lt;/h3&gt;

&lt;p&gt;One of the nicest use cases of Axios interceptors is the ability to add them to a custom axios instance for your project.&lt;/p&gt;

&lt;p&gt;If you are unfamiliar with Axios instances, it is a way to add all the data you need to send with each request automatically. &lt;/p&gt;

&lt;p&gt;For example: in our project we need to always have an access token header and the base URL of our api. We can create a custom instance like this:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight tsx"&gt;&lt;code&gt;&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;customInstance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;axios&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;create&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="na"&gt;baseURL&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="s1"&gt;https://mytestingapi.com/api&lt;/span&gt;&lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;headers&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="s1"&gt;access_token&lt;/span&gt;&lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="s1"&gt;custom_token&lt;/span&gt;&lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;});&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Now whenever we need to perform a request with Axios, we can call &lt;em&gt;customInstance&lt;/em&gt; instead and all our data will be included automatically. &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#adding-interceptors-to-custom-instances"&gt;
  &lt;/a&gt;
  Adding Interceptors To Custom Instances
&lt;/h3&gt;

&lt;p&gt;Adding interceptors to Axios instances is done in the same way you'd do it normally, except instead of using the axios keyword, we'll use the name of our instance.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight tsx"&gt;&lt;code&gt;&lt;span class="nx"&gt;customInstance&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;interceptors&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;request&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;use&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;&lt;span class="cm"&gt;/* do stuff here */&lt;/span&gt;&lt;span class="p"&gt;});&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h3&gt;
  &lt;a href="#conclusion"&gt;
  &lt;/a&gt;
  Conclusion
&lt;/h3&gt;

&lt;p&gt;I hope you found this simple introduction to interceptors to be useful. It is worth checking out the Axios documentation if you want to learn more. &lt;/p&gt;

&lt;p&gt;Also don't forget to follow me on twitter &lt;a class="mentioned-user" href="https://dev.to/codespectremike"&gt;@codespectremike&lt;/a&gt;
 to get notifications for my latest videos and articles.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://twitter.com/codespectreMike"&gt;https://twitter.com/codespectreMike&lt;/a&gt;&lt;/p&gt;

</description>
      <category>webdev</category>
      <category>react</category>
      <category>typescript</category>
      <category>javascript</category>
    </item>
    <item>
      <title>Differences between Javascript and NodeJs</title>
      <author>Maria Antonella ü¶ã</author>
      <pubDate>Sun, 22 Aug 2021 15:50:39 +0000</pubDate>
      <link>https://dev.to/antoomartini/differences-between-javascript-and-nodejs-27m4</link>
      <guid>https://dev.to/antoomartini/differences-between-javascript-and-nodejs-27m4</guid>
      <description>&lt;p&gt;At first it was hard for me to understand the differences because for me, they were the same thing. It was all javascript. But then, I started to understand what each one was used for. For this reason, I share it :)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;‚òò JavaScript is a language that runs inside web browsers as part of the documents loaded by the browser and is used as a client-side development language. It provides the behavior of the pages. Like HTML it provides the semantic structure and CSS the look and feel. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;However, being an interpreted language, it needs an interpreter to run. V8 is Google Chrome's JS engine and 'node' is a front-end that can be used to run JavaScript scripts outside the browser. In other words: &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;‚òò  NodeJs is an open source, cross-platform environment that allows you to create server-side applications and tools using JavaScript. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A short list of comparisons üßêüíª:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;JS
üî¥ Can only be run in the browsers
üî¥ Used on the client-side
üî¥ Capable enough to add HTML and play with the DOM
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;NodeJs
üü° Can be run outside the browser
üü° Used on the server-side.
üü° Does not have the capability to add HTML tags
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



</description>
      <category>javascript</category>
      <category>begginers</category>
      <category>node</category>
      <category>webdev</category>
    </item>
    <item>
      <title>AWS Certified SysOps Administrator SOA-C02 Exam Questions Part 3</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 15:31:42 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-sysops-administrator-soa-c02-exam-questions-part-3-bii</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-sysops-administrator-soa-c02-exam-questions-part-3-bii</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company static website hosted on Amazon S3 was launched recently and is being used. Currently users are experiencing 503 services unavailable errors. Why are these errors occuring?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The request rate to Amazon S3 is too high
B. There is an error with the Amazon RDS database.
C. The requests to Amazon S3 do not have the proper permissions
D. The users are in a different geographical region and Amazon Route53 is restricting access
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company is releasing a new static website hosted on Amazon S3. However, upon navigating to the site, the following error messages is received.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;403 Forbidden - Access Denied
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What change should be made to fix this error?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Add a bucket policy that grants everyone read access to the bucket
B. Add a bucket policy that grants everyone read access to the bucket objects
C. Remove the default bucket policy that denies read access to the bucket
D. Configure cross-origin resource sharing (CORS) on the bucket
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has created an online retail application that is hosted on a fleet of EC2 instances behind of ELB application load balancer, authentication is handled at the individual EC2 instance level. Once a user is authenticated, all request have go to the same EC2 instance. What should the SysOps Administrator enable to meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. ELB TCP listeners
B. ELB Sticky Sessions
C. ELB connection draining
D. ELB cross-zone load balancing
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is managing a large organization with multiple accounts on the Business Support plan all linked to a single payer account. The Administrator wants to be notified automatically of AWS Personal Health Dashboard events. In the main payer account, the Administrator configures Amazon CloudWatch Events triggered by AWS Health events to issue notifications using Amazon SNS, but alerts in the linked accounts failed to trigger. Why did the alerts fail?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Amazon SNS cannot be triggered from the AWS Personal health Dashboard
B. The AWS personal health dashboard only reports events from one a account, not linked account
C. The AWS Personal Health Dashboard must be configured from the payer account only; all events will then roll up into the payer account.
D. AWS Organizations must be used to monitor linked accounts.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The security team has decided that there will be no public internet access to HTTP (TCP port 80 ) because it is moving to HTTPS for all incoming web traffic. The team had asked a SysOps Administrator to provide a report on any security groups that are not compliant. What should the AWS SysOps Administrator do to provide near real time compliance reporting?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Enable AWS Trusted Advisor and show the security team that the security groups unrestricted access will check alarm
B. Schedule and AWS lambda function to run hourly to scan and evaluate all security groups and send report to the security team.
C. Use AWS config to enable the restricted common port rule and add port 80 to parameters
D. Use Amazon Inspector to evaluate the security groups during scans, and send the completed reports to the Security team.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;According to the shared responsibility model, for which of the following Amazon EC2 activities is AWS responsible? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Patching the guest operating system
B. Monitoring memory utilization
C. Configuring network ACLs
D. Patching the hypervisor
E. Maintaining network infrastructure
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company use of AWS Cloud services is quickly growing, so a SysOps Administrator has been asked to generate details of daily spending to share with management. Which method should the Administrator choose to produce this data?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Share the monthly AWS bill with management.
B. Use AWS CloudTrail Logs to access daily costs in JSON format.
C. Set up daily Cost and Usage Report and download the output from Amazon S3.
D. Monitor AWS costs with Amazon Cloud Watch and create billing alerts and notifications.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is managing an application that runs on Amazon EC2 instances behind and application load balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The applications stores data in Amazon RDS MySQL DB instance. The Administrator must ensure that that application stays available if the database becomes unresponsive. How can these requirements be met?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create read replicas for the RDS database and use them in case of a database failure.
B. Create a new RDS instance from the snapshot of the original RDS instance if a failure occurs.
C. Keep a separate RDS database running and switch the endpoint in the web application if a failure occurs.
D. Modify the RDS instance to be a Multi-AZ deployment.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has Sales department and Marketing department. The company uses one AWS account. There is a need to determine what charges are incurred on the AWS platform by each department. There is also a need to receive notifications when a specified cost level is approached or exceeded. Which two actions must a SysOps Administrator take to achieve both requirements with the LEAST amount of administrative overhead? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use AWS Trusted Advisor to obtain a report containing the checked items in the Cost Optimization pillar.
B. Download the detailed billing report, upload it to a database, and match the line items with a list of known resources by department.
C. Create a script by using the AWS CLI to automatically apply tags to existing resources to each department. Schedule the script to run weekly.
D. Use AWS Organizations to create a department Organizational Unit and allow only authorized personnel in each department to create resources.
E. Create a Budget from the Billing and Cost Management console. Specify the budget type a Cost, assign tags for each department, define notifications, and specify any other options as required.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;On a weekly basis, the Administrator for a photo sharing website receives an archive of all files users have uploaded the previous week. These file archives can be as a large as 10TB in size. For legal reasons, these archives must be saved with no possibility of someone deleting or modifying these archives. Occasionally, there may be a need to view the contents, but it is expected that retrieving them can take three or more hours. What should the Administrator do with the weekly archive?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Uploaded the file to Amazon S3 through the AWS management console and apply lifecycle policy to change the storage class to Amazon Glacier.
B. Upload the archive to the Amazon Glacier with the AWS CLI and enable Vault Lock.
C. Create a Linux EC2 instance with an encrypted Amazon EBS volume and copy each weekly archive file for this instance
D. Create a file gateway attached to a file share on an S3 bucket with the storage class S3 Infrequent Access. Upload the archives via the gateway
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to ensure that each department operates within their own isolated environment and that they are only able to use pre-approved services. How can this requirement be met?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Setup an AWS Organization to create accounts for each department and apply services control policies to control access to AWS services.
B. Create IAM roles for each department, and set policies that grant access to specific AWS services.
C. Use the AWS Service Catalog to create catalogs of AWS services that are approved for use by each department.
D. Request that each department create and manage its own AWS account and the resources within it.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An Amazon EC2 instance is unable to connect to an SMTP server in a different subnet. Other instance are successfully communicating with the SMTP server, however VPC flow logs have been enabled on the SMTP server‚Äôs network interface and show the following information.&lt;br&gt;
2223342796652 eni-abe77dab 10.1.1.200 10.100.1.10 1123 25 17 70 48252 1515534437 1515535037 REJECT OK&lt;br&gt;
What can be done to correct problem?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Add the instance to the security group for the SMTP server and ensure that is permitted to communicate over TCP port 25.
B. Disable the iptables service on the SMTP server so that the instance can properly communicate over the network.
C. Install an email client on the instance to ensure that it communicates correctly on TCP port 25 to the SMTP server.
D. Add a rule to the security group for the instance to explicitly permit TCP port 25 outbound to any address.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A web application accepts orders from online users and places the orders into an Amazon SQS queue. Amazon EC2 instances in an EC2 Auto Scaling group read the messages from the queue, process the orders, and email order confirmations to the users. The Auto Scaling group scales up and down based on the queue depth. At the beginning of each business day, users report confirmation emails are delayed. What action will be address this issues?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create a scheduled scaling action to scale up in anticipation of the traffic.
B. Change the Auto Scaling group to scale up and down based on CPU utilization
C. Change the launch configuration to launch larger EC2 instance types
D. Modify the scaling policy to deploy more EC2 instances when scaling up
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An Applications team has successfully deployed an AWS CloudFormation stack consisting of 30 t2-medium Amazon EC2 instances in the us-west-2 Region. When using the same template to launch a stack in us-east-2, the launch failed and rolled back after launching only 10 EC2 instances. What is a possible cause of this failure?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The IAM user did not have privileges to launch the CloudFormation template.
B. The t2 medium EC2 instance service limit was reached
C. An AWS Budgets threshold was breached
D. The application‚Äôs Amazon Machine Image (AMI) is not available in us-east-2
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is receiving multiple reports from customers that they are unable to connect to the company‚Äôs website. which is being served through Amazon CloudFront. Customers are receiving HTTP response codes for both 4XX and 5XX errors. Which metric can the Administrator use to monitor the elevated error rates in CloudFront?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. TotalErrorRate
B. RejectedConnectionCount
C. NetworkTransmitThroughput
D. HealthyHostCount
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization stores sensitive customer information in S3 buckets protected by bucket policies. Recently, there have been reports that unauthorized entities within the company have been trying to access the data on those S3 buckets. The Chief Information Security Officer (CISO) would like to know which buckets are being targeted and determine who is responsible for trying to access that information. Which steps should a Sysops administrator take to meet the CISO requirement? ( Select TWO)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Enable Amazon S3 Analytics on all affected S3 buckets to obtain a report of which buckets are being accessed without authorization.
B. Enable Amazon S3 Server Access Logging on all affected S3 buckets and have the logs stored in a bucket dedicated for logs.
C. Use Amazon Athena to query S3 Analytics reports for HTTP 403 errors, and determine the IAM user or role making the requests.
D. Use Amazon Athena to query the S3 Server Access Logs for HTTP 403 errors, and determine the IAM user or role making the requests.
E. Use Amazon Athena to query the S3 Server Access Logs for HTTP 503 errors, and determine the IAM user or role making the requests.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is responsible for a large fleet of EC2 instances and must know whether any instances will be affected by upcoming hardware maintenance. Which option would provide this information with the LEAST administrative overhead?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Monitor AWS CloudTrail for StopInstances API calls related to upcoming maintenance.
B. Review the Personal Health Dashboard for any scheduled maintenance
C. From the AWS Management Console, list any instances with failed system status checks.
D. Deploy a third-party monitoring solution to provide real-time EC2 instance monitoring.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Malicious traffic is reaching company web servers from a single IP address located in another country. The SysOps Administrator is tasked with blocking this IP address. How should the Administrator implement the restriction?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Edit the security group for the web servers and add a deny entry for the IP address
B. Edit the network access control list for the web server subnet and add a deny entry for the IP address
C. Edit the VPC route table to route the malicious IP address to a black hole
D. Use Amazon CloudFront‚Äôs geo restriction feature to block traffic from the IP address
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company website hosts patches for software that is sold globally. The website runs in AWS and performs well until a large software patch is released. The flood of downloads puts a strain on the web servers and leads to a poor customer experience. What can the SysOps Administrator propose to enhance customer experience, create a more available web platform, and keep costs low?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use an Amazon CloudFront distribution to cache static content, including software patches
B. Increase the size of the NAT instance to improve throughput
C. Scale out of web servers in advance of patch releases to reduce Auto Scaling delays
D. Move the content to IO1 and provision additional IOPS to the volume that contains the software patches
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A website uses Elastic Load Balancing (ELB) in front of several Amazon EC2 instances backed by an Amazon RDS database. The content is dynamically generated for visitors of a webpage based on their geographic location. and is updated daily. Some of the generated objects are large in size and are taking longer to download than they should, resulting in a poor user experience. Which approach will improve the user experience?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Implement Amazon ElastiCache to cache the content and reduce the load on the database.
B. Enable an Amazon CloudFront distribution with Elastic Load Balancing as a custom origin.
C. Use Amazon S3 to store and deliver the content.
D. Enable Auto Scaling for the EC2 instances so that they can scale automatically.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A workload has been moved from a data center to AWS. Previously, vulnerability scans were performed nightly by an external testing company. There is a mandate to continue the vulnerability scans in the AWS environment with third-party testing occurring at least once each month. What solution allows the vulnerability scans to continue without violating the AWS Acceptable Use Policy?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The existing nightly scan can continue with a few changes. The external testing company must be notified of the new IP address of the workload and the security group of the workload must be modified to allow scans from the external company‚Äôs IP range .
B. If the external company is a vendor in the AWS Marketplace, notify them of the new IP address of the workload
C. Submit a penetration testing request every 90 days and have the external company test externally when the request is approved.
D. AWS performs vulnerability testing behind the scenes daily and patches instances as needed. If a vulnerability cannot be automatically addressed, a notification email is distributed.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A web service runs on Amazon EC2 instances behind an Elastic Load Balancing (ELB) load balancer. External clients must whitelist specific public IP addresses in their firewalls to access the service. What load balancer or ELB feature should be used for this application?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Network Load Balancer
B. Application Load Balancer
C. Classic Load Balancer
D. Load balancer target groups
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator receives reports of an Auto Scaling group failing to scale when the nodes running Amazon Linux in the cluster are constrained by high memory utilization. What should the Administrator do to enable scaling to better adapt to the high memory utilization?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create a custom script that pipes memory utilization to Amazon S3, then, scale with an AWS Lambda-powered event
B. Install the Amazon CloudWatch memory monitoring scripts, and create a custom metric based on the script‚Äôs results
C. Increase the minimum size of the cluster to meet memory and application load demands
D. Deploy an Application Load Balancer to more evenly distribute traffic among nodes
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator attempting to delete an Amazon S3 bucket ran the following command: aws s3 rb s3://mybucket The command failed and bucket still exists. The administrator validated that no files existed in the bucket by running aws s3 1s s3://mybucket and getting an empty response. Why is the Administrator unable to delete the bucket, and what must be done to accomplish this task?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The bucket has MFA Delete enabled, and the Administrator must turn it off.
B. The bucket has versioning enabled, and the Administrator must permanently delete the objects‚Äô delete markers.
C. The bucket is storing files in Amazon Glacier, and the Administrator must wait 3-5 hours for the files to delete.
D. The bucket has server-side encryption enabled, and the Administrator must run the aws s3 rb s3://my bucket ‚Äî sse command.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has two AWS accounts: development and production. All applications send logs to a specific Amazon S3 bucket for each account, and the Developers are requesting access to the production account S3 buckets to view the logs. Which is the MOST efficient way to provide the Developers with access?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create an AWS Lambda function with an IAM role attached to it that has access to both accounts‚Äô S3 buckets. Pull the logs from the production S3 bucket to the development S3 bucket.
B. Create IAM users for each Developer on the production account, and add the Developers to an IAM group that provides read-only access to the S3 log bucket.
C. Create an Amazon EC2 bastion host with an IAM role attached to it that has access to the production S3 log bucket, and then provision access for the Developers on the host.
D. Create a resource-based policy for the S3 bucket on the production account that grants access to the development account, and then delegate access in the development account.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A web application runs on Amazon EC2 instances behind an Elastic Load Balancing Application Load Balancer (ALB). The instances run in an Auto Scaling group across multiple Availability Zones. A SysOps Administrator has notice that some EC2 instances show up healthy in the Auto Scaling console but show up as unhealthy in the ALB target console. What could be the issue?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The health check grace period for the Auto Scaling group is set too low; increase it
B. The target group health check is incorrectly configured and needs to be adjusted
C. The user data or AMI used for the Auto Scaling group launch configuration is incorrect
D. The Auto Scaling group health check type is based on EC2 instance health instead of Elastic Load Balancing health checks
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is running critical applications on Amazon EC2 instances. The company needs to ensure its resources are automatically recovered if they become impaired due to an underlying hardware failure. Which service can be used to monitor and recover the EC2 instances?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Amazon EC2 Systems Manager
B. Amazon Inspector
C. AWS CloudFormation
D. Amazon CloudWatch
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company requires that all access from on-premises applications to AWS services go over its AWS Direct Connect connection rather than the public internet. How would a SysOps Administrator implement this requirement?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Implement an IAM policy that uses the aws:sourceConnection condition to allow access for the AWS Direct Connect connection ID only
B. Set up a public virtual interface on the AWS Direct Connect connection
C. Configure AWS Shield to protect the AWS Management Console from being accessed by IP addresses other than those within the data center ranges
D. Update all the VPC network ACLs to allow access from the data center IP ranges
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is required to monitor free space on Amazon EBS volumes attached to Microsoft Windows-based Amazon EC2 instances within a company‚Äôs account. The Administrator must be alerted to potential issues. What should the Administrator do to receive email alerts before low storage space affects EC2 instance performance?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use built-in Amazon CloudWatch metrics, and configure CloudWatch alarms and an Amazon SNS topic for email notifications
B. Use AWS CloudTrail logs and configure the trail to send notifications to an Amazon SNS topic
C. Use the Amazon CloudWatch agent to send disk space metrics, then set up CloudWatch alarms using an Amazon SNS topic
D. Use AWS Trusted Advisor and enable email notification alerts for EC2 disk space
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company‚Äôs Information Security team has requested information on AWS environment compliance for Payment Card Industry (PCI) workloads. They have requested assistance in understanding what specific areas of the PCI standards are the responsibility of the company. Which AWS tool will provide the necessary information?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. AWS Macie
B. AWS Artifact
C. AWS OpsWorks
D. AWS Organizations
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company uses AWS CloudFormation to deploy its application infrastructure. Recently, a user accidentally changed a property of a database in a CloudFormation template and performed a stack update that caused an interruption to the application. A SysOps Administrator must determine how to modify the deployment process to allow the DevOps team to continue to deploy the infrastructure, but prevent against accidental modifications to specific resources. Which solution will meet these requirements?&lt;/p&gt;

&lt;p&gt;A. Set up an AWS Config rule to alert based on changes to any Cloud Formation stack. An AWS Lambda function can then describe the stack to determine if any protected resources were modified and cancel the operation.&lt;br&gt;
B. Set up an Amazon CloudWatch Events event with a rule to trigger based on any CloudFormation API call. An AWS Lambda function can then describe the stack to determine if any protected resources were modified and cancel the operation.&lt;br&gt;
C. Launch the CloudFormation templates using a stack policy with an explicit allow for all resources and an explicit deny of the protected resources with an action of Update:*&lt;br&gt;
D. Attach an IAM policy to the DevOps team role that prevents a CloudFormation stack from updating, with a condition based on the specific Amazon Resource names (ARNs) of the protected resources.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company recently implemented an Amazon S3 lifecycle rule that accidentally deleted objects from one of its S3 buckets. The bucket has S3 versioning enabled. Which actions will restore the objects? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use the AWS Management Console to delete the object delete markers.
B. Create a new lifecycle rule to delete the object delete markers that were created.
C. Use the AWS CLI to delete the object delete markers while specifying the version IDs of the delete markers.
D. Modify the existing lifecycle rule to delete the object delete markers that were created.
E. Use the AWS CLI to delete the object delete markers while specifying the name of the objects only.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An application running on Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones was deployed using an AWS CloudFormation template. The SysOps team has patched the Amazon Machine Image (AMI) version and must update all the EC2 instances to use the new AMI. How can the SysOps Administrator use CloudFormation to apply the new AMI while maintaining a minimum level of active instances to ensure service continuity?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Run the aws cloudformation update-stack command with the ‚Äì rollback-configuration option
B. Update the CloudFormation template with the new AMI ID, then reboot the EC2 instances
C. Deploy a second CloudFormation stack and use Amazon Route 53 to redirect traffic to the new stack
D. Set an AutoScalingUpdate policy in the CloudFormation template to update the stack.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An AWS CodePipeline in us-east-1 returns ‚ÄúInternalError‚Äù with the code ‚ÄúJobFailed‚Äù when launching a deployment using an artifact from an Amazon S3 bucket in us-west-1. What is causing this error?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. S3 Transfer Acceleration is not enabled.
B. The S3 bucket is not in the appropriate region.
C. The S3 bucket is being throttled.
D. There are insufficient permissions on the artifact in Amazon S3.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SySOps Administrator is managing an AWS account where Developers are authorized to launch Amazon EC2 instances to test new code. To limit costs, the Administrator must ensure that the EC2 instances in the account are terminated 24 hours after launch. How should the Administrator meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create an Amazon CloudWatch alarm based on the CPUUtilization metric. When the metric is 0% for 24 hours, trigger an action to terminate the EC2 instance when the alarm is triggered.
B. Create an AWS Lambda function to check all EC2 instances and terminate instances running more than 24 hours. Trigger the function with an Amazon CloudWatch Events event every 15 minutes.
C. Add an action to AWS Trusted Advisor to turn off EC2 instances based on the Low Utilization Amazon EC2 Instances check, terminating instances identified by Trusted Advisor as running for more than 24 hours.
D. Install the unified Amazon CloudWatch agent on every EC2 instance. Configure the agent to terminate instances after they have been running for 24 hours.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator created an Application Load balancer (ALB) and placed two Amazon EC2 instances in the same subnet behind the ALB. During monitoring, the Administrator observes HealthyHostCount drop to 1 in Amazon CloudWatch. What is MOST likely causing this issue?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The EC2 instances are in the same Availability Zone, causing contention between the two.
B. The route tables are not updated to allow traffic to flow between the ALB and the EC2 instances.
C. The ALB health check has failed, and the ALB has taken EC2 instances out of service.
D. The Amazon Route 53 health check has failed, and the ALB has taken EC2 instances out of service.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has centralized all its logs into one Amazon CloudWatch Logs log group. The SysOps Administrator is to alert different teams of any issues relevant to them. What is the MOST efficient approach to accomplish this?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Write a AWS lambda function that will query the logs every minute and contain the logic of which team to notify on which patterns and issues.
B. Set up different metric filters for each team based on patterns and alerts. Each alarm will notify the appropriate notification list.
C. Redesign the aggregation of logs so that each team‚Äôs relevant parts are sent to a separate log group, then subscribe each team to its respective log group.
D. Create an AWS Auto Scaling group of Amazon EC2 instances that will scale based on the amount of ingested log entries. This group will pull streams, look for patterns, and send notifications to relevant teams.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An Amazon S3 bucket in a SysOps Administrator account can be accessed by users in other SWS accounts. How can the Administrator ensure that the bucket is only accessible to members of the Administrator‚Äôs AWS account?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Move the S3 bucket from a public subnet to a private subnet in the Amazon VPC.
B. Change the bucket access control list (ACL) to restrict access to the bucket owner.
C. Enable server-side encryption for all objects in the bucket.
D. Use only Amazon S3 presigned URLs for accessing objects in the bucket.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company received its latest bill with a large increase in the number of requests against Amazon SQS as compared to the month prior. The company is not aware of any changes in its SQS usage. The company is concerned about the cost increase and who or what was making these calls. What should the SysOps Administrator use to validate the calls made to SQS?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. AWS CloudTrail
B. Amazon CloudWatch
C. AWS Cost Explorer
D. Amazon S3 server access logs
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator responsible for an e-commerce web application observes the application does not launch new Amazon EC2 instances at peak times, even though the maximum capacity of the Auto Scaling group has not been reached. What should the Administrator do to identify the underlying problem? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Monitor service limits in AWS trusted Advisor.
B. Analyze VPC Flow Logs.
C. Monitor limits in AWS Systems Manager.
D. Use Amazon inspector to gather performance information.
E. Check the response for RunInstance requests in AWS CloudTrail logs.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A E&lt;/p&gt;

</description>
      <category>awslagi</category>
      <category>aws</category>
      <category>googlecloud</category>
    </item>
    <item>
      <title>AWS Certified SysOps Administrator SOA-C02 Exam Questions Part 2</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 15:30:18 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-sysops-administrator-soa-c02-exam-questions-part-2-3l85</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-sysops-administrator-soa-c02-exam-questions-part-2-3l85</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A fleet of servers must send local logs to Amazon Cloudwatch. How should the servers be configured to meet these requirements ?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Configure AWS Config to forward events to cloudwatch
B. Configure a simple network management protocol (SNMP) agent to forward events to Cloudwatch
C. Install and configure the unified Cloudwatch agent
D. Install and configure the Amazon Inspector agent
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company data retention policy dictates that backups be stored for exactly two years. After that the data must be deleted. How can Amazon EBS snapshots be managed to conform to this data retention policy?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use an Amazon S3 lifecycle policy to delete snapshots older than two years
B. Configure Amazon Inspector to find and delete old EBS Snapshots
C. Schedule an AWS Lambda function using Cloudwatch events to periodically run a scripts to delete old snapshots
D. Configure an Amazon Cloudwatch Alarm to trigger the launch of an AWS Cloudformation template that will clean the older snapshots
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;In configuring an Amazon Route 53 health check, a SysOps Administrator selects ‚ÄòYes‚Äô to the String Matching option in the Advanced Configuration section. In the Search String box, the Administrator types the following text: /html. This is to ensure that the entire page is loading during the health check. Within 5 minutes of enabling the health check, the Administrator receives an alert stating that the check failed. However, when the Administrator navigates to the page, it loads successfully. What is the Most likely cause of this false alarm?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The search string is not HTML encoded
B. The search string must be put in quotes
C. The search string must be escaped with a backslash (\) before the forward slash (/)
D. The search string is not in the first 5120 bytes of the tested page
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator must ensure that AWS Cloudformation deployment changes are properly backend for governance. Which AWS Service should be used to accomplish this?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. AWS Artifact
B. AWS Config
C. Amazon Inspector
D. AWS Trusted Advisor
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Developer created an AWS Lambda function and has asked the SysOps Administrator to make the function run in every 15 minutes . What is the MOST efficient way to accomplish this request?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create an Amazon EC2 instance and schedule a cron to invoke the Lambda function
B. Create a repeat time variable inside the Lambda function to invoke the Lambda function
C. Create a second Lambda function to monitor and invoke the first Lambda function
D. Create an Amazon Cloudwatch scheduled event to invoke the Lambda function
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is analyzing how Reserved Instance discounts are allocated to Amazon EC2 instances across multiple AWS Account. Which AWS tool will provide the details necessary to understand the billing charges?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. AWS Budgets
B. AWS Cost and Usage report
C. AWS Trusted Advisor
D. AWS Organizations
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator wants to prevent Developer from accidentally terminating Amazon EC2 instance. How can this be accomplished?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use AWS Systems Manager to restrict EC2 termination
B. Use AWS Config to restrict EC2 termination
C. Application Amazon Cloudwatch event to prevent EC2 termination
D. Enable termination protection on EC2 instances
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization has developed a new memory intensive application that is deployed to a large Amazon EC2. The application is exhaustion, so the development team wants to monitor memory usage by using Amazon Cloudwatch. What is the MOST efficient way to accomplish this goal?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Deploy the solution to memory-optimized EC2 instances and use the cloudwatch MemoryUtilization metrics
B. Enable the memory monitoring option by using AWS Config
C. Install the AWS System Manager agent on applicable EC2 instances to monitor memory
D. Monitor memory by using a script within the instance and send it to cloudwatch as a custom metric
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization has been running their website on several m2 Linux instances behind a classic load balancer for two years. Application load has been constant and predictable. What should the organization do to reduce costs?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Purchase Reserved instances for the specific m2.instances
B. Change the m2 instances to equivalent m5 types, and purchase Reserved instances for the specific m5 instances
C. Change the classic load balancer to an application load balancer and purchase reserved instances for the specific m2 instances
D. Purchase Spot instances for the specific m2 instances
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator has written an AWS Lambda function to launch new Amazon EC2 instances and deployed it in the us-east-1 region. The Administrator tested it by launching a new t2 nano instance in the us-east-1 region and it performed as expected. However, when the region name was updated in the Lambda function to launch an EC2 instance in the us-west-1 region, it failed. What is causing this error?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The AMI ID must be updated for the us-west-1 region in the Lambda function as well
B. The Lambda function can only launch EC2 instances in the same region where it is deployed
C. The Lambda function does not have the necessary IAM permission to launch more than one EC2 instance
D. The instance type defined in the Lambda function is not available in the us-west-1 region
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company backs up data from data center using a tape gateway on AWS Storage Gateway. The SysOps Administrator must stop a running storage gateway. What process will protect data integrity?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Stop storage gateway and reboot the virtual machine, then restart Storage Gateway
B. Reboot the virtual machine then restart storage gateway
C. Reboot the virtual machine
D. Shutdown the virtual machine and stop storage gateway then turn the virtual machine
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is responsible for a legacy, CPU heavy application. The application can only be scaled vertical. Currently application running on t2.large Amazon EC2 instance. The system is showing 90% CPU usage and significant performance latency. What change should be made to alleviate the performance problem?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Change the EBS volume to provisioned IOPS
B. Upgrade to a compute-optimized instance
C. Add additional t2.large instances to the application
D. Purchase the Reserved Instance
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator runs a web application that is using a microservices approach whereby different responsibilities of the application have been divided in a separate microservice running on a different Amazon EC2 instance. The Administrator has been tasked with reconfiguring the infrastructure to support this approach. How can the Administrator accomplish this with the LEAST administrative overhead?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use Amazon Cloudfront to log the URL and forward the request
B. Use Amazon Cloudfront to rewrite the header base on the micro service and forward the request
C. Use an Application Load Balancer (ALB) and do path-based routing
D. Use a Network Load Balancer (NLB) and do path-based routing
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization is concerned that its Amazon RDS databases are not protected. The solution to address this issue must be low cost, protect against table corruption that could be overlooked for several days, and must offer a 30-day window of protection. How can these requirement must be met?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Enable multi-AZ on the RDS Instance to maintain the data in a second Availability Zone
B. Create a Read Replica of the RDS Instance to maintain the data in a second region
C. Ensure that automated backups are enabled and set the appropriate retention period
D. Enable versioning in RDS to recover altered table data when needed
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company with a dozens of AWS Account wants to ensure that governance rules are being applied across all accounts. The CIO has recommended that AWS Config rules be deployed using an AWS Cloudformation template. How should the requirements be met?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create the Cloudformation stack set then select Cloudformation template and use it to configure the AWS accounts
B. Write a script that iterates over the Company AWS accounts and executes the Cloudformation template in each account
C. Use AWS Organizations to execute the Cloudformation template in all accounts
D. Create a Cloudformation template in the master account of AWS. Organizations and execute the Cloudformation template to create AWS Config rules in all accounts
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company must ensures that any objects uploaded to an s3 bucket must be encrypted. Which of the following actions will meet the requirement? ( SELECT TWO)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Implement AWS Shield to protect again unencrypted objects stored in s3 buckets
B. Implement Object access control list (ACL) to deny unencrypted objects from being uploaded to the S3 bucket
C. Implement Amazon S3 default encryption to make sure that any object being uploaded is encrypted before it is stored
D. Implement Amazon Inspector to inspect objects uploaded to s3 bucket to make sure that they are encrypted
E. Implement S3 bucket policies to deny unencrypted objects from being uploaded to the buckets
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Based on the AWS Shared Responsibility Model, which of the following actions are the responsibility of the customer for an Aurora database?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Performing underlying OS updates
B. Provisioning of storage for database
C. Scheduling maintenance, patches and other updates
D. Executing maintenance, patches and other updates
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company would like to review each change in the infrastructure before deploying updates in its AWS Cloudformation stacks. Which action will allow an Administrator to understand the impact of these changes before implement?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Implement a blue/green strategy using AWS Elastic Beanstalk
B. Perform a canary deployment using a Application Load Balancer and target groups
C. Create a change set for the running stack
D. Submit the update using UpdateStack API call.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization is running multiple applications for their customers. Each application is deployed by running a base AWS CloudFormation template that configures a new VPC. All applications are run in the same AWS account and AWS Region. A SysOps Administrator has noticed that when trying to deploy the same AWS CloudFormation stack, it fails to deploy. What is likely to be the problem?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The Amazon Machine Image used is not Available in that region
B. The AWS Cloudformation template needs to be update to the latest version
C. The VPC configurations parameters have changed and must be updated in the template
D. The account has reached the default limit for VPCs allowed
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator found that newly-deployed Amazon EC2 application server is unable to connect to an Amazon RDS database. VPC Flow Logs and confirming that the flow log is active on the console, the log group cannot be located on Amazon Cloudwatch. What are the MOST likely reasons for this situation? (SELECT TWO)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The Administrator must configure the VPC Flow Logs to have them sent to AWS CloudTrail
B. The Administrator has waited less than ten minutes for the log group to be created in Cloudwatch
C. The account VPC Flow Logs have been disabled by using a service control policy
D. No relevant traffic has been sent since the VPC Flow Logs were created
E. The account has Amazon Guard Duty enabled.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has mandated the use of multi-factor authentication (MFA) for all IAM users, and requires users to make all API-calls using the CLI. However, users are not prompted to enter MFA tokens, and are able to run CLI commands without MFA. In an attempt to enforce MFA, the company attached an IAM policy to all users that denies API calls that have not been authenticated with MFA. What additional step must be taken to ensure that API calls are authenticated using MFA?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Enable MFA on IAM roles and require IAM users to use role credentials to sign API calls.
B. Ask the IAM users to log into the AWS Management Console with MFA before making API calls using the CLI
C. Restricts the IAM users to use of the console, as MFA is not supported for CLI use
D. Require user to use temporary credentials from the get sessions token command to sign API calls
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;p&gt;62.An application running on Amazon EC2 allows users to launch batch jobs for data analysis. The jobs are run asynchronously, and the user is notified when they are complete. While multiple jobs can run concurrently, a user‚Äôs request need not be fulfilled for up to 24 hours. To run a job, the application launches an additional EC2 instance that performs all the analytics calculations. A job takes between 75 and 110 minutes to complete and cannot be interrupted. What is the MOST cost-effective way to run this workload?&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;    A. Run the application on-Demand EC2 instances. Run the jobs on spot instances with a specified duration
    B. Run the application on Reserved instance EC2 instances. Run the jobs on AWS Lambda
    C. Run the application on On-Demand EC2 instances. Run the jobs on On-Demand EC2 instances
    D. Run the application on Reserved instance EC2 instances. Run the jobs on spot instances with a specified duration
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization has two AWS accounts Development and Production. A SysOps Administrator manages access via IAM. Users require in Development should have access to certain resource in Production. How can this be accomplished?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create an IAM role in Production account with the Development account as a trusted entity and then allow those users from Development account to assume the Production account IAM role
B. Create a group of IAM users in the Development account and add Production account service ARNs as resources in the IAM policy
C. Establish a federation between the two accounts using the on-premises Microsoft Active Directory and allow development account to access the Production account through this federation
D. Establish an Amazon Cognito Federated Identity between the two accounts and allow the Development account to access the Production account through this federation
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator has been able to consolidate multiple secure websites onto a single servers and each site is running on a different port. The Administrator now wants to start a duplicate server in a second Availability Zone and put both behind a Load Balancer for high availability. What would be the command line necessary to deploy one of the sites certificates to the load balancer?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. aws kms modify-listener ‚Äìloadbalancer-name my-loadbalancer ‚Äìcertificates CertificateARN arn:aws:iam::123456:server-certificate/my-new-server-cert
B. aws elb set-load-balancer-listener-ssl-cerficate ‚Äìload-balancer-name my-load-balancer ‚Äìload-balaner-port 443 ‚Äìssl-cerficate-id arn:aws:iam::123456:server-certificate/new-server-cert
C. aws ec2 put-ssl-certificate ‚Äìloadbalancer-name my-loadbalancer ‚Äìload-balaner-port 443 ‚Äìssl-cerficate-id arn:aws:iam::123456:server-certificate/new-server-cert
D. aws acm put-ssl-cerficate ‚Äìloadbalancer-name my-loadbalancer ‚Äìload-balaner-port 443 ‚Äìssl-cerficate-id arn:aws:iam::123456:server-certificate/new-server-cert
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An application resides on multiple EC2 instances in public subnets in two Availability Zones. To improve security Application Load Balancer (ALB) in separate subnets and pointed the DNS at the ALB instead of EC2 instances. After the change, traffic is not reaching the instances and an error is being returned from the ALB. What steps must a SysOps Administrator take to resolve this issue and improve the security of the application? (SELECT TWO)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Add the EC2 instances to the ALB target group, configure the health check and ensure that the instances report healthy
B. Add the EC2 instances to an Auto Scaling group, configure the health check to ensure that the instances report healthy and remove the public IPs from the instances
C. Create a new subnet in which EC2 instances and ALB will reside to ensure that they can communicate and remove the public IPs from the instances
D. Change the security group for the EC2 instances to allow access from only the ALB security group and remove the public IPs from the instances
E. Change the security group to allow access from 0.0.0.0/0 which permits access from the ALB
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is received its latest bill with a large increase in the number of request against Amazon SQS as API call action. Admin need to know of any major changes in it SQS usage. The company is concerned about the cost increase and who or what was missing the calls. What should the SysOps Administrator use to validate the calls made to SQS?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Amazon Cloudtrail
B. Amazon Cloudwatch
C. AWS Cost Explorer
D. Amazon S3 Access logs
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;After a particularly high AWS bill, an organization wants to review the use of AWS Services&lt;br&gt;
What AWS Service will allow the SysOps Administrator to quickly view this information to share it and will also forecast equipment ?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. AWS Trusted Advisor
B. Amazon QuickSight
C. AWS Cost and Usage Report
D. AWS Cost Explorer
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator must find a way to setup alerts when Amazon EC2 service limit are close to being reached? How can the Administrator achieve this requirement?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use Amazon Inspector and Amazon Cloudwatch Events
B. Use AWS Trusted Advisor and Amazon Cloudwatch Events
C. Use the Personal Health Dashboard and Cloudwatch Events
D. Use AWS CloudTrail and Cloudwatch Events
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is reviewing AWS Trusted Advisor warnings and encounters a warning for an S3 bucket policy that discussing the issue with the bucket owner, the Administrator realizes the S3 bucket is an origin for an Amazon Cloudfront Which action should the Administrator take to ensure that users access objects in Amazon S3 by using only Cloudfront URL?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Encrypt the S3 bucket content with Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)
B. Create an Origin access identity and grand it permissions to read objects in the S3 buckets
C. Assign an IAM user to the Cloudfront distribution and whitelist the IAM user in the S3 bucket policy
D. Assign an IAM Role to the Cloudfront distribution and whitelist the IAM role in the S3 bucket policy
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An Auto Scaling group scales up and down based on Average CPU Utilization. The alarms is set to trigger a scaling when CPU exceeds 80% for 5 minutes. Currently, the average CPU has been 95% for over two hours and new instances are not being added What could be the issue?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. A Scheduled scaling action has not been defined
B. In the field suspend process ‚Äú ReplacesUnhealthy‚Äù has been selected
C. The maximum size of the Auto Scaling Group is below or at the current group size
D. The HealthCheck Grace Period is set to less than 300 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The Database Administrator team is interested in performing manual backups of an Amazon RDS Oracle DB instance. What step should be taken to perform the backups?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Attach Amazon EBS Volume with Oracle RMAN installed to the RDS Instance
B. Take a snapshot of the EBS volume that is attached to the DB instance
C. Install Oracle Secure backup on the RDS instance and backup the Oracle database to Amazon S3
D. Take a snapshot of the DB Instance
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company has created a separate AWS account for all development work to protect the production environment. In the development environment users request permission to manipulate IAM policies and roles. Corporate policies require that developers are blocked from accessing services. What is the BEST way to grant the developers privileges in the development account while still complying with corporate policies?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create a service control policy in AWS Organizations and apply it to the development account
B. Create a customer managed policy in IAM and apply it in to all users within the development account
C. Create a job function policy in IAM and apply it to all users within the development account
D. Create an IAM Policy and apply it in API Gateway to restrict the development account
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company has a web application that runs on both on-premises and on Amazon EC2 instances. Over time both the on-premises server and EC2 instances is crashing. A SysOps Administrator suspects a memory leak in the application and wants unified method to monitor memory utilization. How can the Administrator track both the EC2 memory utilization and on-premises server memory utilization over time?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Write a script or use a third-party application to report memory utilization for both EC2 instances and on-premises servers.
B. Use Amazon Cloudwatch agent for both Amazon EC2 instances and on-premises servers to report MemoryUtilization metrics to Cloudwatch and set a Cloudwatch alarm for notifications
C. Use Cloudwatch agent for Amazon EC2 instances to report memory Utilization to Cloudwatch and set Cloudwatch Alarms for notifications. Use a third-party application for the on-premises servers.
D. Configure a load balancer to route traffic to both on-premises servers and EC2 instances, then use cloudwatch as the unified view of the metrics for the load balancer.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is using AWS Cloudformation to deploy resources but would like to manually address any errors the template encounters. What should the Administrator add to the template to support the requirement?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Enable Termination Protection on the Stack
B. Set the OnFailure parameter to ‚ÄúDO_NOTHING‚Äù
C. Restrict the IAM permissions for CloudFormation to delete resources
D. Set the DeleteStack API action to ‚ÄúNO‚Äù
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company‚Äôs application stores documents within an Amazon S3 bucket. The application is running on Amazon EC2 in a VPC. A recent change in security requirements states that traffic between the company‚Äôs application and the S3 bucket must never leave the Amazon network. What AWS feature can provide this functionality?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Security Groups
B. NAT gateways
C. Virtual private gateway
D. Amazon VPC endpoints
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization with a large IT department has decided to migrate to AWS . With different jobs functions in departments and is not desirable to give all users access to all AWS resources. Currently the organization handles access via LDAP group membership. What the best method to allow access using current LDAP credentials ?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create an AWS directory service simple AD . Replicate the onpremise LDAP directory to simple AD
B. Create Lambda function to read LDAP groups and automate the creation of IAM users
C. Use AWS Cloud Formations to create IAM roles . Deploy direct connect to allow access to the on-premises LDAP server.
D. Federate the LDAP directory with IAM using SAML. Create different IAM roles correspond to different LDAP group to limit permissions.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An Sysops Administrator must set up notifications for whenever combined billing exceeds a certain threshold for all AWS account within company. The Administrator has set up AWS Organizations and enabled Consolidate billing. Which additionals steps must the Administrator perform to setup the billing alerts?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. On the payer account Enable billing alerts in the Billing and Cost management console ; publish an Amazon SNS message when the billing alerts triggers.
B. On each account Enable billing alerts in the billing and cost management console ; setup a billing alarm in Amazon Cloudwatch; publish an SNS message when the alarm triggers.
C. On the payer account Enable billing alerts in the billing and cost management console; setup a billing alarm in the billing and cost management console to publish an SNS message when the alarm triggers.
D. On the payer account Enable billing alerts in the billing and cost management console; setup billing alarm in Amazon Cloudwatch , publish an SNS message when the alarm triggers.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization has been running their website on several m2 Linux instance behind a classic load balancer for more than two years. Traffic and utilization have been constant and predictable. What should the organization do to reduce cost ?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Purchase reserved instances for the specific m2 instances.
B. Change the m2 instances type to equivalent m5 types and purchase reserved instances for specific m5 instances.
C. Change the classic load balancer to an application load balancer and purchase reserved instances for the specific m2 instances.
D. Purchase spot instances for the specific m2 instances.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An application developers are reporting access denied errors when trying to list the content in s3 bucket with IAM Role ARN ‚Äúarn:aws:iam:11111111:user/application‚Äù. The following s3 bucket policy:&lt;br&gt;
    {&lt;br&gt;
    ‚ÄúId‚Äù: ‚ÄúS3BucketPolicy‚Äù,&lt;br&gt;
    ‚ÄúVersions‚Äù: ‚Äú2012-10-17‚Äù,&lt;br&gt;
    ‚ÄúStatement‚Äù: [&lt;br&gt;
    {&lt;br&gt;
    ‚ÄúSid‚Äù: ‚ÄúList‚Äù,&lt;br&gt;
    ‚ÄúAction‚Äù: {&lt;br&gt;
    ‚Äús3: List*‚Äù&lt;br&gt;
    },&lt;br&gt;
    ‚ÄúEffect‚Äù: ‚ÄúAllow‚Äù,&lt;br&gt;
    ‚ÄúResources‚Äù: {&lt;br&gt;
    ‚Äúarn:aws:s3:::bucketname/*‚Äù&lt;br&gt;
    },&lt;br&gt;
    ‚ÄúPrincipal‚Äù: {&lt;br&gt;
    ‚ÄúAWS‚Äù: {&lt;br&gt;
    ‚Äúarn:aws:iam::11111111:user/application‚Äù&lt;br&gt;
    }&lt;br&gt;
    }&lt;br&gt;
    }&lt;br&gt;
    }&lt;br&gt;
    }&lt;br&gt;
How should a SysOps Administrator modify the S3 bucket policy to fix the issue?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Change the ‚ÄúEffect‚Äù from ‚ÄúAllow‚Äù to ‚ÄúDeny‚Äù
B. Change the ‚ÄúAction‚Äù from ‚ÄúS3:List*‚Äù to ‚ÄúS3:ListBucket‚Äù
C. Change the ‚ÄúResource‚Äù from ‚Äúarn:aws:s3:::bucketname/*‚Äù to ‚Äúarn:aws:s3:::bucketname‚Äù
D. Change the ‚ÄúPrincipal‚Äù from ‚Äúarn:aws:iam::11111111:user/application‚Äù to ‚Äúarn:aws:iam:1111111:role/application‚Äù
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company creates custom AMI images by launching new Amazon EC2 instance from an Amazon Cloudformation template. AMI images is installed software through AWS OpsWorks and take image of each EC2 instance. The process of installing software take a long times, the process stalls due to installations errors. The SysOps administrator must modify the Cloudformation Template so if the process stalls, stacks will rollback. Based on the requirements, what should be added to the template?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Conditions with a timeout set to 4 hours
B. CreationPolicy with a timeout set to 4 hours
C. DependOn with a timeout set to 4 hours
D. MetaData with a timeout set to 4 hours
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

</description>
      <category>aws</category>
      <category>googlecloud</category>
      <category>awslagi</category>
    </item>
    <item>
      <title>AWS Certified Solutions Architect Professional SAP-C01 Exam Questions Part 5</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 14:58:33 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-5-3kog</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-5-3kog</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;For AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company plans to migrate to AWS. A solutions architect uses AWS Application Discovery Service over the fleet and discovers that there is an Oracle data warehouse and several PostgreSQL databases. Which combination of migration patterns will reduce licensing costs and operational overhead? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Lift and shift the Oracle data warehouse to Amazon EC2 using AWS DMS.
       B. Migrate the Oracle data warehouse to Amazon Redshift using AWS SCT and AWS DMS
       C. Lift and shift the PostgreSQL databases to Amazon EC2 using AWS DMS.
       D. Migrate the PostgreSQL databases to Amazon RDS for PostgreSQL using AWS DMS.
       E. Migrate the Oracle data warehouse to an Amazon EMR managed cluster using AWS DMS.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A solutions architect needs to define a reference architecture for a solution for three-tier applications with web, application, and NoSQL data layers. The reference architecture must meet the following requirements:&lt;br&gt;
‚Äì High availability within an AWS Region.&lt;br&gt;
‚Äì Able to fail over in 1 minute to another AWS Region for disaster recovery.&lt;br&gt;
‚Äì Provide the most efficient solution while minimizing the impact on the user experience.&lt;br&gt;
Which combination of steps will meet these requirements? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use an Amazon Route 53 weighted routing policy set to 100/0 across the two selected Regions. Set Time to Live (TTL) to 1 hour.
       B. Use an Amazon Route 53 failover routing policy for failover from the primary Region to the disaster recovery Region. Set Time to Live (TTL) to 30 seconds.
       C. Use a global table within Amazon DynamoDB so data can be accessed in the two selected Regions.
       D. Back up data from an Amazon DynamoDB table in the primary Region every 60 minutes and then write the data to Amazon S3. Use S3 cross-Region replication to copy the data from the primary Region to the disaster recovery Region. Have a script import the data into DynamoDB in a disaster recovery scenario.
       E. Implement a hot standby model using Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use zonal Reserved Instances for the minimum number of servers and On-Demand Instances for any additional resources. Use Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use Spot Instances for the required resources.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A D E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has a Microsoft SQL Server database in its data center and plans to migrate data to Amazon Aurora MySQL. The company has already used the AWS Schema Conversion Tool to migrate triggers, stored procedures and other schema objects to Aurora MySQL. The database contains 1 TB of data and grows less than 1 MB per day. The company‚Äôs data center is connected to AWS through a dedicated 1Gbps AWS Direct Connect connection. The company would like to migrate data to Aurora MySQL and perform reconfigurations with minimal downtime to the applications. Which solution meets the company‚Äôs requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Shut down applications over the weekend. Create an AWS DMS replication instance and task to migrate existing data from SQL Server to Aurora MySQL. Perform application testing and migrate the data to the new database endpoint.
       B. Create an AWS DMS replication instance and task to migrate existing data and ongoing replication from SQL Server to Aurora MySQL. Perform application testing and migrate the data to the new database endpoint.
       C. Create a database snapshot of SQL Server on Amazon S3. Restore the database snapshot from Amazon S3 to Aurora MySQL. Create an AWS DMS replication instance and task for ongoing replication from SQL Server to Aurora MySQL. Perform application testing and migrate the data to the new database endpoint.
       D. Create a SQL Server native backup file on Amazon S3. Create an AWS DMS replication instance and task to restore the SQL Server backup file to Aurora MySQL. Create another AWS DMS task for ongoing replication from SQL Server to Aurora MySQL. Perform application testing and migrate the data to the new database endpoint.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company runs an application on a fleet of Amazon EC2 instances. The application requires low latency and random access to 100 GB of data. The application must be able to access the data at up to 3.000 IOPS. A Development team has configured the EC2 launch template to provision a 100-GB Provisioned IOPS (PIOPS) Amazon EBS volume with 3 000 IOPS provisioned. A Solutions Architect is tasked with lowering costs without impacting performance and durability. Which action should be taken?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an Amazon EFS file system with the performance mode set to Max I/O. Configure the EC2 operating system to mount the EFS file system.
       B. Create an Amazon EFS file system with the throughput mode set to Provisioned. Configure the EC2 operating system to mount the EFS file system.
       C. Update the EC2 launch template to allocate a new 1-TB EBS General Purpose SSO (gp2) volume.                
       D. Update the EC2 launch template to exclude the PIOPS volume. Configure the application to use local instance storage.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company recently transformed its legacy infrastructure provisioning scripts to AWS CloudFormation templates. The newly developed templates are hosted in the company‚Äôs private GitHub repository. Since adopting CloudFormation, the company has encountered several issues with updates to the CloudFormation templates, causing execution or creating an environment. Management is concerned by the increase in errors and has asked a Solutions Architect to design the automated testing of CloudFormation template updates. What should the Solution Architect do to meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use AWS CodePipeline to create a change set from the CloudFormation templates stored in the private GitHub repository. Execute the change set using AWS CodeDeploy. Include a CodePipeline action to test the deployment with testing scripts run by AWS CodeBuild.
       B. Mirror the GitHub repository to AWS CodeCommit using AWS Lambda. Use AWS CodeDeploy to create a change set from the CloudFormation templates and execute it. Have CodeDeploy test the deployment with testing scripts run by AWS CodeBuild.
       C. Use AWS CodePipeline to create and execute a change set from the CloudFormation templates stored in the GitHub repository. Configure a CodePipeline action to be deployed with testing scripts run by AWS CodeBuild.
       D. Mirror the GitHub repository to AWS CodeCommit using AWS Lambda. Use AWS CodeBuild to create a change set from the CloudFormation templates and execute it. Have CodeBuild test the deployment with testing scripts.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has several Amazon EC2 instances to both public and private subnets within a VPC that is not connected to the corporate network. A security group associated with the EC2 instances allows the company to use the Windows remote desktop protocol (RDP) over the internet to access the instances. The security team has noticed connection attempts from unknown sources. The company wants to implement a more secure solution to access the EC2 instances. Which strategy should a solutions architect implement?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Deploy a Linux bastion host on the corporate network that has access to all instances in the VPC.
       B. Deploy AWS Systems Manager Agent on the EC2 instances. Access the EC2 instances using Session Manager restricting access to users with permission.
       C. Deploy a Linux bastion host with an Elastic IP address in the public subnet. Allow access to the bastion host from 0.0.0.0/0.
       D. Establish a Site-to-Site VPN connecting the corporate network to the VPC. Update the security groups to allow access from the corporate network only.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A retail company has a custom .NET web application running on AWS that uses Microsoft SQL Server for the database. The application servers maintain a user‚Äôs session locally. Which combination of architecture changes are needed to ensure all tiers of the solution are highly available? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Refactor the application to store the user‚Äôs session in Amazon ElastiCache. Use Application Load Balancers to distribute the load between application instances.
       B. Set up the database to generate hourly snapshots using Amazon EBS. Configure an Amazon CloudWatch Events rule to launch a new database instance if the primary one fails.
       C. Migrate the database to Amazon RDS for SQL Server. Configure the RDS instance to use a MultiAZ deployment.
       D. Move the .NET content to an Amazon S3 bucket. Configure the bucket for static website hosting.                
       E. Put the application instances in an Auto Scaling group. Configure the Auto Scaling group to create new instances if an instance becomes unhealthy.
       F. Deploy Amazon CloudFront in front of the application tier. Configure CloudFront to serve content from healthy application instances only.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A B E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to improve cost awareness for its Amazon EMR platform. The company has allocated budgets for each team‚Äôs Amazon EMR usage. When a budgetary threshold is reached, a notification should be sent by email to the budget office‚Äôs distribution list. Teams should be able to view their EMR cluster expenses to date. A solutions architect needs to create a solution that ensures the policy is proactively and centrally enforced in a multi-account environment. Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Update the AWS CloudFormation template to include the AWS::Budgets::Budget::resource with the NotificationsWithSubscribers property.
       B. Implement Amazon CloudWatch dashboards for Amazon EMR usage.
       C. Create an EMR bootstrap action that runs at startup that calls the Cost Explorer API to set the budget on the cluster with the GetCostForecast and NotificationsWithSubscribers actions.
       D. Create an AWS Service Catalog portfolio for each team. Add each team‚Äôs Amazon EMR cluster as an AWS CloudFormation template to their Service Catalog portfolio as a Product.
       E. Create an Amazon CloudWatch metric for billing. Create a custom alert when costs exceed the budgetary threshold.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is migrating its on-premises systems to AWS. The user environment consists of the following systems:&lt;br&gt;
‚Äì Windows and Linux virtual machines running on VMware.&lt;br&gt;
‚Äì Physical servers running Red Hat Enterprise Linux.&lt;br&gt;
‚Äì The company wants to be able to perform the following steps before migrating to AWS:&lt;br&gt;
‚Äì Identify dependencies between on-premises systems.&lt;br&gt;
‚Äì Group systems together into applications to build migration plans.&lt;br&gt;
‚Äì Review performance data using Amazon Athena to ensure that Amazon EC2 instances are right-sized.&lt;br&gt;
How can these requirements be met?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Populate the AWS Application Discovery Service import template with information from an on premises configuration management database (CMDB). Upload the completed import template to Amazon S3, then import the data into Application Discovery Service.
       B. Install the AWS Application Discovery Service Discovery Agent on each of the on-premises systems. Allow the Discovery Agent to collect data for a period of time.
       C. Install the AWS Application Discovery Service Discovery Connector on each of the on-premises systems and in VMware vCenter. Allow the Discovery Connector to collect data for one week.
       D. Install the AWS Application Discovery Service Discovery Agent on the physical on-pre-map servers. Install the AWS Application Discovery Service Discovery Connector in VMware vCenter. Allow the Discovery Agent to collect data for a period of time.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to migrate its on-premises data center to the AWS Cloud. This includes thousands of virtualized Linux and Microsoft Windows servers, SAN storage, Java and PHP applications with MYSQL, and Oracle databases. There are many department services hosted either in the same data center or externally. The technical documentation is incomplete and outdated. A solutions architect needs to understand the current environment and estimate the cloud resource costs after the migration. Which tools or services should solutions architects use to plan the cloud migration (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. AWS Application Discovery Service
       B. AWS SMS
       C. AWS x-Ray
       D. AWS Cloud Adoption Readness Tool (CART)
       E. Amazon Inspector
       F. AWS Migration Hub
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B C F&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company decided to purchase Amazon EC2 Reserved Instances. A solutions architect is tasked with implementing a solution where only the master account in AWS Organizations is able to purchase the Reserved Instances. Current and future member accounts should be blocked from purchasing Reserved Instances. Which solution will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an SCP with the Deny effect on the ec2:PurchaseReservedInstancesOffering action. Attach the SCP to the root of the organization.
       B. Create a new organizational unit (OU) Move all current member accounts to the new OU. Create an SCP with the Deny effect on the ec2:PurchaseReservedInstancesOffering action. Attach the SCP to the new OU.
       C. Create an AWS Config rule event that triggers automation that will terminate any Reserved Instances launched by member accounts.
       D. Create two new organizational units (OUs): OU1 and OU2. Move all member accounts to OU2 and the master account to OU1. Create an SCP with the Allow effect on the ec2:PurchaseReservedInstancesOffering action. Attach the SCP to OU1.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is using multiple AWS accounts. The DNS records are stored in a private hosted zone for Amazon Route 53 in Account                A. The company‚Äôs applications and databases are running in Account                B. A solutions architect will deploy a two-tier application in a new VPC. To simplify the configuration, the db.example.com CNAME record set for the Amazon RDS endpoint was created in a private hosted zone for Amazon Route 53. During deployment the application failed to start. Troubleshooting revealed that db.example.com is not resolvable on the Amazon EC2 instance. The solutions architect confirmed that the record set was created correctly in Route 53. Which combination of steps should the solutions architect take to resolve this issue? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Deploy the database on a separate EC2 instance in the new VPC. Create a record set for the instance‚Äôs private IP in the private hosted zone.
       B. Use SSH to connect to the application tier EC2 instance. Add an RDS endpoint IP address to the /etc/resolv conf file.
       C. Create an authorization to associate the private hosted zone in Account A with the new VPC in Account                B.
       D. Create a private hosted zone for the example com domain in Account                B. Configure Route 53 replication between AWS accounts.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A solutions architect needs to advise a company on how to migrate its on-premises data processing application to the AWS Cloud. Currently, users upload input files through a web portal. The web server then stores the uploaded files on NAS and messages the processing server over a message queue. Each media file can take up to 1 hour to process. The company has determined that the number of media files awaiting processing is significantly higher during business hours, with the number of files rapidly declining after business hours. What is the MOST cost-effective migration recommendation?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in an Amazon S3 bucket.
       B. Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, create a new Amazon EC2 instance to pull requests from the queue and process the files. Store the processed files in Amazon EFS. Shut down the EC2 instance after the task is complete.
       C. Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in Amazon EFS.
       D. Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. Use Amazon EC2 instances in an EC2 Auto Seating group to pull requests from the queue and process the files. Scale the EC2 instances based on the SQS queue length. Store the processed files in an Amazon S3 bucket.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A utility company wants to collect usage data every 5 minutes from its smart meters to facilitate time-of-use metering. When a meter sends data to AWS, the data is sent to Amazon API Gateway, processed by an AWS Lambda function and stored in an Amazon DynamoDB table. During the pilot phase, the Lambda functions took from 3 to 5 seconds to complete. As more smart meters are deployed, the Engineers notice the Lambda functions are taking from 1 to 2 minutes to complete. The functions are also increasing in duration as new types of metrics are collected from the devices. There are many ProvisionedThroughputExceededException errors while performing PUT operations on DynamoDB, and there are also many TooManyRequestsException errors from Lambda. Which combination of changes will resolve these issues? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Increase the write capacity units to the DynamoDB table.
       B. Increase the memory available to the Lambda functions.
       C. Increase the payload size from the smart meters to send more data.
       D. Stream the data into an Amazon Kinesis data stream from API Gateway and process the data in batches.
       E. Collect data in an Amazon SQS FIFO queue, which triggers a Lambda function to process each message.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An AWS partner company is building a service in AWS Organizations using its organization named org1. This service requires the partner company to have access to AWS resources in a customer account, which is in a separate organization named org2. The company must establish least privilege security access using an API or command line tool to the customer account. What is the MOST secure way to allow org1 to access resources in org2?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. The customer should provide the partner company with their AWS account access keys to log in and perform the required tasks.
       B. The customer should create an IAM user and assign the required permissions to the IAM user. The customer should then provide the credentials to the partner company to log in and perform the required tasks.
       C. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role‚Äôs Amazon Resource Name (ARN) when requesting access to perform the required tasks.
       D. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role‚Äôs Amazon Resource Name (ARN), including the external ID in the IAM role‚Äôs trust policy, when requesting access to perform the required tasks.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company‚Äôs security compliance requirements state that all Amazon EC2 images must be scanned for vulnerabilities and must pass a CVE assessment. A solutions architect is developing a mechanism to create security- approved AMIs that can be used by developers. Any new AMIs should go through an automated assessment process and be marked as approved before developers can use them. The approved images must be scanned every 30 days to ensure compliance. Which combination of steps should the solutions architect take to meet these requirements while following best practices? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use the AWS Systems Manager EC2 agent to run the CVE assessment on the EC2 instances launched from the AMIs that need to be scanned.
       B. Use AWS Lambda to write automatic approval rules. Store the approved AMI list in AWS Systems Manager Parameter Store. Use Amazon EventBridge to trigger an AWS Systems Manager Automation document on all EC2 instances every 30 days.
       C. Use Amazon Inspector to run the CVE assessment on the EC2 instances launched from the AMIs that need to be scanned.
       D. Use AWS Lambda to write automatic approval rules. Store the approved AMI list in AWS Systems Manager Parameter Store. Use a managed AWS Config rule for continuous scanning on all EC2 instances, and use AWS Systems Manager Automation documents for remediation.
       E. Use AWS CloudTrail to run the CVE assessment on the EC2 instances launched from the AMIs that need to be scanned.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company uses AWS Organizations with a single OU named Production to manage multiple accounts. All accounts are members of the Production OU. Administrators use deny list SCPs in the root of the organization to manage access to restricted services. The company recently acquired a new business unit and invited the new unit‚Äôs existing AWS account to the organization. Once onboarded, the administrators of the new business unit discovered that they are not able to update existing AWS Config rules to meet the company‚Äôs policies. Which option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Remove the organization‚Äôs root SCPs that limit access to AWS Config. Create AWS Service Catalog products for the company‚Äôs standard AWS Config rules and deploy them throughout the organization, including the new account.
       B. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the new account to the Production OU when adjustments to AWS Config are complete.
       C. Convert the organization‚Äôs root SCPs from deny list SCPs to allow list SCPs to allow the required services only. Temporally apply an SCP to the organization‚Äôs root that allows AWS Config actions for principals only in the new account.
       D. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization‚Äôs root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is launching a web-based application in multiple regions around the world. The application consists of both static content stored in a private Amazon S3 bucket and dynamic content hosted in Amazon ECS containers content behind an Application Load Balancer (ALB). The company requires that the static and dynamic application content be accessible through Amazon CloudFront only. Which combination of steps should a solutions architect recommend to restrict direct content access to CloudFront? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a web ACL in AWS WAF with a rule to validate the presence of a custom header and associate the web ACL with the ALB.
       B. Create a web ACL in AWS WAF with a rule to validate the presence of a custom header and associate the web ACL with the CloudFront distribution.
       C. Configure CloudFront to add a custom header to origin requests.
       D. Configure the ALB to add a custom header to HTTP requests.
       E. Update the S3 bucket ACL to allow access from the CloudFront distribution only.
       F. Create a CloudFront Origin Access Identity (OAI) and add it to the CloudFront distribution. Update the S3 bucket policy to allow access to the OAI only.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A D F&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An ecommerce website running on AWS uses an Amazon RDS for MySQL DB instance with General Purpose SSD storage. The developers chose an appropriate instance type based on demand, and configured 100 GB of storage with a sufficient amount of free space. The website was running smoothly for a few weeks until a marketing campaign launched. On the second day of the campaign, users reported long wait times and time outs. Amazon CloudWatch metrics indicated that both reads and writes to the DB instance were experiencing long response times. The CloudWatch metrics show 40% to 50% CPU and memory utilization, and sufficient free storage space is still available. The application server logs show no evidence of database connectivity issues. What could be the root cause of the issue with the marketing campaign?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. It exhausted the I/O credit balance due to provisioning low disk storage during the setup phase.
       B. It caused the data in the tables to change frequently, requiring indexes to be rebuilt to optimize queries.
       C. It exhausted the maximum number of allowed connections to the database instance.
       D. It exhausted the network bandwidth available to the RDS for MySQL DB instances.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A solutions architect has been assigned to migrate a 50 TB Oracle data warehouse that contains sales data from on-premises to Amazon Redshift. Major updates to the sales data occur on the final calendar day of the month. For the remainder of the month, the data warehouse only receives minor daily updates and is primarily used for reading and reporting. Because of this, the migration process must start on the first day of the month and must be complete before the next set of updates occur. This provides approximately 30 days to complete the migration and ensure that the minor daily changes have been synchronized with the Amazon Redshift data warehouse. Because the migration cannot impact normal business network operations, the bandwidth allocated to the migration for moving data over the internet is 50 Mbps. The company wants to keep data migration costs low. Which steps will allow the solutions architect to perform the migration within the specified timeline?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Install Oracle database software on an Amazon EC2 instance. Configure VPN connectivity between AWS and the company‚Äôs data center. Configure the Oracle database running on Amazon EC2 to join the Oracle Real Application Clusters (RAC). When the Oracle database on Amazon EC2 finishes synchronizing, create an AWS DMS ongoing replication task to migrate the data from the Oracle database on Amazon EC2 to Amazon Redshift. Verify the data migration is complete and perform the cut over to Amazon Redshift.
       B. Create an AWS Snowball import job. Export a backup of the Oracle data warehouse. Copy the exported data to the Snowball device. Return the Snowball device to AWS. Create an Amazon RDS for Oracle database and restore the backup file to that RDS instance. Create an AWS DMS task to migrate the data from the RDS for Oracle database to Amazon Redshift. Copy daily incremental backups from Oracle in the data center to the RDS for Oracle database over the internet. Verify the data migration is complete and perform the cut over to Amazon Redshift.
       C. Install Oracle database software on an Amazon EC2 instance. To minimize the migration time, configure VPN connectivity between AWS and the company‚Äôs data center by provisioning a 1 Gbps AWS Direct Connect connection. Configure the Oracle database running on Amazon EC2 to be a read replica of the data center Oracle database. Start the synchronization process between the company‚Äôs on-premises data center and the Oracle database on Amazon EC2. When the Oracle database on Amazon EC2 is synchronized with the on-premises database, create an AWS DMS ongoing replication task to migrate the data from the Oracle database read replica that is running on Amazon EC2 to Amazon Redshift. Verify the data migration is complete and perform the cut over to Amazon Redshift.
       D. Create an AWS Snowball import job. Configure a server in the company‚Äôs data center with an extraction agent. Use AWS SCT to manage the extraction agent and convert the Oracle schema to an Amazon Redshift schema. Create a new project in AWS SCT using the registered data extraction agent. Create a local task and an AWS DMS task in AWS SCT with replication of ongoing changes. Copy data to the Snowball device and return the Snowball device to AWS. Allow AWS DMS to copy data from Amazon S3 to Amazon Redshift. Verify that the data migration is complete and perform the cut over to Amazon Redshift.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

</description>
      <category>awslagi</category>
      <category>googlecloud</category>
      <category>aws</category>
    </item>
    <item>
      <title>AWS Certified Solutions Architect Professional SAP-C01 Exam Questions Part 4</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 14:54:28 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-4-407b</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-4-407b</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;For AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A solutions architect is designing a disaster recovery strategy for a three-tier application. The application has an RTO of 30 minutes and an RPO of 5 minutes for the data tier. The application and web tiers are stateless and leverage a fleet of Amazon EC2 instances. The data tier consists of a 50 TB Amazon Aurora database. Which combination of steps satisfies the RTO and RPO requirements while optimizing costs? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create daily snapshots of the EC2 instances and replicate the snapshots to another Region.
       B. Deploy a hot standby of the application to another Region.
       C. Create snapshots of the Aurora database every 5 minutes.
       D. Create a cross-Region Aurora Replica of the database.
       E. Create an AWS Backup job to replicate data to another Region.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has a primary Amazon S3 bucket that receives thousands of objects every day. The company needs to replicate these objects into several other S3 buckets from various AWS accounts. A solutions architect is designing a new AWS Lambda function that is triggered when an object is created in the main bucket and replicates the object into the target buckets. The objects do not need to be replicated in real time. There is concern that this function may impact other critical Lambda functions due to Lambda‚Äôs regional concurrency limit. How can the solutions architect ensure this new Lambda function will not impact other critical Lambda functions?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Set the new Lambda function reserved concurrency limit to ensure the executions do not impact other critical Lambda functions. Monitor existing critical Lambda functions with Amazon CloudWatch alarms for the Throttles Lambda metric.
       B. Increase the execution timeout of the new Lambda function to 5 minutes. Monitor existing critical Lambda functions with Amazon CloudWatch alarms for the Throttles Lambda metric.
       C. Configure S3 event notifications to add events to an Amazon SQS queue in a separate account. Create the new Lambda function in the same account as the SQS queue and trigger the function when a message arrives in the queue.
       D. Ensure the new Lambda function implements an exponential backoff algorithm. Monitor existing critical Lambda functions with Amazon CloudWatch alarms for the Throttles Lambda metric.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to run a serverless application on AWS. The company plans to provision its application in Docker containers running in an Amazon ECS cluster. The application requires a MySQL database and the company plans to use Amazon RDS. The company has documents that need to be accessed frequently for the first 3 months, and rarely after that. The document must be retained for 7 years. What is the MOST cost-effective solution to meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an ECS cluster using On-Demand Instances. Provision the database and its read replicas in Amazon RDS using Spot Instances. Store the documents in an encrypted EBS volume, and create a cron job to delete the documents after 7 years.
       B. Create an ECS cluster using a fleet of Spot Instances, with Spot Instance draining enabled. Provision the database and its read replicas in Amazon RDS using Reserved Instances. Store the documents in a secured Amazon S3 bucket with a lifecycle policy to move the documents that are older than 3 months to Amazon S3 Glacier, then delete the documents from Amazon S3 Glacier that are more than 7 years old.
       C. Create an ECS cluster using On-Demand Instances. Provision the database and its read replicas in Amazon RDS using On-Demand Instances. Store the documents in Amazon EFS. Create a cron job to move the documents that are older than 3 months to Amazon S3 Glacier. Create an AWS Lambda function to delete the documents in S3 Glacier that are older than 7 years.
       D. Create an ECS cluster using a fleet of Spot Instances with Spot Instance draining enabled. Provision the database and its read replicas in Amazon RDS using On-Demand Instances. Store the documents in a secured Amazon S3 bucket with a lifecycle policy to move the documents that are older than 3 months to Amazon S3 Glacier, then delete the documents in Amazon S3 Glacier after 7 years.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A financial services company receives a regular data feed from its credit card servicing partner. Approximately 5,000 records are sent every 15 minutes in plaintext, delivered over HTTPS directly into an Amazon S3 bucket with server-side encryption. This feed contains sensitive credit card primary account number (PAN) data. The company needs to automatically mask the PAN before sending the data to another S3 bucket for additional internal processing. The company also needs to remove and merge specific fields, and then transform the record into JSON format. Additionally, extra feeds are likely to be added in the future, so any design needs to be easily expandable. Which solutions will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Trigger an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Trigger another Lambda function when new messages arrive in the SQS queue to process the records, writing the results to a temporary location in Amazon S3. Trigger a final Lambda function once the SQS queue is empty to transform the records into JSON format and send the results to another S3 bucket for internal processing.
       B. Trigger an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Configure an AWS Fargate container application to automatically scale to a single instance when the SQS queue contains messages. Have the application process each record, and transform the record into JSON format. When the queue is empty, send the results to another S3 bucket for internal processing and scale down the AWS Fargate instance.
       C. Create an AWS Glue crawler and custom classifier based on the data feed formats and build a table definition to match. Trigger an AWS Lambda function on file delivery to start an AWS Glue ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, have the ETL job send the results to another S3 bucket for internal processing.
       D. Create an AWS Glue crawler and custom classifier based upon the data feed formats and build a table definition to match. Perform an Amazon Athena query on file delivery to start an Amazon EMR ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, send the results to another S3 bucket for internal processing and scale down the EMR cluster.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A media company is serving video files stored in Amazon S3 using Amazon CloudFront. The development team needs access to the logs to diagnose faults and perform service monitoring. The log files from CloudFront may contain sensitive information about users. The company uses a log processing service to remove sensitive information before making the logs available to the development team. The company has the following requirements for the unprocessed logs:&lt;br&gt;
‚Äì The logs must be encrypted at rest and must be accessible by the log processing service only.&lt;br&gt;
‚Äì Only the data protection team can control access to the unprocessed log files.&lt;br&gt;
‚Äì AWS CloudFormation templates must be stored in AWS CodeCommit.&lt;br&gt;
‚Äì AWS CodePipeline must be triggered on commit to perform updates made to CloudFormation templates.&lt;br&gt;
‚Äì CloudFront is already writing the unprocessed logs to an Amazon S3 bucket, and the log processing service is operating against this S3 bucket.&lt;br&gt;
Which combination of steps should a solutions architect take to meet the company‚Äôs requirements? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an AWS KMS key that allows the AWS Logs Delivery account to generate data keys for encryption Configure S3 default encryption to use server-side encryption with KMS managed keys (SSEKMS) on the log storage bucket using the new KMS key. Modify the KMS key policy to allow the log processing service to perform decrypt operations.
       B. Create an AWS KMS key that follows the CloudFront service role to generate data keys for encryption Configure S3 default encryption to use KMS managed keys (SSE-KMS) on the log storage bucket using the new KMS key Modify the KMS key policy to allow the log processing service to perform decrypt operations.
       C. Configure S3 default encryption to use AWS KMS managed keys (SSE-KMS) on the log storage bucket using the AWS Managed S3 KMS key. Modify the KMS key policy to allow the CloudFront service role to generate data keys for encryption Modify the KMS key policy to allow the log processing service to perform decrypt operations.
       D. Create a new CodeCommit repository for the AWS KMS key template. Create an IAM policy to allow commits to the new repository and attach it to the data protection team‚Äôs users. Create a new CodePipeline pipeline with a custom IAM role to perform KMS key updates using CloudFormation Modify the KMS key policy to allow the CodePipeline IAM role to modify the key policy.
       E. Use the existing CodeCommit repository for the AWS KMS key template. Create an IAM policy to allow commits to the new repository and attach it to the data protection team‚Äôs users. Modify the existing CodePipeline pipeline to use a custom IAM role and to perform KMS key updates using CloudFormation. Modify the KMS key policy to allow the CodePipeline IAM role to modify the key policy.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company‚Äôs service for video game recommendations has just gone viral. The company has new users from all over the world. The website for the service is hosted on a set of Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The website consists of static content with different resources being loaded depending on the device type. Users recently reported that the load time for the website has increased. Administrators are reporting high loads on the EC2 instances that host the service. Which set actions should a solutions architect take to improve response times?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create separate Auto Scaling groups based on device types. Switch to Network Load Balancer (NLB). Use the User-Agent HTTP header in the NLB to route to a different set of EC2 instances.
       B. Move content to Amazon S3. Create an Amazon CloudFront distribution to serve content out of the S3 bucket. Use Lambda@Edge to load different resources based on the User-Agent HTTP header.
       C. Create a separate ALB for each device type. Create one Auto Scaling group behind each ALB. Use Amazon Route 53 to route to different ALBs depending on the User-Agent HTTP header.
       D. Move content to Amazon S3. Create an Amazon CloudFront distribution to serve content out of the S3 bucket. Use the User-Agent HTTP header to load different content.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is planning a large event where a promotional offer will be introduced. The company‚Äôs website is hosted on AWS and backed by an Amazon RDS for PostgreSQL DB instance. The website explains the promotion and includes a sign-up page that collects user information and preferences. Management expects large and unpredictable volumes of traffic periodically, which will create many database writes. A solutions architect needs to build a solution that does not change the underlying data model and ensures that submissions are not dropped before they are committed to the database. Which solution meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Immediately before the event, scale up the existing DB instance to meet the anticipated demand. Then scale down after the event.
       B. Use Amazon SQS to decouple the application and database layers. Configure an AWS Lambda function to write items from the queue into the database.
       C. Migrate to Amazon DynamoDB and manage throughput capacity with automatic scaling.
       D. Use Amazon ElastiCache for Memcached to increase write capacity to the DB instance.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A mobile app has become very popular, and usage has gone from a few hundred to millions of users. Users capture and upload images of activities within a city, and provide ratings and recommendations. Data access patterns are unpredictable. The current application is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The application is experiencing slowdowns and costs are growing rapidly. Which changes should a solutions architect make to the application architecture to control costs and improve performance?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an Amazon CloudFront distribution and place the ALB behind the distribution. Store static content in Amazon S3 in an Infrequent Access storage class.
       B. Store static content in an Amazon S3 bucket using the Intelligent Tiering storage class. Use an Amazon CloudFront distribution in front of the S3 bucket and the ALB.
       C. Place AWS Global Accelerator in front of the ALB. Migrate the static content to Amazon EFS, and then run an AWS Lambda function to resize the images during the migration process.
       D. Move the application code to AWS Fargate containers and swap out the EC2 instances with the Fargate containers.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A financial company with multiple departments wants to expand its on-premises environment to the AWS Cloud. The company must retain centralized access control using an existing on premises Active Directory (AD) service. Each department should be allowed to create AWS accounts with preconfigured networking and should have access to only a specific list of approved services. Departments are not permitted to have account administrator permissions. What should a solutions architect do to meet these security requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure AWS Identity and Access Management (IAM) with a SAML identity provider (IdP) linked to the on-premises Active Directory, and create a role to grant access. Configure AWS Organizations with SCPs and create new member accounts. Use AWS CloudFormation templates to configure the member account networking.
       B. Deploy an AWS Control Tower landing zone. Create an AD Connector linked to the on-premises Active Directory. Change the identity source in AWS Single Sign-On to use Active Directory. Allow department administrators to use Account Factory to create new member accounts and networking. Grant the departments AWS power user permissions on the created accounts.
       C. Deploy an Amazon Cloud Directory. Create a two-way trust relationship with the on-premises Active Directory, and create a role to grant access. Set up an AWS Service Catalog to use AWS CloudFormation templates to create the new member accounts and networking. Use IAM roles to allow access to approved AWS services.
       D. Configure AWS Directory Service for Microsoft Active Directory with AWS Single Sign-On. Join the service to the on-premises Active Directory. Use AWS CloudFormation to create new member accounts and networking. Use IAM roles to allow access to approved AWS services.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A large financial company is deploying applications that consist of Amazon EC2 and Amazon RDS instances to the AWS Cloud using AWS CloudFormation. The CloudFormation stack has the following stack policy:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The company wants to ensure that developers do not lose data by accidentally removing or replacing RDS instances when updating the CloudFormation stack. Developers also still need to be able to modify or remove EC2 instances as needed. How should the company change the stack policy to meet these requirements?&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;           A. Modify the statement to specify ‚ÄúEffect‚Äù: ‚ÄúDeny‚Äù, ‚ÄúAction‚Äù:[‚ÄúUpdate:*‚Äù] for all logical RDS resources.                B. Modify the statement to specify ‚ÄúEffect‚Äù: ‚ÄúDeny‚Äù, ‚ÄúAction‚Äù:[‚ÄúUpdate:Delete‚Äù] for all logical RDS resources.
           C. Add a second statement that specifies ‚ÄúEffect‚Äù: ‚ÄúDeny‚Äù, ‚ÄúAction‚Äù:[‚ÄúUpdate:Delete‚Äù, ‚ÄúUpdate:Replace‚Äù] for all logical RDS resources.
           D. Add a second statement that specifies ‚ÄúEffect‚Äù: ‚ÄúDeny‚Äù, ‚ÄúAction‚Äù:[‚ÄúUpdate:*‚Äù] for all logical RDS resources.
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is currently in the design phase of an application that will need an RPO of less than 5 minutes and an RTO of less than 10 minutes. The solutions architecture team is forecasting that the database will store approximately 10 TB of data. As part of the design, they are looking for a database solution that will provide the company with the ability to fail over to a secondary Region. Which solution will meet these business requirements at the LOWEST cost?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Deploy an Amazon Aurora DB cluster and take snapshots of the cluster every 5 minutes. Once a snapshot is complete, copy the snapshot to a secondary Region to serve as a backup in the event of a failure.
       B. Deploy an Amazon RDS instance with a cross-Region read replica in a secondary Region. In the event of a failure, promote the read replica to become the primary.
       C. Deploy an Amazon Aurora DB cluster in the primary Region and another in a secondary Region. Use AWS DMS to keep the secondary Region in sync.
       D. Deploy an Amazon RDS instance with a read replica in the same Region. In the event of a failure, promote the read replica to become the primary.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has a web application that uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. A recent marketing campaign has increased demand. Monitoring software reports that many requests have significantly longer response times than before the marketing campaign. A solutions architect enabled Amazon CloudWatch Logs for API Gateway and noticed that errors are occurring on 20% of the requests. In CloudWatch, the Lambda function Throttles metric represents 1% of the requests and the Errors metric represents 10% of the requests. Application logs indicate that, when errors occur, there is a call to DynamoDB. What change should the solutions architect make to improve the current response times as the web application becomes more popular?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Increase the concurrency limit of the Lambda function
       B. Implement DynamoDB auto scaling on the table
       C. Increase the API Gateway throttle limit
       D. Re-create the DynamoDB table with a better-partitioned primary index
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A European online newspaper service hosts its public-facing WordPress site in a collocated data center in London. The current WordPress infrastructure consists of a load balancer, two web servers, and one MySQL database server. A solutions architect is tasked with designing a solution with the following requirements:&lt;br&gt;
‚Äì Improve the website‚Äôs performance&lt;br&gt;
‚Äì Make the web tier scalable and stateless&lt;br&gt;
‚Äì Improve the database server performance for read-heavy loads&lt;br&gt;
‚Äì Reduce latency for users across Europe and the US Design the new architecture with a goal of 99.9% availability&lt;br&gt;
Which solution meets these requirements while optimizing operational efficiency?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use an Application Load Balancer (ALB) in front of an Auto Scaling group of WordPress Amazon EC2 instances in one AWS Region and three Availability Zones. Configure an Amazon ElastiCache cluster in front of a Multi-AZ Amazon Aurora MySQL DB cluster. Move the WordPress shared files to Amazon EFS. Configure Amazon CloudFront with the ALB as the origin, and select a price class that includes the US and Europe.
       B. Use an Application Load Balancer (ALB) in front of an Auto Scaling group of WordPress Amazon EC2 instances in two AWS Regions and two Availability Zones in each Region. Configure an Amazon ElastiCache cluster in front of a global Amazon Aurora MySQL database. Move the WordPress shared files to Amazon EFS. Configure Amazon CloudFront with the ALB as the origin, and select a price class that includes the US and Europe. Configure EFS cross-Region replication.
       C. Use an Application Load Balancer (ALB) in front of an Auto Scaling group of WordPress Amazon EC2 instances in one AWS Region and three Availability Zones. Configure an Amazon DocumentDB table in front of a Multi-AZ Amazon Aurora MySQL DB cluster. Move the WordPress shared files to Amazon EFS. Configure Amazon CloudFront with the ALB as the origin, and select a price class that includes all global locations.
       D. Use an Application Load Balancer (ALB) in front of an Auto Scaling group of WordPress Amazon EC2 instances in two AWS Regions and three Availability Zones in each Region. Configure an Amazon ElastiCache cluster in front of a global Amazon Aurora MySQL database. Move the WordPress shared files to Amazon FSx with cross-Region synchronization. Configure Amazon CloudFront with the ALB as the origin and a price class that includes the US and Europe.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company built an ecommerce website on AWS using a three-tier web architecture. The application is Java-based and composed of an Amazon CloudFront distribution, an Apache web server layer of Amazon EC2 instances in an Auto Scaling group, and a backend Amazon Aurora MySQL database. Last month, during a promotional sales event, users reported errors and timeouts while adding items to their shopping carts. The operations team recovered the logs created by the web servers and reviewed Aurora DB cluster performance metrics. Some of the web servers were terminated before logs could be collected and the Aurora metrics were not sufficient for query performance analysis. Which combination of steps must the solutions architect take to improve application performance visibility during peak traffic events? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure the Aurora MySQL DB cluster to publish slow query and error logs to Amazon CloudWatch Logs.
       B. Implement the AWS X-Ray SDK to trace incoming HTTP requests on the EC2 instances and implement tracing of SQL queries with the X-Ray SDK for Java.
       C. Configure the Aurora MySQL DB cluster to stream slow query and error logs to Amazon Kinesis
       D. Install and configure an Amazon CloudWatch Logs agent on the EC2 instances to send the Apache logs to CloudWatch Logs.
       E. Enable and configure AWS CloudTrail to collect and analyze application activity from Amazon EC2 and Aurora.                
       F. Enable Aurora MySQL DB cluster performance benchmarking and publish the stream to AWS XRay.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B C E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A solutions architect has an operational workload deployed on Amazon EC2 instances in an Auto Scaling group. The VPC architecture spans two Availability Zones (AZ) with a subnet in each that the Auto Scaling group is targeting. The VPC is connected to an on-premises environment and connectivity cannot be interrupted. The maximum size of the Auto Scaling group is 20 instances in service. The VPC IPv4 addressing is as follows:&lt;br&gt;
‚Äì VPC CIDR: 10.0.0.0/23&lt;br&gt;
‚Äì AZ1 subnet CIDR: 10.0.0.0/24&lt;br&gt;
‚Äì AZ2 subnet CIDR: 10.0.1.0/24&lt;br&gt;
Since deployment, a third AZ has become available in the Region. The solutions architect wants to adopt the new AZ without adding additional IPv4 address space and without service downtime. Which solution will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Update the Auto Scaling group to use the AZ2 subnet only. Delete and re-create the AZ1 subnet using half the previous address space. Adjust the Auto Scaling group to also use the new AZ1 subnet. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Remove the current AZ2 subnet. Create a new AZ2 subnet using the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets.
       B. Terminate the EC2 instances in the AZ1 subnet. Delete and re-create the AZ1 subnet using half the address space. Update the Auto Scaling group to use this new subnet. Repeat this for the second AZ. Define a new subnet in AZ3, then update the Auto Scaling group to target all three new subnets.
       C. Create a new VPC with the same IPv4 address space and define three subnets, with one for each AZ. Update the existing Auto Scaling group to target the new subnets in the new VPC.
       D. Update the Auto Scaling group to use the AZ2 subnet only. Update the AZ1 subnet to have the previous address space. Adjust the Auto Scaling group to also use the AZ1 subnet again. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Update the current AZ2 subnet and assign the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is storing data on premises on a Windows file server. The company produces 5 GB of new data daily. The company migrated part of its Windows-based workload to AWS and needs the data to be available on a file system in the cloud. The company already has established an AWS Direct Connect connection between the on-premises network and AWS. Which data migration strategy should the company use?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use the file gateway option in AWS Storage Gateway to replace the existing Windows file server, and point the existing file share to the new file gateway
       B. Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon FSx
       C. Use AWS Data Pipeline to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS)
       D. Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS)
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company uses AWS Organizations to manage one parent account and nine member accounts. The number of member accounts is expected to grow as the business grows. A security engineer has requested consolidation of AWS CloudTrail logs into the parent account for compliance purposes. Existing logs currently stored in Amazon S3 buckets in each individual member account should not be lost. Future member accounts should comply with the logging strategy. Which operationally efficient solution meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an AWS Lambda function in each member account with a cross-account role. Trigger the Lambda functions when new CloudTrail logs are created and copy the CloudTrail logs to a centralized S3 bucket. Set up an Amazon CloudWatch alarm to alert if CloudTrail is not configured properly.
       B. Configure CloudTrail in each member account to deliver log events to a central S3 bucket. Ensure the central S3 bucket policy allows PutObject access from the member accounts. Migrate existing logs to the central S3 bucket. Set up an Amazon CloudWatch alarm to alert if CloudTrail is not configured properly.
       C. Configure an organization-level CloudTrail in the parent account to deliver log events to a central S3 bucket. Migrate the existing CloudTrail logs from each member account to the central S3 bucket. Delete the existing CloudTrail and logs in the member accounts.
       D. Configure an organization-level CloudTrail in the parent account to deliver log events to a central S3 bucket. Configure CloudTrail in each member account to deliver log events to the central S3 bucket.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A weather service provides high-resolution weather maps from a web application hosted on AWS in the eu-west-1 Region. The weather maps are updated frequently and stored in Amazon S3 along with static HTML content. The web application is fronted by Amazon CloudFront. The company recently expanded to serve users in the us-east-1 Region, and these new users report that viewing their respective weather maps is slow from time to time. Which combination of steps will resolve the us-east-1 performance issues? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure the AWS Global Accelerator endpoint for the S3 bucket in eu-west-1. Configure endpoint groups for TCP ports 80 and 443 in us-east-1.
       B. Create a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1.
       C. Use Lambda@Edge to modify requests from North America to use the S3 Transfer Acceleration endpoint in us-east-1.
       D. Use Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1.
       E. Configure the AWS Global Accelerator endpoint for us-east-1 as an origin on the CloudFront distribution. Use Lambda@Edge to modify requests from North America to use the new origin.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is deploying a public-facing global application on AWS using Amazon CloudFront. The application communicates with an external system. A solutions architect needs to . Which combination of steps will satisfy these requirements? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a public certificate for the required domain in AWS Certificate Manager and deploy it to CloudFront, an Application Load Balancer, and Amazon EC2 instances.
       B. Acquire a public certificate from a third-party vendor and deploy it to CloudFront, an Application Load Balancer, and Amazon EC2 instances.
       C. Provision Amazon EBS encrypted volumes using AWS KMS and ensure explicit encryption of data when writing to Amazon EBS.
       D. Provision Amazon EBS encrypted volumes using AWS KMS.
       E. Use SSL or encrypt data while communicating with the external system using a VPN.
       F. Communicate with the external system using plaintext and use the VPN to encrypt the data in transit.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A C E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company provides a centralized Amazon EC2 application hosted in a single shared VPC. The centralized application must be accessible from client applications running in the VPCs of other business units. The centralized application front end is configured with a Network Load Balancer (NLB) for scalability. Up to 10 business unit VPCs will need to be connected to the shared VPC. Some of the business unit VPC CIDR blocks overlap with the shared VPC, and some overlap with each other. Network connectivity to the centralized application in the shared VPC should be allowed from authorized business unit VPCs only. Which network configuration should a solutions architect use to provide connectivity from the client applications in the business unit VPCs to the centralized application in the shared VPC?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an AWS Transit Gateway. Attach the shared VPC and the authorized business unit VPCs to the transit gateway. Create a single transit gateway route table and associate it with all of the attached VPCs. Allow automatic propagation of routes from the attachments into the route table. Configure VPC routing tables to send traffic to the transit gateway
       B. Create a VPC endpoint service using the centralized application NLB and enable the option to require endpoint acceptance. Create a VPC endpoint in each of the business unit VPCs using the service name of the endpoint service. Accept authorized endpoint requests from the endpoint service console.
       C. Create a VPC peering connection from each business unit VPC to the shared VPC. Accept the VPC peering connections from the shared VPC console. Configure VPC routing tables to send traffic to the VPC peering connection.
       D. Configure a virtual private gateway for the shared VPC and create customer gateways for each of the authorized business unit VPCs. Establish a Site-to-Site VPN connection from the business unit VPCs to the shared VPC. Configure VPC routing tables to send traffic to the VPN connection.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

</description>
      <category>awslagi</category>
      <category>aws</category>
      <category>googlecloud</category>
    </item>
    <item>
      <title>Effortlessly install TailwindCss in a Rails app with Webpack (minimum configuration)</title>
      <author>Vernes</author>
      <pubDate>Sun, 22 Aug 2021 14:26:28 +0000</pubDate>
      <link>https://dev.to/wizardhealth/effortlessly-install-tailwindcss-in-a-rails-app-with-webpack-minimum-configuration-14gg</link>
      <guid>https://dev.to/wizardhealth/effortlessly-install-tailwindcss-in-a-rails-app-with-webpack-minimum-configuration-14gg</guid>
      <description>&lt;p&gt;A while back &lt;a href="https://twitter.com/dhh"&gt;DHH&lt;/a&gt; decided to created a &lt;a href="https://github.com/rails/tailwindcss-rails"&gt;gem&lt;/a&gt; for easily installing TailwindCss into rails apps üôåüèª. This gem could be used to install Tailwind through the asset pipeline as well as using webpack. This &lt;a href="https://github.com/rails/tailwindcss-rails/issues/62#issuecomment-900193727"&gt;changed&lt;/a&gt; later on as contributors wanted to focus their attention to what was the heart of the gem ü•∫.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://i.giphy.com/media/d10dMmzqCYqQ0/giphy.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://i.giphy.com/media/d10dMmzqCYqQ0/giphy.gif"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Well, since a lot of people used this gem to get a new Rails app going with Tailwind without the hassle of configuring everything from scratch (including colleagues from my company), &lt;a href="https://dev.to/wizardhealth"&gt;we decided&lt;/a&gt; to create a &lt;a href="https://github.com/WizardComputer/tailwindcss-rails-webpacker"&gt;new gem&lt;/a&gt; üéâ. This gem installs Tailwind with Webpack and has production purging enabled. Other Webpack specific problems are to be addressed as well.&lt;/p&gt;

</description>
      <category>tailwindcss</category>
      <category>webpack</category>
      <category>rails</category>
      <category>gem</category>
    </item>
    <item>
      <title>AWS Certified Solutions Architect Professional SAP-C01 Exam Questions Part 3</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 14:20:03 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-3-3leg</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-3-3leg</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;For AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has an on-premises monitoring solution using a PostgreSQL database for persistence of events. The database is unable to scale due to heavy ingestion and it frequently runs out of storage. The company wants to create a hybrid solution and has already set up a VPN connection between its network and AWS. The solution should include the following attributes:&lt;br&gt;
‚Äì Managed AWS services to minimize operational complexity.&lt;br&gt;
‚Äì A buffer that automatically scales to match the throughput of data and requires no ongoing administration.&lt;br&gt;
‚Äì A visualization tool to create dashboards to observe events in near-real time. Support for semi-structured JSON data and dynamic schemas.&lt;br&gt;
Which combination of components will enable the company to create a monitoring solution that will satisfy these requirements? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events.
       B. Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events.
       C. Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.
       D. Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards.
       E. Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A life sciences company is using a combination of open source tools to manage data analysis workflows and Docker containers running on servers in its on-premises data center to process genomics data. Sequencing data is generated and stored on a local storage area network (SAN), and then the data is processed. The research and development teams are running into capacity issues and have decided to re-architect their genomics analysis platform on AWS to scale based on workload demands and reduce the turnaround time from weeks to days. The company has a high-speed AWS Direct Connect connection. Sequencers will generate around 200 GB of data for each genome, and individual jobs can take several hours to process the data with ideal compute capacity. The end result will be stored in Amazon S3. The company is expecting 10-15 job requests each day. Which solution meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use regularly scheduled AWS Snowball Edge devices to transfer the sequencing data into AWS. When AWS receives the Snowball Edge device and the data is loaded into Amazon S3, use S3 events to trigger an AWS Lambda function to process the data.
       B. Use AWS Data Pipeline to transfer the sequencing data to Amazon S3. Use S3 events to trigger an Amazon EC2 Auto Scaling group to launch custom-AMI EC2 instances running the Docker containers to process the data.
       C. Use AWS DataSync to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow. Store the Docker images in Amazon Elastic Container Registry (Amazon ECR) and trigger AWS Batch to run the container and process the sequencing data.
       D. Use an AWS Storage Gateway file gateway to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Batch job that executes on Amazon EC2 instances running the Docker containers to process the data.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has five physical data centers in specific locations around the world. Each data center has hundreds of physical servers with a mix of Windows and Linux-based applications and database services. Each data center also has an AWS Direct Connect connection of 10 Gbps to AWS with a company-approved VPN solution to ensure that data transfer is secure. The company needs to shut down the existing data centers as quickly as possible and migrate the servers and applications to AWS. Which solution meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Install the AWS Server Migration Service (AWS SMS) connector onto each physical machine. Use the AWS Management Console to select the servers from the server catalog, and start the replication. Once the replication is complete, launch the Amazon EC2 instances created by the service.
       B. Install the AWS DataSync agent onto each physical machine. Use the AWS Management Console to configure the destination to be an AMI, and start the replication. Once the replication is complete, launch the Amazon EC2 instances created by the service.
       C. Install the CloudEndure Migration agent onto each physical machine. Create a migration blueprint, and start the replication. Once the replication is complete, launch the Amazon EC2 instances in cutover mode.
       D. Install the AWS Application Discovery Service agent onto each physical machine. Use the AWS Migration Hub import option to start the replication. Once the replication is complete, launch the Amazon EC2 instances created by the service.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A security engineer determined that an existing application retrieves credentials to an Amazon RDS for MySQL database from an encrypted file in Amazon S3. For the next version of the application, the security engineer wants to implement the following application design changes to improve security:&lt;br&gt;
‚Äì The database must use strong, randomly generated passwords stored in a secure AWS managed service.&lt;br&gt;
‚Äì The application resources must be deployed through AWS CloudFormation.&lt;br&gt;
‚Äì The application must rotate credentials for the database every 90 days.&lt;br&gt;
A solutions architect will generate a CloudFormation template to deploy the application. Which resources specified in the CloudFormation template will meet the security engineer‚Äôs requirements with the LEAST amount of operational overhead?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Specify a Secrets Manager RotationSchedule resource to rotate the database password every 90 days.
       B. Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Create an AWS Lambda function resource to rotate the database password. Specify a Parameter Store RotationSchedule resource to rotate the database password every 90 days.
       C. Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Create an Amazon EventBridge scheduled rule resource to trigger the Lambda function password rotation every 90 days.
       D. Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Specify an AWS AppSync DataSource resource to automatically rotate the database password every 90 days.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has a three-tier application running on AWS with a web server, an application server, and an Amazon RDS MySQL DB instance. A solutions architect is designing a disaster recovery (DR) solution with an RPO of 5 minutes. Which solution will meet the company‚Äôs requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure AWS Backup to perform cross-Region backups of all servers every 5 minutes. Reprovision the three tiers in the DR Region from the backups using AWS CloudFormation in the event of a disaster.
       B. Maintain another running copy of the web and application server stack in the DR Region using AWS CloudFormation drift detection. Configure cross-Region snapshots of the DB instance to the DR Region every 5 minutes. In the event of a disaster, restore the DB instance using the snapshot in the DR Region.
       C. Use Amazon EC2 Image Builder to create and copy AMIs of the web and application server to both the primary and DR Regions. Create a cross-Region read replica of the DB instance in the DR Region. In the event of a disaster, promote the read replica to become the master and reprovision the servers with AWS CloudFormation using the AMIs.
       D. Create AMIs of the web and application servers in the DR Region. Use scheduled AWS Glue jobs to synchronize the DB instance with another DB instance in the DR Region. In the event of a disaster, switch to the DB instance in the DR Region and reprovision the servers with AWS CloudFormation using the AMIs.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to migrate its corporate data center from on premises to the AWS Cloud. The data center includes physical servers and VMs that use VMware and Hyper-V. An administrator needs to select the correct services to collect data for the initial migration discovery process. The data format should be supported by AWS Migration Hub. The company also needs the ability to generate reports from the data. Which solution meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use the AWS Agentless Discovery Connector for data collection on physical servers and all VMs. Store the collected data in Amazon S3. Query the data with S3 Select. Generate reports by using Kibana hosted on Amazon EC2.
       B. Use the AWS Application Discovery Service agent for data collection on physical servers and all VMs. Store the collected data in Amazon Elastic File System (Amazon EFS). Query the data and generate reports with Amazon Athena.
       C. Use the AWS Application Discovery Service agent for data collection on physical servers and Hyper-V. Use the AWS Agentless Discovery Connector for data collection on VMware. Store the collected data in Amazon S3. Query the data with Amazon Athena. Generate reports by using Amazon QuickSight.
       D. Use the AWS Systems Manager agent for data collection on physical servers. Use the AWS Agentless Discovery Connector for data collection on all VMs. Store, query, and generate reports from the collected data by using Amazon Redshift.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is using Amazon Aurora MySQL for a customer relationship management (CRM) application. The application requires frequent maintenance on the database and the Amazon EC2 instances on which the application runs. For AWS Management Console access, the system administrators authenticate against AWS Identity and Access Management (IAM) using an internal identity provider. For database access, each system administrator has a user name and password that have previously been configured within the database. A recent security audit revealed that the database passwords are not frequently rotated. The company wants to replace the passwords with temporary credentials using the company‚Äôs existing AWS access controls. Which set of options will meet the company‚Äôs requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a new AWS Systems Manager Parameter Store entry for each database password. Enable parameter expiration to invoke an AWS Lambda function to perform password rotation by updating the parameter value. Create an IAM policy allowing each system administrator to retrieve their current password from the Parameter Store. Use the AWS CLI to retrieve credentials when connecting to the database.
       B. Create a new AWS Secrets Manager entry for each database password. Configure password rotation for each secret using an AWS Lambda function in the same VPC as the database cluster. Create an IAM policy allowing each system administrator to retrieve their current password. Use the AWS CLI to retrieve credentials when connecting to the database.
       C. Enable IAM database authentication on the database. Attach an IAM policy to each system administrator‚Äôs role to map the role to the database user name. Install the Amazon Aurora SSL certificate bundle to the system administrators‚Äô certificate trust store. Use the AWS CLI to generate an authentication token used when connecting to the database.
       D. Enable IAM database authentication on the database. Configure the database to use the IAM identity provider to map the administrator roles to the database user. Install the Amazon Aurora SSL certificate bundle to the system administrators‚Äô certificate trust store. Use the AWS CLI to generate an authentication token used when connecting to the database.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company‚Äôs AWS architecture currently uses access keys and secret access keys stored on each instance to access AWS services. Database credentials are hard-coded on each instance. SSH keys for command-line remote access are stored in a secured Amazon S3 bucket. The company has asked its solutions architect to improve the security posture of the architecture without adding operational complexity. Which combination of steps should the solutions architect take to accomplish this? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use Amazon EC2 instance profiles with an IAM role
       B. Use AWS Secrets Manager to store access keys and secret access keys
       C. Use AWS Systems Manager Parameter Store to store database credentials
       D. Use a secure fleet of Amazon EC2 bastion hosts for remote access
       E. Use AWS KMS to store database credentials                
       F. Use AWS Systems Manager Session Manager for remote access
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A B D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to change its internal cloud billing strategy for each of its business units. Currently, the cloud governance team shares reports for overall cloud spending with the head of each business unit. The company uses AWS Organizations to manage the separate AWS accounts for each business unit. The existing tagging standard in Organizations includes the application, environment, and owner. The cloud governance team wants a centralized solution so each business unit receives monthly reports on its cloud spending. The solution should also send notifications for any cloud spending that exceeds a set threshold. Which solution is the MOST cost-effective way to meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in each account to create monthly reports for each business unit.
       B. Configure AWS Budgets in the organization‚Äôs master account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in the organization‚Äôs master account to create monthly reports for each business unit.
       C. Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use the AWS Billing and Cost Management dashboard in each account to create monthly reports for each business unit.
       D. Enable AWS Cost and Usage Reports in the organization‚Äôs master account and configure reports grouped by application, environment, and owner. Create an AWS Lambda function that processes AWS Cost and Usage Reports, sends budget alerts, and sends monthly reports to each business unit‚Äôs email list
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is configuring connectivity to a multi-account AWS environment to support application workloads that serve users in a single geographic region. The workloads depend on a highly available, on-premises legacy system deployed across two locations. It is critical for the AWS workloads to maintain connectivity to the legacy system, and a minimum of 5 Gbps of bandwidth is required. All application workloads within AWS must have connectivity with one another. Which solution will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from a DX partner for each on‚Äìpremises location. Create private virtual interfaces on each connection for each AWS account VPC. Associate the private virtual interface with a virtual private gateway attached to each VPC.
       B. Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from two DX partners for each on-premises location. Create and attach a virtual private gateway for each AWS account VPC. Create a DX gateway in a central network account and associate it with the virtual private gateways. Create a public virtual interface on each DX connection and associate the interface with the DX gateway.
       C. Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from two DX partners for each on-premises location. Create a transit gateway and a DX gateway in a central network account. Create a transit virtual interface for each DX interface and associate them with the DX gateway. Create a gateway association between the DX gateway and the transit gateway.
       D. Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from a DX partner for each on-premises location. Create and attach a virtual private gateway for each AWS account VPC. Create a transit gateway in a central network account and associate it with the virtual private gateways. Create a transit virtual interface on each DX connection and attach the interface to the transit gateway.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A financial company needs to create a separate AWS account for a new digital wallet application. The company uses AWS Organizations to manage its accounts. A solutions architect uses the IAM user Support1 from the master account to create a new member account with &lt;a href="mailto:finance1@example.com"&gt;finance1@example.com&lt;/a&gt; as the email address. What should the solutions architect do to create IAM users in the new member account?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Sign in to the AWS Management Console with AWS account root user credentials by using the 64- character password from the initial AWS Organizations email sent to finance1@example.com. Set up the IAM users as required.
       B. From the master account, switch roles to assume the OrganizationAccountAccessRole role with the account ID of the new member account. Set up the IAM users as required.
       C. Go to the AWS Management Console sign-in page. Choose ‚ÄúSign in using root account credentials.‚Äù Sign in by using the email address finance1@example.com and the master account‚Äôs root password. Set up the IAM users as required.
       D. Go to the AWS Management Console sign-in page. Sign in by using the account ID of the new member account and the Support1 IAM credentials. Set up the IAM users as required.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A financial company needs to create a separate AWS account for a new digital wallet application. The company uses AWS Organizations to manage its accounts. A solutions architect uses the IAM user Support1 from the master account to create a new member account with &lt;a href="mailto:finance1@example.com"&gt;finance1@example.com&lt;/a&gt; as the email address. What should the solutions architect do to create IAM users in the new member account?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Sign in to the AWS Management Console with AWS account root user credentials by using the 64- character password from the initial AWS Organizations email sent to finance1@example.com. Set up the IAM users as required.
       B. From the master account, switch roles to assume the OrganizationAccountAccessRole role with the account ID of the new member account. Set up the IAM users as required.
       C. Go to the AWS Management Console sign-in page. Choose ‚ÄúSign in using root account credentials.‚Äù Sign in by using the email address finance1@example.com and the master account‚Äôs root password. Set up the IAM users as required.
       D. Go to the AWS Management Console sign-in page. Sign in by using the account ID of the new member account and the Support1 IAM credentials. Set up the IAM users as required.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is designing a data processing platform to process a large number of files in an Amazon S3 bucket and store the results in Amazon DynamoDB. These files will be processed once and must be retained for 1 year. The company wants to ensure that the original files and resulting data are highly available in multiple AWS Regions. Which solution will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an S3 CreateObject event notification to copy the file to Amazon Elastic Block Store (Amazon EBS). Use AWS DataSync to sync the files between EBS volumes in multiple Regions. Use an Amazon EC2 Auto Scaling group in multiple Regions to attach the EBS volumes. Process the files and store the results in a DynamoDB global table in multiple Regions. Configure the S3 bucket with an S3 Lifecycle policy to move the files to S3 Glacier after 1 year.
       B. Create an S3 CreateObject event notification to copy the file to Amazon Elastic File System (Amazon EFS). Use AWS DataSync to sync the files between EFS volumes in multiple Regions. Use an AWS Lambda function to process the EFS files and store the results in a DynamoDB global table in multiple Regions. Configure the S3 buckets with an S3 Lifecycle policy to move the files to S3 Glacier after 1 year.
       C. Copy the files to an S3 bucket in another Region by using cross-Region replication. Create an S3 CreateObject event notification on the original bucket to push S3 file paths into Amazon EventBridge (Amazon CloudWatch Events). Use an AWS Lambda function to poll EventBridge (CloudWatch Events) to process each file and store the results in a DynamoDB table in each Region. Configure both S3 buckets to use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class and an S3 Lifecycle policy to delete the files after 1 year.
       D. Copy the files to an S3 bucket in another Region by using cross-Region replication. Create an S3 CreateObject event notification on the original bucket to execute an AWS Lambda function to process each file and store the results in a DynamoDB global table in multiple Regions. Configure both S3 buckets to use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class and an S3 Lifecycle policy to delete the files after 1 year.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is running an Apache Hadoop cluster on Amazon EC2 instances. The Hadoop cluster stores approximately 100 TB of data for weekly operational reports and allows occasional access for data scientists to retrieve data. The company needs to reduce the cost and operational complexity for storing and serving this data. Which solution meets these requirements in the MOST cost-effective manner?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Move the Hadoop cluster from EC2 instances to Amazon EMR. Allow data access patterns to remain the same.
       B. Write a script that resizes the EC2 instances to a smaller instance type during downtime and resizes the instances to a larger instance type before the reports are created.
       C. Move the data to Amazon S3 and use Amazon Athena to query the data for reports. Allow the data scientists to access the data directly in Amazon S3.
       D. Migrate the data to Amazon DynamoDB and modify the reports to fetch data from DynamoDB. Allow the data scientists to access the data directly in DynamoDB.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is building a sensor data collection pipeline in which thousands of sensors write data to an Amazon Simple Queue Service (Amazon SQS) queue every minute. The queue is processed by an AWS Lambda function that extracts a standard set of metrics from the sensor data. The company wants to send the data to Amazon CloudWatch. The solution should allow for viewing individual and aggregate sensor metrics and interactively querying the sensor log data using CloudWatch Logs Insights. What is the MOST cost-effective solution that meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Write the processed data to CloudWatch Logs in the CloudWatch embedded metric format.
       B. Write the processed data to CloudWatch Logs. Then write the data to CloudWatch by using the PutMetricData API call.
       C. Write the processed data to CloudWatch Logs in a structured format. Create a CloudWatch metric filter to parse the logs and publish the metrics to CloudWatch with dimensions to uniquely identify a sensor.
       D. Configure the CloudWatch Logs agent for AWS Lambda. Output the metrics for each sensor in statsd format with tags to uniquely identify a sensor. Write the processed data to CloudWatch Logs.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A car rental company has built a serverless REST API to provide data to its mobile app. The app consists of an Amazon API Gateway API with a Regional endpoint, AWS Lambda functions, and an Amazon Aurora MySQL Serverless DB cluster. The company recently opened the API to mobile apps of partners. A significant increase in the number of requests resulted, causing sporadic database memory errors. Analysis of the API traffic indicates that clients are making multiple HTTP GET requests for the same queries in a short period of time. Traffic is concentrated during business hours, with spikes around holidays and other events. The company needs to improve its ability to support the additional usage while minimizing the increase in costs associated with the solution. Which strategy meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Convert the API Gateway Regional endpoint to an edge-optimized endpoint. Enable caching in the production stage.
       B. Implement an Amazon ElastiCache for Redis cache to store the results of the database calls. Modify the Lambda functions to use the cache.
       C. Modify the Aurora Serverless DB cluster configuration to increase the maximum amount of available memory.
       D. Enable throttling in the API Gateway production stage. Set the rate and burst values to limit the incoming calls.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has application services that have been containerized and deployed on multiple Amazon EC2 instances with public IPs. An Apache Kafka cluster has been deployed to the EC2 instances. A PostgreSQL database has been migrated to Amazon RDS for PostgreSQL. The company expects a significant increase of orders on its platform when a new version of its flagship product is released. What changes to the current architecture will reduce operational overhead and support the product release?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an EC2 Auto Scaling group behind an Application Load Balancer. Create additional read replicas for the DB instance. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3.
       B. Create an EC2 Auto Scaling group behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3.
       C. Deploy the application on a Kubernetes cluster created on the EC2 instances behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution.
       D. Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate and enable auto scaling behind an Application Load Balancer. Create additional read replicas for the DB instance. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company recently completed a large-scale migration to AWS. Development teams that support various business units have their own accounts in AWS Organizations. A central cloud team is responsible for controlling which services and resources can be accessed, and for creating operational strategies for all teams within the company. Some teams are approaching their account service quotas. The cloud team needs to create an automated and operationally efficient solution to proactively monitor service quotas. Monitoring should occur every 15 minutes and send alerts when a team exceeds 80% utilization. Which solution will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a scheduled AWS Config rule to trigger an AWS Lambda function to call the GetServiceQuota API. If any service utilization is above 80%, publish a message to an Amazon Simple Notification Service (Amazon SNS) topic to alert the cloud team. Create an AWS CloudFormation template and deploy the necessary resources to each account.
       B. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that triggers an AWS Lambda function to refresh the AWS Trusted Advisor service limits checks and retrieve the most current utilization and service limit data. If the current utilization is above 80%, publish a message to an Amazon Simple Notification Service (Amazon SNS) topic to alert the cloud team. Create AWS CloudFormation StackSets that deploy the necessary resources to all Organizations accounts.
       C. Create an Amazon CloudWatch alarm that triggers an AWS Lambda function to call the Amazon CloudWatch GetInsightRuleReport API to retrieve the most current utilization and service limit data. If the current utilization is above 80%, publish an Amazon Simple Email Service (Amazon SES) notification to alert the cloud team. Create AWS CloudFormation StackSets that deploy the necessary resources to all Organizations accounts.
       D. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that triggers an AWS Lambda function to refresh the AWS Trusted Advisor service limits checks and retrieve the most current utilization and service limit data. If the current utilization is above 80%, use Amazon Pinpoint to send an alert to the cloud team. Create an AWS CloudFormation template and deploy the necessary resources to each account.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An AWS customer has a web application that runs on premises. The web application fetches data from a third-party API that is behind a firewall. The third party accepts only one public CIDR block in each client‚Äôs allow list. The customer wants to migrate their web application to the AWS Cloud. The application will be hosted on a set of Amazon EC2 instances behind an Application Load Balancer (ALB) in a VPC. The ALB is located in public subnets. The EC2 instances are located in private subnets. NAT gateways provide internet access to the private subnets. How should a solutions architect ensure that the web application can continue to call the third party API after the migration?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Associate a block of customer-owned public IP addresses to the VPC. Enable public IP addressing for public subnets in the VPC.
       B. Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC.
       C. Create Elastic IP addresses from the block of customer-owned IP addresses. Assign the static Elastic IP addresses to the ALB.
       D. Register a block of customer-owned public IP addresses in the AWS account. Set up AWS Global Accelerator to use Elastic IP addresses from the address block. Set the ALB as the accelerator endpoint.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is using AWS Organizations to manage multiple AWS accounts. For security purposes, the company requires the creation of an Amazon Simple Notification Service (Amazon SNS) topic that enables integration with a third-party alerting system in all the Organizations member accounts. A solutions architect used an AWS CloudFormation template to create the SNS topic and stack sets to automate the deployment of CloudFormation stacks. Trusted access has been enabled in Organizations. What should the solutions architect do to deploy the CloudFormation StackSets in all AWS accounts?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a stack set in the Organizations member accounts. Use service-managed permissions. Set deployment options to deploy to an organization. Use CloudFormation StackSets drift detection.
       B. Create stacks in the Organizations member accounts. Use self-service permissions. Set deployment options to deploy to an organization. Enable the CloudFormation StackSets automatic deployment.
       C. Create a stack set in the Organizations master account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets automatic deployment.
       D. Create stacks in the Organizations master account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets drift detection.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to provide a desktop as a service (DaaS) to a number of employees using Amazon WorkSpaces. WorkSpaces will need to access files and services hosted on premises with authorization based on the company‚Äôs Active Directory. Network connectivity will be provided through an existing AWS Direct Connect connection. The solution has the following requirements: Credentials from Active Directory should be used to access on-premises files and services. Credentials from Active Directory should not be stored outside the company. End users should have a single sign-on (SSO) to on-premises files and services once connected to WorkSpaces. Which strategy should the solutions architect use for end user authentication?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) directory within the WorkSpaces VPC. Use the Active Directory Migration Tool (ADMT) with the Password Export Server to copy users from the on-premises Active Directory to AWS Managed Microsoft AD. Set up a one-way trust allowing users from AWS Managed Microsoft AD to access resources in the on-premises Active Directory. Use AWS Managed Microsoft AD as the directory for WorkSpaces.
       B. Create a service account in the on-premises Active Directory with the required permissions. Create an AD Connector in AWS Directory Service to be deployed on premises using the service account to communicate with the on-premises Active Directory. Ensure the required TCP ports are open from the WorkSpaces VPC to the on-premises AD Connector. Use the AD Connector as the directory for WorkSpaces.
       C. Create a service account in the on-premises Active Directory with the required permissions. Create an AD Connector in AWS Directory Service within the WorkSpaces VPC using the service account to communicate with the on-premises Active Directory. Use the AD Connector as the directory for WorkSpaces.
       D. Create an AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) directory in the AWS Directory Service within the WorkSpaces VPC. Set up a one-way trust allowing users from the on-premises Active Directory to access resources in the AWS Managed Microsoft AD. Use AWS Managed Microsoft AD as the directory for WorkSpaces. Create an identity provider with AWS Identity and Access Management (IAM) from an on-premises ADFS server. Allow users from this identity provider to assume a role with a policy allowing them to run WorkSpaces.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

</description>
      <category>awslagi</category>
      <category>aws</category>
      <category>googlecloud</category>
    </item>
  </channel>
</rss>
