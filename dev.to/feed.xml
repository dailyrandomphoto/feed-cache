<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>DEV Community</title>
    <author>DEV Community</author>
    <description>A constructive and inclusive social network for software developers. With you every step of your journey.</description>
    <link>https://dev.to</link>
    <language>en</language>
    <item>
      <title>How to Install Pardus GNOME 21</title>
      <author>Ali Orhun Akkirman</author>
      <pubDate>Sun, 22 Aug 2021 16:09:00 +0000</pubDate>
      <link>https://dev.to/aciklab/how-to-install-pardus-gnome-21-15gm</link>
      <guid>https://dev.to/aciklab/how-to-install-pardus-gnome-21-15gm</guid>
      <description>&lt;h1&gt;
  &lt;a href="#pardus-21x"&gt;
  &lt;/a&gt;
  Pardus 21.x
&lt;/h1&gt;

&lt;p&gt;Pardus 21 was released on August 21, 2021! Pardus 21 was released in 3 different environments as XFCE, GNOME and Server. Version 21.x will receive 5 minor updates over approximately 2 years and will be "Supported" until May 1, 2025&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#downloading-pardus-21-gnome-iso"&gt;
  &lt;/a&gt;
  Downloading Pardus 21 GNOME ISO
&lt;/h1&gt;

&lt;p&gt;You can &lt;a href="https://www.pardus.org.tr/surumler/"&gt;download&lt;/a&gt; the GNOME desktop environment ISO disk image of the Pardus 21.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--jutic66o--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/puvuegfzb1qfbaqowxe7.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--jutic66o--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/puvuegfzb1qfbaqowxe7.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#installing-pardus-21"&gt;
  &lt;/a&gt;
  Installing Pardus 21
&lt;/h1&gt;

&lt;p&gt;You can reach the following screen by starting your Pardus 21 ISO in various virtual or physical environments.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--EpumblCS--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/e76z33ji4ksllqgu4vn0.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--EpumblCS--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/e76z33ji4ksllqgu4vn0.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After choosing the language setting on the relevant screen, it would be appropriate to select "Pardus Live" to open an environment that you can use to test your own computer with the Pardus operating system. Of course, if this media is running on a USB stick or similar environment, it will obviously be slower in speed, so it will be more efficient after the actual installation.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--uTrjVN-_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/wmpnf624a07nbxmvjpu2.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--uTrjVN-_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/wmpnf624a07nbxmvjpu2.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;When "Pardus Live" is opened, an empty GNOME like the one below will welcome you. Then you can switch to the Installation Wizard with the "Install Pardus" icon on the desktop.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--Iu2DiY91--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/357w2ov9ycwfiwmoqq2w.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Iu2DiY91--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/357w2ov9ycwfiwmoqq2w.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In the following steps, you can go through the Language, Time Zone and Keyboard selection steps according to your wishes.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--pyB1mDZW--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rwafu7u8grmbyix09hup.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--pyB1mDZW--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rwafu7u8grmbyix09hup.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--dbvmCyEz--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/an4r8wzgu59b8mykkk53.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--dbvmCyEz--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/an4r8wzgu59b8mykkk53.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--6DpPDif_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/gzrx78l67wrfo1xrotxm.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--6DpPDif_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/gzrx78l67wrfo1xrotxm.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After this step, "name", "username" and "password" must be entered for the local administrator account with "sudo" on your system. On the same screen, you are also asked to type the "computer name".&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--3I87IJFm--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/p6io7ly307rjaeupnnx2.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--3I87IJFm--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/p6io7ly307rjaeupnnx2.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Then comes the most important step, how to do the disk partitioning. If you are using another operating system on your disk or if you want to partition manually, you have to do it manually, but in this article we will apply the situation on a blank disk. Therefore, you need to select the disk you want to install in the "Manual Partitioning" option.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--TvgkClCa--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/eawfyl8rfrfsuwjuycdk.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--TvgkClCa--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/eawfyl8rfrfsuwjuycdk.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It warns you again that all data on the disk will be deleted when you make the selection and proceed. Indeed, this is the most important step. If you have data to lose, you need to be careful. If you're using a virtual machine, it won't delete anything from your real physical machine. After this step, if you haven't made any adjustments to your disk, it will ask one more question and just say "Yes".&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--d5Ykfz5x--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8i1qkfuaim3lu00150d4.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--d5Ykfz5x--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8i1qkfuaim3lu00150d4.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--XdzoFxha--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/c3b8pfttzkpqi9i5hawb.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--XdzoFxha--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/c3b8pfttzkpqi9i5hawb.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Then the summary part of the installation will be shared with you. Up until this step, nothing has been done about the installation except erasing the disk.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--XpczEG7W--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/bzdhful8ff5i9ampitub.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--XpczEG7W--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/bzdhful8ff5i9ampitub.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As soon as you click Next, the installation will begin.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s---YfS40Cp--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/7iyiga3c2y49g09i3vh6.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s---YfS40Cp--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/7iyiga3c2y49g09i3vh6.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This step, which will vary depending on your SSD or HDD disk performance, will take between 2-10 minutes. Then the reboot screen will appear.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--5AvueNYN--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vf5kmysv95ydor8ewqp8.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--5AvueNYN--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vf5kmysv95ydor8ewqp8.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In the new version of Pardus, the GRUB login screen has also become a successful design.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--1Phc1JVh--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2porhd5vtyhg3wki0l3o.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--1Phc1JVh--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2porhd5vtyhg3wki0l3o.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Then, when you see the GDM screen open, your installation has been completed successfully. You can start your session with the "your password" you entered during installation.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--bHStZjij--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1tcset89cqe5og534rq7.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--bHStZjij--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1tcset89cqe5og534rq7.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After starting the session, the "Welcome application" appears and you can organize your desktop according to your own taste.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--kEvSkbSj--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/brv7vkr61qoav65qnili.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--kEvSkbSj--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/brv7vkr61qoav65qnili.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Have a good time...&lt;/p&gt;

</description>
      <category>pardus</category>
      <category>gnome</category>
      <category>yirmibir</category>
      <category>21</category>
    </item>
    <item>
      <title>Differences between Javascript and NodeJs</title>
      <author>Maria Antonella ü¶ã</author>
      <pubDate>Sun, 22 Aug 2021 15:50:39 +0000</pubDate>
      <link>https://dev.to/antoomartini/differences-between-javascript-and-nodejs-27m4</link>
      <guid>https://dev.to/antoomartini/differences-between-javascript-and-nodejs-27m4</guid>
      <description>&lt;p&gt;At first it was hard for me to understand the differences because for me, they were the same thing. It was all javascript. But then, I started to understand what each one was used for. For this reason, I share it :)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;‚òò JavaScript is a language that runs inside web browsers as part of the documents loaded by the browser and is used as a client-side development language. It provides the behavior of the pages. Like HTML it provides the semantic structure and CSS the look and feel. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;However, being an interpreted language, it needs an interpreter to run. V8 is Google Chrome's JS engine and 'node' is a front-end that can be used to run JavaScript scripts outside the browser. In other words: &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;‚òò  NodeJs is an open source, cross-platform environment that allows you to create server-side applications and tools using JavaScript. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A short list of comparisons üßêüíª:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;JS
üî¥ Can only be run in the browsers
üî¥ Used on the client-side
üî¥ Capable enough to add HTML and play with the DOM
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;NodeJs
üü° Can be run outside the browser
üü° Used on the server-side.
üü° Does not have the capability to add HTML tags
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



</description>
      <category>javascript</category>
      <category>begginers</category>
      <category>node</category>
      <category>webdev</category>
    </item>
    <item>
      <title>AWS Certified SysOps Administrator SOA-C02 Exam Questions Part 3</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 15:31:42 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-sysops-administrator-soa-c02-exam-questions-part-3-bii</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-sysops-administrator-soa-c02-exam-questions-part-3-bii</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company static website hosted on Amazon S3 was launched recently and is being used. Currently users are experiencing 503 services unavailable errors. Why are these errors occuring?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The request rate to Amazon S3 is too high
B. There is an error with the Amazon RDS database.
C. The requests to Amazon S3 do not have the proper permissions
D. The users are in a different geographical region and Amazon Route53 is restricting access
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company is releasing a new static website hosted on Amazon S3. However, upon navigating to the site, the following error messages is received.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;403 Forbidden - Access Denied
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What change should be made to fix this error?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Add a bucket policy that grants everyone read access to the bucket
B. Add a bucket policy that grants everyone read access to the bucket objects
C. Remove the default bucket policy that denies read access to the bucket
D. Configure cross-origin resource sharing (CORS) on the bucket
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has created an online retail application that is hosted on a fleet of EC2 instances behind of ELB application load balancer, authentication is handled at the individual EC2 instance level. Once a user is authenticated, all request have go to the same EC2 instance. What should the SysOps Administrator enable to meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. ELB TCP listeners
B. ELB Sticky Sessions
C. ELB connection draining
D. ELB cross-zone load balancing
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is managing a large organization with multiple accounts on the Business Support plan all linked to a single payer account. The Administrator wants to be notified automatically of AWS Personal Health Dashboard events. In the main payer account, the Administrator configures Amazon CloudWatch Events triggered by AWS Health events to issue notifications using Amazon SNS, but alerts in the linked accounts failed to trigger. Why did the alerts fail?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Amazon SNS cannot be triggered from the AWS Personal health Dashboard
B. The AWS personal health dashboard only reports events from one a account, not linked account
C. The AWS Personal Health Dashboard must be configured from the payer account only; all events will then roll up into the payer account.
D. AWS Organizations must be used to monitor linked accounts.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The security team has decided that there will be no public internet access to HTTP (TCP port 80 ) because it is moving to HTTPS for all incoming web traffic. The team had asked a SysOps Administrator to provide a report on any security groups that are not compliant. What should the AWS SysOps Administrator do to provide near real time compliance reporting?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Enable AWS Trusted Advisor and show the security team that the security groups unrestricted access will check alarm
B. Schedule and AWS lambda function to run hourly to scan and evaluate all security groups and send report to the security team.
C. Use AWS config to enable the restricted common port rule and add port 80 to parameters
D. Use Amazon Inspector to evaluate the security groups during scans, and send the completed reports to the Security team.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;According to the shared responsibility model, for which of the following Amazon EC2 activities is AWS responsible? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Patching the guest operating system
B. Monitoring memory utilization
C. Configuring network ACLs
D. Patching the hypervisor
E. Maintaining network infrastructure
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company use of AWS Cloud services is quickly growing, so a SysOps Administrator has been asked to generate details of daily spending to share with management. Which method should the Administrator choose to produce this data?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Share the monthly AWS bill with management.
B. Use AWS CloudTrail Logs to access daily costs in JSON format.
C. Set up daily Cost and Usage Report and download the output from Amazon S3.
D. Monitor AWS costs with Amazon Cloud Watch and create billing alerts and notifications.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is managing an application that runs on Amazon EC2 instances behind and application load balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The applications stores data in Amazon RDS MySQL DB instance. The Administrator must ensure that that application stays available if the database becomes unresponsive. How can these requirements be met?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create read replicas for the RDS database and use them in case of a database failure.
B. Create a new RDS instance from the snapshot of the original RDS instance if a failure occurs.
C. Keep a separate RDS database running and switch the endpoint in the web application if a failure occurs.
D. Modify the RDS instance to be a Multi-AZ deployment.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has Sales department and Marketing department. The company uses one AWS account. There is a need to determine what charges are incurred on the AWS platform by each department. There is also a need to receive notifications when a specified cost level is approached or exceeded. Which two actions must a SysOps Administrator take to achieve both requirements with the LEAST amount of administrative overhead? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use AWS Trusted Advisor to obtain a report containing the checked items in the Cost Optimization pillar.
B. Download the detailed billing report, upload it to a database, and match the line items with a list of known resources by department.
C. Create a script by using the AWS CLI to automatically apply tags to existing resources to each department. Schedule the script to run weekly.
D. Use AWS Organizations to create a department Organizational Unit and allow only authorized personnel in each department to create resources.
E. Create a Budget from the Billing and Cost Management console. Specify the budget type a Cost, assign tags for each department, define notifications, and specify any other options as required.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;On a weekly basis, the Administrator for a photo sharing website receives an archive of all files users have uploaded the previous week. These file archives can be as a large as 10TB in size. For legal reasons, these archives must be saved with no possibility of someone deleting or modifying these archives. Occasionally, there may be a need to view the contents, but it is expected that retrieving them can take three or more hours. What should the Administrator do with the weekly archive?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Uploaded the file to Amazon S3 through the AWS management console and apply lifecycle policy to change the storage class to Amazon Glacier.
B. Upload the archive to the Amazon Glacier with the AWS CLI and enable Vault Lock.
C. Create a Linux EC2 instance with an encrypted Amazon EBS volume and copy each weekly archive file for this instance
D. Create a file gateway attached to a file share on an S3 bucket with the storage class S3 Infrequent Access. Upload the archives via the gateway
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to ensure that each department operates within their own isolated environment and that they are only able to use pre-approved services. How can this requirement be met?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Setup an AWS Organization to create accounts for each department and apply services control policies to control access to AWS services.
B. Create IAM roles for each department, and set policies that grant access to specific AWS services.
C. Use the AWS Service Catalog to create catalogs of AWS services that are approved for use by each department.
D. Request that each department create and manage its own AWS account and the resources within it.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An Amazon EC2 instance is unable to connect to an SMTP server in a different subnet. Other instance are successfully communicating with the SMTP server, however VPC flow logs have been enabled on the SMTP server‚Äôs network interface and show the following information.&lt;br&gt;
2223342796652 eni-abe77dab 10.1.1.200 10.100.1.10 1123 25 17 70 48252 1515534437 1515535037 REJECT OK&lt;br&gt;
What can be done to correct problem?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Add the instance to the security group for the SMTP server and ensure that is permitted to communicate over TCP port 25.
B. Disable the iptables service on the SMTP server so that the instance can properly communicate over the network.
C. Install an email client on the instance to ensure that it communicates correctly on TCP port 25 to the SMTP server.
D. Add a rule to the security group for the instance to explicitly permit TCP port 25 outbound to any address.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A web application accepts orders from online users and places the orders into an Amazon SQS queue. Amazon EC2 instances in an EC2 Auto Scaling group read the messages from the queue, process the orders, and email order confirmations to the users. The Auto Scaling group scales up and down based on the queue depth. At the beginning of each business day, users report confirmation emails are delayed. What action will be address this issues?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create a scheduled scaling action to scale up in anticipation of the traffic.
B. Change the Auto Scaling group to scale up and down based on CPU utilization
C. Change the launch configuration to launch larger EC2 instance types
D. Modify the scaling policy to deploy more EC2 instances when scaling up
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An Applications team has successfully deployed an AWS CloudFormation stack consisting of 30 t2-medium Amazon EC2 instances in the us-west-2 Region. When using the same template to launch a stack in us-east-2, the launch failed and rolled back after launching only 10 EC2 instances. What is a possible cause of this failure?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The IAM user did not have privileges to launch the CloudFormation template.
B. The t2 medium EC2 instance service limit was reached
C. An AWS Budgets threshold was breached
D. The application‚Äôs Amazon Machine Image (AMI) is not available in us-east-2
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is receiving multiple reports from customers that they are unable to connect to the company‚Äôs website. which is being served through Amazon CloudFront. Customers are receiving HTTP response codes for both 4XX and 5XX errors. Which metric can the Administrator use to monitor the elevated error rates in CloudFront?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. TotalErrorRate
B. RejectedConnectionCount
C. NetworkTransmitThroughput
D. HealthyHostCount
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization stores sensitive customer information in S3 buckets protected by bucket policies. Recently, there have been reports that unauthorized entities within the company have been trying to access the data on those S3 buckets. The Chief Information Security Officer (CISO) would like to know which buckets are being targeted and determine who is responsible for trying to access that information. Which steps should a Sysops administrator take to meet the CISO requirement? ( Select TWO)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Enable Amazon S3 Analytics on all affected S3 buckets to obtain a report of which buckets are being accessed without authorization.
B. Enable Amazon S3 Server Access Logging on all affected S3 buckets and have the logs stored in a bucket dedicated for logs.
C. Use Amazon Athena to query S3 Analytics reports for HTTP 403 errors, and determine the IAM user or role making the requests.
D. Use Amazon Athena to query the S3 Server Access Logs for HTTP 403 errors, and determine the IAM user or role making the requests.
E. Use Amazon Athena to query the S3 Server Access Logs for HTTP 503 errors, and determine the IAM user or role making the requests.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is responsible for a large fleet of EC2 instances and must know whether any instances will be affected by upcoming hardware maintenance. Which option would provide this information with the LEAST administrative overhead?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Monitor AWS CloudTrail for StopInstances API calls related to upcoming maintenance.
B. Review the Personal Health Dashboard for any scheduled maintenance
C. From the AWS Management Console, list any instances with failed system status checks.
D. Deploy a third-party monitoring solution to provide real-time EC2 instance monitoring.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Malicious traffic is reaching company web servers from a single IP address located in another country. The SysOps Administrator is tasked with blocking this IP address. How should the Administrator implement the restriction?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Edit the security group for the web servers and add a deny entry for the IP address
B. Edit the network access control list for the web server subnet and add a deny entry for the IP address
C. Edit the VPC route table to route the malicious IP address to a black hole
D. Use Amazon CloudFront‚Äôs geo restriction feature to block traffic from the IP address
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company website hosts patches for software that is sold globally. The website runs in AWS and performs well until a large software patch is released. The flood of downloads puts a strain on the web servers and leads to a poor customer experience. What can the SysOps Administrator propose to enhance customer experience, create a more available web platform, and keep costs low?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use an Amazon CloudFront distribution to cache static content, including software patches
B. Increase the size of the NAT instance to improve throughput
C. Scale out of web servers in advance of patch releases to reduce Auto Scaling delays
D. Move the content to IO1 and provision additional IOPS to the volume that contains the software patches
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A website uses Elastic Load Balancing (ELB) in front of several Amazon EC2 instances backed by an Amazon RDS database. The content is dynamically generated for visitors of a webpage based on their geographic location. and is updated daily. Some of the generated objects are large in size and are taking longer to download than they should, resulting in a poor user experience. Which approach will improve the user experience?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Implement Amazon ElastiCache to cache the content and reduce the load on the database.
B. Enable an Amazon CloudFront distribution with Elastic Load Balancing as a custom origin.
C. Use Amazon S3 to store and deliver the content.
D. Enable Auto Scaling for the EC2 instances so that they can scale automatically.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A workload has been moved from a data center to AWS. Previously, vulnerability scans were performed nightly by an external testing company. There is a mandate to continue the vulnerability scans in the AWS environment with third-party testing occurring at least once each month. What solution allows the vulnerability scans to continue without violating the AWS Acceptable Use Policy?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The existing nightly scan can continue with a few changes. The external testing company must be notified of the new IP address of the workload and the security group of the workload must be modified to allow scans from the external company‚Äôs IP range .
B. If the external company is a vendor in the AWS Marketplace, notify them of the new IP address of the workload
C. Submit a penetration testing request every 90 days and have the external company test externally when the request is approved.
D. AWS performs vulnerability testing behind the scenes daily and patches instances as needed. If a vulnerability cannot be automatically addressed, a notification email is distributed.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A web service runs on Amazon EC2 instances behind an Elastic Load Balancing (ELB) load balancer. External clients must whitelist specific public IP addresses in their firewalls to access the service. What load balancer or ELB feature should be used for this application?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Network Load Balancer
B. Application Load Balancer
C. Classic Load Balancer
D. Load balancer target groups
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator receives reports of an Auto Scaling group failing to scale when the nodes running Amazon Linux in the cluster are constrained by high memory utilization. What should the Administrator do to enable scaling to better adapt to the high memory utilization?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create a custom script that pipes memory utilization to Amazon S3, then, scale with an AWS Lambda-powered event
B. Install the Amazon CloudWatch memory monitoring scripts, and create a custom metric based on the script‚Äôs results
C. Increase the minimum size of the cluster to meet memory and application load demands
D. Deploy an Application Load Balancer to more evenly distribute traffic among nodes
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator attempting to delete an Amazon S3 bucket ran the following command: aws s3 rb s3://mybucket The command failed and bucket still exists. The administrator validated that no files existed in the bucket by running aws s3 1s s3://mybucket and getting an empty response. Why is the Administrator unable to delete the bucket, and what must be done to accomplish this task?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The bucket has MFA Delete enabled, and the Administrator must turn it off.
B. The bucket has versioning enabled, and the Administrator must permanently delete the objects‚Äô delete markers.
C. The bucket is storing files in Amazon Glacier, and the Administrator must wait 3-5 hours for the files to delete.
D. The bucket has server-side encryption enabled, and the Administrator must run the aws s3 rb s3://my bucket ‚Äî sse command.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has two AWS accounts: development and production. All applications send logs to a specific Amazon S3 bucket for each account, and the Developers are requesting access to the production account S3 buckets to view the logs. Which is the MOST efficient way to provide the Developers with access?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create an AWS Lambda function with an IAM role attached to it that has access to both accounts‚Äô S3 buckets. Pull the logs from the production S3 bucket to the development S3 bucket.
B. Create IAM users for each Developer on the production account, and add the Developers to an IAM group that provides read-only access to the S3 log bucket.
C. Create an Amazon EC2 bastion host with an IAM role attached to it that has access to the production S3 log bucket, and then provision access for the Developers on the host.
D. Create a resource-based policy for the S3 bucket on the production account that grants access to the development account, and then delegate access in the development account.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A web application runs on Amazon EC2 instances behind an Elastic Load Balancing Application Load Balancer (ALB). The instances run in an Auto Scaling group across multiple Availability Zones. A SysOps Administrator has notice that some EC2 instances show up healthy in the Auto Scaling console but show up as unhealthy in the ALB target console. What could be the issue?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The health check grace period for the Auto Scaling group is set too low; increase it
B. The target group health check is incorrectly configured and needs to be adjusted
C. The user data or AMI used for the Auto Scaling group launch configuration is incorrect
D. The Auto Scaling group health check type is based on EC2 instance health instead of Elastic Load Balancing health checks
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is running critical applications on Amazon EC2 instances. The company needs to ensure its resources are automatically recovered if they become impaired due to an underlying hardware failure. Which service can be used to monitor and recover the EC2 instances?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Amazon EC2 Systems Manager
B. Amazon Inspector
C. AWS CloudFormation
D. Amazon CloudWatch
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company requires that all access from on-premises applications to AWS services go over its AWS Direct Connect connection rather than the public internet. How would a SysOps Administrator implement this requirement?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Implement an IAM policy that uses the aws:sourceConnection condition to allow access for the AWS Direct Connect connection ID only
B. Set up a public virtual interface on the AWS Direct Connect connection
C. Configure AWS Shield to protect the AWS Management Console from being accessed by IP addresses other than those within the data center ranges
D. Update all the VPC network ACLs to allow access from the data center IP ranges
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is required to monitor free space on Amazon EBS volumes attached to Microsoft Windows-based Amazon EC2 instances within a company‚Äôs account. The Administrator must be alerted to potential issues. What should the Administrator do to receive email alerts before low storage space affects EC2 instance performance?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use built-in Amazon CloudWatch metrics, and configure CloudWatch alarms and an Amazon SNS topic for email notifications
B. Use AWS CloudTrail logs and configure the trail to send notifications to an Amazon SNS topic
C. Use the Amazon CloudWatch agent to send disk space metrics, then set up CloudWatch alarms using an Amazon SNS topic
D. Use AWS Trusted Advisor and enable email notification alerts for EC2 disk space
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company‚Äôs Information Security team has requested information on AWS environment compliance for Payment Card Industry (PCI) workloads. They have requested assistance in understanding what specific areas of the PCI standards are the responsibility of the company. Which AWS tool will provide the necessary information?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. AWS Macie
B. AWS Artifact
C. AWS OpsWorks
D. AWS Organizations
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company uses AWS CloudFormation to deploy its application infrastructure. Recently, a user accidentally changed a property of a database in a CloudFormation template and performed a stack update that caused an interruption to the application. A SysOps Administrator must determine how to modify the deployment process to allow the DevOps team to continue to deploy the infrastructure, but prevent against accidental modifications to specific resources. Which solution will meet these requirements?&lt;/p&gt;

&lt;p&gt;A. Set up an AWS Config rule to alert based on changes to any Cloud Formation stack. An AWS Lambda function can then describe the stack to determine if any protected resources were modified and cancel the operation.&lt;br&gt;
B. Set up an Amazon CloudWatch Events event with a rule to trigger based on any CloudFormation API call. An AWS Lambda function can then describe the stack to determine if any protected resources were modified and cancel the operation.&lt;br&gt;
C. Launch the CloudFormation templates using a stack policy with an explicit allow for all resources and an explicit deny of the protected resources with an action of Update:*&lt;br&gt;
D. Attach an IAM policy to the DevOps team role that prevents a CloudFormation stack from updating, with a condition based on the specific Amazon Resource names (ARNs) of the protected resources.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company recently implemented an Amazon S3 lifecycle rule that accidentally deleted objects from one of its S3 buckets. The bucket has S3 versioning enabled. Which actions will restore the objects? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use the AWS Management Console to delete the object delete markers.
B. Create a new lifecycle rule to delete the object delete markers that were created.
C. Use the AWS CLI to delete the object delete markers while specifying the version IDs of the delete markers.
D. Modify the existing lifecycle rule to delete the object delete markers that were created.
E. Use the AWS CLI to delete the object delete markers while specifying the name of the objects only.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An application running on Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones was deployed using an AWS CloudFormation template. The SysOps team has patched the Amazon Machine Image (AMI) version and must update all the EC2 instances to use the new AMI. How can the SysOps Administrator use CloudFormation to apply the new AMI while maintaining a minimum level of active instances to ensure service continuity?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Run the aws cloudformation update-stack command with the ‚Äì rollback-configuration option
B. Update the CloudFormation template with the new AMI ID, then reboot the EC2 instances
C. Deploy a second CloudFormation stack and use Amazon Route 53 to redirect traffic to the new stack
D. Set an AutoScalingUpdate policy in the CloudFormation template to update the stack.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An AWS CodePipeline in us-east-1 returns ‚ÄúInternalError‚Äù with the code ‚ÄúJobFailed‚Äù when launching a deployment using an artifact from an Amazon S3 bucket in us-west-1. What is causing this error?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. S3 Transfer Acceleration is not enabled.
B. The S3 bucket is not in the appropriate region.
C. The S3 bucket is being throttled.
D. There are insufficient permissions on the artifact in Amazon S3.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SySOps Administrator is managing an AWS account where Developers are authorized to launch Amazon EC2 instances to test new code. To limit costs, the Administrator must ensure that the EC2 instances in the account are terminated 24 hours after launch. How should the Administrator meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create an Amazon CloudWatch alarm based on the CPUUtilization metric. When the metric is 0% for 24 hours, trigger an action to terminate the EC2 instance when the alarm is triggered.
B. Create an AWS Lambda function to check all EC2 instances and terminate instances running more than 24 hours. Trigger the function with an Amazon CloudWatch Events event every 15 minutes.
C. Add an action to AWS Trusted Advisor to turn off EC2 instances based on the Low Utilization Amazon EC2 Instances check, terminating instances identified by Trusted Advisor as running for more than 24 hours.
D. Install the unified Amazon CloudWatch agent on every EC2 instance. Configure the agent to terminate instances after they have been running for 24 hours.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator created an Application Load balancer (ALB) and placed two Amazon EC2 instances in the same subnet behind the ALB. During monitoring, the Administrator observes HealthyHostCount drop to 1 in Amazon CloudWatch. What is MOST likely causing this issue?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The EC2 instances are in the same Availability Zone, causing contention between the two.
B. The route tables are not updated to allow traffic to flow between the ALB and the EC2 instances.
C. The ALB health check has failed, and the ALB has taken EC2 instances out of service.
D. The Amazon Route 53 health check has failed, and the ALB has taken EC2 instances out of service.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has centralized all its logs into one Amazon CloudWatch Logs log group. The SysOps Administrator is to alert different teams of any issues relevant to them. What is the MOST efficient approach to accomplish this?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Write a AWS lambda function that will query the logs every minute and contain the logic of which team to notify on which patterns and issues.
B. Set up different metric filters for each team based on patterns and alerts. Each alarm will notify the appropriate notification list.
C. Redesign the aggregation of logs so that each team‚Äôs relevant parts are sent to a separate log group, then subscribe each team to its respective log group.
D. Create an AWS Auto Scaling group of Amazon EC2 instances that will scale based on the amount of ingested log entries. This group will pull streams, look for patterns, and send notifications to relevant teams.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An Amazon S3 bucket in a SysOps Administrator account can be accessed by users in other SWS accounts. How can the Administrator ensure that the bucket is only accessible to members of the Administrator‚Äôs AWS account?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Move the S3 bucket from a public subnet to a private subnet in the Amazon VPC.
B. Change the bucket access control list (ACL) to restrict access to the bucket owner.
C. Enable server-side encryption for all objects in the bucket.
D. Use only Amazon S3 presigned URLs for accessing objects in the bucket.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company received its latest bill with a large increase in the number of requests against Amazon SQS as compared to the month prior. The company is not aware of any changes in its SQS usage. The company is concerned about the cost increase and who or what was making these calls. What should the SysOps Administrator use to validate the calls made to SQS?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. AWS CloudTrail
B. Amazon CloudWatch
C. AWS Cost Explorer
D. Amazon S3 server access logs
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator responsible for an e-commerce web application observes the application does not launch new Amazon EC2 instances at peak times, even though the maximum capacity of the Auto Scaling group has not been reached. What should the Administrator do to identify the underlying problem? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Monitor service limits in AWS trusted Advisor.
B. Analyze VPC Flow Logs.
C. Monitor limits in AWS Systems Manager.
D. Use Amazon inspector to gather performance information.
E. Check the response for RunInstance requests in AWS CloudTrail logs.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A E&lt;/p&gt;

</description>
      <category>awslagi</category>
      <category>aws</category>
      <category>googlecloud</category>
    </item>
    <item>
      <title>AWS Certified SysOps Administrator SOA-C02 Exam Questions Part 2</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 15:30:18 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-sysops-administrator-soa-c02-exam-questions-part-2-3l85</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-sysops-administrator-soa-c02-exam-questions-part-2-3l85</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A fleet of servers must send local logs to Amazon Cloudwatch. How should the servers be configured to meet these requirements ?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Configure AWS Config to forward events to cloudwatch
B. Configure a simple network management protocol (SNMP) agent to forward events to Cloudwatch
C. Install and configure the unified Cloudwatch agent
D. Install and configure the Amazon Inspector agent
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company data retention policy dictates that backups be stored for exactly two years. After that the data must be deleted. How can Amazon EBS snapshots be managed to conform to this data retention policy?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use an Amazon S3 lifecycle policy to delete snapshots older than two years
B. Configure Amazon Inspector to find and delete old EBS Snapshots
C. Schedule an AWS Lambda function using Cloudwatch events to periodically run a scripts to delete old snapshots
D. Configure an Amazon Cloudwatch Alarm to trigger the launch of an AWS Cloudformation template that will clean the older snapshots
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;In configuring an Amazon Route 53 health check, a SysOps Administrator selects ‚ÄòYes‚Äô to the String Matching option in the Advanced Configuration section. In the Search String box, the Administrator types the following text: /html. This is to ensure that the entire page is loading during the health check. Within 5 minutes of enabling the health check, the Administrator receives an alert stating that the check failed. However, when the Administrator navigates to the page, it loads successfully. What is the Most likely cause of this false alarm?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The search string is not HTML encoded
B. The search string must be put in quotes
C. The search string must be escaped with a backslash (\) before the forward slash (/)
D. The search string is not in the first 5120 bytes of the tested page
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator must ensure that AWS Cloudformation deployment changes are properly backend for governance. Which AWS Service should be used to accomplish this?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. AWS Artifact
B. AWS Config
C. Amazon Inspector
D. AWS Trusted Advisor
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Developer created an AWS Lambda function and has asked the SysOps Administrator to make the function run in every 15 minutes . What is the MOST efficient way to accomplish this request?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create an Amazon EC2 instance and schedule a cron to invoke the Lambda function
B. Create a repeat time variable inside the Lambda function to invoke the Lambda function
C. Create a second Lambda function to monitor and invoke the first Lambda function
D. Create an Amazon Cloudwatch scheduled event to invoke the Lambda function
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is analyzing how Reserved Instance discounts are allocated to Amazon EC2 instances across multiple AWS Account. Which AWS tool will provide the details necessary to understand the billing charges?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. AWS Budgets
B. AWS Cost and Usage report
C. AWS Trusted Advisor
D. AWS Organizations
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator wants to prevent Developer from accidentally terminating Amazon EC2 instance. How can this be accomplished?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use AWS Systems Manager to restrict EC2 termination
B. Use AWS Config to restrict EC2 termination
C. Application Amazon Cloudwatch event to prevent EC2 termination
D. Enable termination protection on EC2 instances
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization has developed a new memory intensive application that is deployed to a large Amazon EC2. The application is exhaustion, so the development team wants to monitor memory usage by using Amazon Cloudwatch. What is the MOST efficient way to accomplish this goal?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Deploy the solution to memory-optimized EC2 instances and use the cloudwatch MemoryUtilization metrics
B. Enable the memory monitoring option by using AWS Config
C. Install the AWS System Manager agent on applicable EC2 instances to monitor memory
D. Monitor memory by using a script within the instance and send it to cloudwatch as a custom metric
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization has been running their website on several m2 Linux instances behind a classic load balancer for two years. Application load has been constant and predictable. What should the organization do to reduce costs?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Purchase Reserved instances for the specific m2.instances
B. Change the m2 instances to equivalent m5 types, and purchase Reserved instances for the specific m5 instances
C. Change the classic load balancer to an application load balancer and purchase reserved instances for the specific m2 instances
D. Purchase Spot instances for the specific m2 instances
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator has written an AWS Lambda function to launch new Amazon EC2 instances and deployed it in the us-east-1 region. The Administrator tested it by launching a new t2 nano instance in the us-east-1 region and it performed as expected. However, when the region name was updated in the Lambda function to launch an EC2 instance in the us-west-1 region, it failed. What is causing this error?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The AMI ID must be updated for the us-west-1 region in the Lambda function as well
B. The Lambda function can only launch EC2 instances in the same region where it is deployed
C. The Lambda function does not have the necessary IAM permission to launch more than one EC2 instance
D. The instance type defined in the Lambda function is not available in the us-west-1 region
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company backs up data from data center using a tape gateway on AWS Storage Gateway. The SysOps Administrator must stop a running storage gateway. What process will protect data integrity?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Stop storage gateway and reboot the virtual machine, then restart Storage Gateway
B. Reboot the virtual machine then restart storage gateway
C. Reboot the virtual machine
D. Shutdown the virtual machine and stop storage gateway then turn the virtual machine
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is responsible for a legacy, CPU heavy application. The application can only be scaled vertical. Currently application running on t2.large Amazon EC2 instance. The system is showing 90% CPU usage and significant performance latency. What change should be made to alleviate the performance problem?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Change the EBS volume to provisioned IOPS
B. Upgrade to a compute-optimized instance
C. Add additional t2.large instances to the application
D. Purchase the Reserved Instance
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator runs a web application that is using a microservices approach whereby different responsibilities of the application have been divided in a separate microservice running on a different Amazon EC2 instance. The Administrator has been tasked with reconfiguring the infrastructure to support this approach. How can the Administrator accomplish this with the LEAST administrative overhead?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use Amazon Cloudfront to log the URL and forward the request
B. Use Amazon Cloudfront to rewrite the header base on the micro service and forward the request
C. Use an Application Load Balancer (ALB) and do path-based routing
D. Use a Network Load Balancer (NLB) and do path-based routing
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization is concerned that its Amazon RDS databases are not protected. The solution to address this issue must be low cost, protect against table corruption that could be overlooked for several days, and must offer a 30-day window of protection. How can these requirement must be met?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Enable multi-AZ on the RDS Instance to maintain the data in a second Availability Zone
B. Create a Read Replica of the RDS Instance to maintain the data in a second region
C. Ensure that automated backups are enabled and set the appropriate retention period
D. Enable versioning in RDS to recover altered table data when needed
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company with a dozens of AWS Account wants to ensure that governance rules are being applied across all accounts. The CIO has recommended that AWS Config rules be deployed using an AWS Cloudformation template. How should the requirements be met?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create the Cloudformation stack set then select Cloudformation template and use it to configure the AWS accounts
B. Write a script that iterates over the Company AWS accounts and executes the Cloudformation template in each account
C. Use AWS Organizations to execute the Cloudformation template in all accounts
D. Create a Cloudformation template in the master account of AWS. Organizations and execute the Cloudformation template to create AWS Config rules in all accounts
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company must ensures that any objects uploaded to an s3 bucket must be encrypted. Which of the following actions will meet the requirement? ( SELECT TWO)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Implement AWS Shield to protect again unencrypted objects stored in s3 buckets
B. Implement Object access control list (ACL) to deny unencrypted objects from being uploaded to the S3 bucket
C. Implement Amazon S3 default encryption to make sure that any object being uploaded is encrypted before it is stored
D. Implement Amazon Inspector to inspect objects uploaded to s3 bucket to make sure that they are encrypted
E. Implement S3 bucket policies to deny unencrypted objects from being uploaded to the buckets
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Based on the AWS Shared Responsibility Model, which of the following actions are the responsibility of the customer for an Aurora database?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Performing underlying OS updates
B. Provisioning of storage for database
C. Scheduling maintenance, patches and other updates
D. Executing maintenance, patches and other updates
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company would like to review each change in the infrastructure before deploying updates in its AWS Cloudformation stacks. Which action will allow an Administrator to understand the impact of these changes before implement?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Implement a blue/green strategy using AWS Elastic Beanstalk
B. Perform a canary deployment using a Application Load Balancer and target groups
C. Create a change set for the running stack
D. Submit the update using UpdateStack API call.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization is running multiple applications for their customers. Each application is deployed by running a base AWS CloudFormation template that configures a new VPC. All applications are run in the same AWS account and AWS Region. A SysOps Administrator has noticed that when trying to deploy the same AWS CloudFormation stack, it fails to deploy. What is likely to be the problem?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The Amazon Machine Image used is not Available in that region
B. The AWS Cloudformation template needs to be update to the latest version
C. The VPC configurations parameters have changed and must be updated in the template
D. The account has reached the default limit for VPCs allowed
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator found that newly-deployed Amazon EC2 application server is unable to connect to an Amazon RDS database. VPC Flow Logs and confirming that the flow log is active on the console, the log group cannot be located on Amazon Cloudwatch. What are the MOST likely reasons for this situation? (SELECT TWO)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. The Administrator must configure the VPC Flow Logs to have them sent to AWS CloudTrail
B. The Administrator has waited less than ten minutes for the log group to be created in Cloudwatch
C. The account VPC Flow Logs have been disabled by using a service control policy
D. No relevant traffic has been sent since the VPC Flow Logs were created
E. The account has Amazon Guard Duty enabled.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has mandated the use of multi-factor authentication (MFA) for all IAM users, and requires users to make all API-calls using the CLI. However, users are not prompted to enter MFA tokens, and are able to run CLI commands without MFA. In an attempt to enforce MFA, the company attached an IAM policy to all users that denies API calls that have not been authenticated with MFA. What additional step must be taken to ensure that API calls are authenticated using MFA?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Enable MFA on IAM roles and require IAM users to use role credentials to sign API calls.
B. Ask the IAM users to log into the AWS Management Console with MFA before making API calls using the CLI
C. Restricts the IAM users to use of the console, as MFA is not supported for CLI use
D. Require user to use temporary credentials from the get sessions token command to sign API calls
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;p&gt;62.An application running on Amazon EC2 allows users to launch batch jobs for data analysis. The jobs are run asynchronously, and the user is notified when they are complete. While multiple jobs can run concurrently, a user‚Äôs request need not be fulfilled for up to 24 hours. To run a job, the application launches an additional EC2 instance that performs all the analytics calculations. A job takes between 75 and 110 minutes to complete and cannot be interrupted. What is the MOST cost-effective way to run this workload?&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;    A. Run the application on-Demand EC2 instances. Run the jobs on spot instances with a specified duration
    B. Run the application on Reserved instance EC2 instances. Run the jobs on AWS Lambda
    C. Run the application on On-Demand EC2 instances. Run the jobs on On-Demand EC2 instances
    D. Run the application on Reserved instance EC2 instances. Run the jobs on spot instances with a specified duration
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization has two AWS accounts Development and Production. A SysOps Administrator manages access via IAM. Users require in Development should have access to certain resource in Production. How can this be accomplished?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create an IAM role in Production account with the Development account as a trusted entity and then allow those users from Development account to assume the Production account IAM role
B. Create a group of IAM users in the Development account and add Production account service ARNs as resources in the IAM policy
C. Establish a federation between the two accounts using the on-premises Microsoft Active Directory and allow development account to access the Production account through this federation
D. Establish an Amazon Cognito Federated Identity between the two accounts and allow the Development account to access the Production account through this federation
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator has been able to consolidate multiple secure websites onto a single servers and each site is running on a different port. The Administrator now wants to start a duplicate server in a second Availability Zone and put both behind a Load Balancer for high availability. What would be the command line necessary to deploy one of the sites certificates to the load balancer?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. aws kms modify-listener ‚Äìloadbalancer-name my-loadbalancer ‚Äìcertificates CertificateARN arn:aws:iam::123456:server-certificate/my-new-server-cert
B. aws elb set-load-balancer-listener-ssl-cerficate ‚Äìload-balancer-name my-load-balancer ‚Äìload-balaner-port 443 ‚Äìssl-cerficate-id arn:aws:iam::123456:server-certificate/new-server-cert
C. aws ec2 put-ssl-certificate ‚Äìloadbalancer-name my-loadbalancer ‚Äìload-balaner-port 443 ‚Äìssl-cerficate-id arn:aws:iam::123456:server-certificate/new-server-cert
D. aws acm put-ssl-cerficate ‚Äìloadbalancer-name my-loadbalancer ‚Äìload-balaner-port 443 ‚Äìssl-cerficate-id arn:aws:iam::123456:server-certificate/new-server-cert
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An application resides on multiple EC2 instances in public subnets in two Availability Zones. To improve security Application Load Balancer (ALB) in separate subnets and pointed the DNS at the ALB instead of EC2 instances. After the change, traffic is not reaching the instances and an error is being returned from the ALB. What steps must a SysOps Administrator take to resolve this issue and improve the security of the application? (SELECT TWO)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Add the EC2 instances to the ALB target group, configure the health check and ensure that the instances report healthy
B. Add the EC2 instances to an Auto Scaling group, configure the health check to ensure that the instances report healthy and remove the public IPs from the instances
C. Create a new subnet in which EC2 instances and ALB will reside to ensure that they can communicate and remove the public IPs from the instances
D. Change the security group for the EC2 instances to allow access from only the ALB security group and remove the public IPs from the instances
E. Change the security group to allow access from 0.0.0.0/0 which permits access from the ALB
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is received its latest bill with a large increase in the number of request against Amazon SQS as API call action. Admin need to know of any major changes in it SQS usage. The company is concerned about the cost increase and who or what was missing the calls. What should the SysOps Administrator use to validate the calls made to SQS?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Amazon Cloudtrail
B. Amazon Cloudwatch
C. AWS Cost Explorer
D. Amazon S3 Access logs
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;After a particularly high AWS bill, an organization wants to review the use of AWS Services&lt;br&gt;
What AWS Service will allow the SysOps Administrator to quickly view this information to share it and will also forecast equipment ?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. AWS Trusted Advisor
B. Amazon QuickSight
C. AWS Cost and Usage Report
D. AWS Cost Explorer
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator must find a way to setup alerts when Amazon EC2 service limit are close to being reached? How can the Administrator achieve this requirement?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Use Amazon Inspector and Amazon Cloudwatch Events
B. Use AWS Trusted Advisor and Amazon Cloudwatch Events
C. Use the Personal Health Dashboard and Cloudwatch Events
D. Use AWS CloudTrail and Cloudwatch Events
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is reviewing AWS Trusted Advisor warnings and encounters a warning for an S3 bucket policy that discussing the issue with the bucket owner, the Administrator realizes the S3 bucket is an origin for an Amazon Cloudfront Which action should the Administrator take to ensure that users access objects in Amazon S3 by using only Cloudfront URL?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Encrypt the S3 bucket content with Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)
B. Create an Origin access identity and grand it permissions to read objects in the S3 buckets
C. Assign an IAM user to the Cloudfront distribution and whitelist the IAM user in the S3 bucket policy
D. Assign an IAM Role to the Cloudfront distribution and whitelist the IAM role in the S3 bucket policy
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An Auto Scaling group scales up and down based on Average CPU Utilization. The alarms is set to trigger a scaling when CPU exceeds 80% for 5 minutes. Currently, the average CPU has been 95% for over two hours and new instances are not being added What could be the issue?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. A Scheduled scaling action has not been defined
B. In the field suspend process ‚Äú ReplacesUnhealthy‚Äù has been selected
C. The maximum size of the Auto Scaling Group is below or at the current group size
D. The HealthCheck Grace Period is set to less than 300 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The Database Administrator team is interested in performing manual backups of an Amazon RDS Oracle DB instance. What step should be taken to perform the backups?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Attach Amazon EBS Volume with Oracle RMAN installed to the RDS Instance
B. Take a snapshot of the EBS volume that is attached to the DB instance
C. Install Oracle Secure backup on the RDS instance and backup the Oracle database to Amazon S3
D. Take a snapshot of the DB Instance
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company has created a separate AWS account for all development work to protect the production environment. In the development environment users request permission to manipulate IAM policies and roles. Corporate policies require that developers are blocked from accessing services. What is the BEST way to grant the developers privileges in the development account while still complying with corporate policies?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create a service control policy in AWS Organizations and apply it to the development account
B. Create a customer managed policy in IAM and apply it in to all users within the development account
C. Create a job function policy in IAM and apply it to all users within the development account
D. Create an IAM Policy and apply it in API Gateway to restrict the development account
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company has a web application that runs on both on-premises and on Amazon EC2 instances. Over time both the on-premises server and EC2 instances is crashing. A SysOps Administrator suspects a memory leak in the application and wants unified method to monitor memory utilization. How can the Administrator track both the EC2 memory utilization and on-premises server memory utilization over time?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Write a script or use a third-party application to report memory utilization for both EC2 instances and on-premises servers.
B. Use Amazon Cloudwatch agent for both Amazon EC2 instances and on-premises servers to report MemoryUtilization metrics to Cloudwatch and set a Cloudwatch alarm for notifications
C. Use Cloudwatch agent for Amazon EC2 instances to report memory Utilization to Cloudwatch and set Cloudwatch Alarms for notifications. Use a third-party application for the on-premises servers.
D. Configure a load balancer to route traffic to both on-premises servers and EC2 instances, then use cloudwatch as the unified view of the metrics for the load balancer.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A SysOps Administrator is using AWS Cloudformation to deploy resources but would like to manually address any errors the template encounters. What should the Administrator add to the template to support the requirement?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Enable Termination Protection on the Stack
B. Set the OnFailure parameter to ‚ÄúDO_NOTHING‚Äù
C. Restrict the IAM permissions for CloudFormation to delete resources
D. Set the DeleteStack API action to ‚ÄúNO‚Äù
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company‚Äôs application stores documents within an Amazon S3 bucket. The application is running on Amazon EC2 in a VPC. A recent change in security requirements states that traffic between the company‚Äôs application and the S3 bucket must never leave the Amazon network. What AWS feature can provide this functionality?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Security Groups
B. NAT gateways
C. Virtual private gateway
D. Amazon VPC endpoints
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization with a large IT department has decided to migrate to AWS . With different jobs functions in departments and is not desirable to give all users access to all AWS resources. Currently the organization handles access via LDAP group membership. What the best method to allow access using current LDAP credentials ?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Create an AWS directory service simple AD . Replicate the onpremise LDAP directory to simple AD
B. Create Lambda function to read LDAP groups and automate the creation of IAM users
C. Use AWS Cloud Formations to create IAM roles . Deploy direct connect to allow access to the on-premises LDAP server.
D. Federate the LDAP directory with IAM using SAML. Create different IAM roles correspond to different LDAP group to limit permissions.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An Sysops Administrator must set up notifications for whenever combined billing exceeds a certain threshold for all AWS account within company. The Administrator has set up AWS Organizations and enabled Consolidate billing. Which additionals steps must the Administrator perform to setup the billing alerts?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. On the payer account Enable billing alerts in the Billing and Cost management console ; publish an Amazon SNS message when the billing alerts triggers.
B. On each account Enable billing alerts in the billing and cost management console ; setup a billing alarm in Amazon Cloudwatch; publish an SNS message when the alarm triggers.
C. On the payer account Enable billing alerts in the billing and cost management console; setup a billing alarm in the billing and cost management console to publish an SNS message when the alarm triggers.
D. On the payer account Enable billing alerts in the billing and cost management console; setup billing alarm in Amazon Cloudwatch , publish an SNS message when the alarm triggers.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An organization has been running their website on several m2 Linux instance behind a classic load balancer for more than two years. Traffic and utilization have been constant and predictable. What should the organization do to reduce cost ?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Purchase reserved instances for the specific m2 instances.
B. Change the m2 instances type to equivalent m5 types and purchase reserved instances for specific m5 instances.
C. Change the classic load balancer to an application load balancer and purchase reserved instances for the specific m2 instances.
D. Purchase spot instances for the specific m2 instances.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An application developers are reporting access denied errors when trying to list the content in s3 bucket with IAM Role ARN ‚Äúarn:aws:iam:11111111:user/application‚Äù. The following s3 bucket policy:&lt;br&gt;
    {&lt;br&gt;
    ‚ÄúId‚Äù: ‚ÄúS3BucketPolicy‚Äù,&lt;br&gt;
    ‚ÄúVersions‚Äù: ‚Äú2012-10-17‚Äù,&lt;br&gt;
    ‚ÄúStatement‚Äù: [&lt;br&gt;
    {&lt;br&gt;
    ‚ÄúSid‚Äù: ‚ÄúList‚Äù,&lt;br&gt;
    ‚ÄúAction‚Äù: {&lt;br&gt;
    ‚Äús3: List*‚Äù&lt;br&gt;
    },&lt;br&gt;
    ‚ÄúEffect‚Äù: ‚ÄúAllow‚Äù,&lt;br&gt;
    ‚ÄúResources‚Äù: {&lt;br&gt;
    ‚Äúarn:aws:s3:::bucketname/*‚Äù&lt;br&gt;
    },&lt;br&gt;
    ‚ÄúPrincipal‚Äù: {&lt;br&gt;
    ‚ÄúAWS‚Äù: {&lt;br&gt;
    ‚Äúarn:aws:iam::11111111:user/application‚Äù&lt;br&gt;
    }&lt;br&gt;
    }&lt;br&gt;
    }&lt;br&gt;
    }&lt;br&gt;
    }&lt;br&gt;
How should a SysOps Administrator modify the S3 bucket policy to fix the issue?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Change the ‚ÄúEffect‚Äù from ‚ÄúAllow‚Äù to ‚ÄúDeny‚Äù
B. Change the ‚ÄúAction‚Äù from ‚ÄúS3:List*‚Äù to ‚ÄúS3:ListBucket‚Äù
C. Change the ‚ÄúResource‚Äù from ‚Äúarn:aws:s3:::bucketname/*‚Äù to ‚Äúarn:aws:s3:::bucketname‚Äù
D. Change the ‚ÄúPrincipal‚Äù from ‚Äúarn:aws:iam::11111111:user/application‚Äù to ‚Äúarn:aws:iam:1111111:role/application‚Äù
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Company creates custom AMI images by launching new Amazon EC2 instance from an Amazon Cloudformation template. AMI images is installed software through AWS OpsWorks and take image of each EC2 instance. The process of installing software take a long times, the process stalls due to installations errors. The SysOps administrator must modify the Cloudformation Template so if the process stalls, stacks will rollback. Based on the requirements, what should be added to the template?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;A. Conditions with a timeout set to 4 hours
B. CreationPolicy with a timeout set to 4 hours
C. DependOn with a timeout set to 4 hours
D. MetaData with a timeout set to 4 hours
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

</description>
      <category>aws</category>
      <category>googlecloud</category>
      <category>awslagi</category>
    </item>
    <item>
      <title>AWS Certified Solutions Architect Professional SAP-C01 Exam Questions Part 5</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 14:58:33 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-5-3kog</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-5-3kog</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;For AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company plans to migrate to AWS. A solutions architect uses AWS Application Discovery Service over the fleet and discovers that there is an Oracle data warehouse and several PostgreSQL databases. Which combination of migration patterns will reduce licensing costs and operational overhead? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Lift and shift the Oracle data warehouse to Amazon EC2 using AWS DMS.
       B. Migrate the Oracle data warehouse to Amazon Redshift using AWS SCT and AWS DMS
       C. Lift and shift the PostgreSQL databases to Amazon EC2 using AWS DMS.
       D. Migrate the PostgreSQL databases to Amazon RDS for PostgreSQL using AWS DMS.
       E. Migrate the Oracle data warehouse to an Amazon EMR managed cluster using AWS DMS.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A solutions architect needs to define a reference architecture for a solution for three-tier applications with web, application, and NoSQL data layers. The reference architecture must meet the following requirements:&lt;br&gt;
‚Äì High availability within an AWS Region.&lt;br&gt;
‚Äì Able to fail over in 1 minute to another AWS Region for disaster recovery.&lt;br&gt;
‚Äì Provide the most efficient solution while minimizing the impact on the user experience.&lt;br&gt;
Which combination of steps will meet these requirements? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use an Amazon Route 53 weighted routing policy set to 100/0 across the two selected Regions. Set Time to Live (TTL) to 1 hour.
       B. Use an Amazon Route 53 failover routing policy for failover from the primary Region to the disaster recovery Region. Set Time to Live (TTL) to 30 seconds.
       C. Use a global table within Amazon DynamoDB so data can be accessed in the two selected Regions.
       D. Back up data from an Amazon DynamoDB table in the primary Region every 60 minutes and then write the data to Amazon S3. Use S3 cross-Region replication to copy the data from the primary Region to the disaster recovery Region. Have a script import the data into DynamoDB in a disaster recovery scenario.
       E. Implement a hot standby model using Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use zonal Reserved Instances for the minimum number of servers and On-Demand Instances for any additional resources. Use Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use Spot Instances for the required resources.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A D E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has a Microsoft SQL Server database in its data center and plans to migrate data to Amazon Aurora MySQL. The company has already used the AWS Schema Conversion Tool to migrate triggers, stored procedures and other schema objects to Aurora MySQL. The database contains 1 TB of data and grows less than 1 MB per day. The company‚Äôs data center is connected to AWS through a dedicated 1Gbps AWS Direct Connect connection. The company would like to migrate data to Aurora MySQL and perform reconfigurations with minimal downtime to the applications. Which solution meets the company‚Äôs requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Shut down applications over the weekend. Create an AWS DMS replication instance and task to migrate existing data from SQL Server to Aurora MySQL. Perform application testing and migrate the data to the new database endpoint.
       B. Create an AWS DMS replication instance and task to migrate existing data and ongoing replication from SQL Server to Aurora MySQL. Perform application testing and migrate the data to the new database endpoint.
       C. Create a database snapshot of SQL Server on Amazon S3. Restore the database snapshot from Amazon S3 to Aurora MySQL. Create an AWS DMS replication instance and task for ongoing replication from SQL Server to Aurora MySQL. Perform application testing and migrate the data to the new database endpoint.
       D. Create a SQL Server native backup file on Amazon S3. Create an AWS DMS replication instance and task to restore the SQL Server backup file to Aurora MySQL. Create another AWS DMS task for ongoing replication from SQL Server to Aurora MySQL. Perform application testing and migrate the data to the new database endpoint.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company runs an application on a fleet of Amazon EC2 instances. The application requires low latency and random access to 100 GB of data. The application must be able to access the data at up to 3.000 IOPS. A Development team has configured the EC2 launch template to provision a 100-GB Provisioned IOPS (PIOPS) Amazon EBS volume with 3 000 IOPS provisioned. A Solutions Architect is tasked with lowering costs without impacting performance and durability. Which action should be taken?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an Amazon EFS file system with the performance mode set to Max I/O. Configure the EC2 operating system to mount the EFS file system.
       B. Create an Amazon EFS file system with the throughput mode set to Provisioned. Configure the EC2 operating system to mount the EFS file system.
       C. Update the EC2 launch template to allocate a new 1-TB EBS General Purpose SSO (gp2) volume.                
       D. Update the EC2 launch template to exclude the PIOPS volume. Configure the application to use local instance storage.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company recently transformed its legacy infrastructure provisioning scripts to AWS CloudFormation templates. The newly developed templates are hosted in the company‚Äôs private GitHub repository. Since adopting CloudFormation, the company has encountered several issues with updates to the CloudFormation templates, causing execution or creating an environment. Management is concerned by the increase in errors and has asked a Solutions Architect to design the automated testing of CloudFormation template updates. What should the Solution Architect do to meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use AWS CodePipeline to create a change set from the CloudFormation templates stored in the private GitHub repository. Execute the change set using AWS CodeDeploy. Include a CodePipeline action to test the deployment with testing scripts run by AWS CodeBuild.
       B. Mirror the GitHub repository to AWS CodeCommit using AWS Lambda. Use AWS CodeDeploy to create a change set from the CloudFormation templates and execute it. Have CodeDeploy test the deployment with testing scripts run by AWS CodeBuild.
       C. Use AWS CodePipeline to create and execute a change set from the CloudFormation templates stored in the GitHub repository. Configure a CodePipeline action to be deployed with testing scripts run by AWS CodeBuild.
       D. Mirror the GitHub repository to AWS CodeCommit using AWS Lambda. Use AWS CodeBuild to create a change set from the CloudFormation templates and execute it. Have CodeBuild test the deployment with testing scripts.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has several Amazon EC2 instances to both public and private subnets within a VPC that is not connected to the corporate network. A security group associated with the EC2 instances allows the company to use the Windows remote desktop protocol (RDP) over the internet to access the instances. The security team has noticed connection attempts from unknown sources. The company wants to implement a more secure solution to access the EC2 instances. Which strategy should a solutions architect implement?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Deploy a Linux bastion host on the corporate network that has access to all instances in the VPC.
       B. Deploy AWS Systems Manager Agent on the EC2 instances. Access the EC2 instances using Session Manager restricting access to users with permission.
       C. Deploy a Linux bastion host with an Elastic IP address in the public subnet. Allow access to the bastion host from 0.0.0.0/0.
       D. Establish a Site-to-Site VPN connecting the corporate network to the VPC. Update the security groups to allow access from the corporate network only.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A retail company has a custom .NET web application running on AWS that uses Microsoft SQL Server for the database. The application servers maintain a user‚Äôs session locally. Which combination of architecture changes are needed to ensure all tiers of the solution are highly available? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Refactor the application to store the user‚Äôs session in Amazon ElastiCache. Use Application Load Balancers to distribute the load between application instances.
       B. Set up the database to generate hourly snapshots using Amazon EBS. Configure an Amazon CloudWatch Events rule to launch a new database instance if the primary one fails.
       C. Migrate the database to Amazon RDS for SQL Server. Configure the RDS instance to use a MultiAZ deployment.
       D. Move the .NET content to an Amazon S3 bucket. Configure the bucket for static website hosting.                
       E. Put the application instances in an Auto Scaling group. Configure the Auto Scaling group to create new instances if an instance becomes unhealthy.
       F. Deploy Amazon CloudFront in front of the application tier. Configure CloudFront to serve content from healthy application instances only.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A B E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to improve cost awareness for its Amazon EMR platform. The company has allocated budgets for each team‚Äôs Amazon EMR usage. When a budgetary threshold is reached, a notification should be sent by email to the budget office‚Äôs distribution list. Teams should be able to view their EMR cluster expenses to date. A solutions architect needs to create a solution that ensures the policy is proactively and centrally enforced in a multi-account environment. Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Update the AWS CloudFormation template to include the AWS::Budgets::Budget::resource with the NotificationsWithSubscribers property.
       B. Implement Amazon CloudWatch dashboards for Amazon EMR usage.
       C. Create an EMR bootstrap action that runs at startup that calls the Cost Explorer API to set the budget on the cluster with the GetCostForecast and NotificationsWithSubscribers actions.
       D. Create an AWS Service Catalog portfolio for each team. Add each team‚Äôs Amazon EMR cluster as an AWS CloudFormation template to their Service Catalog portfolio as a Product.
       E. Create an Amazon CloudWatch metric for billing. Create a custom alert when costs exceed the budgetary threshold.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is migrating its on-premises systems to AWS. The user environment consists of the following systems:&lt;br&gt;
‚Äì Windows and Linux virtual machines running on VMware.&lt;br&gt;
‚Äì Physical servers running Red Hat Enterprise Linux.&lt;br&gt;
‚Äì The company wants to be able to perform the following steps before migrating to AWS:&lt;br&gt;
‚Äì Identify dependencies between on-premises systems.&lt;br&gt;
‚Äì Group systems together into applications to build migration plans.&lt;br&gt;
‚Äì Review performance data using Amazon Athena to ensure that Amazon EC2 instances are right-sized.&lt;br&gt;
How can these requirements be met?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Populate the AWS Application Discovery Service import template with information from an on premises configuration management database (CMDB). Upload the completed import template to Amazon S3, then import the data into Application Discovery Service.
       B. Install the AWS Application Discovery Service Discovery Agent on each of the on-premises systems. Allow the Discovery Agent to collect data for a period of time.
       C. Install the AWS Application Discovery Service Discovery Connector on each of the on-premises systems and in VMware vCenter. Allow the Discovery Connector to collect data for one week.
       D. Install the AWS Application Discovery Service Discovery Agent on the physical on-pre-map servers. Install the AWS Application Discovery Service Discovery Connector in VMware vCenter. Allow the Discovery Agent to collect data for a period of time.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to migrate its on-premises data center to the AWS Cloud. This includes thousands of virtualized Linux and Microsoft Windows servers, SAN storage, Java and PHP applications with MYSQL, and Oracle databases. There are many department services hosted either in the same data center or externally. The technical documentation is incomplete and outdated. A solutions architect needs to understand the current environment and estimate the cloud resource costs after the migration. Which tools or services should solutions architects use to plan the cloud migration (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. AWS Application Discovery Service
       B. AWS SMS
       C. AWS x-Ray
       D. AWS Cloud Adoption Readness Tool (CART)
       E. Amazon Inspector
       F. AWS Migration Hub
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B C F&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company decided to purchase Amazon EC2 Reserved Instances. A solutions architect is tasked with implementing a solution where only the master account in AWS Organizations is able to purchase the Reserved Instances. Current and future member accounts should be blocked from purchasing Reserved Instances. Which solution will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an SCP with the Deny effect on the ec2:PurchaseReservedInstancesOffering action. Attach the SCP to the root of the organization.
       B. Create a new organizational unit (OU) Move all current member accounts to the new OU. Create an SCP with the Deny effect on the ec2:PurchaseReservedInstancesOffering action. Attach the SCP to the new OU.
       C. Create an AWS Config rule event that triggers automation that will terminate any Reserved Instances launched by member accounts.
       D. Create two new organizational units (OUs): OU1 and OU2. Move all member accounts to OU2 and the master account to OU1. Create an SCP with the Allow effect on the ec2:PurchaseReservedInstancesOffering action. Attach the SCP to OU1.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is using multiple AWS accounts. The DNS records are stored in a private hosted zone for Amazon Route 53 in Account                A. The company‚Äôs applications and databases are running in Account                B. A solutions architect will deploy a two-tier application in a new VPC. To simplify the configuration, the db.example.com CNAME record set for the Amazon RDS endpoint was created in a private hosted zone for Amazon Route 53. During deployment the application failed to start. Troubleshooting revealed that db.example.com is not resolvable on the Amazon EC2 instance. The solutions architect confirmed that the record set was created correctly in Route 53. Which combination of steps should the solutions architect take to resolve this issue? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Deploy the database on a separate EC2 instance in the new VPC. Create a record set for the instance‚Äôs private IP in the private hosted zone.
       B. Use SSH to connect to the application tier EC2 instance. Add an RDS endpoint IP address to the /etc/resolv conf file.
       C. Create an authorization to associate the private hosted zone in Account A with the new VPC in Account                B.
       D. Create a private hosted zone for the example com domain in Account                B. Configure Route 53 replication between AWS accounts.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A solutions architect needs to advise a company on how to migrate its on-premises data processing application to the AWS Cloud. Currently, users upload input files through a web portal. The web server then stores the uploaded files on NAS and messages the processing server over a message queue. Each media file can take up to 1 hour to process. The company has determined that the number of media files awaiting processing is significantly higher during business hours, with the number of files rapidly declining after business hours. What is the MOST cost-effective migration recommendation?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in an Amazon S3 bucket.
       B. Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, create a new Amazon EC2 instance to pull requests from the queue and process the files. Store the processed files in Amazon EFS. Shut down the EC2 instance after the task is complete.
       C. Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in Amazon EFS.
       D. Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. Use Amazon EC2 instances in an EC2 Auto Seating group to pull requests from the queue and process the files. Scale the EC2 instances based on the SQS queue length. Store the processed files in an Amazon S3 bucket.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A utility company wants to collect usage data every 5 minutes from its smart meters to facilitate time-of-use metering. When a meter sends data to AWS, the data is sent to Amazon API Gateway, processed by an AWS Lambda function and stored in an Amazon DynamoDB table. During the pilot phase, the Lambda functions took from 3 to 5 seconds to complete. As more smart meters are deployed, the Engineers notice the Lambda functions are taking from 1 to 2 minutes to complete. The functions are also increasing in duration as new types of metrics are collected from the devices. There are many ProvisionedThroughputExceededException errors while performing PUT operations on DynamoDB, and there are also many TooManyRequestsException errors from Lambda. Which combination of changes will resolve these issues? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Increase the write capacity units to the DynamoDB table.
       B. Increase the memory available to the Lambda functions.
       C. Increase the payload size from the smart meters to send more data.
       D. Stream the data into an Amazon Kinesis data stream from API Gateway and process the data in batches.
       E. Collect data in an Amazon SQS FIFO queue, which triggers a Lambda function to process each message.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An AWS partner company is building a service in AWS Organizations using its organization named org1. This service requires the partner company to have access to AWS resources in a customer account, which is in a separate organization named org2. The company must establish least privilege security access using an API or command line tool to the customer account. What is the MOST secure way to allow org1 to access resources in org2?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. The customer should provide the partner company with their AWS account access keys to log in and perform the required tasks.
       B. The customer should create an IAM user and assign the required permissions to the IAM user. The customer should then provide the credentials to the partner company to log in and perform the required tasks.
       C. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role‚Äôs Amazon Resource Name (ARN) when requesting access to perform the required tasks.
       D. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role‚Äôs Amazon Resource Name (ARN), including the external ID in the IAM role‚Äôs trust policy, when requesting access to perform the required tasks.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company‚Äôs security compliance requirements state that all Amazon EC2 images must be scanned for vulnerabilities and must pass a CVE assessment. A solutions architect is developing a mechanism to create security- approved AMIs that can be used by developers. Any new AMIs should go through an automated assessment process and be marked as approved before developers can use them. The approved images must be scanned every 30 days to ensure compliance. Which combination of steps should the solutions architect take to meet these requirements while following best practices? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use the AWS Systems Manager EC2 agent to run the CVE assessment on the EC2 instances launched from the AMIs that need to be scanned.
       B. Use AWS Lambda to write automatic approval rules. Store the approved AMI list in AWS Systems Manager Parameter Store. Use Amazon EventBridge to trigger an AWS Systems Manager Automation document on all EC2 instances every 30 days.
       C. Use Amazon Inspector to run the CVE assessment on the EC2 instances launched from the AMIs that need to be scanned.
       D. Use AWS Lambda to write automatic approval rules. Store the approved AMI list in AWS Systems Manager Parameter Store. Use a managed AWS Config rule for continuous scanning on all EC2 instances, and use AWS Systems Manager Automation documents for remediation.
       E. Use AWS CloudTrail to run the CVE assessment on the EC2 instances launched from the AMIs that need to be scanned.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company uses AWS Organizations with a single OU named Production to manage multiple accounts. All accounts are members of the Production OU. Administrators use deny list SCPs in the root of the organization to manage access to restricted services. The company recently acquired a new business unit and invited the new unit‚Äôs existing AWS account to the organization. Once onboarded, the administrators of the new business unit discovered that they are not able to update existing AWS Config rules to meet the company‚Äôs policies. Which option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Remove the organization‚Äôs root SCPs that limit access to AWS Config. Create AWS Service Catalog products for the company‚Äôs standard AWS Config rules and deploy them throughout the organization, including the new account.
       B. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the new account to the Production OU when adjustments to AWS Config are complete.
       C. Convert the organization‚Äôs root SCPs from deny list SCPs to allow list SCPs to allow the required services only. Temporally apply an SCP to the organization‚Äôs root that allows AWS Config actions for principals only in the new account.
       D. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization‚Äôs root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is launching a web-based application in multiple regions around the world. The application consists of both static content stored in a private Amazon S3 bucket and dynamic content hosted in Amazon ECS containers content behind an Application Load Balancer (ALB). The company requires that the static and dynamic application content be accessible through Amazon CloudFront only. Which combination of steps should a solutions architect recommend to restrict direct content access to CloudFront? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a web ACL in AWS WAF with a rule to validate the presence of a custom header and associate the web ACL with the ALB.
       B. Create a web ACL in AWS WAF with a rule to validate the presence of a custom header and associate the web ACL with the CloudFront distribution.
       C. Configure CloudFront to add a custom header to origin requests.
       D. Configure the ALB to add a custom header to HTTP requests.
       E. Update the S3 bucket ACL to allow access from the CloudFront distribution only.
       F. Create a CloudFront Origin Access Identity (OAI) and add it to the CloudFront distribution. Update the S3 bucket policy to allow access to the OAI only.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A D F&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An ecommerce website running on AWS uses an Amazon RDS for MySQL DB instance with General Purpose SSD storage. The developers chose an appropriate instance type based on demand, and configured 100 GB of storage with a sufficient amount of free space. The website was running smoothly for a few weeks until a marketing campaign launched. On the second day of the campaign, users reported long wait times and time outs. Amazon CloudWatch metrics indicated that both reads and writes to the DB instance were experiencing long response times. The CloudWatch metrics show 40% to 50% CPU and memory utilization, and sufficient free storage space is still available. The application server logs show no evidence of database connectivity issues. What could be the root cause of the issue with the marketing campaign?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. It exhausted the I/O credit balance due to provisioning low disk storage during the setup phase.
       B. It caused the data in the tables to change frequently, requiring indexes to be rebuilt to optimize queries.
       C. It exhausted the maximum number of allowed connections to the database instance.
       D. It exhausted the network bandwidth available to the RDS for MySQL DB instances.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A solutions architect has been assigned to migrate a 50 TB Oracle data warehouse that contains sales data from on-premises to Amazon Redshift. Major updates to the sales data occur on the final calendar day of the month. For the remainder of the month, the data warehouse only receives minor daily updates and is primarily used for reading and reporting. Because of this, the migration process must start on the first day of the month and must be complete before the next set of updates occur. This provides approximately 30 days to complete the migration and ensure that the minor daily changes have been synchronized with the Amazon Redshift data warehouse. Because the migration cannot impact normal business network operations, the bandwidth allocated to the migration for moving data over the internet is 50 Mbps. The company wants to keep data migration costs low. Which steps will allow the solutions architect to perform the migration within the specified timeline?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Install Oracle database software on an Amazon EC2 instance. Configure VPN connectivity between AWS and the company‚Äôs data center. Configure the Oracle database running on Amazon EC2 to join the Oracle Real Application Clusters (RAC). When the Oracle database on Amazon EC2 finishes synchronizing, create an AWS DMS ongoing replication task to migrate the data from the Oracle database on Amazon EC2 to Amazon Redshift. Verify the data migration is complete and perform the cut over to Amazon Redshift.
       B. Create an AWS Snowball import job. Export a backup of the Oracle data warehouse. Copy the exported data to the Snowball device. Return the Snowball device to AWS. Create an Amazon RDS for Oracle database and restore the backup file to that RDS instance. Create an AWS DMS task to migrate the data from the RDS for Oracle database to Amazon Redshift. Copy daily incremental backups from Oracle in the data center to the RDS for Oracle database over the internet. Verify the data migration is complete and perform the cut over to Amazon Redshift.
       C. Install Oracle database software on an Amazon EC2 instance. To minimize the migration time, configure VPN connectivity between AWS and the company‚Äôs data center by provisioning a 1 Gbps AWS Direct Connect connection. Configure the Oracle database running on Amazon EC2 to be a read replica of the data center Oracle database. Start the synchronization process between the company‚Äôs on-premises data center and the Oracle database on Amazon EC2. When the Oracle database on Amazon EC2 is synchronized with the on-premises database, create an AWS DMS ongoing replication task to migrate the data from the Oracle database read replica that is running on Amazon EC2 to Amazon Redshift. Verify the data migration is complete and perform the cut over to Amazon Redshift.
       D. Create an AWS Snowball import job. Configure a server in the company‚Äôs data center with an extraction agent. Use AWS SCT to manage the extraction agent and convert the Oracle schema to an Amazon Redshift schema. Create a new project in AWS SCT using the registered data extraction agent. Create a local task and an AWS DMS task in AWS SCT with replication of ongoing changes. Copy data to the Snowball device and return the Snowball device to AWS. Allow AWS DMS to copy data from Amazon S3 to Amazon Redshift. Verify that the data migration is complete and perform the cut over to Amazon Redshift.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

</description>
      <category>awslagi</category>
      <category>googlecloud</category>
      <category>aws</category>
    </item>
    <item>
      <title>AWS Certified Solutions Architect Professional SAP-C01 Exam Questions Part 4</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 14:54:28 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-4-407b</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-4-407b</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;For AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A solutions architect is designing a disaster recovery strategy for a three-tier application. The application has an RTO of 30 minutes and an RPO of 5 minutes for the data tier. The application and web tiers are stateless and leverage a fleet of Amazon EC2 instances. The data tier consists of a 50 TB Amazon Aurora database. Which combination of steps satisfies the RTO and RPO requirements while optimizing costs? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create daily snapshots of the EC2 instances and replicate the snapshots to another Region.
       B. Deploy a hot standby of the application to another Region.
       C. Create snapshots of the Aurora database every 5 minutes.
       D. Create a cross-Region Aurora Replica of the database.
       E. Create an AWS Backup job to replicate data to another Region.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has a primary Amazon S3 bucket that receives thousands of objects every day. The company needs to replicate these objects into several other S3 buckets from various AWS accounts. A solutions architect is designing a new AWS Lambda function that is triggered when an object is created in the main bucket and replicates the object into the target buckets. The objects do not need to be replicated in real time. There is concern that this function may impact other critical Lambda functions due to Lambda‚Äôs regional concurrency limit. How can the solutions architect ensure this new Lambda function will not impact other critical Lambda functions?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Set the new Lambda function reserved concurrency limit to ensure the executions do not impact other critical Lambda functions. Monitor existing critical Lambda functions with Amazon CloudWatch alarms for the Throttles Lambda metric.
       B. Increase the execution timeout of the new Lambda function to 5 minutes. Monitor existing critical Lambda functions with Amazon CloudWatch alarms for the Throttles Lambda metric.
       C. Configure S3 event notifications to add events to an Amazon SQS queue in a separate account. Create the new Lambda function in the same account as the SQS queue and trigger the function when a message arrives in the queue.
       D. Ensure the new Lambda function implements an exponential backoff algorithm. Monitor existing critical Lambda functions with Amazon CloudWatch alarms for the Throttles Lambda metric.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to run a serverless application on AWS. The company plans to provision its application in Docker containers running in an Amazon ECS cluster. The application requires a MySQL database and the company plans to use Amazon RDS. The company has documents that need to be accessed frequently for the first 3 months, and rarely after that. The document must be retained for 7 years. What is the MOST cost-effective solution to meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an ECS cluster using On-Demand Instances. Provision the database and its read replicas in Amazon RDS using Spot Instances. Store the documents in an encrypted EBS volume, and create a cron job to delete the documents after 7 years.
       B. Create an ECS cluster using a fleet of Spot Instances, with Spot Instance draining enabled. Provision the database and its read replicas in Amazon RDS using Reserved Instances. Store the documents in a secured Amazon S3 bucket with a lifecycle policy to move the documents that are older than 3 months to Amazon S3 Glacier, then delete the documents from Amazon S3 Glacier that are more than 7 years old.
       C. Create an ECS cluster using On-Demand Instances. Provision the database and its read replicas in Amazon RDS using On-Demand Instances. Store the documents in Amazon EFS. Create a cron job to move the documents that are older than 3 months to Amazon S3 Glacier. Create an AWS Lambda function to delete the documents in S3 Glacier that are older than 7 years.
       D. Create an ECS cluster using a fleet of Spot Instances with Spot Instance draining enabled. Provision the database and its read replicas in Amazon RDS using On-Demand Instances. Store the documents in a secured Amazon S3 bucket with a lifecycle policy to move the documents that are older than 3 months to Amazon S3 Glacier, then delete the documents in Amazon S3 Glacier after 7 years.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A financial services company receives a regular data feed from its credit card servicing partner. Approximately 5,000 records are sent every 15 minutes in plaintext, delivered over HTTPS directly into an Amazon S3 bucket with server-side encryption. This feed contains sensitive credit card primary account number (PAN) data. The company needs to automatically mask the PAN before sending the data to another S3 bucket for additional internal processing. The company also needs to remove and merge specific fields, and then transform the record into JSON format. Additionally, extra feeds are likely to be added in the future, so any design needs to be easily expandable. Which solutions will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Trigger an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Trigger another Lambda function when new messages arrive in the SQS queue to process the records, writing the results to a temporary location in Amazon S3. Trigger a final Lambda function once the SQS queue is empty to transform the records into JSON format and send the results to another S3 bucket for internal processing.
       B. Trigger an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Configure an AWS Fargate container application to automatically scale to a single instance when the SQS queue contains messages. Have the application process each record, and transform the record into JSON format. When the queue is empty, send the results to another S3 bucket for internal processing and scale down the AWS Fargate instance.
       C. Create an AWS Glue crawler and custom classifier based on the data feed formats and build a table definition to match. Trigger an AWS Lambda function on file delivery to start an AWS Glue ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, have the ETL job send the results to another S3 bucket for internal processing.
       D. Create an AWS Glue crawler and custom classifier based upon the data feed formats and build a table definition to match. Perform an Amazon Athena query on file delivery to start an Amazon EMR ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, send the results to another S3 bucket for internal processing and scale down the EMR cluster.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A media company is serving video files stored in Amazon S3 using Amazon CloudFront. The development team needs access to the logs to diagnose faults and perform service monitoring. The log files from CloudFront may contain sensitive information about users. The company uses a log processing service to remove sensitive information before making the logs available to the development team. The company has the following requirements for the unprocessed logs:&lt;br&gt;
‚Äì The logs must be encrypted at rest and must be accessible by the log processing service only.&lt;br&gt;
‚Äì Only the data protection team can control access to the unprocessed log files.&lt;br&gt;
‚Äì AWS CloudFormation templates must be stored in AWS CodeCommit.&lt;br&gt;
‚Äì AWS CodePipeline must be triggered on commit to perform updates made to CloudFormation templates.&lt;br&gt;
‚Äì CloudFront is already writing the unprocessed logs to an Amazon S3 bucket, and the log processing service is operating against this S3 bucket.&lt;br&gt;
Which combination of steps should a solutions architect take to meet the company‚Äôs requirements? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an AWS KMS key that allows the AWS Logs Delivery account to generate data keys for encryption Configure S3 default encryption to use server-side encryption with KMS managed keys (SSEKMS) on the log storage bucket using the new KMS key. Modify the KMS key policy to allow the log processing service to perform decrypt operations.
       B. Create an AWS KMS key that follows the CloudFront service role to generate data keys for encryption Configure S3 default encryption to use KMS managed keys (SSE-KMS) on the log storage bucket using the new KMS key Modify the KMS key policy to allow the log processing service to perform decrypt operations.
       C. Configure S3 default encryption to use AWS KMS managed keys (SSE-KMS) on the log storage bucket using the AWS Managed S3 KMS key. Modify the KMS key policy to allow the CloudFront service role to generate data keys for encryption Modify the KMS key policy to allow the log processing service to perform decrypt operations.
       D. Create a new CodeCommit repository for the AWS KMS key template. Create an IAM policy to allow commits to the new repository and attach it to the data protection team‚Äôs users. Create a new CodePipeline pipeline with a custom IAM role to perform KMS key updates using CloudFormation Modify the KMS key policy to allow the CodePipeline IAM role to modify the key policy.
       E. Use the existing CodeCommit repository for the AWS KMS key template. Create an IAM policy to allow commits to the new repository and attach it to the data protection team‚Äôs users. Modify the existing CodePipeline pipeline to use a custom IAM role and to perform KMS key updates using CloudFormation. Modify the KMS key policy to allow the CodePipeline IAM role to modify the key policy.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company‚Äôs service for video game recommendations has just gone viral. The company has new users from all over the world. The website for the service is hosted on a set of Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The website consists of static content with different resources being loaded depending on the device type. Users recently reported that the load time for the website has increased. Administrators are reporting high loads on the EC2 instances that host the service. Which set actions should a solutions architect take to improve response times?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create separate Auto Scaling groups based on device types. Switch to Network Load Balancer (NLB). Use the User-Agent HTTP header in the NLB to route to a different set of EC2 instances.
       B. Move content to Amazon S3. Create an Amazon CloudFront distribution to serve content out of the S3 bucket. Use Lambda@Edge to load different resources based on the User-Agent HTTP header.
       C. Create a separate ALB for each device type. Create one Auto Scaling group behind each ALB. Use Amazon Route 53 to route to different ALBs depending on the User-Agent HTTP header.
       D. Move content to Amazon S3. Create an Amazon CloudFront distribution to serve content out of the S3 bucket. Use the User-Agent HTTP header to load different content.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is planning a large event where a promotional offer will be introduced. The company‚Äôs website is hosted on AWS and backed by an Amazon RDS for PostgreSQL DB instance. The website explains the promotion and includes a sign-up page that collects user information and preferences. Management expects large and unpredictable volumes of traffic periodically, which will create many database writes. A solutions architect needs to build a solution that does not change the underlying data model and ensures that submissions are not dropped before they are committed to the database. Which solution meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Immediately before the event, scale up the existing DB instance to meet the anticipated demand. Then scale down after the event.
       B. Use Amazon SQS to decouple the application and database layers. Configure an AWS Lambda function to write items from the queue into the database.
       C. Migrate to Amazon DynamoDB and manage throughput capacity with automatic scaling.
       D. Use Amazon ElastiCache for Memcached to increase write capacity to the DB instance.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A mobile app has become very popular, and usage has gone from a few hundred to millions of users. Users capture and upload images of activities within a city, and provide ratings and recommendations. Data access patterns are unpredictable. The current application is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The application is experiencing slowdowns and costs are growing rapidly. Which changes should a solutions architect make to the application architecture to control costs and improve performance?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an Amazon CloudFront distribution and place the ALB behind the distribution. Store static content in Amazon S3 in an Infrequent Access storage class.
       B. Store static content in an Amazon S3 bucket using the Intelligent Tiering storage class. Use an Amazon CloudFront distribution in front of the S3 bucket and the ALB.
       C. Place AWS Global Accelerator in front of the ALB. Migrate the static content to Amazon EFS, and then run an AWS Lambda function to resize the images during the migration process.
       D. Move the application code to AWS Fargate containers and swap out the EC2 instances with the Fargate containers.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A financial company with multiple departments wants to expand its on-premises environment to the AWS Cloud. The company must retain centralized access control using an existing on premises Active Directory (AD) service. Each department should be allowed to create AWS accounts with preconfigured networking and should have access to only a specific list of approved services. Departments are not permitted to have account administrator permissions. What should a solutions architect do to meet these security requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure AWS Identity and Access Management (IAM) with a SAML identity provider (IdP) linked to the on-premises Active Directory, and create a role to grant access. Configure AWS Organizations with SCPs and create new member accounts. Use AWS CloudFormation templates to configure the member account networking.
       B. Deploy an AWS Control Tower landing zone. Create an AD Connector linked to the on-premises Active Directory. Change the identity source in AWS Single Sign-On to use Active Directory. Allow department administrators to use Account Factory to create new member accounts and networking. Grant the departments AWS power user permissions on the created accounts.
       C. Deploy an Amazon Cloud Directory. Create a two-way trust relationship with the on-premises Active Directory, and create a role to grant access. Set up an AWS Service Catalog to use AWS CloudFormation templates to create the new member accounts and networking. Use IAM roles to allow access to approved AWS services.
       D. Configure AWS Directory Service for Microsoft Active Directory with AWS Single Sign-On. Join the service to the on-premises Active Directory. Use AWS CloudFormation to create new member accounts and networking. Use IAM roles to allow access to approved AWS services.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A large financial company is deploying applications that consist of Amazon EC2 and Amazon RDS instances to the AWS Cloud using AWS CloudFormation. The CloudFormation stack has the following stack policy:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The company wants to ensure that developers do not lose data by accidentally removing or replacing RDS instances when updating the CloudFormation stack. Developers also still need to be able to modify or remove EC2 instances as needed. How should the company change the stack policy to meet these requirements?&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;           A. Modify the statement to specify ‚ÄúEffect‚Äù: ‚ÄúDeny‚Äù, ‚ÄúAction‚Äù:[‚ÄúUpdate:*‚Äù] for all logical RDS resources.                B. Modify the statement to specify ‚ÄúEffect‚Äù: ‚ÄúDeny‚Äù, ‚ÄúAction‚Äù:[‚ÄúUpdate:Delete‚Äù] for all logical RDS resources.
           C. Add a second statement that specifies ‚ÄúEffect‚Äù: ‚ÄúDeny‚Äù, ‚ÄúAction‚Äù:[‚ÄúUpdate:Delete‚Äù, ‚ÄúUpdate:Replace‚Äù] for all logical RDS resources.
           D. Add a second statement that specifies ‚ÄúEffect‚Äù: ‚ÄúDeny‚Äù, ‚ÄúAction‚Äù:[‚ÄúUpdate:*‚Äù] for all logical RDS resources.
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is currently in the design phase of an application that will need an RPO of less than 5 minutes and an RTO of less than 10 minutes. The solutions architecture team is forecasting that the database will store approximately 10 TB of data. As part of the design, they are looking for a database solution that will provide the company with the ability to fail over to a secondary Region. Which solution will meet these business requirements at the LOWEST cost?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Deploy an Amazon Aurora DB cluster and take snapshots of the cluster every 5 minutes. Once a snapshot is complete, copy the snapshot to a secondary Region to serve as a backup in the event of a failure.
       B. Deploy an Amazon RDS instance with a cross-Region read replica in a secondary Region. In the event of a failure, promote the read replica to become the primary.
       C. Deploy an Amazon Aurora DB cluster in the primary Region and another in a secondary Region. Use AWS DMS to keep the secondary Region in sync.
       D. Deploy an Amazon RDS instance with a read replica in the same Region. In the event of a failure, promote the read replica to become the primary.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has a web application that uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. A recent marketing campaign has increased demand. Monitoring software reports that many requests have significantly longer response times than before the marketing campaign. A solutions architect enabled Amazon CloudWatch Logs for API Gateway and noticed that errors are occurring on 20% of the requests. In CloudWatch, the Lambda function Throttles metric represents 1% of the requests and the Errors metric represents 10% of the requests. Application logs indicate that, when errors occur, there is a call to DynamoDB. What change should the solutions architect make to improve the current response times as the web application becomes more popular?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Increase the concurrency limit of the Lambda function
       B. Implement DynamoDB auto scaling on the table
       C. Increase the API Gateway throttle limit
       D. Re-create the DynamoDB table with a better-partitioned primary index
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A European online newspaper service hosts its public-facing WordPress site in a collocated data center in London. The current WordPress infrastructure consists of a load balancer, two web servers, and one MySQL database server. A solutions architect is tasked with designing a solution with the following requirements:&lt;br&gt;
‚Äì Improve the website‚Äôs performance&lt;br&gt;
‚Äì Make the web tier scalable and stateless&lt;br&gt;
‚Äì Improve the database server performance for read-heavy loads&lt;br&gt;
‚Äì Reduce latency for users across Europe and the US Design the new architecture with a goal of 99.9% availability&lt;br&gt;
Which solution meets these requirements while optimizing operational efficiency?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use an Application Load Balancer (ALB) in front of an Auto Scaling group of WordPress Amazon EC2 instances in one AWS Region and three Availability Zones. Configure an Amazon ElastiCache cluster in front of a Multi-AZ Amazon Aurora MySQL DB cluster. Move the WordPress shared files to Amazon EFS. Configure Amazon CloudFront with the ALB as the origin, and select a price class that includes the US and Europe.
       B. Use an Application Load Balancer (ALB) in front of an Auto Scaling group of WordPress Amazon EC2 instances in two AWS Regions and two Availability Zones in each Region. Configure an Amazon ElastiCache cluster in front of a global Amazon Aurora MySQL database. Move the WordPress shared files to Amazon EFS. Configure Amazon CloudFront with the ALB as the origin, and select a price class that includes the US and Europe. Configure EFS cross-Region replication.
       C. Use an Application Load Balancer (ALB) in front of an Auto Scaling group of WordPress Amazon EC2 instances in one AWS Region and three Availability Zones. Configure an Amazon DocumentDB table in front of a Multi-AZ Amazon Aurora MySQL DB cluster. Move the WordPress shared files to Amazon EFS. Configure Amazon CloudFront with the ALB as the origin, and select a price class that includes all global locations.
       D. Use an Application Load Balancer (ALB) in front of an Auto Scaling group of WordPress Amazon EC2 instances in two AWS Regions and three Availability Zones in each Region. Configure an Amazon ElastiCache cluster in front of a global Amazon Aurora MySQL database. Move the WordPress shared files to Amazon FSx with cross-Region synchronization. Configure Amazon CloudFront with the ALB as the origin and a price class that includes the US and Europe.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company built an ecommerce website on AWS using a three-tier web architecture. The application is Java-based and composed of an Amazon CloudFront distribution, an Apache web server layer of Amazon EC2 instances in an Auto Scaling group, and a backend Amazon Aurora MySQL database. Last month, during a promotional sales event, users reported errors and timeouts while adding items to their shopping carts. The operations team recovered the logs created by the web servers and reviewed Aurora DB cluster performance metrics. Some of the web servers were terminated before logs could be collected and the Aurora metrics were not sufficient for query performance analysis. Which combination of steps must the solutions architect take to improve application performance visibility during peak traffic events? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure the Aurora MySQL DB cluster to publish slow query and error logs to Amazon CloudWatch Logs.
       B. Implement the AWS X-Ray SDK to trace incoming HTTP requests on the EC2 instances and implement tracing of SQL queries with the X-Ray SDK for Java.
       C. Configure the Aurora MySQL DB cluster to stream slow query and error logs to Amazon Kinesis
       D. Install and configure an Amazon CloudWatch Logs agent on the EC2 instances to send the Apache logs to CloudWatch Logs.
       E. Enable and configure AWS CloudTrail to collect and analyze application activity from Amazon EC2 and Aurora.                
       F. Enable Aurora MySQL DB cluster performance benchmarking and publish the stream to AWS XRay.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B C E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A solutions architect has an operational workload deployed on Amazon EC2 instances in an Auto Scaling group. The VPC architecture spans two Availability Zones (AZ) with a subnet in each that the Auto Scaling group is targeting. The VPC is connected to an on-premises environment and connectivity cannot be interrupted. The maximum size of the Auto Scaling group is 20 instances in service. The VPC IPv4 addressing is as follows:&lt;br&gt;
‚Äì VPC CIDR: 10.0.0.0/23&lt;br&gt;
‚Äì AZ1 subnet CIDR: 10.0.0.0/24&lt;br&gt;
‚Äì AZ2 subnet CIDR: 10.0.1.0/24&lt;br&gt;
Since deployment, a third AZ has become available in the Region. The solutions architect wants to adopt the new AZ without adding additional IPv4 address space and without service downtime. Which solution will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Update the Auto Scaling group to use the AZ2 subnet only. Delete and re-create the AZ1 subnet using half the previous address space. Adjust the Auto Scaling group to also use the new AZ1 subnet. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Remove the current AZ2 subnet. Create a new AZ2 subnet using the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets.
       B. Terminate the EC2 instances in the AZ1 subnet. Delete and re-create the AZ1 subnet using half the address space. Update the Auto Scaling group to use this new subnet. Repeat this for the second AZ. Define a new subnet in AZ3, then update the Auto Scaling group to target all three new subnets.
       C. Create a new VPC with the same IPv4 address space and define three subnets, with one for each AZ. Update the existing Auto Scaling group to target the new subnets in the new VPC.
       D. Update the Auto Scaling group to use the AZ2 subnet only. Update the AZ1 subnet to have the previous address space. Adjust the Auto Scaling group to also use the AZ1 subnet again. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Update the current AZ2 subnet and assign the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is storing data on premises on a Windows file server. The company produces 5 GB of new data daily. The company migrated part of its Windows-based workload to AWS and needs the data to be available on a file system in the cloud. The company already has established an AWS Direct Connect connection between the on-premises network and AWS. Which data migration strategy should the company use?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use the file gateway option in AWS Storage Gateway to replace the existing Windows file server, and point the existing file share to the new file gateway
       B. Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon FSx
       C. Use AWS Data Pipeline to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS)
       D. Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS)
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company uses AWS Organizations to manage one parent account and nine member accounts. The number of member accounts is expected to grow as the business grows. A security engineer has requested consolidation of AWS CloudTrail logs into the parent account for compliance purposes. Existing logs currently stored in Amazon S3 buckets in each individual member account should not be lost. Future member accounts should comply with the logging strategy. Which operationally efficient solution meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an AWS Lambda function in each member account with a cross-account role. Trigger the Lambda functions when new CloudTrail logs are created and copy the CloudTrail logs to a centralized S3 bucket. Set up an Amazon CloudWatch alarm to alert if CloudTrail is not configured properly.
       B. Configure CloudTrail in each member account to deliver log events to a central S3 bucket. Ensure the central S3 bucket policy allows PutObject access from the member accounts. Migrate existing logs to the central S3 bucket. Set up an Amazon CloudWatch alarm to alert if CloudTrail is not configured properly.
       C. Configure an organization-level CloudTrail in the parent account to deliver log events to a central S3 bucket. Migrate the existing CloudTrail logs from each member account to the central S3 bucket. Delete the existing CloudTrail and logs in the member accounts.
       D. Configure an organization-level CloudTrail in the parent account to deliver log events to a central S3 bucket. Configure CloudTrail in each member account to deliver log events to the central S3 bucket.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A weather service provides high-resolution weather maps from a web application hosted on AWS in the eu-west-1 Region. The weather maps are updated frequently and stored in Amazon S3 along with static HTML content. The web application is fronted by Amazon CloudFront. The company recently expanded to serve users in the us-east-1 Region, and these new users report that viewing their respective weather maps is slow from time to time. Which combination of steps will resolve the us-east-1 performance issues? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure the AWS Global Accelerator endpoint for the S3 bucket in eu-west-1. Configure endpoint groups for TCP ports 80 and 443 in us-east-1.
       B. Create a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1.
       C. Use Lambda@Edge to modify requests from North America to use the S3 Transfer Acceleration endpoint in us-east-1.
       D. Use Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1.
       E. Configure the AWS Global Accelerator endpoint for us-east-1 as an origin on the CloudFront distribution. Use Lambda@Edge to modify requests from North America to use the new origin.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is deploying a public-facing global application on AWS using Amazon CloudFront. The application communicates with an external system. A solutions architect needs to . Which combination of steps will satisfy these requirements? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a public certificate for the required domain in AWS Certificate Manager and deploy it to CloudFront, an Application Load Balancer, and Amazon EC2 instances.
       B. Acquire a public certificate from a third-party vendor and deploy it to CloudFront, an Application Load Balancer, and Amazon EC2 instances.
       C. Provision Amazon EBS encrypted volumes using AWS KMS and ensure explicit encryption of data when writing to Amazon EBS.
       D. Provision Amazon EBS encrypted volumes using AWS KMS.
       E. Use SSL or encrypt data while communicating with the external system using a VPN.
       F. Communicate with the external system using plaintext and use the VPN to encrypt the data in transit.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A C E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company provides a centralized Amazon EC2 application hosted in a single shared VPC. The centralized application must be accessible from client applications running in the VPCs of other business units. The centralized application front end is configured with a Network Load Balancer (NLB) for scalability. Up to 10 business unit VPCs will need to be connected to the shared VPC. Some of the business unit VPC CIDR blocks overlap with the shared VPC, and some overlap with each other. Network connectivity to the centralized application in the shared VPC should be allowed from authorized business unit VPCs only. Which network configuration should a solutions architect use to provide connectivity from the client applications in the business unit VPCs to the centralized application in the shared VPC?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an AWS Transit Gateway. Attach the shared VPC and the authorized business unit VPCs to the transit gateway. Create a single transit gateway route table and associate it with all of the attached VPCs. Allow automatic propagation of routes from the attachments into the route table. Configure VPC routing tables to send traffic to the transit gateway
       B. Create a VPC endpoint service using the centralized application NLB and enable the option to require endpoint acceptance. Create a VPC endpoint in each of the business unit VPCs using the service name of the endpoint service. Accept authorized endpoint requests from the endpoint service console.
       C. Create a VPC peering connection from each business unit VPC to the shared VPC. Accept the VPC peering connections from the shared VPC console. Configure VPC routing tables to send traffic to the VPC peering connection.
       D. Configure a virtual private gateway for the shared VPC and create customer gateways for each of the authorized business unit VPCs. Establish a Site-to-Site VPN connection from the business unit VPCs to the shared VPC. Configure VPC routing tables to send traffic to the VPN connection.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

</description>
      <category>awslagi</category>
      <category>aws</category>
      <category>googlecloud</category>
    </item>
    <item>
      <title>Effortlessly install TailwindCss in a Rails app with Webpack (minimum configuration)</title>
      <author>Vernes</author>
      <pubDate>Sun, 22 Aug 2021 14:26:28 +0000</pubDate>
      <link>https://dev.to/wizardhealth/effortlessly-install-tailwindcss-in-a-rails-app-with-webpack-minimum-configuration-14gg</link>
      <guid>https://dev.to/wizardhealth/effortlessly-install-tailwindcss-in-a-rails-app-with-webpack-minimum-configuration-14gg</guid>
      <description>&lt;p&gt;A while back &lt;a href="https://twitter.com/dhh"&gt;DHH&lt;/a&gt; decided to created a &lt;a href="https://github.com/rails/tailwindcss-rails"&gt;gem&lt;/a&gt; for easily installing TailwindCss into rails apps üôåüèª. This gem could be used to install Tailwind through the asset pipeline as well as using webpack. This &lt;a href="https://github.com/rails/tailwindcss-rails/issues/62#issuecomment-900193727"&gt;changed&lt;/a&gt; later on as contributors wanted to focus their attention to what was the heart of the gem ü•∫.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://i.giphy.com/media/d10dMmzqCYqQ0/giphy.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://i.giphy.com/media/d10dMmzqCYqQ0/giphy.gif"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Well, since a lot of people used this gem to get a new Rails app going with Tailwind without the hassle of configuring everything from scratch (including colleagues from my company), &lt;a href="https://dev.to/wizardhealth"&gt;we decided&lt;/a&gt; to create a &lt;a href="https://github.com/WizardComputer/tailwindcss-rails-webpacker"&gt;new gem&lt;/a&gt; üéâ. This gem installs Tailwind with Webpack and has production purging enabled. Other Webpack specific problems are to be addressed as well.&lt;/p&gt;

</description>
      <category>tailwindcss</category>
      <category>webpack</category>
      <category>rails</category>
      <category>gem</category>
    </item>
    <item>
      <title>AWS Certified Solutions Architect Professional SAP-C01 Exam Questions Part 3</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 14:20:03 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-3-3leg</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-3-3leg</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;For AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has an on-premises monitoring solution using a PostgreSQL database for persistence of events. The database is unable to scale due to heavy ingestion and it frequently runs out of storage. The company wants to create a hybrid solution and has already set up a VPN connection between its network and AWS. The solution should include the following attributes:&lt;br&gt;
‚Äì Managed AWS services to minimize operational complexity.&lt;br&gt;
‚Äì A buffer that automatically scales to match the throughput of data and requires no ongoing administration.&lt;br&gt;
‚Äì A visualization tool to create dashboards to observe events in near-real time. Support for semi-structured JSON data and dynamic schemas.&lt;br&gt;
Which combination of components will enable the company to create a monitoring solution that will satisfy these requirements? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events.
       B. Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events.
       C. Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.
       D. Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards.
       E. Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A life sciences company is using a combination of open source tools to manage data analysis workflows and Docker containers running on servers in its on-premises data center to process genomics data. Sequencing data is generated and stored on a local storage area network (SAN), and then the data is processed. The research and development teams are running into capacity issues and have decided to re-architect their genomics analysis platform on AWS to scale based on workload demands and reduce the turnaround time from weeks to days. The company has a high-speed AWS Direct Connect connection. Sequencers will generate around 200 GB of data for each genome, and individual jobs can take several hours to process the data with ideal compute capacity. The end result will be stored in Amazon S3. The company is expecting 10-15 job requests each day. Which solution meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use regularly scheduled AWS Snowball Edge devices to transfer the sequencing data into AWS. When AWS receives the Snowball Edge device and the data is loaded into Amazon S3, use S3 events to trigger an AWS Lambda function to process the data.
       B. Use AWS Data Pipeline to transfer the sequencing data to Amazon S3. Use S3 events to trigger an Amazon EC2 Auto Scaling group to launch custom-AMI EC2 instances running the Docker containers to process the data.
       C. Use AWS DataSync to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow. Store the Docker images in Amazon Elastic Container Registry (Amazon ECR) and trigger AWS Batch to run the container and process the sequencing data.
       D. Use an AWS Storage Gateway file gateway to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Batch job that executes on Amazon EC2 instances running the Docker containers to process the data.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has five physical data centers in specific locations around the world. Each data center has hundreds of physical servers with a mix of Windows and Linux-based applications and database services. Each data center also has an AWS Direct Connect connection of 10 Gbps to AWS with a company-approved VPN solution to ensure that data transfer is secure. The company needs to shut down the existing data centers as quickly as possible and migrate the servers and applications to AWS. Which solution meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Install the AWS Server Migration Service (AWS SMS) connector onto each physical machine. Use the AWS Management Console to select the servers from the server catalog, and start the replication. Once the replication is complete, launch the Amazon EC2 instances created by the service.
       B. Install the AWS DataSync agent onto each physical machine. Use the AWS Management Console to configure the destination to be an AMI, and start the replication. Once the replication is complete, launch the Amazon EC2 instances created by the service.
       C. Install the CloudEndure Migration agent onto each physical machine. Create a migration blueprint, and start the replication. Once the replication is complete, launch the Amazon EC2 instances in cutover mode.
       D. Install the AWS Application Discovery Service agent onto each physical machine. Use the AWS Migration Hub import option to start the replication. Once the replication is complete, launch the Amazon EC2 instances created by the service.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A security engineer determined that an existing application retrieves credentials to an Amazon RDS for MySQL database from an encrypted file in Amazon S3. For the next version of the application, the security engineer wants to implement the following application design changes to improve security:&lt;br&gt;
‚Äì The database must use strong, randomly generated passwords stored in a secure AWS managed service.&lt;br&gt;
‚Äì The application resources must be deployed through AWS CloudFormation.&lt;br&gt;
‚Äì The application must rotate credentials for the database every 90 days.&lt;br&gt;
A solutions architect will generate a CloudFormation template to deploy the application. Which resources specified in the CloudFormation template will meet the security engineer‚Äôs requirements with the LEAST amount of operational overhead?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Specify a Secrets Manager RotationSchedule resource to rotate the database password every 90 days.
       B. Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Create an AWS Lambda function resource to rotate the database password. Specify a Parameter Store RotationSchedule resource to rotate the database password every 90 days.
       C. Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Create an Amazon EventBridge scheduled rule resource to trigger the Lambda function password rotation every 90 days.
       D. Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Specify an AWS AppSync DataSource resource to automatically rotate the database password every 90 days.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has a three-tier application running on AWS with a web server, an application server, and an Amazon RDS MySQL DB instance. A solutions architect is designing a disaster recovery (DR) solution with an RPO of 5 minutes. Which solution will meet the company‚Äôs requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure AWS Backup to perform cross-Region backups of all servers every 5 minutes. Reprovision the three tiers in the DR Region from the backups using AWS CloudFormation in the event of a disaster.
       B. Maintain another running copy of the web and application server stack in the DR Region using AWS CloudFormation drift detection. Configure cross-Region snapshots of the DB instance to the DR Region every 5 minutes. In the event of a disaster, restore the DB instance using the snapshot in the DR Region.
       C. Use Amazon EC2 Image Builder to create and copy AMIs of the web and application server to both the primary and DR Regions. Create a cross-Region read replica of the DB instance in the DR Region. In the event of a disaster, promote the read replica to become the master and reprovision the servers with AWS CloudFormation using the AMIs.
       D. Create AMIs of the web and application servers in the DR Region. Use scheduled AWS Glue jobs to synchronize the DB instance with another DB instance in the DR Region. In the event of a disaster, switch to the DB instance in the DR Region and reprovision the servers with AWS CloudFormation using the AMIs.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to migrate its corporate data center from on premises to the AWS Cloud. The data center includes physical servers and VMs that use VMware and Hyper-V. An administrator needs to select the correct services to collect data for the initial migration discovery process. The data format should be supported by AWS Migration Hub. The company also needs the ability to generate reports from the data. Which solution meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use the AWS Agentless Discovery Connector for data collection on physical servers and all VMs. Store the collected data in Amazon S3. Query the data with S3 Select. Generate reports by using Kibana hosted on Amazon EC2.
       B. Use the AWS Application Discovery Service agent for data collection on physical servers and all VMs. Store the collected data in Amazon Elastic File System (Amazon EFS). Query the data and generate reports with Amazon Athena.
       C. Use the AWS Application Discovery Service agent for data collection on physical servers and Hyper-V. Use the AWS Agentless Discovery Connector for data collection on VMware. Store the collected data in Amazon S3. Query the data with Amazon Athena. Generate reports by using Amazon QuickSight.
       D. Use the AWS Systems Manager agent for data collection on physical servers. Use the AWS Agentless Discovery Connector for data collection on all VMs. Store, query, and generate reports from the collected data by using Amazon Redshift.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is using Amazon Aurora MySQL for a customer relationship management (CRM) application. The application requires frequent maintenance on the database and the Amazon EC2 instances on which the application runs. For AWS Management Console access, the system administrators authenticate against AWS Identity and Access Management (IAM) using an internal identity provider. For database access, each system administrator has a user name and password that have previously been configured within the database. A recent security audit revealed that the database passwords are not frequently rotated. The company wants to replace the passwords with temporary credentials using the company‚Äôs existing AWS access controls. Which set of options will meet the company‚Äôs requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a new AWS Systems Manager Parameter Store entry for each database password. Enable parameter expiration to invoke an AWS Lambda function to perform password rotation by updating the parameter value. Create an IAM policy allowing each system administrator to retrieve their current password from the Parameter Store. Use the AWS CLI to retrieve credentials when connecting to the database.
       B. Create a new AWS Secrets Manager entry for each database password. Configure password rotation for each secret using an AWS Lambda function in the same VPC as the database cluster. Create an IAM policy allowing each system administrator to retrieve their current password. Use the AWS CLI to retrieve credentials when connecting to the database.
       C. Enable IAM database authentication on the database. Attach an IAM policy to each system administrator‚Äôs role to map the role to the database user name. Install the Amazon Aurora SSL certificate bundle to the system administrators‚Äô certificate trust store. Use the AWS CLI to generate an authentication token used when connecting to the database.
       D. Enable IAM database authentication on the database. Configure the database to use the IAM identity provider to map the administrator roles to the database user. Install the Amazon Aurora SSL certificate bundle to the system administrators‚Äô certificate trust store. Use the AWS CLI to generate an authentication token used when connecting to the database.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company‚Äôs AWS architecture currently uses access keys and secret access keys stored on each instance to access AWS services. Database credentials are hard-coded on each instance. SSH keys for command-line remote access are stored in a secured Amazon S3 bucket. The company has asked its solutions architect to improve the security posture of the architecture without adding operational complexity. Which combination of steps should the solutions architect take to accomplish this? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use Amazon EC2 instance profiles with an IAM role
       B. Use AWS Secrets Manager to store access keys and secret access keys
       C. Use AWS Systems Manager Parameter Store to store database credentials
       D. Use a secure fleet of Amazon EC2 bastion hosts for remote access
       E. Use AWS KMS to store database credentials                
       F. Use AWS Systems Manager Session Manager for remote access
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A B D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to change its internal cloud billing strategy for each of its business units. Currently, the cloud governance team shares reports for overall cloud spending with the head of each business unit. The company uses AWS Organizations to manage the separate AWS accounts for each business unit. The existing tagging standard in Organizations includes the application, environment, and owner. The cloud governance team wants a centralized solution so each business unit receives monthly reports on its cloud spending. The solution should also send notifications for any cloud spending that exceeds a set threshold. Which solution is the MOST cost-effective way to meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in each account to create monthly reports for each business unit.
       B. Configure AWS Budgets in the organization‚Äôs master account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in the organization‚Äôs master account to create monthly reports for each business unit.
       C. Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use the AWS Billing and Cost Management dashboard in each account to create monthly reports for each business unit.
       D. Enable AWS Cost and Usage Reports in the organization‚Äôs master account and configure reports grouped by application, environment, and owner. Create an AWS Lambda function that processes AWS Cost and Usage Reports, sends budget alerts, and sends monthly reports to each business unit‚Äôs email list
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is configuring connectivity to a multi-account AWS environment to support application workloads that serve users in a single geographic region. The workloads depend on a highly available, on-premises legacy system deployed across two locations. It is critical for the AWS workloads to maintain connectivity to the legacy system, and a minimum of 5 Gbps of bandwidth is required. All application workloads within AWS must have connectivity with one another. Which solution will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from a DX partner for each on‚Äìpremises location. Create private virtual interfaces on each connection for each AWS account VPC. Associate the private virtual interface with a virtual private gateway attached to each VPC.
       B. Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from two DX partners for each on-premises location. Create and attach a virtual private gateway for each AWS account VPC. Create a DX gateway in a central network account and associate it with the virtual private gateways. Create a public virtual interface on each DX connection and associate the interface with the DX gateway.
       C. Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from two DX partners for each on-premises location. Create a transit gateway and a DX gateway in a central network account. Create a transit virtual interface for each DX interface and associate them with the DX gateway. Create a gateway association between the DX gateway and the transit gateway.
       D. Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from a DX partner for each on-premises location. Create and attach a virtual private gateway for each AWS account VPC. Create a transit gateway in a central network account and associate it with the virtual private gateways. Create a transit virtual interface on each DX connection and attach the interface to the transit gateway.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A financial company needs to create a separate AWS account for a new digital wallet application. The company uses AWS Organizations to manage its accounts. A solutions architect uses the IAM user Support1 from the master account to create a new member account with &lt;a href="mailto:finance1@example.com"&gt;finance1@example.com&lt;/a&gt; as the email address. What should the solutions architect do to create IAM users in the new member account?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Sign in to the AWS Management Console with AWS account root user credentials by using the 64- character password from the initial AWS Organizations email sent to finance1@example.com. Set up the IAM users as required.
       B. From the master account, switch roles to assume the OrganizationAccountAccessRole role with the account ID of the new member account. Set up the IAM users as required.
       C. Go to the AWS Management Console sign-in page. Choose ‚ÄúSign in using root account credentials.‚Äù Sign in by using the email address finance1@example.com and the master account‚Äôs root password. Set up the IAM users as required.
       D. Go to the AWS Management Console sign-in page. Sign in by using the account ID of the new member account and the Support1 IAM credentials. Set up the IAM users as required.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A financial company needs to create a separate AWS account for a new digital wallet application. The company uses AWS Organizations to manage its accounts. A solutions architect uses the IAM user Support1 from the master account to create a new member account with &lt;a href="mailto:finance1@example.com"&gt;finance1@example.com&lt;/a&gt; as the email address. What should the solutions architect do to create IAM users in the new member account?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Sign in to the AWS Management Console with AWS account root user credentials by using the 64- character password from the initial AWS Organizations email sent to finance1@example.com. Set up the IAM users as required.
       B. From the master account, switch roles to assume the OrganizationAccountAccessRole role with the account ID of the new member account. Set up the IAM users as required.
       C. Go to the AWS Management Console sign-in page. Choose ‚ÄúSign in using root account credentials.‚Äù Sign in by using the email address finance1@example.com and the master account‚Äôs root password. Set up the IAM users as required.
       D. Go to the AWS Management Console sign-in page. Sign in by using the account ID of the new member account and the Support1 IAM credentials. Set up the IAM users as required.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is designing a data processing platform to process a large number of files in an Amazon S3 bucket and store the results in Amazon DynamoDB. These files will be processed once and must be retained for 1 year. The company wants to ensure that the original files and resulting data are highly available in multiple AWS Regions. Which solution will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an S3 CreateObject event notification to copy the file to Amazon Elastic Block Store (Amazon EBS). Use AWS DataSync to sync the files between EBS volumes in multiple Regions. Use an Amazon EC2 Auto Scaling group in multiple Regions to attach the EBS volumes. Process the files and store the results in a DynamoDB global table in multiple Regions. Configure the S3 bucket with an S3 Lifecycle policy to move the files to S3 Glacier after 1 year.
       B. Create an S3 CreateObject event notification to copy the file to Amazon Elastic File System (Amazon EFS). Use AWS DataSync to sync the files between EFS volumes in multiple Regions. Use an AWS Lambda function to process the EFS files and store the results in a DynamoDB global table in multiple Regions. Configure the S3 buckets with an S3 Lifecycle policy to move the files to S3 Glacier after 1 year.
       C. Copy the files to an S3 bucket in another Region by using cross-Region replication. Create an S3 CreateObject event notification on the original bucket to push S3 file paths into Amazon EventBridge (Amazon CloudWatch Events). Use an AWS Lambda function to poll EventBridge (CloudWatch Events) to process each file and store the results in a DynamoDB table in each Region. Configure both S3 buckets to use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class and an S3 Lifecycle policy to delete the files after 1 year.
       D. Copy the files to an S3 bucket in another Region by using cross-Region replication. Create an S3 CreateObject event notification on the original bucket to execute an AWS Lambda function to process each file and store the results in a DynamoDB global table in multiple Regions. Configure both S3 buckets to use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class and an S3 Lifecycle policy to delete the files after 1 year.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is running an Apache Hadoop cluster on Amazon EC2 instances. The Hadoop cluster stores approximately 100 TB of data for weekly operational reports and allows occasional access for data scientists to retrieve data. The company needs to reduce the cost and operational complexity for storing and serving this data. Which solution meets these requirements in the MOST cost-effective manner?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Move the Hadoop cluster from EC2 instances to Amazon EMR. Allow data access patterns to remain the same.
       B. Write a script that resizes the EC2 instances to a smaller instance type during downtime and resizes the instances to a larger instance type before the reports are created.
       C. Move the data to Amazon S3 and use Amazon Athena to query the data for reports. Allow the data scientists to access the data directly in Amazon S3.
       D. Migrate the data to Amazon DynamoDB and modify the reports to fetch data from DynamoDB. Allow the data scientists to access the data directly in DynamoDB.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is building a sensor data collection pipeline in which thousands of sensors write data to an Amazon Simple Queue Service (Amazon SQS) queue every minute. The queue is processed by an AWS Lambda function that extracts a standard set of metrics from the sensor data. The company wants to send the data to Amazon CloudWatch. The solution should allow for viewing individual and aggregate sensor metrics and interactively querying the sensor log data using CloudWatch Logs Insights. What is the MOST cost-effective solution that meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Write the processed data to CloudWatch Logs in the CloudWatch embedded metric format.
       B. Write the processed data to CloudWatch Logs. Then write the data to CloudWatch by using the PutMetricData API call.
       C. Write the processed data to CloudWatch Logs in a structured format. Create a CloudWatch metric filter to parse the logs and publish the metrics to CloudWatch with dimensions to uniquely identify a sensor.
       D. Configure the CloudWatch Logs agent for AWS Lambda. Output the metrics for each sensor in statsd format with tags to uniquely identify a sensor. Write the processed data to CloudWatch Logs.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A car rental company has built a serverless REST API to provide data to its mobile app. The app consists of an Amazon API Gateway API with a Regional endpoint, AWS Lambda functions, and an Amazon Aurora MySQL Serverless DB cluster. The company recently opened the API to mobile apps of partners. A significant increase in the number of requests resulted, causing sporadic database memory errors. Analysis of the API traffic indicates that clients are making multiple HTTP GET requests for the same queries in a short period of time. Traffic is concentrated during business hours, with spikes around holidays and other events. The company needs to improve its ability to support the additional usage while minimizing the increase in costs associated with the solution. Which strategy meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Convert the API Gateway Regional endpoint to an edge-optimized endpoint. Enable caching in the production stage.
       B. Implement an Amazon ElastiCache for Redis cache to store the results of the database calls. Modify the Lambda functions to use the cache.
       C. Modify the Aurora Serverless DB cluster configuration to increase the maximum amount of available memory.
       D. Enable throttling in the API Gateway production stage. Set the rate and burst values to limit the incoming calls.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has application services that have been containerized and deployed on multiple Amazon EC2 instances with public IPs. An Apache Kafka cluster has been deployed to the EC2 instances. A PostgreSQL database has been migrated to Amazon RDS for PostgreSQL. The company expects a significant increase of orders on its platform when a new version of its flagship product is released. What changes to the current architecture will reduce operational overhead and support the product release?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an EC2 Auto Scaling group behind an Application Load Balancer. Create additional read replicas for the DB instance. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3.
       B. Create an EC2 Auto Scaling group behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3.
       C. Deploy the application on a Kubernetes cluster created on the EC2 instances behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution.
       D. Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate and enable auto scaling behind an Application Load Balancer. Create additional read replicas for the DB instance. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company recently completed a large-scale migration to AWS. Development teams that support various business units have their own accounts in AWS Organizations. A central cloud team is responsible for controlling which services and resources can be accessed, and for creating operational strategies for all teams within the company. Some teams are approaching their account service quotas. The cloud team needs to create an automated and operationally efficient solution to proactively monitor service quotas. Monitoring should occur every 15 minutes and send alerts when a team exceeds 80% utilization. Which solution will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a scheduled AWS Config rule to trigger an AWS Lambda function to call the GetServiceQuota API. If any service utilization is above 80%, publish a message to an Amazon Simple Notification Service (Amazon SNS) topic to alert the cloud team. Create an AWS CloudFormation template and deploy the necessary resources to each account.
       B. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that triggers an AWS Lambda function to refresh the AWS Trusted Advisor service limits checks and retrieve the most current utilization and service limit data. If the current utilization is above 80%, publish a message to an Amazon Simple Notification Service (Amazon SNS) topic to alert the cloud team. Create AWS CloudFormation StackSets that deploy the necessary resources to all Organizations accounts.
       C. Create an Amazon CloudWatch alarm that triggers an AWS Lambda function to call the Amazon CloudWatch GetInsightRuleReport API to retrieve the most current utilization and service limit data. If the current utilization is above 80%, publish an Amazon Simple Email Service (Amazon SES) notification to alert the cloud team. Create AWS CloudFormation StackSets that deploy the necessary resources to all Organizations accounts.
       D. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that triggers an AWS Lambda function to refresh the AWS Trusted Advisor service limits checks and retrieve the most current utilization and service limit data. If the current utilization is above 80%, use Amazon Pinpoint to send an alert to the cloud team. Create an AWS CloudFormation template and deploy the necessary resources to each account.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An AWS customer has a web application that runs on premises. The web application fetches data from a third-party API that is behind a firewall. The third party accepts only one public CIDR block in each client‚Äôs allow list. The customer wants to migrate their web application to the AWS Cloud. The application will be hosted on a set of Amazon EC2 instances behind an Application Load Balancer (ALB) in a VPC. The ALB is located in public subnets. The EC2 instances are located in private subnets. NAT gateways provide internet access to the private subnets. How should a solutions architect ensure that the web application can continue to call the third party API after the migration?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Associate a block of customer-owned public IP addresses to the VPC. Enable public IP addressing for public subnets in the VPC.
       B. Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC.
       C. Create Elastic IP addresses from the block of customer-owned IP addresses. Assign the static Elastic IP addresses to the ALB.
       D. Register a block of customer-owned public IP addresses in the AWS account. Set up AWS Global Accelerator to use Elastic IP addresses from the address block. Set the ALB as the accelerator endpoint.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is using AWS Organizations to manage multiple AWS accounts. For security purposes, the company requires the creation of an Amazon Simple Notification Service (Amazon SNS) topic that enables integration with a third-party alerting system in all the Organizations member accounts. A solutions architect used an AWS CloudFormation template to create the SNS topic and stack sets to automate the deployment of CloudFormation stacks. Trusted access has been enabled in Organizations. What should the solutions architect do to deploy the CloudFormation StackSets in all AWS accounts?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a stack set in the Organizations member accounts. Use service-managed permissions. Set deployment options to deploy to an organization. Use CloudFormation StackSets drift detection.
       B. Create stacks in the Organizations member accounts. Use self-service permissions. Set deployment options to deploy to an organization. Enable the CloudFormation StackSets automatic deployment.
       C. Create a stack set in the Organizations master account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets automatic deployment.
       D. Create stacks in the Organizations master account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets drift detection.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to provide a desktop as a service (DaaS) to a number of employees using Amazon WorkSpaces. WorkSpaces will need to access files and services hosted on premises with authorization based on the company‚Äôs Active Directory. Network connectivity will be provided through an existing AWS Direct Connect connection. The solution has the following requirements: Credentials from Active Directory should be used to access on-premises files and services. Credentials from Active Directory should not be stored outside the company. End users should have a single sign-on (SSO) to on-premises files and services once connected to WorkSpaces. Which strategy should the solutions architect use for end user authentication?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) directory within the WorkSpaces VPC. Use the Active Directory Migration Tool (ADMT) with the Password Export Server to copy users from the on-premises Active Directory to AWS Managed Microsoft AD. Set up a one-way trust allowing users from AWS Managed Microsoft AD to access resources in the on-premises Active Directory. Use AWS Managed Microsoft AD as the directory for WorkSpaces.
       B. Create a service account in the on-premises Active Directory with the required permissions. Create an AD Connector in AWS Directory Service to be deployed on premises using the service account to communicate with the on-premises Active Directory. Ensure the required TCP ports are open from the WorkSpaces VPC to the on-premises AD Connector. Use the AD Connector as the directory for WorkSpaces.
       C. Create a service account in the on-premises Active Directory with the required permissions. Create an AD Connector in AWS Directory Service within the WorkSpaces VPC using the service account to communicate with the on-premises Active Directory. Use the AD Connector as the directory for WorkSpaces.
       D. Create an AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) directory in the AWS Directory Service within the WorkSpaces VPC. Set up a one-way trust allowing users from the on-premises Active Directory to access resources in the AWS Managed Microsoft AD. Use AWS Managed Microsoft AD as the directory for WorkSpaces. Create an identity provider with AWS Identity and Access Management (IAM) from an on-premises ADFS server. Allow users from this identity provider to assume a role with a policy allowing them to run WorkSpaces.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

</description>
      <category>awslagi</category>
      <category>aws</category>
      <category>googlecloud</category>
    </item>
    <item>
      <title>AWS Certified Solutions Architect Professional SAP-C01 Exam Questions Part 2</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 14:17:47 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-2-44p2</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-2-44p2</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;For AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A social media company has a serverless application stack that consists of CloudFront, API Gateway and Lambda functions. The company has hired you as an AWS Certified Solutions Architect Professional to improve the current deployment process which creates a new version of the Lambda function and then runs an AWS CLI script for deployment. In case the new version errors out, then another CLI script is invoked to deploy the previous working version of the Lambda function. The company has mandated you to decrease the time to deploy new versions of the Lambda functions and also reduce the time to detect and rollback when errors are identified. Which of the following solutions would you suggest for the given use-case?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. In case the Lambda function errors out, rollback the CloudFormation change set to the previous version.
       B. Use Serverless Application Model (SAM) and leverage the built-in traffic-shifting feature of SAM to deploy the new Lambda version via CodeDeploy and use pre-traffic and post-traffic test functions to verify code. Rollback in case CloudWatch alarms are triggered.
       C. Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. Use pre-traffic and post-traffic test functions of the change set to verify the deployment. Rollback in case CloudWatch alarms are triggered.
       D. Set up and deploy a CloudFormation stack containing a new API Gateway endpoint that points to the new Lambda version. Test the updated CloudFront origin that points to this new API Gateway endpoint and in case errors are detected then revert the CloudFront origin to the previous working API Gateway endpoint.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A financial services company runs more than 400 core-banking microservices on AWS, using services including Amazon Elastic Compute Cloud (Amazon EC2). Amazon Elastic Block Store (Amazon EBS). and Amazon Simple Storage Service (Amazon S3). The company also segregates parts of its infrastructure using separate AWS accounts, so if one account is compromised, critical parts of the infrastructure in other accounts remain unaffected. The bank uses one account for production, one for non-production, and one for storing and managing users‚Äô login information and roles within AWS. The privileges that are assigned in the user account then allow users to read or write to production and non-production accounts. The company has set up ‚ÄúAWS Organizations‚Äù to manage several of these scenarios. The company wants to provide shared and centrally-managed VPCs to all business units for certain applications that need a high degree of interconnectivity. As a solutions architect, which of the following options would you choose to facilitate this use-case?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations.
       B. Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations.
       C. Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations.
       D. Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The CTO at a multi-national retail company is pursuing an IT re-engineering effort to set up a hybrid network architecture that would facilitate the company‚Äôs envisaged long-term data center migration from multiple on-premises data centers to the AWS Cloud. The current on-premises data centers are in different locations and are inter-linked via a private fiber. Due to the unique constraints of the existing legacy applications, using NAT is not an option. During the migration period, many critical applications will need access to other applications deployed in both the on-premises data centers and AWS Cloud. As a Solutions Architect Professional, which of the following options would you suggest to set up a hybrid network architecture that is secure, highly available and supports high bandwidth for a multi-Region deployment post-migration?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Set up multiple software VPN connections between AWS cloud and the on-premises data centers Configure each subnet‚Äôs traffic through different VPN connections for redundancy. Make sure that no VPC CIDR blocks overlap one another or the on-premises network.
       B. Set up a Direct Connect as primary connection for all on-premises data centers with another VPN as backup. Configure both connections to use the same virtual private gateway and BGP. Make sure that no VPC CIDR blocks overlap one another or the on-premises network.
       C. Set up multiple hardware VPN connections between AWS cloud and the on-premises data centers Configure each subnet‚Äôs traffic through different VPN connections for redundancy. Make sure that no VPC CIDR blocks overlap one another or the on-premises network.
       D. Set up a Direct Connect to each on-premises data center from different service providers and configure routing to failover to the other on-premises data center‚Äôs Direct Connect in case one
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;connection fails. Make sure that no VPC CIDR blocks overlap one another or the on-premises network.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A big data analytics company analyzes customer movement data for brick-and-mortar retailers to help them optimize marketing. merchandising, and operations performance by measuring foot traffic, store visits, walk-by conversion, bounce rate, visit duration, and customer loyalty. The company leverages its proprietary analytics workflows built on Redshift to correlate traffic with marketing campaigns and to help retailers optimize hours for peak traffic, among other activities. The company has hired you as an AWS Certified Solutions Architect Professional to review the company‚Äôs Redshift cluster, which has now become an integral part of its technology solutions. You have been asked to improve the reliability and availability of the cluster in case of a disaster and provide options to ensure that if an issue arises, the cluster can either operate or be restored within five hours. Which of the following would you suggest as the BEST solution to meet the business needs in the most cost-effective way?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Set up two identical Amazon Redshift clusters in different regions in a primary-secondary configuration. Create a cron job to run the UNLOAD command every five hours to export data for all tables in primary cluster to S3. Use cross-region replication from the primary region to secondary region. Create another cron job to ingest the data for all tables from S3 into the secondary cluster using the LOAD command.
       B. Set up a CloudFormation stack set for Redshift cluster creation so it can be launched in another Region and configure Amazon Redshift to automatically copy snapshots for the cluster to the other AWS Region. In case of a disaster, restore the cluster in the other AWS Region from that Region‚Äôs snapshot.
       C. Set up two identical Amazon Redshift clusters in different regions in a primary-secondary configuration. Develop a solution using the Kinesis Data Streams to collect the data prior to ingestion into the primary Redshift cluster and stream the data to the secondary cluster.
       D. Configure the Amazon Redshift cluster to make use of Auto Scaling groups with the nodes in the cluster spread across multiple Availability Zones (AZs). In case of a disaster, the nodes in the other AZs will ensure reliability and availability.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The engineering team at a social media company is building an ElasticSearch based index for all the existing files in S3. To build this index, it only needs to read the first 250 bytes of each object in S3. which contains some metadata about the content of the file itself. There are over 100,000 files in your S3 bucket, adding up to 50TB of data. As a Solutions Architect Professional, which of the following solutions can be used to build this index MOST efficiently? (Select two)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an application that will traverse the 53 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in ElasticSearch.
       B. Use the Database Migration Service to load the entire data from 53 to ElasticSearch and then ElasticSearch would automatically build the index.
       C. Create an application that will use the 53 Select ScanRange parameter to get the first 250 bytes and store that information in ElasticSearch.
       D. Create an application that will traverse the S3 bucket read the entire files one by one, extract the first 250 bytes, and store that information in ElasticSearch.
       E. Use the ElasticSearch Import feature to load the entire data from 53 to ElasticSearch and then ElasticSearch would automatically build the index.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An analytics company has developed a solution for location intelligence for mobile advertising wherein the company‚Äôs flagship product extracts contextual intelligence generated from large amounts of location based data such as place data, event data, digital, mobile and social data as well as a diversity of other location specific information. The product then performs a series of data cleansing. normalization, analysis, and machine learning processes to extract an understanding of the type of audience present in a specific location at a specific time enabling the ability to audience target in mobile highly effectively. The company wants to leverage ElastiCache for Redis in cluster mode to enhance the performance and scalability of its existing two-tier application architecture. The ElastiCache cluster is configured to listen on port 6379. The company has hired you as an AWS Certified Solutions Architect Professional to build a secure solution so that the cache data is secure and protected from unauthorized access. Which of the following steps would address the given use-case? (Select three)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Enable CloudTrail to monitor the API Calls for the ElastiCache cluster .
       B. Enable CloudWatch Logs to monitor the security credentials for the ElastiCache cluster.
       C. Configure the security group for the ElastiCache cluster with the required rules to allow outbound traffic to the cluster‚Äôs clients on port 6379.
       D. Configure the ElastiCache cluster to have both in-transit as well as at-rest encryption.
       E. Create the cluster with auth-token parameter and make sure that the parameter is included in all subsequent commands to the cluster.
       F. Configure the security group for the ElastiCache cluster with the required rules to allow inbound traffic from the cluster itself as well as from the cluster‚Äôs clients on port 6379.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C D F&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A medical technology company has recently set up a hybrid cloud between its on-premises data centers and AWS Cloud. The engineering team at the company has developed a Media Archiving and Communication System application that runs on AWS to support real-time collaboration among radiologists and other specialists. The company uses Amazon S3 to aggregate the raw medical images and video footage from its research teams across the world to discover tremendous medical insights. The technical teams at the overseas research facilities have reported huge delays in uploading large video files to the destination S3 bucket. As a Solutions Architect Professional, which of the following would you recommend as the MOST cost-effective solutions to improve the file upload speed into S3? (Select two)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use AWS Global Accelerator for faster file uploads into the destination 53 bucket.
       B. Create multiple AWS direct connect connections between the AWS Cloud and research facilities running in the on-premises data centers. Use the direct connect connections for faster file uploads into S3.
       C. Use multipart uploads for faster file uploads into the destination S3 bucket.
       D. Create multiple site-to-site VPN connections between the AWS Cloud and research facilities running in the on-premises data centers. Use these VPN connections for faster file uploads into 53.
       E. Use Amazon 53 Transfer Acceleration to enable faster file uploads into the destination S3 bucket.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A retail company has hired you as an AWS Certified Solutions Architect Professional to provide consultancy for managing a serverless application that consists of multiple API gateways, Lambda functions, S3 buckets and DynamoDB tables. The company is getting reports from customers that some of the application components seem to be lagging while loading dynamic images and some are timing out with the ‚Äú504 Gateway Timeout‚Äù error. As part of your investigations to identify the root cause behind this issue, you can confirm that DynamoDB monitoring metrics are at acceptable levels. Which of the following steps would you recommend to address these application issues? (Select two)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Process and analyze the VPC Flow Logs to determine if there is packet loss between the Lambda function and S3.
       B. Process and analyze the Amazon CloudWatch Logs for Lambda function to determine processing times for requested images at pre-configured intervals.
       C. Process and analyze the AWS X-Ray traces and analyze HTTP methods to determine the root cause of the HTTP errors.
       D. Enable execution logging for the API Gateway. Process and analyze the execution logs in the API Gateway for HTTP errors to determine the root cause of the errors.
       E. Enable access logging for the API Gateway. Process and analyze the access logs in the API Gateway for HTTP errors to determine the root cause of the errors.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An oncology research company has a goal to eradicate cancer by developing personalized immunotherapies to fight multiple cancer types. Recently the company has migrated some of its IT infrastructure to AWS Cloud and is looking for a solution to enable real-time data transfer between AWS and its data centers to reduce the turnaround time for patient results thereby enabling the company to identify tumor antigens and manufacture personalized immunotherapies faster. The company wants to build a patient results archival solution such that only the most frequently accessed results are available as cached data locally while backing up all results on Amazon S3. As a Solutions Architect Professional, which of the following solutions would you recommend for this use-case?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use AWS Volume Gateway Stored Volume to store the most frequently accessed results locally for low-latency access while storing the full volume with all results in its Amazon S3 service bucket.
       B. Use AWS Snowball Edge Storage Optimized device to store the most frequently accessed results locally for low-latency access while storing the full backup of results in an Amazon S3 bucket.
       C. Use AWS direct connect to store the most frequently accessed results locally for low-latency access while storing the full backup of results in an Amazon S3 bucket.
       D. Use AWS Volume Gateway Cached Volume to store the most frequently accessed results locally for low-latency access while storing the full volume with all results in its Amazon S3 service bucket.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A global loT technology company develops solutions using connected sensors and intelligent analytics to empower more than 17,000 municipalities in 50 countries to be smarter by improving transportation capacity. safety. cost-effectiveness and performance. The product team at the company is looking to build features to simplify each step of building an API and streamline collaboration so you can create better APIs. As part of its research, the product team has figured out a market need to support both stateful and stateless client-server communications via the APIs developed using its platform. You have been hired by the company as an AWS Certified Solutions Architect Professional to build a solution to fulfill this market need using AWS API Gateway. Which of the following would you recommend to the company?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. API Gateway creates RESTful APIs that enable stateless client-server communication and API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server.
       B. API Gateway creates RESTful APIs that enable stateless client-server communication and API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol, which enables stateless, full-duplex communication between client and server.
       C. API Gateway creates RESTful APIs that enable stateful client-server communication and API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol which enables stateful, full-duplex communication between client and server.
       D. API Gateway creates RESTful APIs that enable stateful client-server communication and API Gateway also creates WebSocket APIs that adhere to the WebSocket protocol which enables stateless, full-duplex communication between client and server.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A social media company is transitioning its IT infrastructure from its on-premises data center to the AWS Cloud. The company wants to move its data artifacts, 200 TB in total size, to Amazon S3 on the AWS Cloud in the shortest possible time. The company has hired you as an AWS Certified Solutions Architect Professional to provide consultancy for this data migration. In terms of the networking infrastructure, the company has a 500 Mbps Direct Connect connection to the AWS Cloud as well as an IPSec based AWS VPN connection using the public internet that supports a bandwidth of 1 Gbps. Which of the following solutions would you recommend to address the given use-case?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Leverage the 500 Mbps Direct Connect connection to transfer the data to S3 over the dedicated connection.
       B. Leverage S3 Transfer Acceleration to transfer the data to S3.
       C. Leverage the 1Gbps IPSec based AWS VPN connection to transfer the data to S3 over the public internet.
       D. Order three AWS Snowball Edge appliances, split and transfer the data to these three appliances and ship them to AWS which will then copy the data from the Snowball Edge appliances to S3.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A healthcare technology solutions company recently faced a security event resulting in an S3 bucket with sensitive data containing Personally Identifiable Information (PII) for patients being made public. The company policy mandates never to have public 53 objects so the Governance and Compliance team must be notified immediately as soon as any public objects are identified. The company has hired you as an AWS Certified Solutions Architect Professional to help build a solution that detects the presence of a public 53 object, which in turn sets off an alarm to trigger notifications and then automatically remediates the said object. Which of the following solutions would you implement in tandem to meet the requirements of the given use-case? (Select two)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Enable object-level logging for 53. When a PutObject API call is made with a public-read permission, use 53 event notifications to trigger a Lambda that sends a notification via SNS
       B. Enable object-level logging for S3. Set up a CloudWatch event pattern when a PutObject API call with public-read permission is detected in the AWS CloudTrail logs and set the target as an SNS topic for downstream notifications.
       C. Configure a Lambda function as one of the SNS topic subscribers, which is invoked to secure the objects in the S3 bucket.
       D. Leverage AWS Trusted Advisor to check for 53 bucket public-read permissions and invoke a Lambda function to send a notification via SNS as soon as a public object is uploaded.
       E. Leverage AWS Access Analyzer to check for S3 bucket public-read permissions and invoke a Lambda function to send a notification via SNS as soon as a public object is uploaded.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The DevOps team at a leading SaaS company is planning to release the major upgrade of its flagship CRM application in a week. The team is testing the alpha release of the application running on 20 EC2 instances managed by an Auto Scaling group in subnet 172.20.0.0/24 within VPC x with CIDR block 172.20.0.0/16. The team has noticed connection timeout errors in the application logs while connecting to a MySQL database running on an EC2 instance in the same region in subnet 172.30.0.0/24 within VPC Y with CIDR block 172.30.0.0/16. The IP of the database instance is hard-coded in the application instances. As a Solutions Architect Professional, which of the following solutions would you recommend to the DevOps team to solve the problem in a secure way with minimal maintenance and overhead? (Select two)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Set up a VPC peering connection between the two VPCs and add a route to the routing table of VPC Y that points to the IP address range of 172.20.0.0/16.
       B. Create and attach virtual private gateways for both VPCs and set up default routes to the customer gateways for both VPCs. Assign an Elastic IP for the EC2 instance running MySQL database in VPC Y. Update the application instances to connect to this Elastic IP.
       C. Set up a VPC peering connection between the two VPCs and add a route to the routing table of VPC X that points to the IP address range of 172.30.0.0/16.
       D. Create and attach internet gateways for both VPCs and set up default routes to the Internet gateways for both VPCs. Assign an Elastic IP for the EC2 instance running MySQL database in VPC Y. Update the application instances to connect to this Elastic IP.
       E. Create and attach NAT gateways for both VPCs and set up routes to the NAT gateways for both VPCs. Assign an Elastic IP for the EC2 instance running MySQL database in VPC Y. Update the application instances to connect to this Elastic IP.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A leading Internet-of-Things (loT) solutions company needs to develop a platform that would analyze real-time clickstream events from embedded sensors in consumer electronic devices. The company has hired you as an AWS Certified Solutions Architect Professional to consult the engineering team and develop a solution using the AWS Cloud. The company wants to use clickstream data to perform data science, develop algorithms, and create visualizations and dashboards to support the business stakeholders. Each of these groups would work independently and would need real-time access to this clickstream data for their applications. Which of the following options would provide a highly available and fault-tolerant solution to capture the clickstream events from the source and also provide a simultaneous feed of the data stream to the downstream applications?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use AWS Kinesis Data Analytics to facilitate multiple applications consume and analyze same streaming data concurrently and independently.
       B. Use Amazon SQS to facilitate multiple applications process same streaming data concurrently and independently.
       C. Use AWS Kinesis Data Firehose to allow applications to consume the same streaming data concurrently and independently.
       D. Use AWS Kinesis Data Streams to facilitate multiple applications consume same streaming data concurrently and independently.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A digital media company has hired you as an AWS Certified Solutions Architect Professional to optimize the architecture for its backup solution for applications running on the AWS Cloud. Currently, all of the applications running on AWS use at least two Availability Zones (AZs). The updated backup policy at the company mandates that all nightly backups for its data are durably stored in at least two geographically distinct Regions for Production and Disaster Recovery (DR) and the backup processes for both Regions must be fully automated. The new backup solution must ensure that the backup is available to be restored immediately for the Production Region and should be restored within 24 hours in the DR Region. Which of the following represents the MOST cost-effective solution that will address the given use-case?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a backup process to persist all the data to a large Amazon EBS volume attached to the backup server in the Production Region. Run nightly cron jobs to snapshot these volumes and then copy these snapshots to the DR Region.
       B. Create a backup process to persist all the data to Amazon Glacier in the Production Region. Set up cross-Region replication of this data to Amazon Glacier in the DR Region to ensure minimum possible costs in both Regions.
       C. Create a backup process to persist all the data to an 53 bucket A using S3 standard storage class in the Production Region Set up cross-Region replication of this S3 bucket A to an S3 bucket B using S3 standard storage class in the DR Region and set up a lifecycle policy in the DR Region to immediately move this data to Amazon Glacier.
       D. Create a backup process to persist all the data to an S3 bucket A using S3 standard storage class in the Production Region. Set up cross-Region replication of this S3 bucket A to an S3 bucket B using S3 standard-IA storage class in the DR Region and set up a lifecycle policy in the DR Region to immediately move this data to Amazon Glacier
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The engineering team at a data analytics company is currently optimizing a production workload on AWS that is I/O intensive with frequent read/write/update operations and it‚Äôs currently constrained on the IOPS. This workload consists of a single-tier with 15 r6g.8xlarge instances, each with 3 TB gp2 volume. The number of processing jobs has increased recently. resulting in an increase in latency as well. The team has concluded that they need to increase the IOPS by 3,000 for each of the instances for the application to perform efficiently. As an AWS Certified Solutions Architect Professional, which of the following solutions will you suggest to meet the performance goal in the MOST cost-efficient way?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Modify the size of the gp2 volume for each instance from 3 TB to 4 TB.
       B. Set up a new Amazon S3 bucket and migrate all the data to this new bucket Configure each instance to access this S3 bucket and use it for storage.
       C. Provision a new EFS file system and migrate all the data to this new file system. Mount this file system on all 15 instances.
       D. Modify the type of Amazon EBS volume on each instance from gp2 to io1 and set provisioned IOPS to 12,000.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A web-hosting startup manages more than 500 public web applications on AWS Cloud which are deployed in a single AWS Region. The fully qualified domain names (FQDNs) of all of the applications are configured to use HTTPS and are served via Application Load Balancers (ALBs). These ALBs are configured to use public SSL/TLS certificates. The startup has hired you as an AWS Certified Solutions Architect Professional to migrate the web applications to a multi-Region architecture. You must ensure that all HTTPS services continue to work without interruption. Which of the following solutions would you suggest to address these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Generate a separate certificate for each FQDN in each AWS Region using AWS KMS. Associate the certificates with the corresponding ALBs in the relevant AWS Region.
       B. Generate a separate certificate for each FQDN in each AWS Region using AWS Certificate Manager. Associate the certificates with the corresponding ALBs in the relevant AWS Region.
       C. Set up the key pairs and then generate the certificate for each FQDN via AWS KMS. Associate the same FQDN certificate with the ALBs in the relevant AWS Regions.
       D. Generate a certificate for each FQDN via AWS Certificate Manager. Associate the same FQDN certificate with the ALBs in the relevant AWS Regions.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A leading club in the Major League Baseball runs a web platform that boasts over 50,000 pages and over 100 million digitized photographs. It is available in six languages and maintains up-to-date information for the season. The engineering team has built a notification system on the web platform using SNS notifications which are then handled by a Lambda function for end-user delivery. During the off-season, the notification systems need to handle about 100 requests per second. During the peak baseball season, the rate touches about 5000 requests per second and it is noticed that a significant number of the notifications are not being delivered to the end-users on the web platform. As a Solutions Architect Professional, which of the following would you suggest as the BEST fit solution to address this issue?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. The engineering team needs to provision more servers running the Lambda service.
       B. Amazon SNS has hit a concurrency limit, so the team needs to contact AWS support to raise the account limit.
       C. The engineering team needs to provision more servers running the SNS service.
       D. Amazon SNS message deliveries to AWS Lambda have crossed the account concurrency quota for Lambda, so the team needs to contact AWS support to raise the account limit.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A data analytics company needs to set up a data lake on Amazon S3 for a financial services client. The data lake is split in raw and curated zones. For compliance reasons, the source data needs to be kept for a minimum of 5 years. The source data arrives in the raw zone and is then processed via an AWS Glue based ETL job into the curated zone. The business analysts run ad-hoc queries only on the data in the curated zone using Athena. The team is concerned about the cost of data storage in both the raw and curated zones as the data is increasing at a rate of 2 TB daily in each zone. Which of the following options would you implement together as the MOST cost-optimal solution? (Select two)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use Glue ETL job to write the transformed data in the curated zone using CSV format
       B. Use Glue ETL job to write the transformed data in the curated zone using a compressed file format.
       C. Create a Lambda function based job to delete the raw zone data after 1 day.
       D. Setup a lifecycle policy to transition the curated zone data into Glacier Deep Archive after 1 day of object creation.
       E. Setup a lifecycle policy to transition the raw zone data into Glacier Deep Archive after 1 day of object creation.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An e-commerce company has hired an AWS Certified Solutions Architect Professional to transform a standard three-tier web application architecture in AWS. Currently. the web and application tiers run on EC2 instances and the database tier runs on RDS MySQL. The company wants to redesign the web and application tiers to use API Gateway with Lambda Functions with the final goal of deploying the new application within 6 months. As an immediate short-term task, the Engineering Manager has mandated the Solutions Architect to reduce costs for the existing stack. Which of the following options should the Solutions Architect recommend as the MOST cost-effective and reliable solution?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Provision On-Demand Instances for the web and application tiers and Reserved Instances for the database tier.
       B. Provision Reserved Instances for the web and application tiers and On-Demand Instances for the database tier.
       C. Provision Spot Instances for the web and application tiers and Reserved Instances for the database tier.
       D. Provision Reserved Instances for the web, application and database tiers.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A leading internet television network company uses AWS Cloud for analytics, recommendation engines and video transcoding. To monitor and optimize this network, the engineering team at the company has developed a solution for ingesting. augmenting. and analyzing the multiple terabytes of data its network generates daily in the form of virtual private cloud (VPC) flow logs. This would enable the company to identify performance-improvement opportunities such as identifying apps that are communicating across regions and collocating them. The VPC flow logs data is funneled into Kinesis Data Streams which further acts as the source of a delivery stream for Kinesis Firehose. The engineering team has now configured a Kinesis Agent to send the VPC flow logs data from another set of network devices to the same Firehose delivery stream. They noticed that data is not reaching Firehose as expected. As a Solutions Architect Professional, which of the following options would you identify as the MOST plausible root cause behind this issue?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Kinesis Agent can only write to Kinesis Data Streams, not to Kinesis Firehose.
       B. Kinesis Firehose delivery stream has reached its limit and needs to be scaled manually.
       C. The data sent by Kinesis Agent is lost because of a configuration error.
       D. Kinesis Agent cannot write to a Kinesis Firehose for which the delivery stream source is already set as Kinesis Data Streams.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The DevOps team at a leading social media company uses Chef to automate the configurations of servers in the on-premises data center. The CTO at the company now wants to migrate the IT infrastructure to AWS Cloud with minimal changes to the server configuration workflows and at the same time account for less operational overhead post-migration to AWS. The company has hired you as an AWS Certified Solutions Architect Professional to recommend a solution for this migration. Which of the following solutions would you recommend to address the given use-case?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Replatform the IT infrastructure to AWS Cloud by leveraging AWS OpsWorks as a configuration management service to automate the configurations of servers on AWS.
       B. Replatform the IT infrastructure to AWS Cloud by leveraging AWS Config as a configuration management service to automate the configurations of servers on AWS.
       C. Rehost the IT infrastructure to AWS Cloud by leveraging AWS Elastic Beanstalk as a configuration management service to automate the configurations of servers on AWS.
       D. Rehost the IT infrastructure to AWS Cloud by leveraging AWS OpsWorks as a configuration management service to automate the configurations of servers on AWS.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Wall Street based trading firm uses AWS Cloud for its IT infrastructure. The firm runs several trading-risk simulation applications, developing complex algorithms to simulate diverse scenarios in order to evaluate the financial health of its customers. The firm stores customers‚Äô financial records on Amazon S3. The engineering team needs to implement an archival solution based on Amazon S3 Glacier to enforce regulatory and compliance controls on data access. As a Solutions Architect Professional, which of the following solutions would you recommend?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use S3 Glacier to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls.
       B. Use S3 Glacier to store the sensitive archived data and then use an S3 lifecycle policy to enforce compliance controls.
       C. Use S3 Glacier vault to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls.
       D. Use S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A global apparel, footwear, and accessories retailer uses Amazon S3 for centralized storage of the static media assets such as images and videos for its products. The product planning specialists typically upload and download video files (about 100MB each) to the same S3 bucket as part of their day to day work. Initially, the product planning specialists were based out of a single region and there were no performance issues. However, as the company grew and started running offices from multiple countries, it resulted in poor latency for accessing and uploading data to/from S3. The company wants to continue with the serverless solution for its storage requirements but wants to improve its performance. As a solutions architect, which of the following solutions do you propose to address this issue? (Select two)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Spin up EC2 instances in each region where the company has an office Create a daily job to transfer S3 data into EBS volumes attached to the EC2 instances.
       B. Enable Amazon S3 Transfer Acceleration for the S3 bucket This would speed up uploads as well as downloads for the video files.
       C. Create new S3 buckets in every region where the company has an office, so that each office can maintain its storage for the media assets.
       D. Use Amazon CloudFront distribution with origin as the S3 bucket This would speed up uploads as well as downloads for the video files.
       E. Move S3 data into EFS file system created in a US region, connect to EFS file system from EC2 instances in other AWS regions using an inter-region VPC peering connection.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A healthcare company provides solutions for diagnostic, treatment and preventative care in the US. The company uses data to drive decisions, and when its on-premises database solutions couldn‚Äôt handle the amount of data in 37 million records, the company migrated to Amazon Redshift. The engineering team at the company is now working on the Disaster Recovery (DR) plans for the Redshift cluster deployed in the eu-west-1 Region. The existing cluster is encrypted via AWS KMS and the team wants to copy the Redshift snapshots to another Region to meet the DR requirements. As a Solutions Architect Professional, which of the following solutions would you suggest to address the given use-case?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region snapshots in the source Region.
       B. Create a snapshot copy grant in the source Region for a KMS key in the source Region. Configure Redshift cross-Region snapshots in the destination Region.
       C. Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region replication in the source Region.
       D. Create an IAM role in destination Region with access to the KMS key in the source Region. Create a snapshot copy grant in the destination Region for this KMS key in the source Region. Configure Redshift cross-Region snapshots in the source Region.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A global SaaS company has recently migrated its technology infrastructure from its on-premises data center to AWS Cloud. The engineering team has provisioned an RDS MySQL DB cluster for the company‚Äôs flagship application. An analytics workload also runs on the same database which publishes near real-time reports for the management of the company. When the analytics workload runs, it slows down the SaaS application as well, resulting in bad user experience. As a Solutions Architect Professional, which of the following would you recommend as the MOST cost-optimal solution to fix this issue?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Enable Multi-AZ for the RDS database and run the analytics workload on the standby database.
       B. For Disaster Recovery purposes, create a Read Replica in another Region as the Master database and point the analytics workload there.
       C. Migrate the analytics application to AWS Lambda.
       D. Create a Read Replica in the same Region as the Master database and point the analytics workload there
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A global biomedicine company has built a Genomics Solution on AWS Cloud to advance neurological disease research and identify potential new disease therapies, find new gene targets and better understand neurological disease biology. The company‚Äôs labs generate hundreds of terabytes of research data daily. To further accelerate the innovation process, the engineering team at the company wants to move most of the on-premises data into Amazon S3, Amazon EFS, and Amazon FSx for Windows File Server easily. quickly. and cost-effectively. The team would like to automate and accelerate online data transfers to these AWS storage services. As a Solutions Architect Professional, which of the following solutions would you recommend as the BEST fit?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use AWS DataSync to automate and accelerate online data transfers to the given AWS storage services.
       B. Use AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services.
       C. Use File Gateway to automate and accelerate online data transfers to the given AWS storage services.
       D. Use AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A multi-national retail company has built a hub-and-spoke network with AWS Transit Gateway. VPCs have been provisioned into multiple AWS accounts to facilitate network isolation and to enable delegated network administration. The organization is looking at a cost-effective, quick and secure way of maintaining this distributed architecture so that it provides access to services required by workloads in each of the VPCs. As a Solutions Architect Professional, which of the following options would you recommend for the given use-case?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use Fully meshed VPC Peers.
       B. Use Centralized VPC Endpoints for connecting with multiple VPCs also known as shared services VPC.
       C. Use Transit VPC to reduce cost and share the resources across VPCs.
       D. Use VPCs connected with AWS Direct Connect.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A financial services company has multiple AWS accounts hosting its portfolio of IT applications that serve the company‚Äôs retail and enterprise customers. A CloudWatch Logs agent is installed on each of the EC2 instances running these IT applications. The company wants to aggregate all security events in a centralized AWS account dedicated to log storage. The centralized operations team at the company needs to perform near-real-time gathering and collating events across multiple AWS accounts. As a Solutions Architect Professional, which of the following solutions would you suggest to meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Set up CloudWatch Logs streams in each application AWS account to forward events to CloudWatch Logs in the centralized logging AWS account In the centralized logging AWS account subscribe a Kinesis Data Firehose stream to Amazon CloudWatch Events and further use the Firehose stream to store the log data in S3.
       B. Set up CloudWatch Logs agents to publish data to a Kinesis Data Firehose stream in the centralized logging AWS account Create a Lambda function to read messages from the stream and push messages to Kinesis Data Firehose and then store the data in S3.
       C. Set up a new IAM role in each application AWS account with permissions to view CloudWatch Logs. Create a Lambda function to assume this new role and perform an hourly export of each AWS account‚Äôs CloudWatch Logs data to an S3 bucket in the centralized logging AWS account.
       D. Set up Kinesis Data Streams in the logging account and then subscribe the stream to CloudWatch Logs streams in each application AWS account via subscription filters. Configure an Amazon Kinesis Data Firehose delivery stream with the Data Streams as its source and persist the log data in an Amazon 53 bucket inside the logging AWS account.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A social learning platform allows students to connect with other students as well as experts and professionals from academic, research institutes and industry. The goal of the company‚Äôs platform, developed on AWS Cloud, is to assist students pursuing higher education learn and develop skills in a manner unencumbered by socio-economic, location and resource barriers. The engineering team at the company manages 5 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for PostgreSQL DB cluster. As an AWS Certified Solutions Architect Professional, you have been asked to make the database cluster resilient from a disaster recovery perspective. Which of the following features will help you prepare for database disaster recovery? (Select two)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region.
       B. Use database cloning feature of the RDS DB cluster.
       C. Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions.
       D. Use RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage
       E. Use cross-Region Read Replicas.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Big Data Analytics company has built a custom data warehousing solution for a large airline by using Amazon Redshift. The solution helps the airline to analyze the international and domestic flight reservations, ticket issuing and boarding information, aircraft operation records, and cargo transportation records. As part of the cost optimizations, the airline now wants to move any historical data (any data older than a year) into S3, as the daily analytical reports consume data for just the last one year. However, the analysts at multiple divisions of the airline want to retain the ability to cross-reference this historical data along with the daily reports. The airline wants to develop a solution with the LEAST amount of effort and MINIMUM cost. As a Solutions Architect Professional, which option would you recommend to address this use-case?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Set up access to the historical data via Athena. The analytics team can run historical data queries on Athena and continue the daily reporting on Redshift. In case the reports need to be cross-referenced, the analytics team needs to export these in flat files and then do further analysis.
       B. Use Glue ETL job to load the S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Redshift.
       C. Use Redshift Spectrum to create Redshift cluster tables pointing to the underlying historical data in S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift.
       D. Use the Redshift COPY command to load the S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Redshift.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A multi-national bank has recently migrated to AWS Cloud to utilize dedicated instances that are physically isolated at the host hardware level from instances that belong to other AWS accounts. The bank‚Äôs flagship application is hosted on a fleet of EC2 instances which are part of an Auto Scaling group (ASG). The ASG uses a Launch Configuration (LC-A) with ‚Äúdedicated‚Äù instance placement tenancy but the VPC (VPC-A) used by the Launch Configuration LC-A has the instance tenancy set to default. Later the engineering team creates a new Launch Configuration (LC-B) with ‚Äúdefault‚Äù instance placement tenancy but the VPC (VPC-B) used by the Launch Configuration LC-B has the instance tenancy set to dedicated. As a Solutions Architect Professional, which of the following options would you identify as correct regarding the instances launched via Launch Configuration LC-A and Launch Configuration LC-B?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. The instances launched by both Launch Configuration LC-A and Launch Configuration LC-B will have default instance tenancy.
       B. The instances launched by Launch Configuration LC-A will have default instance tenancy while the instances launched by the Launch Configuration LC-B will have dedicated instance tenancy.
       C. The instances launched by both Launch Configuration LC-A and Launch Configuration LC-B will have dedicated instance tenancy.
       D. The instances launched by Launch Configuration LC-A will have dedicated instance tenancy while the instances launched by the Launch Configuration LC-B will have default instance tenancy.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The world‚Äôs largest cable company uses AWS in a hybrid environment to innovate and deploy features for its flagship video product, XFINITY X1, several times a week. The company uses AWS products such as Amazon Virtual Private Cloud (Amazon VPC) and Amazon Direct Connect to deliver the scalability and security needed for rapidly innovating in a hybrid environment. As part of an internal product roadmap, the engineering team at the company has created a private hosted zone and associated it with a virtual private cloud (VPC). However, the domain names remain unresolved, resulting in errors. As a Solutions Architect Professional, which of the following Amazon VPC configuration options would you use to get the private hosted zone to work?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. DNS hostnames and DNS resolution should be enabled for private hosted zones.
       B. There is a private hosted zone and a Resolver rule that routes traffic to your network for the same domain name resulting in an ambiguous routing rule.
       C. The private and public hosted zones should not have overlapping namespaces.
       D. Name server (NS) record and Start Of Authority (SOA) records should have the correct configurations.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A web hosting company‚Äôs CFO recently analyzed the company‚Äôs monthly bill for the AWS account for the development environment and identified an opportunity to reduce the cost for AWS Elastic Beanstalk infrastructure in use. The CFO in consultation with the CTO has hired you as an AWS Certified Solutions Architect Professional to design a highly available solution that will provision an Elastic Beanstalk environment in the morning and terminate it at the end of the day. The solution should be designed with minimal operational overhead with a focus on minimizing costs. The solution should also facilitate the increased use of Elastic Beanstalk environments among different development teams and must provide a one-stop scheduler solution for all teams to keep the operational costs as low as possible. Which of the following solution designs will you suggest to address these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Set up separate Lambda functions to provision and terminate the Elastic Beanstalk environment. Configure a Lambda execution role granting the required Elastic Beanstalk environment permissions and assign the role to the Lambda functions. Configure cron expression based Amazon CloudWatch Events rules to trigger the Lambda functions.
       B. Provision an EC2 Micro instance. Configure an IAM role with the required Elastic Beanstalk environment permissions and attach it to the instance profile. Create scripts on the instance to provision and terminate the Elastic Beanstalk environment Set up cron jobs on the instance to execute the scripts.
       C. Leverage the activity task of an AWS Step Function to provision and terminate the Elastic Beanstalk environment Create a role for the Step Function to allow it to provision and terminate the Elastic Beanstalk environment Execute the Step Function daily and use the ‚Äúwait state‚Äù to control the start and stop time.
       D. Configure the Elastic Beanstalk environment to use custom commands in the EC2 instance user data. Leverage the scheduled action for an Auto Scaling group to scale-out EC2 instances in the morning and scale-in the instance count to 0 to terminate the EC2 instances at the end of the day.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A multi-national digital media company wants to exit out of the business of owning and maintaining its own IT infrastructure so it can redeploy resources toward innovation in Artificial Intelligence and other areas to create a better customer experience. As part of this digital transformation the media company wants to archive about 9 PB of data in its on-premises data center to durable long term storage. As a Solutions Architect Professional, what is your recommendation to migrate and store this data in the quickest and MOST cost-optimal way?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Transfer the on-premises data into a Snowmobile device Copy the Snowmobile data into Amazon S3 and create a lifecycle policy to transition the data into AWS Glacier.
       B. Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices Copy the Snowball Edge data directly into AWS Glacier.
       C. Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices Copy the Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into AWS Glacier.
       D. Transfer the on-premises data into a Snowmobile device. Copy the Snowmobile data directly into AWS Glacier.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A web development studio runs hundreds of Proof-of-Concept (PoC) and demo applications on virtual machines running on an on-premises server. Many of the applications are simple PHP, JavaScript or Python web applications which are no longer actively developed and serve little traffic. As a Solutions Architect Professional, which of the following approaches would you suggest to migrate these applications to AWS with the LOWEST infrastructure cost and LEAST migration effort?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Leverage AWS Server Migration Service (SMS) to create AMis for each virtual machine and run each application on a dedicated EC2 instance.
       B. Migrate the application code to use a serverless stack comprising of Lambda functions and DynamoDB.
       C. Leverage VM Import/Export to create AMis for each virtual machine and run them in single-instance AWS Elastic Beanstalk environments by configuring a custom image.
       D. Dockerize each application and then deploy to an ECS cluster running behind an Application Load Balancer.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A project uses two AWS accounts for accessing various AWS services. The engineering team has just configured an Amazon S3 bucket in the first AWS account for writing data from the Amazon Redshift cluster provisioned in the second AWS account. The team has noticed that the files created in the S3 bucket using UNLOAD command from the Redshift cluster are not accessible to the users present in the same AWS account as the S3 bucket. What could be the reason for this denial of permission for resources belonging to the same AWS account?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. By default an S3 object is owned by the AWS account that uploaded it So the S3 bucket owner will not implicitly have access to the objects written by Redshift cluster.
       B. When objects are uploaded to S3 bucket from a different AWS account the S3 bucket owner will get implicit permissions to access these objects. It is an upload error that can be fixed by providing manual access from AWS console.
       C. When two different AWS accounts are accessing an S3 bucket, both the accounts need to share the bucket policies, explicitly defining the actions possible for each account An erroneous policy can lead to such permission failures.
       D. The owner of an S3 bucket has implicit access to all objects in his bucket Permissions are set on objects after they are completely copied to the target location Since the owner is unable to access the uploaded files, it is possible that the write operation is still in progress.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A gaming company runs its flagship application with an SLA of 99.99%. Global users access the application 24/7. The application is currently hosted on the on-premises data centers and it routinely fails to meet its SLA, especially when hundreds of thousands of users access the application concurrently. The engineering team has also received complaints from some users about high latency. As a Solutions Architect Professional, how would you redesign this application for scalability and also allow for automatic failover at the lowest possible cost?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure Route 53 latency-based routing to route to the nearest Region and activate the health checks. Host the website on S3 in each Region and use API Gateway with AWS Lambda for the application layer. Set up the data layer using DynamoDB global tables with DAX for caching.
       B. Configure Route 53 geolocation-based routing to route to the nearest Region and activate the health checks. Host the website behind a Network Load Balancer. (NLB) with targets as ECS containers using Fargate. Repeat this configuration of NLB with ECS containers using Fargate in multiple Regions. Use Aurora Global database as the data layer.
       C. Configure a combination of Route 53 failover routing with geolocation-based routing. Host the website behind an Application Load Balancer (ALB) with targets as EC2 instances that are automatically scaled via Auto-Scaling Group (ASG). Repeat this configuration of ALB with EC2 instances as targets that are scaled via ASG in multiple Regions. Use a Multi-AZ deployment with RDS MySQL as the data layer.
       D. Configure Route 53 round-robin routing policy to distribute load evenly across all Regions and activate the health checks. Host the website behind a Network Load to Balancer (NLB) with targets as ECS containers using Fargate. Repeat this configuration of NLB with ECS containers using Fargate in multiple Regions. Use Aurora Global database as the data layer.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A solo entrepreneur is working on a new digital media startup and wants to have a hands-on understanding of the comparative pricing for various storage types available on AWS Cloud. The entrepreneur has created a test file of size 5 GB with some random data. Next, he uploads this test file into AWS S3 Standard storage class, provisions an EBS volume (General Purpose SSD (gp2)) with 50 GB of provisioned storage and copies the test file into the EBS volume, and lastly copies the test file into an EFS Standard Storage filesystem. At the end of the month, he analyses the bill for costs incurred on the respective storage types for the test file. What of the following represents the correct order of the storage charges incurred for the test file on these three storage types?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Cost of test file storage on S3 Standard &amp;lt; Cost of test file storage on EBS &amp;lt; Cost of test file storage on EFS.
       B. Cost of test file storage on EBS &amp;lt; Cost of test file storage on S3 Standard &amp;lt; Cost of test file storage on EFS.
       C. Cost of test file storage on S3 Standard &amp;lt; Cost of test file storage on EFS &amp;lt; Cost of test file storage on EBS.
       D. Cost of test file storage on EFS &amp;lt; Cost of test file storage on S3 Standard &amp;lt; Cost of test file storage on EBS.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Silicon Valley based education technology startup is moving its IT operations from an on-premises data center to AWS Cloud. Its flagship product is a comprehensive learning management system that is offered in three configurations: on-premises, hosted, and fully managed software as a service (SaaS). For its SaaS version, the startup was initially using a major commercial database in a managed-hosting environment. However, this resulted in high licensing costs and required the startup to invest significant resources in day-to-day database management. That‚Äôs why it chose to migrate to the open-source MySQL database running on Amazon RDS. The engineering team at the startup is evaluating the Multi-AZ and Read Replica capabilities of RDS MySQL vs Aurora MySQL before they implement the solution in their production environment. The startup has hired you as an AWS Certified Solutions Architect Professional to provide a detailed report on this technical requirement. Which of the following would you identify as correct regarding the given use-case?(Select three)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. The primary and standby DB instances are upgraded at the same time for RDS MySQL Multi-AZ. All instances are upgraded at the same time for Aurora MySQL.
       B. Read Replicas can be manually promoted to a standalone database instance for RDS MySQL whereas Read Replicas for Aurora MySQL can be promoted to the primary instance.
       C. Read Replicas can be manually promoted to a standalone database instance for Aurora MySQL whereas Read Replicas for RDS MySQL can be promoted to the primary instance.
       D. Multi-AZ deployments for Aurora MySQL follow synchronous replication whereas. Multi-AZ deployments for RDS MySQL follow asynchronous replication.
       E. Multi-AZ deployments for RDS MySQL follow synchronous replication whereas Multi-AZ deployments for Aurora MySQL follow asynchronous replication.
       F. Database engine version upgrades happen on primary for Aurora MySQL whereas all instances are updated together for RDS MySQL.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A B E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The DevOps team for a CRM SaaS company wants to implement a patching plan on AWS Cloud for a large mixed fleet of Windows and Linux servers. The patching plan has to be auditable and must be implemented securely to ensure compliance with the company‚Äôs business requirements. As a Solutions Architect Professional, which of the following options would you recommend to address these requirements with MINIMAL effort? (Select two)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Apply patch baselines using the AWS-RunPatchBaseline SSM document.
       B. Set up Systems Manager on all instances to manage patching. Test patches in pre-production and then deploy as a maintenance window task with the appropriate approval.
       C. Configure OpsWorks automatic patching support for all applications which will keep the os up-to-date following the initial installation. Set up AWS Config to provide audit and compliance reporting.
       D. Set up an OS-native patching service to manage the update frequency and release approval for all instances. Set up AWS Config to provide audit and compliance reporting.
       E. Apply patch baselines using the AWS-ApplyPatchBaseline SSM document.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A mobility company in Latin America uses Elastic Load Balancing to distribute traffic across multiple Amazon EC2 instances. Auto Scaling groups start and stop Amazon EC2 machines based on the number of incoming requests. The company has recently started operations in a new AWS Region and is setting up an Application Load Balancer for its fleet of EC2 instances spread across two Availability Zones, with one instance as a target in Availability Zone x and four instances as targets in Availability Zone Y. The company is doing benchmarking for server performance in the new Region for the case when cross-zone load balancing is enabled compared to the case when cross-zone load balancing is disabled. As a Solutions Architect Professional, which of the following traffic distribution outcomes would you identify as correct?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. With cross-zone load balancing enabled one instance in Availability Zone X receives no traffic and four instances in Availability Zone Y receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone X receives 50% traffic and four instances in Availability Zone Y receive 12.5% traffic each.
       B. With cross-zone load balancing enabled one instance in Availability Zone X receives 20% traffic and four instances in Availability Zone Y receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone X receives 50% traffic and four instances in Availability Zone Y receive 12.5% traffic each.
       C. With cross-zone load balancing enabled one instance in Availability Zone X receives 50% traffic and four instances in Availability Zone Y receive 12.5% traffic each. With cross-zone load balancing disabled one instance in Availability Zone X receives 20% traffic and four instances in Availability Zone Y receive 20% traffic each.
       D. With cross-zone load balancing enabled one instance in Availability Zone X receives 20% traffic and four instances in Availability Zone Y receive 20% traffic each. With cross-zone load balancing disabled one instance in Availability Zone X receives no traffic and four instances in Availability Zone Y receive 25% traffic each.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A multi-national retail company wants to modernize its applications and minimize its data center infrastructure. The company wants to explore a hybrid cloud environment with AWS so that it can start leveraging AWS services for some of its data analytics workflows. The engineering team at the retail company wants to establish a dedicated, encrypted, low latency. and high throughput connection between its data center and AWS Cloud. The engineering team has set aside sufficient time to account for the operational overhead of establishing this connection. Which of the following options represents the MOST optimal solution with the LEAST infrastructure set up required for provisioning the end to end connection?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use AWS Direct Connect to establish a connection between the data center and AWS Cloud.
       B. Use site-to-site VPN to establish a connection between the data center and AWS Cloud.
       C. Use AWS Direct Connect along with a site-to-site VPN to establish a connection between the data center and AWS Cloud.
       D. Use VPC transit gateway to establish a connection between the data center and AWS Cloud.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A mobile app based social media company is using Amazon CloudFront to deliver media-rich content to its audience across the world. The Content Delivery Network (CDN) offers a multi-tier cache by default, with regional edge caches that improve latency and lower the load on the origin servers when the object is not already cached at the edge. However, there are certain content types that bypass the regional edge cache and go directly to the origin. Which of the following content types skip the regional edge cache? (Select two)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. E-commerce assets such as product photos.
       B. User-generated videos.
       C. Dynamic content, as determined at request time (cache-behavior configured to forward all headers).
       D. Static content such as style sheets, JavaScript files.
       E. Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A social media company has configured a CloudFront distribution to distribute both static and dynamic content from a web application that needs user authorization and session tracking for dynamic content. The web application is running behind an Application Load Balancer. The cache behavior for the CloudFront distribution has been configured to forward the Authorization, Host, and Date HTTP whitelist headers as well as forward a session cookie to the origin. All other cache behavior settings are set to their default value. A valid ACM certificate is applied to the CloudFront distribution as well as to the HTTPS listener for the Application Load Balancer with the CloudFront origin protocol policy configured to HTTPS only. As a Solutions Architect Professional, you have noted that the cache miss rate for the distribution is very high. You have been mandated to improve the cache hit rate for this distribution without causing the SSL/TLS handshake between CloudFront and the Application Load Balancer to fail. Which of the following will you recommend?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Remove the Date and Authorization HTTPS headers from the whitelist headers section of the cache behavior. Then update the cache behavior to use presigned cookies for authorization.
       B. Remove the Host HTTP header from the whitelist headers section and remove the session cookie from the whitelist cookies section for the default cache behavior.Enable automatic object compression and use Lambda@Edge viewer request events for user authorization.
       C. Create separate cache behaviors for static and dynamic content Remove the Date as well as Host HTTP headers from the whitelist headers section on both of the cache behaviors. Remove the session cookie from the whitelist cookies section for static content as well as the Authorization HTTP header from the whitelist headers section for cache behavior configured for static content.
       D. Create separate cache behaviors for static and dynamic content Remove the Date HTTP header from the whitelist headers section on both of the cache behaviors. Remove the session cookie from the whitelist cookies section for static content as well as the Authorization HTTP header from the whitelist headers section for cache behavior configured for static content.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A leading gaming company runs multiple game platforms that need to store game state, player data, session history. and leaderboards. The company is looking to move to AWS Cloud to scale reliably to millions of concurrent users and requests while ensuring consistently low latency measured in single-digit milliseconds. The engineering team at the company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company‚Äôs leaderboard requires high availability. low latency. and real-time processing to deliver customizable user data for the community of its users. As an AWS Certified Solutions Architect Professional, which of the following solutions would you recommend? (Select two)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Develop the leaderboard using RDS Aurora as it meets the in-memory. High availability. low latency requirements.
       B. Develop the leaderboard using DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory. high availability, low latency requirements.
       C. Develop the leaderboard using ElastiCache Redis as it meets the in-memory, high availability, low latency requirements.
       D. Develop the leaderboard using AWS Neptune as it meets the in-memory, high availability, low latency requirements.
       E. Develop the leaderboard using DynamoDB as it meets the in-memory, high availability, low latency requirements.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A retail company recently saw a huge spike in its monthly AWS spend. Upon further investigation, it was found that some developers had accidentally launched Amazon RDS instances in unexpected Regions. The company has hired you as an AWS Certified Solutions Architect Professional to establish best practices around least privileges for developers and control access to on-premises as well as AWS Cloud resources using Active Directory. The company has mandated you to institute a mechanism to control costs by restricting the level of access that developers have to the AWS Management Console without impacting their productivity. The company would also like to allow developers to launch RDS instances only in us-east-1 Region without limiting access to other services in any Region. How can you help the company achieve the new security mandate while minimizing the operational burden on the DevOps team?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Set up an IAM user for each developer and add them to the developer IAM group that has the PowerUserAccess managed policy attached to it. Attach a customer-managed policy that allows the developers access to RDS only in us- east-1 Region.
       B. Configure SAML-based authentication tied to an IAM role that has the AdministrativeAccess managed policy attached to it. Attach a customer-managed policy that denies access to RDS in any AWS Region except us-east-1.
       C. Configure SAML-based authentication tied to an IAM role that has the PowerUserAccess managed policy attached to it. Attach a customer-managed policy that denies access to RDS in any AWS Region except us-east-1.
       D. Configure SAML-based authentication tied to an IAM role that has a PowerUserAccess managed policy and a customer-managed policy that denies all the developers access to any AWS services except AWS Service Catalog. Within AWS Service Catalog. create a product containing only RDS service in us-east-1 region.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A leading community marketplace company allows property owners and travelers to connect with each other for the purpose of renting unique vacation spaces around the world. The engineering team at the company uses Amazon MySQL RDS DB cluster because it simplifies much of the time-consuming administrative tasks typically associated with databases. The team uses Multi-Availability Zone (Multi-AZ) deployment to further automate its database replication and augment data durability. The current cluster configuration also uses Read Replicas. An intern has joined the team and wants to understand the replication capabilities for Multi-AZ as well as Read Replicas for the given RDS cluster. As a Solutions Architect Professional, which of the following capabilities would you identify as correct for the given database?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Multi-AZ follows synchronous replication and spans at least two Availability Zones within a single region. Read Replicas follow asynchronous replication and can be within an Availability Zone Cross-AZ, or Cross-Region.
       B. Multi-AZ follows asynchronous replication and spans at least two Availability Zones within a single region. Read Replicas follow asynchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region.
       C. Multi-AZ follows asynchronous replication and spans at least two Availability Zones within a single region. Read Replicas follow synchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region.
       D. Multi-AZ follows asynchronous replication and spans one Availability Zone within a single region. Read Replicas follow synchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An Internet-of-Things (loT) company has developed an end-to-end cloud-based solution that provides customers with integrated loT functionality in devices including baby monitors, security cameras and entertainment systems. The company is using Kinesis Data Streams (KDS) to process loT data from these devices. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams. As a Solutions Architect Professional, which of the following would you recommend to improve the performance for the given use-case?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Swap out Kinesis Data Streams with SQS FIFO queues to support the desired read throughput for the downstream applications
       B. Swap out Kinesis Data Streams with SQS Standard queues to support the desired read throughput for the downstream applications.
       C. Use Enhanced Fanout feature of Kinesis Data Streams to support the desired read throughput for the downstream applications.
       D. Swap out Kinesis Data Streams with Kinesis Data Firehose to support the desired read throughput for the downstream applications.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A leading medical imaging equipment and diagnostic imaging solutions provider uses AWS Cloud to run its healthcare data flows through more than 500,000 medical imaging devices globally. The solutions provider stores close to one petabyte of medical imaging data on Amazon S3 to provide the durability and reliability needed for their critical data. A research assistant working with the radiology department is trying to upload a high-resolution image into 53 via the public internet. The image size is approximately 5GB. The research assistant is using S3 Transfer Acceleration (S3TA) for faster image upload. It turns out that S3TA did not result in an accelerated transfer. Given this scenario, which of the following is correct regarding the charges for this image transfer?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. The research assistant does not need to pay any transfer charges for the image upload.
       B. The research assistant only needs to pay S3 transfer charges for the image upload.
       C. The research assistant only needs to pay S3TA transfer charges for the image upload.
       D. The research assistant needs to pay both S3 transfer charges and S3TA transfer charges for the image upload.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A social media company has its corporate headquarters in New York with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters. As a Solutions Architect Professional, which of the following solutions would you recommend to meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Set up VPC Peering between branch offices and corporate headquarters which will enable branch offices to send and receive data with each other as well as with their corporate headquarters.
       B. Set up VPC CloudHub between branch offices and corporate headquarters which will enable branch offices to send and receive data with each other as well as with their corporate headquarters.
       C. Configure VPC Endpoints between branch offices and corporate headquarters which will enable branch offices to send and receive data with each other as well as with their corporate headquarters.
       D. Configure Public Virtual Interfaces (VIFs) between branch offices and corporate headquarters which will enable branch offices to send and receive data with each other as well as with their corporate headquarters.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A world-leading video creation and distribution company has recently migrated to AWS Cloud for digitally transforming their movie business. The company wants to speed up its media distribution process and improve data security while also reducing costs and eliminating errors. The company wants to set up a Digital Cinema Network that would allow it to connect the space-constrained movie theater environment to content stored in Amazon S3 as well as to accelerate the online distribution of movies and advertising to theaters in 38 key media markets worldwide. The company also wants to do an accelerated online migration of hundreds of terabytes of files from their on-premises data center to Amazon S3 and then establish a mechanism to access the migrated data for ongoing updates from the on-premises applications. As a Solutions Architect Professional, which of the following would you select as the MOST performant solution for the given use-case?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use AWS DataSync to migrate existing data to Amazon S3 as well as access the S3 data for ongoing updates.
       B. Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use S3 Transfer Acceleration for ongoing updates from the on-premises applications.
       C. Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications.
       D. Use S3 Transfer Acceleration to migrate existing data to Amazon S3 and then use DataSync for ongoing updates from the on-premises applications.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An e-commerce company runs a data archival workflow once a month for its on- premises data center which is connected to the AWS Cloud over a minimally used 10-Gbps Direct Connect connection using a private virtual interface to its virtual private cloud (VPC). The company internet connection is 200 Mbps, and the usual archive size is around 140 TB that is created on the first Friday of a month. The archive must be transferred and available in Amazon S3 by the next Monday morning. As a Solutions Architect Professional, which of the following options would you recommend as the LEAST expensive way to address the given use-case?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Order multiple AWS Snowball Edge appliances, transfer the data in parallel to these appliances and ship them to AWS which will then copy the data from the Snowball Edge appliances to S3.
       B. Configure a private virtual interface on the 10-Gbps Direct Connect connection and then copy the data securely to S3 over the connection.
       C. Configure a public virtual interface on the 10-Gbps Direct Connect connection and then copy the data to S3 over the connection.
       D. Configure a VPC endpoint for S3 and then leverage the Direct Connect connection for data transfer with VPC endpoint as the target.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The engineering team at a retail company has deployed a fleet of EC2 instances under an Auto Scaling group (ASG). The instances under the ASG span two Availability Zones (AZ) within the&lt;br&gt;
eu-west-1 region. All the incoming requests are handled by an Application Load Balancer (ALB) that routes the requests to the EC2 instances under the ASG. A planned migration went wrong last week when two instances (belonging to AZ 1) were manually terminated and desired capacity was reduced causing the Availability Zones to become unbalanced. Later that day. another instance (belonging to AZ 2) was detected as unhealthy by the Application Load Balancer‚Äôs health check. Which of the following options represent the correct outcomes for the aforesaid events? (Select two)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance.
       B. As the Availability Zones got unbalanced Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing. Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched.
       C. As the Availability Zones got unbalanced Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones When rebalancing Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application.
       D. Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it.
       E. Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An online florist and gift retailer serves customers in the US as well as Europe. The company recently decided to go all-in on AWS and use the platform to host its website, order and stock management systems and fulfillment applications. The company wants to migrate its on-premises Oracle database to Aurora MySQL. The company has hired an AWS Certified Solutions Architect Professional to carry out the migration with minimal downtime using AWS DMS. The company has mandated that the migration must have minimal impact on the performance of the source database and the Solutions Architect must validate that the data was migrated accurately from the source to the target before the cutover. Which of the following solutions will MOST effectively address this use-case?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use the table metrics of the DMS task to verify the statistics for tables being migrated including the DDL statements completed.
       B. Use AWS Schema Conversion Tool for the migration task so it can compare the source and target data and report any mismatches.
       C. Configure DMS premigration assessment on the migration task so the assessment can compare the source and target data and report any mismatches.
       D. Configure DMS data validation on the migration task so it can compare the source and target data for the DMS task and report any mismatches.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A health and beauty online retailer ships thousands of orders daily to 85 countries worldwide with more than 25,000 items and carries inventory from 600 different manufacturers. The company processes thousands of online orders each day from these countries and its website is localized in 15 languages. As a global online business, the company‚Äôs website faces continual security threats and challenges in the form of HTTP flood attacks, distributed denial of service (DDoS) attacks, rogue robots that flood its website with traffic, SQL-injection attacks designed to extract data and cross-site scripting attacks (XSS). Most of these attacks originate from certain countries. Therefore, the company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on EC2 instances running under an Application Load Balancer (ALB) with AWS WAF. As a Solutions Architect Professional, which of the following solutions would you suggest as the BEST fit for the given use-case? (Select two)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a deny rule for the blocked countries in the NACL associated with each of the EC2 instances.
       B. Use WAF IP set statement that specifies the IP addresses that you want to allow through.
       C. Use WAF geo match statement listing the countries that you want to block.
       D. Use ALB gec match statement listing the countries that you want to block.
       E. Use ALB IP set statement that specifies the IP addresses that you want to allow through.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A leading pharmaceutical company has significant investments in running Oracle and PostgreSQL services on Amazon RDS which provide their scientists with near real- time analysis of millions of rows of manufacturing data generated by continuous manufacturing equipment with 1,600 data points per row. The business analytics team has been running ad-hoc queries on these databases to prepare daily reports for senior management. The engineering team has observed that the database performance takes a hit whenever these reports are run by the analytics team. To facilitate the business analytics reporting. the engineering team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift. As a Solutions Architect Professional, which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use AWS Glue to replicate the data from the databases into Amazon Redshift.
       B. Use Amazon EMR to replicate the data from the databases into Amazon Redshift.
       C. Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift.
       D. Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A leading telecommunications company has built a portfolio of Software-as-a-Service applications focusing on voice, video, chat, contact center, and enterprise-class API solutions powered by one global cloud communications platform. As part of this strategy. they have developed their multi-cloud storage (MCS) solution on Amazon RDS for MySQL but it‚Äôs running into performance issues despite using Read Replicas. The company has hired you as an AWS Certified Solutions Architect Professional to address these performance-related challenges on an urgent basis without moving away from the underlying relational database schema. The company has branch offices across the world, and it needs the solution to work on a global scale. Which of the following will you recommend as the MOST cost-effective and high-performance solution?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Spin up EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases.
       B. Use Amazon Aurora Global Database to enable fast local reads with low latency in each region.
       C. Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region.
       D. Spin up a Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An IT company wants to move all its clients belonging to the regulated and security- sensitive industries such as financial services and healthcare to the AWS Cloud as it wants to leverage the out-of-box security-specific capabilities offered by AWS. The Security team at the company is developing a framework to validate the adoption of AWS best practices and industry-recognized compliance standards. The AWS Management Console is the preferred method for the in-house teams wanting to provision resources. You have been hired as an AWS Certified Solutions Architect Professional to spearhead this strategic initiative. Which of the following strategies would you adopt to address these business requirements for continuously assessing, auditing and monitoring the configurations of AWS resources? (Select two)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Leverage CloudWatch Events near-real-time capabilities to monitor system events patterns to trigger Lambda functions to automatically revert non-authorized changes in AWS resources. Send notifications via SNS topics to improve the incidence response time.
       B. Leverage CloudTrail integration with SNS to automatically notify unauthorized API activities. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services. Use Lambda functions to automatically revert non-authorized changes in AWS resources.
       C. Leverage CloudWatch Logs agent to collect all the AWS SDK logs. Search the log data using a pre-defined set of filter patterns that match mutating API calls. Use CloudWatch alarms to send notifications via SNS when unintended changes are performed. Archive log data by using a batch export to Amazon S3 and analyze via Athena.
       D. Leverage Config rules to audit changes to AWS resources and monitor the compliance of the configuration by running the evaluations for the rule at a frequency that you choose. Develop AWS Config custom rules to establish a test-driven development approach by triggering the evaluation when any resource that matches the rule‚Äôs scope changes in configuration.
       E. Enable trails and set up CloudTrail events to review and monitor management activities of all AWS accounts via logging into CloudWatch Logs using a KMS key. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An international integrated property management company wants to improve employee communication and productivity by using SharePoint to deploy a content and collaboration platform with document and records management functionality. The company wants to establish an AWS Direct Connect link to connect the AWS Cloud with the internal corporate network using AWS Storage Gateway. Using AWS Direct Connect would enable the company to deliver on its performance benchmark requirements including a three second or less response time for sending small documents across the internal network. To facilitate this goal, the company wants to be able to resolve DNS queries for any resources in the on-premises network from the AWS VPC and also resolve any DNS queries for resources in the AWS VPC from the on-premises network. As a Solutions Architect Professional, which of the following solutions would you recommend for this use-case? (Select two)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an inbound endpoint on Route 53 Resolver and then Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint.
       B. Create an outbound endpoint on Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Route 53 Resolver via this endpoint.
       C. Create a universal endpoint on Route 53 Resolver and then Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint.
       D. Create an inbound endpoint on Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Route 53 Resolver via this endpoint.
       E. Create an outbound endpoint on Route 53 Resolver and then Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A silicon valley based unicorn startup recently launched a video-sharing social networking service called KitKot. The startup uses AWS Cloud to manage the IT infrastructure. Users upload video files up to 1 GB in size to a single EC2 instance based application server which stores them on a shared EFS file system. Another set of EC2 instances managed via an Auto Scaling group. periodically scans the EFS share directory for new files to process and generate new videos (for thumbnails and composite visual effects) according to the video processing instructions that are uploaded alongside the raw video files. Post-processing, the raw video files are deleted from the EFS file system and the results are stored in an S3 bucket. Links to the processed video files are sent via in-app notifications to the users. The startup has recently found that even as more instances are added to the Auto Scaling Group, many files are processed twice, therefore image processing speed is not improved. As an AWS Certified Solutions Architect Professional, what would you recommend to improve the reliability of the solution as well as eliminate the redundant processing of video files?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an hourly cron job on the application server to synchronize the contents of the EFS share with S3. Trigger a Lambda function every time a file is uploaded to S3 and process the video file to store the results in another S3 bucket. Once the file is processed leverage CloudWatch Events to trigger an SNS notification to send an in-app notification to the user containing the links to the processed files.
       B. Refactor the application to run from Amazon 53 instead of the EFS file system and upload the video files directly to an S3 bucket via an API Gateway based REST APL Configure an S3 trigger to invoke a Lambda function each time a file is uploaded and the Lambda in turn processes the video and stores the processed files in another bucket. Leverage CloudWatch Events to trigger an SNS notification to send an in-app notification to the user containing the links to the processed files.
       C. Refactor the application to run from S3 instead of EFS and upload the video files directly to an S3 bucket. Configure an S3 trigger to invoke a Lambda function on each video file upload to S3 that puts a message in an SQS queue containing the link and the video processing instructions. Change the video processing application to read from the SQS queue and the S3 bucket. Configure the queue depth metric to scale the size of the Auto Scaling group for video processing instances. Leverage CloudWatch Events to trigger an SNS notification to the user containing the links to the processed files.
       D. Refactor the application to run from S3 instead of EFS and upload the video files directly to an S3 bucket. Set CloudWatch Events to trigger a Lambda function on each file upload that puts a message in an SQS queue containing the link and the video processing instructions. Change the video processing application to read from SQS queue for new files and configure the queue depth metric to scale instances in the video processing Auto Scaling group. Leverage CloudWatch Events to trigger an SNS notification to the user containing the links to the processed files.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A leading mobility company wants to use AWS for its connected cab application that would collect sensor data from its electric cab fleet to give drivers dynamically updated map information. The company would like to build its new sensor service by leveraging fully serverless components that are provisioned and managed automatically by AWS. The development team at the company does not want an option that requires the capacity to be manually provisioned, as it does not want to respond manually to changing volumes of sensor data. The company has hired you as an AWS Certified Solutions Architect Professional to provide consultancy for this strategic initiative. Given these constraints, which of the following solutions would you suggest as the BEST fit to develop this service?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Ingest the sensor data in an Amazon SQS standard queue, which is polled by an application running on an EC2 instance and the data is written into an auto-scaled DynamoDB table for downstream processing.
       B. Ingest the sensor data in a Kinesis Data Stream, which is polled by a Lambda function in batches and the data is written into an auto-scaled DynamoDB table for downstream processing.
       C. Ingest the sensor data in an Amazon SQS standard queue, which is polled by a Lambda function in batches and the data is written into an auto-scaled DynamoDB table for downstream processing.
       D. Ingest the sensor data in a Kinesis Data Stream, which is polled by an application running on an EC2 instance and the data is written into an auto-scaled DynamoDB table for downstream processing.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An e-commerce company is planning to migrate its IT infrastructure from the on-premises data center to AWS Cloud to ramp up its capabilities well in time for the upcoming Holiday Sale season. The company‚Äôs CTO has hired you as an AWS Certified Solutions Architect Professional to design a distributed, highly available and loosely coupled order processing application. The application is responsible for receiving and processing orders before storing them in a DynamoDB table. The application has seen sporadic traffic spikes in the past and the CTO wants the application to be able to scale during marketing campaigns to process the orders with minimal disruption. Which of the following options would you recommend as the MOST reliable solution to address these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Ingest the orders in an SQS queue and trigger a Lambda function to process them.
       B. Ingest the orders via a Step Function state machine and trigger an ECS container to process them.
       C. Push the orders to Kinesis Data Streams and use Amazon EC2 instances to process them.
       D. Push the orders to an SNS topic and subscribe a Lambda function to process them.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An e-commerce company has hired an AWS Certified Solutions Architect Professional to design a dual-tier storage layer for its flagship application running on EC2 instances. One of the tiers of this storage layer is a data tier that should support a POSIX file system shared across many systems. The other tier of this storage layer is a service tier that supports static file content that requires block storage with more than 100k IOPS. Which of the following solutions represent the BEST combination of AWS services for this use-case? (Select two)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use EC2 Instance Store as the service tier of the storage layer.
       B. Use EBS volumes with Provisioned IOPS as the service tier of the storage layer.
       C. Use Amazon S3 as the data tier of the storage layer.
       D. Use EC2 Instance Store as the data tier of the storage layer.
       E. Use EFS as the data tier of the storage layer.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A blog hosting company has an existing SaaS product architected as an on-premises three-tier web application. The blog content is posted and updated several times a day by multiple authors, so the Linux web servers serve content from a centralized file share on a NAS server. The CTO at the company has done an extensive technical review and highlighted to the company management that the existing infrastructure is not optimized. The company would like to migrate to AWS so that the resources can be dynamically scaled in response to load. The on-premises infrastructure and AWS Cloud are connected using Direct Connect. As a Solutions Architect Professional, which of the following solutions would you recommend to the company so that it can migrate the web infrastructure to AWS without delaying the content updation process?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Attach an EFS file system to the on-premises servers to act as the NAS server. Mount the same EFS file system to the AWS based web servers running on EC2 instances to serve the content.
       B. Set up an on-premises file gateway using Storage Gateway to replace the NAS server and then replicate the existing content to AWS On the AWS Cloud mount the same Storage Gateway bucket to the EC2 instance based web servers to serve the content.
       C. Provision a cluster of EC2 instances based web servers running behind an Application Load Balancer on AWS Share an EBS volume among all instances for accessing the content Develop custom code to periodically synchronize this volume with the NAS server.
       D. Provision EC2 instances based web servers with an Auto Scaling group. Create a nightly data transfer batch job to update the web server instances from the NAS server.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A big data analytics company is leveraging AWS Cloud to process Internet of Things (loT) sensor data from the field devices of an agricultural sciences company. The analytics company stores the loT sensor data in Amazon DynamoDB tables. To detect anomalous behaviors and respond quickly. all changes to the items stored in the DynamoDB tables must be logged in near real-time. As an AWS Certified Solutions Architect Professional, which of the following solutions would you recommend to meet the requirements of the given use-case so that it requires minimal custom development and infrastructure maintenance?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Set up DynamoDB Streams to capture and send updates to a Lambda function that outputs records to Kinesis Data Analytics. (KDA) via Kinesis Data Streams (KDS). Detect and analyze anomalies in KDA and send notifications via SNS.
       B. Configure event patterns in CloudWatch Events to capture DynamoDB API call events and set up Lambda function as a target to analyze anomalous behavior. Send SNS notifications when anomalous behaviors are detected.
       C. Set up Cloud Trail to capture all API calls that update the DynamoDB tables. Leverage CloudTrail event filtering to analyze anomalous behaviors and send SNS notifications in case anomalies are detected
       D. Set up DynamoDB Streams to capture and send updates to a Lambda function that outputs records directly to Kinesis Data Analytics (KDA). Detect and analyze anomalies in KDA and send notifications via SNS.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A financial services provider recently migrated to AWS Cloud as it needed high-powered computing to run financial simulations to value and manage insurance retirement products by leveraging its financial simulation platform to reduce simulation time by leveraging GPU optimized instances. The DevOps team at the company has provisioned a new GPU optimized EC2 instance x by choosing all default options in the AWS management console. The team can ping instance x from other instances in the VPC. The other instances were also created using the default options. The next day. the team launches another GPU optimized instance Y by creating a new security group and attaching it to instance Y. All other configuration options for instance Y are chosen as default. However, the team is not able to ping instance Y from other instances in the VPC. As a Solutions Architect Professional, which of the following would you identify as the root cause of the issue?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Instance x is in the default security group. The default rules for the default security group allow no inbound traffic from all sources Instance Y is in a new security group. The default rules for a security group that you create allow inbound traffic from all sources.
       B. Instance x is in the default security group. The default rules for the default security group allow no inbound traffic from network interfaces (and their associated instances) that are assigned to the same security group. Instance Y is in a new security group. The default rules for a security group that you create allow inbound traffic from all sources.
       C. Instance x is in the default security group. The default rules for the default security group allow inbound traffic from all sources Instance Y is in a new security group. The default rules for a security group that you create allow no inbound traffic.
       D. Instance x is in the default security group. The default rules for the default security group allow inbound traffic from network interfaces (and their associated instances) that are assigned to the same security group. Instance Y is in a new security group. The default rules for a security group that you create allow no inbound traffic.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A US-based retailer wants to ensure website availability as the company‚Äôs traditional infrastructure hasn‚Äôt been easy to scale. By moving its e-commerce platform to AWS, the company. which sees 880,000 unique visitors/day, can scale with demand and has improved availability. Last year, the company handled record Black Friday orders of nearly 10,000 orders/hour. The engineering team at the company now wants to finetune the disaster recovery strategy for its database tier. To kick-off the engagement, as an AWS Certified Solutions Architect Professional, you have been asked to implement a disaster recovery strategy for all the Amazon RDS databases that the company owns. Which of the following points do you need to consider for creating a robust recovery plan? (Select three)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Recovery time objective (RTO), expressed in hours, represents how much data you could lose when a disaster happens.
       B. You can share automated Amazon RDS snapshots with up to 20 AWS accounts.
       C. Automated backups are limited to a single AWS Region while manual snapshots and Read Replicas are supported across multiple Regions.
       D. Recovery time objective (RTO) represents the number of hours it takes, to return the Amazon RDS database to a working state after a disaster.
       E. Similar to an Amazon RDS Multi-AZ configuration, failover to a Read Replica is an automated process that requires no manual intervention after initial configurations.
       F. Database snapshots are user-initiated backups of your complete DB instance that serve as full backups. These snapshots can be copied and shared to different Regions and accounts.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C D F&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A global healthcare company wants to develop a solution called Health Information Systems (HIS) on AWS Cloud that would allow the providers, payers, and government agencies to collaborate, anticipate and navigate the changing healthcare landscape. While pursuing this endeavor, the company would like to decrease its IT operational overhead so it could focus more intently on its core business healthcare analytics. The solution should help the company eliminate the bottleneck created by manual provisioning of development pipelines while adhering to crucial governance and control requirements. As a means to this end, the company has set up ‚ÄúAWS Organizations‚Äù to manage several of these scenarios and would like to use Service Control Policies (SCP) for central control over the maximum available permissions for the various accounts in their organization. This allows the organization to ensure that all accounts stay within the organization‚Äôs access control guidelines. As a Solutions Architect Professional, which of the following scenarios would you identify as correct regarding the given use-case? (Select three)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can‚Äôt perform that action.
       B. SCPs do not affect service-linked role.
       C. If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can still perform that action.
       D. SCPs affect all users and roles in attached accounts, including the root user.
       E. SCPs affect service-linked roles.
       F. SCPs affect all users and roles in attached accounts, excluding the root user.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A B D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A leading car information and shopping platform helps more than 20 million web and mobile users each month browse automobile dealer inventory. read vehicle reviews, and consume other automobile-related content by leveraging its library of 50 million vehicle photos uploaded by auto dealers. The company is planning a key update with even better image quality and faster load times on the company‚Äôs website as well as mobile apps but the existing image-handling solution based on Cloudera MapReduce clusters is not the right tool for the job. The company now wants to switch to a serverless solution on AWS Cloud. As part of this process, the engineering team has been studying various best practices for serverless solutions. They intend to use AWS Lambda extensively and are looking at the salient features to consider when using Lambda as the backbone for the serverless architecture. As a Solutions Architect Professional, which of the following would you identify as key considerations for a serverless architecture? (Select three)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Since Lambda functions can scale extremely quickly, it‚Äôs a good idea to deploy a CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold.
       B. Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of Lambda functions.
       C. The bigger your deployment package, the slower your Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual Lambda package.
       D. If you intend to reuse code in more than one Lambda function, you should consider creating a Lambda Layer for the reusable code.
       E. Serverless architecture and containers complement each other and you should leverage Docker containers within the Lambda functions
       F. By default Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once a Lambda function is VPC-enabled it will need a route through a NAT gateway in a public subnet to access public resources.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A D F&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A digital media company wants to use AWS Cloudfront to manage its content. Firstly. it would like to allow only those new users who have paid the annual subscription fee the ability to download the application installation file. Secondly. only the subscribers should be able to view the files in the members area. As a Solutions Architect Professional, which of the following would you recommend as the MOST optimal solutions to deliver restricted content to the bona fide end users? (Select two)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use CloudFront signed URLs to restrict access to the application installation file.
       B. Use CloudFront signed cookies to restrict access to all the files in the members‚Äô area of the website.
       C. Use CloudFront signed cookies to restrict access to the application installation file.
       D. Require HTTPS for communication between CloudFront and your 53 origin.
       E. Use CloudFront signed URLs to restrict access to all the files in the members area of the website.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A Wall Street based trading firm is modernizing its message queuing system by migrating from self-managed message-oriented middleware systems to Amazon SQS. The firm is using SQS to migrate several trading applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The development team at the firm expects a peak rate of about 2,400 transactions per second to be processed via SQS. It is important that the messages are processed in the order they are received. Which of the following options can be used to implement this system in the most cost-effective way?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use Amazon SQS standard queue to process the messages.
       B. Use Amazon SQS FIFO queue in batch mode of 4 transactions per operation to process the transactions at the peak rate
       C. Use Amazon SQS FIFO queue in batch mode of 8 transactions per operation to process the transactions at the peak rate.
       D. Use Amazon SQS FIFO queue in batch mode of 12 transactions per operation to process the transactions at the peak rate.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A leading hotel reviews website has a repository of more than one million high-quality digital images. When this massive volume of images became too cumbersome to handle in-house, the company decided to offload the content to a central repository on Amazon S3 as part of its hybrid cloud strategy. The company now wants to reprocess its entire collection of photographic images to change the watermarks. The company wants to use Amazon EC2 instances and Amazon SQS in an integrated workflow to generate the sizes they need for each photo. The team wants to process a few thousand photos each night, using Amazon EC2 Spot Instances. The team uses Amazon SQS to communicate the photos that need to be processed and the status of the jobs. To handle certain sensitive photos, the team wants to postpone the delivery of certain messages to the queue by one minute while all other messages need to be delivered immediately to the queue. As a Solutions Architect Professional, which of the following solutions would you suggest to the company to handle the workflow for sensitive photos?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use message timers to postpone the delivery of certain messages to the queue by one minute.
       B. Use delay queues to postpone the delivery of certain messages to the queue by one minute.
       C. Use visibility timeout to postpone the delivery of certain messages to the queue by one minute.
       D. Use dead-letter queues to postpone the delivery of certain messages to the queue by one minute.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A digital marketing company uses S3 to store artifacts that may only be accessible to an EC2 instance x in a given VPC. The security team at the company is apprehensive about an attack vector wherein any team member with access to this instance could also set up an EC2 instance in another VPC to access these artifacts. As an AWS Certified Solutions Architect Professional, which of the following solutions will you recommend to prevent such unauthorized access to the artifacts in S3?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure an S3 VPC endpoint and create an S3 bucket policy to allow access only from this VPC endpoint.
       B. Set up a highly restricted Security Group for the EC2 instance X and create an S3 bucket policy to allow access only from this Security Group.
       C. Set up an IAM role that allows access to the artifacts in S3 and create an S3 bucket policy to allow access only from this role attached to the instance profile.
       D. Attach an Elastic IP to the EC2 instance X and create an S3 bucket policy to allow access only from this Elastic IP.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An e-commerce company wants to test its blue-green deployment on the customer base in the next couple of days. Most of the customers use mobile phones which are prone to DNS caching. The company has only two days left before the big sale will be launched. As a Solutions Architect Professional, which of the following methods would you suggest to test the deployment on as many users as possible in the given time frame?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use Elastic Load Balancer to distribute traffic across deployments.
       B. Use Route 53 weighted routing to spread traffic across different deployments.
       C. Use AWS CodeDeploy deployment options to choose the right deployment.
       D. Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

</description>
      <category>awslagi</category>
      <category>aws</category>
      <category>googlecloud</category>
    </item>
    <item>
      <title>AWS Certified Solutions Architect Professional SAP-C01 Exam Questions Part 1</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 14:15:25 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-1-3jk7</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-1-3jk7</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;For AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt;&lt;br&gt;&lt;br&gt;
For GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;1.Company A has hired you to assist with the migration of an interactive website that allows registered users to rate local restaurants. Updates to the ratings are displayed on the home page, and ratings are updated in real time. Althoughthe website is not very popular today, the company anticipates that it willgrow rapidly over the next few weeks. Theywant the site to be highly available. The current architecture consists of a single Windows Server 2008 R2 web server and a MySQL database running on Linux. Both reside inside an on-premises hypervisor. What would be the most efficient way to transfer the application to AWS, ensuring performance and high-availability?&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;           A. Use AWS VM Import/Export to create an Amazon Elastic Compute Cloud (EC2) Amazon Machine Image (AMI) of the web server. Configure Auto Scaling to launch two web servers in us-west-1a and two in us-est-1b. Launch a Multi-AZ MySQL Amazon Relational Database Service (RDS) instance in us-west-1b. Import the data into Amazon RDS from the latest MySQL backup. Use Amazon Route_53 to create a hosted zone and point an A record to the elastic load balancer
           B. Export web files to an Amazon S3 bucket in us-west-1. Run the website directly out of Amazon S3. Launch a multi-AZ MySQL Amazon RDS instance in us-west-1a. Import the data into Amazon RDS from the latest MySQL backup. Use Route 53 and create an alias record pointing to the elastic load balancer
           C. Use AWS VM Import/Export to create an Amazon EC2 AMI of the web server. Configure auto-scaling to launch two web servers in us-west-1a and two in us-west-1b. Launch a multi-AZ MySQL Amazon RDS instance in us-west-1a. Import the data into Amazon RDS from the latest MySQL backup. Create an elastic load balancer to front your web servers. Use Amazon Route 53 and create an A record pointing to the elastic load balancer
           D. Launch two Windows Server 2008 R2 instances in us-west-1b and two in Us-west-1a. Copy the web files from on premises web server to each Amazon EC2 web server, using Amazon S3 as the repository. Launch a multi-AZ MySQL Amazon RDS instance in us-west-2a. Import the data into Amazon RDS from the latest MySQL backup. Create an elastic load balancer to front your web servers. Use Route 53 and create an alias record pointing to the elastic load balancer.
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A marketing research company has developed a tracking system that collects user behavior during web marketing campaigns on behalf of their customers all over the world. The tracking system consists of an auto-scaled group of Amazon Elastic Compute Cloud (EC2) instances behind an elastic load balancer (ELB), and the collected data is stored in Amazon DynamoDB. After the campaign is terminated, the tracking system is torn down and the data is moved to Amazon Redshift, where it is aggregated, analyzed and used to generate detailed reports. The company wants to be able to instantiate new tracking systems in any region without any manual intervention and therefore adopted AWS CloudFormation. What needs to be done to make sure that the AWS CloudFormation template works in every AWS region? (Choose 2 answers)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Avoid using DeletionPolicies for EBS snapshots
       B. The names of the Amazon DynamoDB tables must be different in every target region
       C. Use the built-in Mappings and FindInMap functions of AWS CloudFormation to refer to the AMI ID set in the ImageId attribute of the Auto Scaling::LaunchConfiguration resource
       D. IAM users with the right to start AWS CloudFormation stacks must be defined for every target region.
       E. Use the built-in function of AWS CloudFormation to set the AvailabilityZone attribute of the ELB resource
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A development team that is currently doing a nightly six-hour build which is lengthening over time on-premises with a large and mostly underutilized server would like to transition to a continuous integration model of development on AWS with multiple builds triggered within the same day. However, they areare concerned about cost, security, and how to integrate with existing on-premises applications such as their LDAP and email servers which cannot move off-premises. The development environment needs a source code repository, a project management system with a MySQL database, resources for performing the builds, and a storage location for QA to pick up builds from. What AWS services combination would you recommend to meet the development team‚Äôs requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. A Bastion host Amazon Elastic Compute Cloud (EC2) instance running a VPN server for access from on-premises, Amazon EC2 for the source code repository with attached Amazon Elastic Block Store (EBS) volumes, Amazon EC2 and Amazon Relational Database Service (RDS) MySQL for the project management system, EIPs for the source code repository and project management system, Amazon Simple Queue Service (SQS) for a build queue, An Auto Scaling group of Amazon EC2 instances for performing builds, and Amazon Simple Email Service for sending the build output
       B. An AWS Storage Gateway for connecting on-premises software applications with cloud-based storage securely, Amazon EC2 for the source code repository with attached Amazon EBS volumes, Amazon EC2 and Amazon RDS MySQL for the project management system, EIPs for the source code repository and project management system, Amazon Simple Notification Service (SNS) for a notification-initiated build, An Auto Scaling group of Amazon EC2 instances for performing builds, and Amazon S3 for the build output.
       C. An AWS Storage Gateway for connecting on-premises software applications with cloud-based storage securely, Amazon EC2 for the source code repository with attached Amazon EBS volumes, Amazon EC2 and Amazon RDS MySQL for the project management system, EIPs for the source code repository and project management system, Amazon SQS for a build queue, An Amazon Elastic MapReduce (EMR) cluster of Amazon EC2 instances for performing builds, and Amazon CloudFront for the build output.
       D. A VPC with a VPN Gateway back to their on-premises servers, Amazon EC2 for the source-code repository with attached Amazon EBS volumes, Amazon EC2 and Amazon RDS MySQL for the project management system, EIPs for the source code repository and project management system, SQS for a build queue, An Auto Scaling group of EC2 instances for performing builds, and S3 for the build output
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A large enterprise wants to adopt CloudFormation to automate administrative tasks and implement the security principles of least priviledge and separation of duties. They have identified the following roles with the corresponding tasks in the company:&lt;/li&gt;
&lt;li&gt; Network administrators: create, modify and delete VPCs, subnets, NACLs, routing tables, and security groups application operators: deploy complete application stacks (ELB, Auto-Scaling groups, RDS) whereas all resources must be deployed in the VPCs managed by the network administrators.&lt;/li&gt;
&lt;li&gt;Both groups must maintain their own CloudFormation templates and should be able to create, update and delete only their own CloudFormation stacks. &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The company has followed your advice to create two IAM groups, one for applications and one for - networks. Both IAM groups are attached to IAM policies that grant rights to perform the necessary task of each group as well as the creation, update and deletion of CloudFormation stacks. &lt;br&gt;
Given setup and requirements, which statements represent valid design considerations? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Network stack updates will fail upon attempts to delete a subnet with EC2 instances
       B. Restricting the launch of EC2 instances into VPCs requires resource level permissions in the IAM policy of the application group
       C. Nesting network stacks within application stacks simplifies management and debugging, but requires resource level permissions in the IAM policy of the network group
       D. Unless resource level permissions are used on the cloudformation:DeleteStack action, network administrators could tear down application stacks
       E. The application stack cannot be deleted before all network stacks are deleted
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To enable end-to-end HTTPS connections from the userÀàs browser to the origin via CloudFront, which of the following options would be valid? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use a self signed certificate in the origin and CloudFront default certificate in CloudFront
       B. Use the CloudFront default certificate in both the origin and CloudFront
       C. Use third-party CA certificate in the origin and CloudFront default certificate in CloudFront
       D. Use third-party CA certificate in both the origin and CloudFront
       E. Use a self signed certificate in both the origin and CloudFront
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A customer is runningan application in US-West (Northern California) region and wants to setup disaster recovery failover to the Asian Pacific (Singapore) region.The customer isinterested in achieving a low Recovery Point Objective (RPO) foran Amazon Relational DatabaseService(RDS) multi-AZ MySQL database instance. Which approach is best suited to this need?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Synchronous replication
       B. Asynchronous replication
       C. Route53 health checks
       D. Copying of RDS incremental snapshots
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A document storage company is deploying their application to AWS and changing their business model to support both Free Tier and Premium Tier users. The Premium Tier &lt;br&gt;
users will be allowed to store up to 200GB of data and Free Tier customers will be allowed to store only 5GB. The customer expects that billions of files will be stored. All users need to be alerted when approaching 75 percent quota utilization and again at 90 percent quota use. To support the Free Tier and Premium Tier users, how should they architect their application?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. The company should utilize an Amazon Simple Workflow Service activity worker that updates the userÀàs used data counter in Amazon DynamoDB. The Activity Worker will use Simple Email Service to send an email if the counter increases above the appropriate thresholds.
       B. The company should deploy an Amazon Relational Database Service (RDS) relational database with a stored objects table that has a row for each stored object along with the size of each object. The upload server will query the aggregate consumption of the user in question (by first determining the files stored by the user, and then querying the stored objects table for respective file sizes) and send an email via Amazon Simple Email Service if the thresholds are breached.
       C. The company should write both the content length and the username of the files owner as S3 metadata for the object. They should then create a a file watcher to iterate over each object and aggregate the size for each user and send a notification via Amazon Simple Queue Service to an emailing service if the storage threshold is exceeded
       D. The company should create two separate Amazon Simple Storage Service buckets, one for data storage for Free Tier Users, and another for data storage for Premium Tier users. An Amazon Simple Workflow Service activity worker will query all objects for a given user based on the bucket the data is stored in and aggregate storage. The activity worker will notify the user via Amazon Simple Notification Service when necessary.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A public archives organization is about to move a pilot application they are running on AWS into production. You have been hired to analyze their application architecture and give cost-saving recommendations. The application displays scanned historical documents. Each document is split into individual image tiles at multiple zoom levels to improve responsiveness and ease of use for the end users. At maximum zoom level the average document will be 8000x 6000 pixels in size, split into multiple 40pxx 40px image tiles. The tiles are batch processed by Amazon Elastic Compute Cloud (EC2) instances, and put into an Amazon Simple Storage Service(S3) bucket.A browser-based JavaScript viewer fetches tiles from the Amazon (S3) bucket and displays them to users as they zoom and pan around each document. The average storage size of all zoom levels for a document is approximately 30MB of JPEG tiles. Originals of each document are archived in Amazon Glacier. The company expects to process and host over 500,000 scanned documents in the first year. What are your recommendations? Choose 3 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Deploy an Amazon CloudFront distribution in front of the Amazon S3 tiles bucket
       B. Increase the size (width/height) of the individual tiles at the maximum zoom level
       C. Store the maximum zoom level in the low cost Amazon S3 Glacier option and only retrieve the most frequently access tiles as they are requested by users.
       D. Use Amazon S3 Reduced Redundancy Storage for each zoom level.
       E. Decrease the size (width/height) of the individual tiles at the maximum zoom level.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A B D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Your multi-national customer wants to rewrite a website portal to ‚Äútake advantage of AWS best practices‚Äù. Other information that you have for this large Enterprise customer is as follows:&lt;/li&gt;
&lt;li&gt;Part of the portal is an employee-only section, and authentication must be against the corporate Active Directory.
‚Ä¢ You used a web analytics website to discover that on average there were 140,000 visitors per month over the past year, a peak of 187,000 unique visitors last month, and a minimum of 109,000 unique visitors two months ago. You have no information about what percentage of these visitors represents employees who signed into the portal.&lt;/li&gt;
&lt;li&gt; The web analytics website also revealed that traffic breakdown is 40 percent South America, 50 percent North America, and 10 percent other.&lt;/li&gt;
&lt;li&gt;The customer‚Äôs primary data center is located in Sao Paulo Brazil.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Their chief technology officer believes that response time for logging in to the employee portal is a primary metric, because employees complain that the current website is too slow in this regard.&lt;br&gt;
When you present your proposed application architecture to the customer, which of the following should you propose as part of the architecture? Choose 3 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. A three-subnet VPC, with an AD controller in the AWS region. The AWS AD controller will be part of the primary AD controller‚Äôs forest, and will synchronize with the corporate controller over a dedicated pipe to the corporate data center
       B. Do not use Amazon CloudFront, because the employees who log in to the portal have unique (private) session data that should not be cached in a content delivery network.
       C. A three-subnet VPC, with all AD calls traversing a dedicated pipe to the corporate data center
       D. Establish the AWS presence in the US-EAST region, with a dedicated pipe to the corporate data center.
       E. Establish the AWS presence in multiple regions: SA-EAST, and also US-EAST, with a dedicated pipe from both SA-EAST and US-EAST to the corporate data center ‚Äì and also a dedicated connection between regions. Replicate data as needed between the regions. Use a geo load balancer to determine which region is primary for a given user.
       F. Use Amazon CloudFront to cache pages for users at the nearest edge location.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A D F&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;For a 3-tier, customer facing, inclement weather site utilizing a MySQL database running in a Region which has two AZs (Availability Zone), which architecture provides fault tolerance within the Region for the application that minimally requires 6 web tier servers and 6 application tier servers running in the web and application tiers and one MySQL database?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. A web tier deployed in 2 AZs with 6 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (Elastic Load Balancer), and an application tier deployed in 2 AZs with 6 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB, and a Multi-AZ RDS (Relational Database Service) deployment
       B. A web tier deployed in 2 AZs with 3 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (Elastic Load Balancer), and an application tier deployed in 2 AZs with 3 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB, and a Multi-AZ RDS (Relational Database Service) deployment.
       C. A web tier deployed in 2 AZs with 3 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (Elastic Load Balancer), and an application tier deployed in 2 AZs with 6 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB, and one RDS (Relational Database Service) instance deployed with read replicas in the other AZ.
       D. A web tier deployed in 1 AZ with 6 EC2 (Elastic Compute Cloud) instances inside an Auto Scaling Group behind an ELB (Elastic Load Balancer), and an application tier deployed in the same AZ with 6 EC2 instances inside an Auto Scaling Group behind an ELB, and a Multi-AZ RDS (Relational Database Service) deployment, with 6 stopped web tier EC2 instances and 6 stopped application tier EC2 instances all in the other AZ ready to be started if any of the running instances in the first AZ fails.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A gaming company adopted AWS Cloud Formation to automate load-testing of their games. They have created an AWS Cloud Formation template for each gaming environment and one for the load-testing stack. The load-testing stack creates an Amazon Relational Database Service (RDS) Postgres database and two web servers running on Amazon Elastic Compute Cloud (EC2) that send HTTP requests, measure response times, and write the results into the database. A test run usually takes between 15 and 30 minutes. Once the tests are done, the AWS CloudFormation stacks are torn down immediately. The test results written to the Amazon RDS database must remain accessible for visualization and analysis. Select possible solutions that allow access to the test results after the AWS Cloud Formation load-testing stack is deleted. Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Define an update policy to prevent deletion of the Amazon RDS database after the AWS CloudFormation stack is deleted.
       B. Define a deletion policy of type Snapshot for the Amazon RDS resource to assure that the RDS database can be restored after the AWS CloudFormation stack is deleted.
       C. Define automated backups with a backup retention period of 30 days for the Amazon RDS database and perform point-in-time recovery of the database after the AWS CloudFormation stack is deleted.
       D. Define an Amazon RDS Read-Replica in the load-testing AWS CloudFormation stack and define a dependency relation between master and replica via the DependsOn attribute
       E. Define a deletion policy of type Retain for the Amazon RDS resource to assure that the RDS database is not deleted with the AWS CloudFormation stack.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You are an architect for a news-sharing mobile application. Anywhere in the world, your users can see local news on topics they choose. They can post pictures and videos from inside the application. Since the application is being used on a mobile phone, connection stability is required for uploading content, and delivery should be quick. Content is accessed a lot in the first minutes after it has been posted, but is quickly replaced by new content before disappearing. The local nature of the news means that 90 percent of the uploaded content is then read locally (less than a hundred kilometers from where it was posted). What solution will optimize the user experience when users upload and view content (by minimizing page load times and minimizing upload times)?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Upload and store the content in a central Amazon Simple Storage Service (S3) bucket, and use an Amazon CloudFront Distribution for content delivery.
       B. Upload and store the content in an Amazon Simple Storage Service (S3) bucket in the region closest to the user, and use multiple Amazon CloudFront distributions for content delivery
       C. Upload the content to an Amazon Elastic Compute Cloud (EC2) instance in the region closest to the user, send the content to a central Amazon Simple Storage Service (S3) bucket, and use an Amazon CloudFront distribution for content delivery.
       D. Use an Amazon CloudFront distribution for uploading the content to a central Amazon Simple Storage Service (S3) bucket and for content delivery.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A customer is deploying an SSL enabled Web application to AWS and would like to implement a separation of roles between the EC2 service administrators that are entited to login to Instances as well as making API calls and the security officers who will maintain and have exclusive access to the applicationÀàs X.509 certificate that contains the private key. Which configuration option could satisfy the above requirement?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure the web servers to retrieve the certificate upon boot from an CloudHSM that is managed by the security officers.
       B. Configure system permissions on the web servers to restrict access to the certificate only to the authorized security officers.
       C. Configure IAM policies authorizing access to the certificate store only to the security officers and terminate SSL on an ELB.
       D. Upload the certificate on an S3 bucket owned by the security officers and accessible only by the EC2 Role of the web servers.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You are designing security inside your VPC. You are considering the options for establishing separate security zones, and enforcing network traffic rules across the different zones to limit which instances can communicate. How would you accomplish these requirements? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure multiple subnets in your VPC, one for each zone. Configure routing within your VPC in such a way that each subnet only has routes to other subnets with which it needs to communicate, and doesnÀàt have routes to subnets with which it shouldnÀàt be able to communicate.
       B. Configure your instances to use pre-set IP addresses with an IP address range for every security zone. Configure NACLs to explicitly allow or deny communication between the different IP address ranges, as required for interzone communication.
       C. Configure a security group for every zone. Configure a default allow all rule. Configure explicit deny rules for the zones that shouldnÀàt be able to communicate with one another
       D. Configure a security group for every zone. Configure allow rules only between zones that need to be able to communicate with one another. Use the implicit deny all rule to block any other traffic.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your company currently has a highly available web application running in production. The application‚Äôs web front-end utilizes an Elastic Load Balancerand Auto Scaling across three Availability Zones.During peak load, your web servers operate at 90% utilization and leverage a combination of Heavy Utilization Reserved Instances for steady state load and On-Demand and Spot Instances for peak load. You are tasked with designing a cost effective architecture to allow the application to recover quickly in the event that an Availability Zoneis unavailable during peak load. Which option provides the most cost effective high availability architectural design for this application?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Continue to run your web front-end at 90% utilization, but leverage a high bid price strategy to cover the loss of any of the other Availability Zones during peak load.
       B. Increase use of spot instances to cost effectively scale the web front-end across all Availability Zones to lower aggregate utilization levels that will allow an Availability Zone to fail during peak load without affecting the application‚Äôs availability.
       C. Increase Auto Scaling capacity and scaling thresholds to allow the web front-end to cost effectively scale across all Availability Zones to lower aggregate utilization levels that will allow an Availability Zone to fail during peak load without affecting the application‚Äôs availability.
       D. Continue to run your web front-end at 90% utilization, but purchase an appropriate number of light utilization RIs in each Availability Zone to cover the loss of any of the other Availability Zones during peak load.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An Enterprise customer is starting their migration to the cloud, their main reason for migrating is agility, and they want to make their internal Microsoft Active Directory available to any applications running on AWS; this is so internal users only have to remember one set of credentials and as a central point of user control for leavers and joiners. How could they make their Active Directory secure, and highly available, with minimal on-premises infrastructure changes, in the most cost and time-efficient way? Choose the most appropriate:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Using Amazon Elastic Compute Cloud (EC2), they could create a DMZ using a security group; within the security group they could provision two smaller Amazon EC2 instances that are running Openswan for resilient IPSEC tunnels, and two larger instances that are domain controllers; they would use multiple Availability Zones
       B. Using VPC, they could create an extension to their data center and make use of resilient hardware IPSEC tunnels; they could then have two domain controller instances that are joined to their existing domain and reside within different subnets, in different Availability Zones.
       C. Within the customerÀàs existing infrastructure, they could provision new hardware to run Active Directory Federation Services; this would present Active Directory as a SAML2 endpoint on the internet; any new application on AWS could be written to authenticate using SAML2.
       D. The customer could create a stand-alone VPC with its own Active Directory Domain Controllers; two domain controller instances could be configured, one in each Availability Zone; new applications would authenticate with those domain controllers.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An AWS customer is deploying a web application that is composed of a front-end running on Amazon EC2 and confidential data that is stored on Amazon S3. The customers security policy requires that the all access operations to this sensitive data must be authenticated and authorized by a centralized access management system that is operated by a separate security team. In addition, the web application team that owns and administers the EC2 web front-end instances is prohibited from having any ability to access the data that circumvents this centralized access management system. Which of the following configurations will support these requirements:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure the web application to authenticate end-users against the centralized access management system. Have the web application provision trusted users STS tokens entitling the download of approved data directly from Amazon S3.
       B. Encrypt the data on Amazon S3 using a CloudHSM that is operated by the separate security team. Configure the web application to integrate with the CloudHSM for decrypting approved data access operations for trusted end-users.
       C. Configure the web application to authenticate end-users against the centralized access management system using SAML. Have the end-users authenticate to IAM using their SAML token and download the approved data directly from Amazon S3.
       D. Have the separate security team create an IAM Role that is entitled to access the data on Amazon S3. Have the web application team provision their instances with this Role while denying their IAM users access to the data on Amazon S3.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You have been asked to design network connectivity between your existing data centers and AWS. Your application‚Äôs EC2 instances must be able to connect to existing backend resources located in your data center. Network traffic between AWS and your data centers will start small, but ramp up to 10s of GB per second over the course of several months. The success of your application is dependent upon getting to market quickly. Which of the following design options will allow you to meet your objectives?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Quickly submit a DirectConnect request to provision a 1 Gbps cross connect between your data center and VPC, then increase the number or size of your DirectConnect connections as needed
       B. Quickly create an internal ELB for your backend applications, submit a DirectConnect request to provision a 1 Gbps cross connect between your data center and VPC, then increase the number or size of your DirectConnect connections as needed
       C. Allocate EIPs and an Internet Gateway for your VPC instances to use for quick, temporary access to your backend applications, then provision a VPN connection between a VPC and existing on-premises equipment.
       D. Provision a VPN connection between a VPC and existing on-premises equipment, submit a DirectConnect partner request to provision cross connects between your data center and the DirectConnect location, then cut over from the VPN connection to one or more DirectConnect connections as needed.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You have an application running on an EC2 instance which will allow users to download files from a private S3 bucket using a pre-signed URL. Before generating the URL, the application should verify the existence of the file in S3. How should the application use AWS credentials to access the S3 bucket securely?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an IAM user for the application with permissions that allow list access to the S3 bucket; launch the instance as the IAM user, and retrieve the IAM user‚Äôs credentials from the EC2 instance user data.
       B. Create an IAM role for EC2 that allows list access to objects in the S3 bucket; launch the instance with the role, and retrieve the role‚Äôs credentials from the EC2 instance metadata
       C. Use the AWS account access keys; the application retrieves the credentials from the source code of the application.
       D. Create an IAM user for the application with permissions that allow list access to the S3 bucket; the application retrieves the IAM user credentials from a temporary directory with permissions that allow read access only to the application user.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A startup deploys its photo-sharing site in a VPC. An elastic load balancer distributes web traffic across two subnets. The load balancer session stickiness is configured to use the AWS-generated session cookie, with a session TTL of 5 minutes. The web server Auto Scaling group is configured as min-size=4, max-size=4. The startup is preparing for a public launch, by running load-testing software installed on a single Amazon Elastic Compute Cloud (EC2) instance running in us-west-2a. After 60 minutes of load-testing, the web server logs show the following:&lt;br&gt;
+‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-+‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-+&lt;br&gt;
| # of HTTP requests | # of HTTP requests |&lt;br&gt;
WEBSERVER LOGS | from load-tester | from private beta users |&lt;br&gt;
+‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-+&lt;br&gt;
| webserver #1 (subnet in us-west-2a): | 19,210 | 434 |&lt;br&gt;
| webserver #2 (subnet in us-west-2a): | 21,790 | 490 |&lt;br&gt;
| webserver #3 (subnet in us-west-2b): | 0 | 410 |&lt;br&gt;
| webserver #4 (subnet in us-west-2b): | 0 | 428 |&lt;br&gt;
+‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî+‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-+‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-+&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Which recommendations can help ensure that load-testing HTTP requests are evenly distributed across the four webservers?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Choose 2 answers&lt;br&gt;
           B. Launch and run the load-tester Amazon EC2 instance from us-east-1 instead.&lt;br&gt;
           C. Use a third-party load-testing service which offers globally distributed test clients.&lt;br&gt;
           D. Configure Elastic Load Balancing and Auto Scaling to distribute across us-west-2a and us-west-2b.&lt;br&gt;
           E. Configure Elastic Load Balancing session stickiness to use the app-specific session cookie&lt;br&gt;
           F. Re-configure the load-testing software to re-resolve DNS for each web request.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C F&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To meet regulatory requirements, a pharmaceuticals company needs to archive data after a drug trial test is concluded. Each drug trial test may generate up to several thousands of files, with compressed file sizes ranging from 1 byte to 100MB. Once archived, data rarely needs to be restored, and on the rare occasion when restoration is needed, the company has 24 hours to restore specific files that match certain metadata. Searches must be possible by numeric file ID, drug name, participant names, date ranges, and other metadata. Which is the most cost-effective architectural approach that can meet the requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Store individual compressed files and search metadata in Amazon Simple Storage Service (S3). Create a lifecycle rule to move the data to Amazon Glacier, after a certain number of days. When restoring data, query the Amazon S3 bucket for files matching the search criteria, and retrieve the file to S3 reduced redundancy in order to move it back to S3 Standard class.
       B. Store individual files in Amazon Glacier, using the file ID as the archive name. When restoring data, query the Amazon Glacier vault for files matching the search criteria.
       C. First, compress and then concatenate all files for a completed drug trial test into a single Amazon Glacier archive. Store the associated byte ranges for the compressed files along with other search metadata in an Amazon RDS database with regular snapshotting. When restoring data, query the RDS database for files that match the search criteria, and create restored files from the retrieved byte ranges
       D. Store individual files in Amazon S3, and store search metadata in an Amazon Relational Database Service (RDS) multi-AZ database. Create a lifecycle rule to move the data to Amazon Glacier after a certain number of days. When restoring data, query the Amazon RDS database for files matching the search criteria, and move the files matching the search criteria back to S3 Standard class.
       E. Store individual files in Amazon Glacier, and store the search metadata in an Amazon RDS multi-AZ database. When restoring data, query the Amazon RDS database for files matching the search criteria, and retrieve the archive name that matches the file ID returned from the database query.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You have been asked to virtually extend two existing data centers into AWS to support a highly available application that depends on existing, on-premises resources located in multiple data centers and static content that is served from an Amazon Simple Storage Service (S3) bucket. Your design currently includes a dual-tunnel VPN connection between your CGW and VGW. Which component of your architecture represents a potential single point of failure that you should consider changing to make the solution more highly available?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Add another CGW in a different data center and create another dual-tunnel VPN connection
       B. Add a second VGW in a different Availability Zone, and a CGW in a different data center, and create another dual-tunnel.
       C. No changes are necessary: the network architecture is currently highly available
       D. Add another VGW in a different Availability Zone and create another dual-tunnel VPN connection
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your company has recently extended its datacenter into a VPC on AWS to add burst computing capacity as needed. Members of your Network Operations Center need to be able to go to the AWS Management Console and administer Amazon EC2 instances as necessary. You donÀàt want to create new IAM users for each NOC member and make those users sign in again to the AWS Management Console. Which option below will meet the needs for your NOC members&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use Web Identity Federation to retrieve AWS temporary security credentials to enable your NOC members to sign in to the AWS Management Console.
       B. Use your on-premises SAML 2.0-compliant identity provider (IdP) to retrieve temporary security credentials to enable NOC members to sign in to the AWS Management Console.
       C. Use OAuth 2.0 to retrieve temporary AWS security credentials to enable your NOC members to sign in to the AWS Management Console.
       D. Use your on-premises SAML 2.0-compliant identity provider (IdP) to grant the NOC members federated access to the AWS Management Console via the AWS single sign-on (SSO) endpoint.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A media production company wants to deliver high-definition raw video material for preproduction and dubbing to customers all around the world. They would like to use Amazon CloudFront for their scenario, and they require the ability to limit downloads per customer and video file to a configurable number. A CloudFront download distribution with TTL = 0 was already setup to make sure all client HTTP requests hit an authentication backend on Amazon Elastic Compute Cloud (EC2)/Amazon Relational Database Service (RDS) first, which is responsible for restricting the number of downloads. Content is stored in Amazon Simple Storage Service (S3) and configured to be accessible only via CloudFront. What else needs to be done to achieve an architecture that meets the requirements? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Enable CloudFront logging into an Amazon S3 bucket, leverage Amazon Elastic MapReduce (EMR) to analyze CloudFront logs to determine the number of downloads per customer, and return the content S3 URL unless the download limit is reached.
       B. Enable CloudFront logging into an Amazon S3 bucket, let the authentication backend determine the number of downloads per customer by parsing those logs, and return the content S3 URL unless the download limit is reached
       C. Enable URL parameter forwarding, let the authentication backend count the number of downloads per customer in Amazon RDS, and return the content S3 URL unless the download limit is reached
       D. Configure a list of trusted signers, let the authentication backend count the number of download requests per customer in Amazon RDS, and return a dynamically signed URL unless the download limit is reached.
       E. Enable URL parameter forwarding, let the authentication backend count the number of downloads per customer in Amazon RDS, and invalidate the CloudFront distribution as soon as the download limit is reached.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your customer is implementing a video on-demand streaming platform on AWS. The requirements are; support for multiple devices such as iOS, Android, and PC as client devices, using a standard client player, using streaming technology (not download,) and scalable architecture with cost effectiveness. Which architecture meets the requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Store the video contents to Amazon Simple Storage Service (S3) as an origin server. Configure the Amazon CloudFront distribution with a streaming option to stream the video contents
       B. Store the video contents to Amazon S3 as an origin server. Configure the Amazon CloudFront distribution with a download option to stream the video contents.
       C. Launch a streaming server on Amazon Elastic Compute Cloud (EC2) (for example, Adobe Media Server), and store the video contents as an origin server. Configure the Amazon CloudFront distribution with a download option to stream the video contents.
       D. Launch a streaming server on Amazon EC2(for example, Adobe Media Server), and store the video contents as an origin server. Launch and configure the required amount of streaming servers on Amazon EC2 as an edge server to stream the video contents.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A research scientist is planning for the one-time launch of an Elastic MapReduce cluster and is encouraged by her manager to minimize costs. The cluster is designed to ingest 200TB of genomics data with a total of 100 Amazon Elastic Compute Cloud (EC2) instances and is expected to run for around four hours. The resulting data set must be stored temporarily until archived into an Amazon Relational Database Service (RDS) Oracle instance. Which option will help save the mostmoney while meeting requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Deploy on-demand master, core and task nodes and store ingest and output files in Amazon Simple Storage Service (S3) Reduced Redundancy Storage (RRS).
       B. Store the ingest files in Amazon S3 RRS and store the output files in S3. Deploy Reserved Instances for the master, and core nodes and on-demand for the task nodes.
       C. Store ingest and output files in Amazon S3. Deploy on-demand for the master, and core nodes and spot for the task nodes.
       D. Optimize by deploying a combination of on-demand, RI, and spot-pricing models for the master, core, and task nodes. Store ingest and output files in Amazon S3 with a lifecycle policy that archives them to Amazon Glacier.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your social media monitoring application uses a Python app running on AWS Elastic Beanstalk to inject tweets, Facebook updates and RSS feeds into an Amazon Kinesis stream. A second AWS Elastic Beanstalk app generates key performance indicators into an Amazon DynamoDB table and powers a dashboard application.&lt;br&gt;
What is the most efficient option to prevent any data loss for this application?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Add a second Amazon Kinesis stream in another Availability Zone and use AWS data pipeline to replicate data across Kinesis streams.
       B. Add a third AWS Elastic Beanstalk app that uses the Amazon Kinesis S3 connector to archive data from Amazon Kinesis into Amazon S3.
       C. Use AWS Data Pipeline to replicate your DynamoDB tables into another region.
       D. Use the second AWS Elastic Beanstalk app to store a backup of Kinesis data onto Amazon Elastic Block Store (EBS), and then create snapshots from your Amazon EBS volumes.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You tried to integrate two subsystems (front-end and back-end) with an HTTP interface to one large system. These subsystems don‚Äôt store any state inside. All state is stored in an Amazon DynamoDB table. You have launched each of these two subsystems from a separate AMI. Black box testing has shown that these servers have stopped running and are issuing malformed requests that do not meet HTTP specifications from the client. Your developers have discover and fixed this issue, and you deploy the fix to the two subsystems as soon as possible without service disruption. What are the most effective options to deploy the fixes? Choose 3 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use VPC.
       B. Use AWS OpsWorks auto healing for both the front-end and back-end instance pair
       C. Use Elastic Load Balancing in front of the front-end subsystem and Auto Scaling to keep the specified number of instances
       D. Use Elastic Load Balancing in front of the back-end subsystem and Auto Scaling to keep specified number of instances.
       E. Use Amazon CloudFront which accesses the front-end server when origin fetch
       F. Use Amazon Simple Queue Service SQS between the front-end and back-end subsystems
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B C D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;When deploying a highly available 2-tier web application on AWS, which combination of AWS Services meets the requirements?&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;AWS Direct Connect&lt;br&gt;
2.Amazon Route 53&lt;br&gt;
3.AWS Storage Gateway&lt;br&gt;
4.Elastic Load Balancing&lt;br&gt;
5.Amazon EC2&lt;br&gt;
6.Auto Scaling&lt;br&gt;
7.Amazon VPC&lt;br&gt;
8.AWS Cloud Trail&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. 2,4,5 and 6
       B. 3,4,5 and 8
       C. 1,2,5 and 6
       D. 1 through 8
       E. 1,3,5 and 7
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your customer needs to create an application to allow contractors to upload videos to Amazon Simple Storage Service (S3) so they can be transcoded into a different format. She creates AWS Identity and Access Management (IAM) users for her application developers, and in just one week, they have the application hosted on a fleet of Amazon Elastic Compute Cloud (EC2) instances. The attached IAM role is assigned to the instances. As expected, a contractor who authenticates to the application is given a pre-signed URL that points to the location for video upload. However, contractors are reporting that they cannot upload their videos. Which of the following are valid reasons for this behavior? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. The IAM role does not explicitly grant permission to upload the object
       B. The contractorsÀà accounts have not been granted ‚Äúwrite‚Äù access to the S3 bucket.
       C. The application is not using valid security credentials to generate the pre-signed URL.
       D. The developers do not have access to upload objects to the S3 bucket
       E. The S3 bucket still has the associated default permissions
       F. The pre-signed URL has expired.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C F&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your company runs a complex customer relations management system that consists of around 10 different software components all backed by the same Amazon Relational Database Service (RDS) database. You adopted AWS OpsWorks to simplify management and deployment of that application and created an AWS OpsWorks stack with layers for each of the individual components. An internal security policy requires that all instances should run on the latest Amazon Linux AMI and that instances must be replaced within one month after the latest Amazon Linux AMI has been released. AMI replacements should be done without incurring application downtime or capacity problems. You decide to write a script to be run as soon as a new Amazon Linux AMI is released. Which solutions support the security policy and meet your requirements? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a new stack and layers with identical configuration, add instances with the latest Amazon Linux AMI specified as a custom AMI to the new layers, switch DNS to the new stack, and tear down the old stack
       B. Identify all Amazon Elastic Compute Cloud (EC2) instances of your AWS OpsWorks stack, stop each instance, replace the AMI ID property with the ID of the latest Amazon Linux AMI ID, and restart the instance. To avoid down time, make sure not more than one instance is stopped at the same time.
       C. Specify the latest Amazon Linux AMI as a custom AMI at the stack level, terminate instances of the stack and let AWS OpsWorks launch new instances with the new AMI.
       D. Add new instances with the latest Amazon Linux AMI specified as a custom AMI to all AWS OpsWorks layers of your stack, and terminate the old ones.
       E. Assign a custom recipe to each layer which replaces the underlying AMI. Use AWS OpsWorks life-cycle events to incrementally execute this custom recipe and update the instances with the new AMI.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A utility company is building an application that stores data coming from more than 10,000 sensors. Each sensor has a unique ID and will send a datapoint (approximately 1 KB) every 10 minutes throughout the day. Each datapoint contains the information coming from the sensor as well as a timestamp. This company would like to query information coming from a particular sensor for the past week very rapidly and would like to delete all data that is older thanfour weeks. Using Amazon DynamoDB for its scalability and rapidity, how would you implement this in the most cost-effective way?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. One table for each week, with a primary key that is the concatenation of the sensor ID and the timestamp
       B. One table for each week, with a primary key that is the sensor ID, and a hash key that is the timestamp
       C. One table, with a primary key that is the concatenation of the sensor ID and the timestamp
       D. One table, with a primary key that is the sensor ID, and a hash key that is the timestamp
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your company sells consumer devices and needs to record the first activation of all sold devices. Devices are not activated until the information is written on a persistent database. Activation data is very important for your company and must be analyzed daily with a MapReduce job. The execution time of the data analysis process must be less than three hours per day. Devices are usually sold evenly during the year, but when a new device model is out, there is a predictable peak in activations, that is, for a few days there are 10 times or even 100 times more activations than in the average day. Which of the following databases and analysis framework would you implement to better optimize costs and performance for this workload?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Amazon Relational Database Service and Amazon Elastic MapReduce with Spot Instances
       B. Amazon DynamoDB and Amazon Elastic MapReduce with Spot Instances
       C. Amazon Relational Database Service and Amazon Elastic MapReduce with Reserved Instances
       D. Amazon DynamoDB and Amazon Elastic MapReduce with Reserved Instances
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You are moving an existing traditional system to AWS, and during the migration discover that there is a master server which is a single point of failure. Having examined the implementation of the master server you realize there is not enough time during migration to re-engineer it to be highly available, though you do discover that it stores its state in a local MySQL database. In order to minimize down-time you select RDS to replace the local database and configure master to use it, what steps would best allow you to create aself-healing architecture:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Replicate the local database into a RDS Read Replica. Place the master node into a multi-AZ auto-scaling group with a minimum of one and a maximum of one with health checks.
       B. Migrate the local database into a multi-AZ RDS database. Place the master node into a Cross-Zone ELB with a minimum of one and a maximum of one with health checks.
       C. Replicate the local database into a RDS Read Replica. Place the master node into a Cross-Zone ELB with a minimum of one and a maximum of one with health checks.
       D. Migrate the local database into a multi-AZ RDS database. Place the master node into a multi-AZ auto-scaling group with a minimum of one and a maximum of one with health checks.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A customer is in the process of deploying multiple applications to AWS that are owned and operated by different development teams. Each development team maintains the authorization offits users independently from other teams. The customerÀàs information security team would like to be able to delegate user authorization to the individual development teams but independently apply restrictions to the users permissions based on factors such as the userÀàs device and location . For example, the information security team would like to grant read-only permissions to a user who is defined by the development team as read/write whenever the user is authenticating from outside the corporate network. What steps can the information security team take to implement this capability?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Operate an authentication service that generates AWS Security Token Service (STS) tokens with IAM policies from application-defined IAM roles.
       B. Add additional IAM policies to the application IAM roles that deny user privileges based on information security policy.
       C. Enable federation with the internal LDAP directory and grant the application teams permissions to modify users.
       D. Configure IAM policies that restrict modification of the application IAM roles only to the information security team.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You are designing a file-sharing service. This service will have millions of files in it. Revenue for the service will come from fees based on how much storage a user is using. You also want to store metadata on each file, such as title, description and whether the object is public or private. How do you achieve all of these goals in a way that is economical and can scale to millions of users?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Store all files in Amazon Simple Storage Service (S3). Create a bucket for each user. Store metadata in the filename of each object, and access it with LIST commands against the S3 API.
       B. Store all files in Amazon S3. Create Amazon DynamoDB tables for the corresponding key-value pairs on the associated metadata, when objects are uploaded.
       C. Create a striped set of 4000 IOPS Elastic Load Balancing volumes to store the data. Use a database running in Amazon Relational Database Service (RDS) to store the metadata.
       D. Create a striped set of 4000 IOPS Elastic Load Balancing volumes to store the data. Create Amazon DynamoDB tables for the corresponding key-value pairs on the associated metadata, when objects are uploaded.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your company has been contracted to develop and operate a website that tracks NBA basketball statistics. Statistical data to derive reports like ‚Äúbest game-winning shots from the regular season‚Äù and more frequently built reports like ‚Äútop shots of the game‚Äù need to be stored durably for repeated lookup. Leveraging social media techniques, NBA fans submit and vote on new report types from the existing data set so the system needs to accommodate variability in data queries and new static reports must be generated and posted daily. Initial research in the design phase indicates that there will be over 3 million report queries on game day by end users and other applications that use this application as a data source. It is expected that this system will gain in popularity over time and reach peaks of 10-15 million report queries of the system on game days. Select the answer that will allow your application to best meet these requirements while minimizing costs.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Launch a multi-AZ MySQL Amazon Relational Database Service (RDS) Read Replica connected to your multi AZ master database and generate reports by querying the Read Replica. Perform a daily table cleanup.
       B. Generate reports from a multi-AZ MySQL Amazon RDS deployment and have an offline task put reports in Amazon Simple Storage Service (S3) and use CloudFront to cache the content. Use a TTL to expire objects daily.
       C. Implement a multi-AZ MySQL RDS deployment and have the application generate reports from Amazon ElastiCache for in-memory performance results. Utilize the default expire parameter for items in the cache.
       D. Query a multi-AZ MySQL RDS instance and store the results in a DynamoDB table. Generate reports from the DynamoDB table. Remove stale tables daily.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;YouÀàve been tasked with moving an ecommerce web application from a customerÀàs datacenter into a VPC. The application must be fault tolerant and well as highly scalable. Moreover, the customer is adamant that service interruptions not affect the user experience. As you near launch, you discover that the application currently uses multicast to share session state between web servers. In order to handle session state within the VPC, you choose to:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Store session state in Amazon ElastiCache for Redis
       B. Enable session stickiness via Elastic Load Balancing
       C. Create a mesh VPN between instances and allow multicast on it.
       D. Store session state in Amazon Relational Database Service
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Your application is leveraging IAM Roles for EC2 for accessing objects stored in S3. Which two of the following IAM policies control access to your S3 objects?&lt;br&gt;
Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. An IAM trust policy allows the EC2 instance to assume an EC2 instance role
       B. An IAM access policy allows the EC2 role to access S3 objects
       C. An IAM bucket policy allows the EC2 role to access S3 objects
       D. An IAM trust policy allows applications running on the EC2 instance to assume an EC2 role
       E. An IAM trust policy allows applications running on the EC2 instance to access S3 objects
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You are designing Internet connectivity for your VPC. The Web servers must be available on the Internet. The application must have a highly available architecture. Which alternatives should you consider? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure ELB with an EIP. Place all your Web servers behind ELB. Configure a Route53 A record that points to the EIP.
       B. Place all your Web servers behind ELB. Configure a Route53 CNAME to point to the ELB DNS name.
       C. Configure a CloudFront distribution and configure the origin to point to the private IP addresses of your Web servers. Configure a Route53 CNAME record to your CloudFront distribution.
       D. Configure a NAT instance in your VPC. Create a default route via the NAT instance and associate it with all subnets. Configure a DNS A record that points to the NAT Instance public IP address.
       E. Assign EIPs to all Web servers. Configure a Route53 record set with all EIPs, with health checks and DNS failover.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B E&lt;/p&gt;

</description>
      <category>awslagi</category>
      <category>aws</category>
      <category>googlecloud</category>
    </item>
    <item>
      <title>Injecting pre-rendered widgets/content</title>
      <author>dave </author>
      <pubDate>Sun, 22 Aug 2021 14:03:16 +0000</pubDate>
      <link>https://dev.to/davethebrit/injecting-pre-rendered-widgets-content-2bjb</link>
      <guid>https://dev.to/davethebrit/injecting-pre-rendered-widgets-content-2bjb</guid>
      <description>&lt;p&gt;Hi! First time member and post. I had a question that I tried to find an answer for, but likely wasn't searching the right terms.&lt;/p&gt;

&lt;p&gt;We have two teams building React experiences. One who is building the core page, and another that owns widget experiences (think carousels that are componentized, have data and specific logic attached). Is there a concept within React of "injecting" these carousels into the body of a page? Does this impact SSR, performance or security?&lt;/p&gt;

&lt;p&gt;Many thanks in advance&lt;/p&gt;

</description>
      <category>react</category>
    </item>
    <item>
      <title>Hello World in Spring Boot</title>
      <author>Atharva Siddhabhatti</author>
      <pubDate>Sun, 22 Aug 2021 13:52:59 +0000</pubDate>
      <link>https://dev.to/atharvasiddhabhatti/hello-world-in-spring-boot-3oma</link>
      <guid>https://dev.to/atharvasiddhabhatti/hello-world-in-spring-boot-3oma</guid>
      <description>&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--h1zAVQX9--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/853hl8twca6nzmzf0pi7.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--h1zAVQX9--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/853hl8twca6nzmzf0pi7.png" alt="Alt Spring Boot"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#installation"&gt;
  &lt;/a&gt;
  Installation
&lt;/h2&gt;

&lt;p&gt;Clone the repository from here &lt;a href="https://github.com/atharvasiddhabhatti/Springboot-Playground/tree/main/springboot-hello-world"&gt;Click Here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Import Maven based project in any of your Favourite IDE.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;./mvnw spring-boot:run
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#output"&gt;
  &lt;/a&gt;
  Output
&lt;/h2&gt;

&lt;p&gt;Open in Browser&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;http://localhost:5000/hello-world
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;http://localhost:5000/helloworld-bean
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#usage"&gt;
  &lt;/a&gt;
  Usage
&lt;/h2&gt;

&lt;h3&gt;
  &lt;a href="#helloworldcontrollerjava"&gt;
  &lt;/a&gt;
  HelloWorldController.java
&lt;/h3&gt;

&lt;p&gt;helloWorld() method returning a string "Hello World".&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight java"&gt;&lt;code&gt;&lt;span class="nd"&gt;@GetMapping&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"/hello-world"&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="nc"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;helloWorld&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;"Hello World"&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;helloWorldBean() method returning a bean.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight java"&gt;&lt;code&gt;&lt;span class="nd"&gt;@GetMapping&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"/helloworld-bean"&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="nc"&gt;HelloWorldBean&lt;/span&gt; &lt;span class="nf"&gt;helloWorldBean&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nf"&gt;HelloWorldBean&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Hello World"&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;helloWorld1() method printing Hello World, 'name' using @PathVariable annotation&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight java"&gt;&lt;code&gt;&lt;span class="nd"&gt;@GetMapping&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"/helloworld/{name}"&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="nc"&gt;HelloWorldBean&lt;/span&gt; &lt;span class="nf"&gt;helloWorl1&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nd"&gt;@PathVariable&lt;/span&gt; &lt;span class="nc"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nf"&gt;HelloWorldBean&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nc"&gt;String&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;format&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Hello World, %s"&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;
  &lt;a href="#credits"&gt;
  &lt;/a&gt;
  Credits
&lt;/h2&gt;

&lt;p&gt;&lt;a href="https://www.udemy.com/user/in28minutes/"&gt;in28Minutes&lt;/a&gt;&lt;/p&gt;

</description>
      <category>java</category>
      <category>springboot</category>
      <category>backend</category>
    </item>
  </channel>
</rss>
