<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>DEV Community</title>
    <author>DEV Community</author>
    <description>A constructive and inclusive social network for software developers. With you every step of your journey.</description>
    <link>https://dev.to</link>
    <language>en</language>
    <item>
      <title>OCSP and CRL: what could go wrong?</title>
      <author>Cossack Labs</author>
      <pubDate>Tue, 18 Jan 2022 18:06:47 +0000</pubDate>
      <link>https://dev.to/cossacklabs/ocsp-and-crl-what-could-go-wrong-1ped</link>
      <guid>https://dev.to/cossacklabs/ocsp-and-crl-what-could-go-wrong-1ped</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Read this post to learn everything to know about TLS certificate revocation protocols: OCSP and CRL. &lt;br&gt;
How to use OCSP and CRL for validating TLS certificates in Go apps? Even though Golang has native support for TLS, it has extremely limited support for OCSP and CRL. So, what should you do then?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Online_Certificate_Status_Protocol"&gt;OCSP (Online Certificate Status Protocol)&lt;/a&gt;, &lt;a href="https://datatracker.ietf.org/doc/html/rfc6960"&gt;RFC6960&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Certificate_revocation_list"&gt;CRL (Certificate Revocation List)&lt;/a&gt;, &lt;a href="https://datatracker.ietf.org/doc/html/rfc5280"&gt;RFC5280&lt;/a&gt; provide a way to verify whether the TLS certificate was revoked by a Certificate Authority before the app establishes secure communication with a service that uses this certificate.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--zCkRumPn--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rp9i805jn9alflc7nu14.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--zCkRumPn--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/rp9i805jn9alflc7nu14.png" alt="OCSP and CRL" width="880" height="744"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#what-could-go-wrong-in-using-ocsp-and-crl"&gt;
  &lt;/a&gt;
  What could go wrong in using OCSP and CRL?
&lt;/h2&gt;

&lt;p&gt;Here's just a quick list:&lt;/p&gt;

&lt;p&gt;üî¥  &lt;strong&gt;Establishing session before validating the TLS certificate&lt;/strong&gt; (don't be tempted to "optimise" things if OCSP and CRL cause visible delays in the application work).&lt;br&gt;
üî¥  &lt;strong&gt;Unreachable CRL / OCSP responders&lt;/strong&gt; (this could destabilise the whole solution, as applications can be stuck in limbo, not communicating with other services while validating the certificates).&lt;br&gt;
üî¥  &lt;strong&gt;Accepting "Unknown" status as valid&lt;/strong&gt; (if the application treats OCSP "Unknown" status as "the certificate is still valid", it might continue talking to the already malicious service, treating it as a valid).&lt;br&gt;
üî¥  &lt;strong&gt;Certificate revocation checks are too rare&lt;/strong&gt; (balance performance and security, mind a threat model and security requirements).&lt;br&gt;
üî¥  &lt;strong&gt;Lack of signature validation&lt;/strong&gt; (it's essential to verify the CRL response signature to prevent potential attackers from tampering with the list).&lt;br&gt;
üî¥  &lt;strong&gt;CRL cache poisoning&lt;/strong&gt; (if CRL is cached in a local file without any integrity checks, the application won't recognise that the file was changed).&lt;br&gt;
üî¥  &lt;strong&gt;Delta CRLs&lt;/strong&gt; (if implemented poorly, issues with network connectivity might result in the application not receiving some delta CRLs and continuing to trust the revoked certificates).&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#who-needs-ocsp-and-crl"&gt;
  &lt;/a&gt;
  Who needs OCSP and CRL?
&lt;/h2&gt;

&lt;p&gt;We stumbled upon intricacies in OCSP and CRL when building &lt;a href="https://www.cossacklabs.com/acra/"&gt;Acra database security suite&lt;/a&gt;, an application that sits between the app and the database and encrypts/decrypts sensitive data. The support of OCSP and CRL is crucial for Acra to prevent unauthorised connections from malicious or misconfigured apps to sensitive data. &lt;/p&gt;

&lt;p&gt;To meet our security model, we had to implement OCSP/CRL verification in Golang ourselves. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#see-ocsp-and-crl-implementation-examples"&gt;
  &lt;/a&gt;
  See OCSP and CRL implementation examples
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;üí° As it was not easy but interesting task, we put our findings into a new engineering blog post: &lt;a href="https://www.cossacklabs.com/blog/tls-validation-implementing-ocsp-and-crl-in-go/"&gt;TLS validation: implement OCSP and CRL verifiers in Go&lt;/a&gt;. Follow the link to learn more about OCSP/CRL design, implementation and security tips, example code and popular mistakes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you're a Golang engineer that works closely with TLS, our blog post will give you lots of moments to think about. &lt;/p&gt;

&lt;p&gt;To illustrate everything we‚Äôve posted above, we created minimalistic OCSP and CRL implementations in Go. &lt;/p&gt;

&lt;p&gt;Check out the &lt;a href="https://cossacklabs.com/blog/tls-validation-implementing-ocsp-and-crl-in-go/#ocsp-crl-live-examples"&gt;4. Live examples&lt;/a&gt; chapter with all the scripts to generate TLS certificates, OCSP responder based on OpenSSL, and Golang server-side and client-side apps. Feel free to review and run examples to see how the verification works with valid and revoked certificates.&lt;/p&gt;

</description>
      <category>security</category>
      <category>go</category>
      <category>development</category>
      <category>encryption</category>
    </item>
    <item>
      <title>How-to setup a HA/DR database in AWS? [9 - Generate a random value]</title>
      <author>Maxime Guilbert</author>
      <pubDate>Tue, 18 Jan 2022 17:40:51 +0000</pubDate>
      <link>https://dev.to/adaendra/how-to-setup-a-hadr-database-in-aws-9-generate-a-random-value-5g8a</link>
      <guid>https://dev.to/adaendra/how-to-setup-a-hadr-database-in-aws-9-generate-a-random-value-5g8a</guid>
      <description>&lt;p&gt;In this part &lt;em&gt;(and the last part of the serie)&lt;/em&gt;, we will see how to generate a random value.&lt;/p&gt;

&lt;p&gt;Really useful to generate unique names for a snapshot for example, you will see that there are multiple definitions available, depending on what you need.&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#random-id"&gt;
  &lt;/a&gt;
  Random Id
&lt;/h2&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="k"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"random_id"&lt;/span&gt; &lt;span class="s2"&gt;"rdm_id"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;byte_length&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Declared as this, it will generate for you an 8 bytes long id which can be retrieve in :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;base64&lt;/strong&gt; : random_id.rdm_id.id =&amp;gt; MDc3NDA2OGE5YTNhMjc5MQ==&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;decimal digits&lt;/strong&gt; : random_id.rdm_id.dec =&amp;gt; 537061447926687633&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;hexadecimal digits&lt;/strong&gt; : random_id.rdm_id.hex =&amp;gt; 0774068a9a3a2791&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But it you are doing it like this, &lt;strong&gt;the random_id will always be the same!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So to be sure the value change when something append, you can use the &lt;strong&gt;keepers&lt;/strong&gt; parameter.&lt;/p&gt;

&lt;p&gt;With this, you can say "if the parameter X and Y change, so the random value must change too".&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In this example the random value will change each time the arn of the global cluster example will change.&lt;/em&gt;&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="k"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"random_id"&lt;/span&gt; &lt;span class="s2"&gt;"rdm_id"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;byte_length&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;

  &lt;span class="nx"&gt;keepers&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;first&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;aws_rds_global_cluster&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;example&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;arn&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;And if you want this value to change each time the script is executed, use a timestamp :&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="k"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"random_id"&lt;/span&gt; &lt;span class="s2"&gt;"rdm_id"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;byte_length&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;

  &lt;span class="nx"&gt;keepers&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;first&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;timestamp&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;






&lt;h2&gt;
  &lt;a href="#other-random-possibilities"&gt;
  &lt;/a&gt;
  Other random possibilities
&lt;/h2&gt;

&lt;p&gt;Following the same pattern, Terraform let you to generate random &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;integer
&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="k"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"random_integer"&lt;/span&gt; &lt;span class="s2"&gt;"priority"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;min&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
  &lt;span class="nx"&gt;max&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;50000&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;ul&gt;
&lt;li&gt;password
&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="k"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"random_password"&lt;/span&gt; &lt;span class="s2"&gt;"password"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;length&lt;/span&gt;           &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;
  &lt;span class="nx"&gt;special&lt;/span&gt;          &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;
  &lt;span class="nx"&gt;override_special&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"_%@"&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;ul&gt;
&lt;li&gt;string
&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="k"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"random_string"&lt;/span&gt; &lt;span class="s2"&gt;"random"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;length&lt;/span&gt;           &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;
  &lt;span class="nx"&gt;special&lt;/span&gt;          &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;
  &lt;span class="nx"&gt;override_special&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"/@¬£&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;ul&gt;
&lt;li&gt;UUID
&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="k"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"random_uuid"&lt;/span&gt; &lt;span class="s2"&gt;"test"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;ul&gt;
&lt;li&gt;A shuffled sublist
&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="k"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"random_shuffle"&lt;/span&gt; &lt;span class="s2"&gt;"az"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;input&lt;/span&gt;        &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"us-west-1a"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"us-west-1c"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"us-west-1d"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"us-west-1e"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="nx"&gt;result_count&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;em&gt;Here you will retrieve a sublist of the input list, with only 2 items.&lt;/em&gt;&lt;/p&gt;




&lt;h3&gt;
  &lt;a href="#links"&gt;
  &lt;/a&gt;
  Links
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Terraform doc random_id : &lt;a href="https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/id"&gt;https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/id&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Terraform doc random_integer : &lt;a href="https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/integer"&gt;https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/integer&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Terraform doc random_password : &lt;a href="https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/password"&gt;https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/password&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Terraform doc random_shuffle : &lt;a href="https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/shuffle"&gt;https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/shuffle&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Terraform doc random_string : &lt;a href="https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/string"&gt;https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/string&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Terraform doc random_uuid : &lt;a href="https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/uuid"&gt;https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/uuid&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;




&lt;p&gt;I hope it will help you and you liked this serie! üç∫&lt;/p&gt;

</description>
      <category>sre</category>
      <category>aws</category>
      <category>terraform</category>
      <category>devops</category>
    </item>
    <item>
      <title>How-to setup a HA/DR database in AWS? [8 - Multiple instances in multiple regions]</title>
      <author>Maxime Guilbert</author>
      <pubDate>Tue, 18 Jan 2022 17:40:28 +0000</pubDate>
      <link>https://dev.to/adaendra/how-to-setup-a-hadr-database-in-aws-8-multiple-instances-in-multiple-regions-210d</link>
      <guid>https://dev.to/adaendra/how-to-setup-a-hadr-database-in-aws-8-multiple-instances-in-multiple-regions-210d</guid>
      <description>&lt;p&gt;In this part of the serie, we will see how to create &lt;strong&gt;in one Terraform script&lt;/strong&gt; and in a single execution, how to create elements in multiple regions.&lt;/p&gt;

&lt;p&gt;This kind of tips is really helpful if you want to create at once all your infra and/or if you want to setup some disaster recovery.&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#how-to-do-it"&gt;
  &lt;/a&gt;
  How to do it?
&lt;/h2&gt;

&lt;p&gt;To do it, it's pretty simple. We have 2 steps to follow :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;declare multiple providers&lt;/li&gt;
&lt;li&gt;declare which provider we want to use for each resource&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#declare-multiple-providers"&gt;
  &lt;/a&gt;
  Declare multiple providers
&lt;/h3&gt;

&lt;p&gt;In your current script to create something on AWS, you should have an &lt;strong&gt;AWS Provider&lt;/strong&gt; with the &lt;strong&gt;region&lt;/strong&gt; where it should be created.&lt;/p&gt;

&lt;p&gt;So, copy/paste this block for all your regions. Then add an alias to each of them to be able to differentiate them.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="k"&gt;provider&lt;/span&gt; &lt;span class="s2"&gt;"aws"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;alias&lt;/span&gt;  &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"frankfurt"&lt;/span&gt;
  &lt;span class="nx"&gt;region&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"eu-central-1"&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;provider&lt;/span&gt; &lt;span class="s2"&gt;"aws"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;alias&lt;/span&gt;  &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"sydney"&lt;/span&gt;
  &lt;span class="nx"&gt;region&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"ap-southeast-2"&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;






&lt;h3&gt;
  &lt;a href="#declare-which-provider-we-want-to-use-for-each-resource"&gt;
  &lt;/a&gt;
  Declare which provider we want to use for each resource
&lt;/h3&gt;

&lt;p&gt;In each resource you have, add the &lt;strong&gt;provider&lt;/strong&gt; parameter to link each of them to the correct provider.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="k"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"aws_rds_cluster_instance"&lt;/span&gt; &lt;span class="s2"&gt;"test_frankfurt"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="k"&gt;provider&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;aws&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;frankfurt&lt;/span&gt;
  &lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"aws_rds_cluster_instance"&lt;/span&gt; &lt;span class="s2"&gt;"test_sydney"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="k"&gt;provider&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;aws&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;sydney&lt;/span&gt;
  &lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;






&lt;p&gt;And that's it! You are now able to deploy your complete infra on multiple regions at once!&lt;/p&gt;

&lt;p&gt;I hope it will help you! üç∫&lt;/p&gt;

&lt;p&gt;And see you soon for the next part of this serie. üòÄ&lt;/p&gt;

</description>
      <category>aws</category>
      <category>terraform</category>
      <category>sre</category>
      <category>devops</category>
    </item>
    <item>
      <title>How-to setup a HA/DR database in AWS? [7 - Dynamic Terraform backend definition]</title>
      <author>Maxime Guilbert</author>
      <pubDate>Tue, 18 Jan 2022 17:40:20 +0000</pubDate>
      <link>https://dev.to/adaendra/how-to-setup-a-hadr-database-in-aws-7-dynamic-terraform-backend-definition-3aga</link>
      <guid>https://dev.to/adaendra/how-to-setup-a-hadr-database-in-aws-7-dynamic-terraform-backend-definition-3aga</guid>
      <description>&lt;p&gt;In this post we will see something which can be really useful to use the same script in multiple environment or for multiple projects without modifying the code!&lt;/p&gt;




&lt;p&gt;To do this magic trick, we will define dynamically the backend that we will use to store the terraform state.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#backend-declaration"&gt;
  &lt;/a&gt;
  Backend declaration
&lt;/h2&gt;

&lt;p&gt;First, we need to declare in the script which kind of backend we want to use.&lt;/p&gt;

&lt;p&gt;In our example, it will be with &lt;strong&gt;AWS S3&lt;/strong&gt;.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="k"&gt;terraform&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="p"&gt;...&lt;/span&gt;
  &lt;span class="nx"&gt;backend&lt;/span&gt; &lt;span class="s2"&gt;"s3"&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
  &lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;






&lt;h2&gt;
  &lt;a href="#dynamic-magic"&gt;
  &lt;/a&gt;
  Dynamic magic
&lt;/h2&gt;

&lt;p&gt;So to have a dynamic backend definition, you have to add declare some parameters as options of the &lt;strong&gt;terraform init&lt;/strong&gt; command.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#backend-parameters"&gt;
  &lt;/a&gt;
  Backend parameters
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;bucket&lt;/strong&gt; : Name of the S3 bucket where you want to store the Terraform state.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;key&lt;/strong&gt; : Path of the Terraform state file in the S3 bucket. Generally it is this parameter who will be updated to ensure you that each terraform state is located in a different place.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;region&lt;/strong&gt; : Region where the bucket is&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;encrypt&lt;/strong&gt; : A boolean to know if you want to encrypt the file &lt;em&gt;(if you have a doubt, set it to true)&lt;/em&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#command-option"&gt;
  &lt;/a&gt;
  Command option
&lt;/h3&gt;

&lt;p&gt;To give the backend parameters, you have to way to do it :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;declare a parameter with a key=value&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;-backend-config='bucket=test'&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;declare a tfvars file which contains some/all the configurations&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;-backend-config='configs/configs_backend.tfvars'&lt;/p&gt;
&lt;/blockquote&gt;




&lt;p&gt;Then you just have to give the right parameters when using the terraform commands, and you will be able to define multiple infra with one script!&lt;/p&gt;




&lt;p&gt;I hope it will help you! üç∫&lt;/p&gt;

&lt;p&gt;And see you soon for the next part of this serie. üòÄ&lt;/p&gt;

</description>
      <category>aws</category>
      <category>sre</category>
      <category>terraform</category>
      <category>devops</category>
    </item>
    <item>
      <title>How-to setup a HA/DR database in AWS? [6 - Create from snapshot]</title>
      <author>Maxime Guilbert</author>
      <pubDate>Tue, 18 Jan 2022 17:40:11 +0000</pubDate>
      <link>https://dev.to/adaendra/how-to-setup-a-hadr-database-in-aws-6-create-from-snapshot-2mbf</link>
      <guid>https://dev.to/adaendra/how-to-setup-a-hadr-database-in-aws-6-create-from-snapshot-2mbf</guid>
      <description>&lt;p&gt;In this post we will see how to create an AWS RDS Global database from a snapshot.&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#why-create-a-database-from-a-snapshot-if-we-already-have-the-dr"&gt;
  &lt;/a&gt;
  Why create a database from a snapshot if we already have the DR?
&lt;/h2&gt;

&lt;p&gt;For sure, if you have a DR system, you might be have to create your global database from a snapshot every day &lt;em&gt;(I hope for you so)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;But when you are delivering a huge update on your database, you maybe want to have something to go back to the last state before, or if you are migrating data from a database to another.&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#how-to-create-a-global-database-from-a-snapshot-"&gt;
  &lt;/a&gt;
  How to create a global database from a snapshot ?
&lt;/h2&gt;

&lt;p&gt;If you have checked the documentation, you have seen a parameter called &lt;strong&gt;snapshot_identifier&lt;/strong&gt; in the &lt;strong&gt;aws_rds_cluster&lt;/strong&gt; definition.&lt;/p&gt;

&lt;p&gt;If you define a value here and run your script, you will have all the elements created but the cluster with the snapshot won't be linked to the global database.&lt;/p&gt;

&lt;p&gt;In the solution presented in the last post, we did :&lt;br&gt;
1 - the creation of the global cluster&lt;br&gt;
2 - then we create the principal cluster&lt;br&gt;
3 - and then we create all the other clusters&lt;/p&gt;

&lt;p&gt;But to make it work, we need to change this order and the links.&lt;/p&gt;

&lt;p&gt;We need to create first the principal cluster from the snapshot, without links to the global database.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This one&lt;/em&gt;&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;global_cluster_identifier = aws_rds_global_cluster.example.id
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Then, we have to create the global cluster. But this time, we will say to the global cluster that it needs to be based on the primary cluster.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="k"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"aws_rds_global_cluster"&lt;/span&gt; &lt;span class="s2"&gt;"example"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;global_cluster_identifier&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"global-test"&lt;/span&gt;

  &lt;span class="nx"&gt;source_db_cluster_identifier&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;aws_rds_cluster&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;default&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;arn&lt;/span&gt;
  &lt;span class="nx"&gt;force_destroy&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;In this global cluster definition, we can see that a lot of parameters desapeared &lt;em&gt;(like engine, database name...)&lt;/em&gt; and it's because the global cluster will take the configurations of the primary cluster!&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note : If you are creating a cluster from a snapshot, the &lt;strong&gt;master_username&lt;/strong&gt; won't be override by the one you defined in your script, it will keep the one defined in the snapshot, but &lt;strong&gt;master_username&lt;/strong&gt; yes.&lt;/p&gt;
&lt;/blockquote&gt;




&lt;p&gt;I hope it will help you! üç∫&lt;/p&gt;

&lt;p&gt;And see you soon for the next part of this serie. üòÄ&lt;/p&gt;

</description>
      <category>terraform</category>
      <category>sre</category>
      <category>aws</category>
      <category>devops</category>
    </item>
    <item>
      <title>How-to setup a HA/DR database in AWS? [5 - DR database]</title>
      <author>Maxime Guilbert</author>
      <pubDate>Tue, 18 Jan 2022 17:40:03 +0000</pubDate>
      <link>https://dev.to/adaendra/how-to-setup-a-hadr-database-in-aws-5-dr-database-278b</link>
      <guid>https://dev.to/adaendra/how-to-setup-a-hadr-database-in-aws-5-dr-database-278b</guid>
      <description>&lt;p&gt;In this part of the serie, we will see how to &lt;em&gt;(finally)&lt;/em&gt; create a database with a system of disaster recovery.&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#create-a-global-database"&gt;
  &lt;/a&gt;
  Create a global database
&lt;/h2&gt;

&lt;p&gt;Now that we have a cluster of databases, we can easily create multiple of them in multiple regions and link them to a &lt;strong&gt;Global Database&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;So at least, we need to create 2 clusters in 2 regions and we will create an &lt;strong&gt;aws_rds_global_cluster&lt;/strong&gt;. &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#definition-of-awsrdsglobalcluster"&gt;
  &lt;/a&gt;
  Definition of aws_rds_global_cluster
&lt;/h3&gt;



&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="k"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"aws_rds_global_cluster"&lt;/span&gt; &lt;span class="s2"&gt;"example"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;global_cluster_identifier&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"global-test"&lt;/span&gt;
  &lt;span class="nx"&gt;engine&lt;/span&gt;                    &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"aurora"&lt;/span&gt;
  &lt;span class="nx"&gt;engine_version&lt;/span&gt;            &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"5.6.mysql_aurora.1.22.2"&lt;/span&gt;
  &lt;span class="nx"&gt;database_name&lt;/span&gt;             &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"example_db"&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;In this example, we can see that some common parameters are defined here as the engine, its version or the db name to keep an uniformity among all the databases.&lt;/p&gt;




&lt;h3&gt;
  &lt;a href="#updates-in-the-clusters-definitions"&gt;
  &lt;/a&gt;
  Updates in the clusters definitions
&lt;/h3&gt;

&lt;p&gt;Due to the fact that you will use a global database, you have to link your clusters to the global one.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="nx"&gt;global_cluster_identifier&lt;/span&gt; &lt;span class="err"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;aws_rds_global_cluster&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;example&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Then, on all the secondary cluster, you can add &lt;strong&gt;depends_on&lt;/strong&gt; to be sure that they will be created after the global one and the primary one.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="nx"&gt;depends_on&lt;/span&gt; &lt;span class="err"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="nx"&gt;aws_rds_global_cluster&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;example&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nx"&gt;aws_rds_cluster&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;default&lt;/span&gt;
  &lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Also, some parameters are not required anymore &lt;em&gt;(because they are now defined in the global database)&lt;/em&gt; like :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;master_username&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;master_password&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;database_name&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then, if you wanted to created all the elements in a single Terraform script, you will have to declare multiple providers for each region where you want to create a cluster. &lt;em&gt;(The complete point will be explain in a following post)&lt;/em&gt;&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#using-the-global-cluster"&gt;
  &lt;/a&gt;
  Using the Global cluster
&lt;/h2&gt;

&lt;h3&gt;
  &lt;a href="#access-to-the-database"&gt;
  &lt;/a&gt;
  Access to the database
&lt;/h3&gt;

&lt;p&gt;The global cluster don't have a specific endpoint to expose the database with Read/Write rights and another for all the read-only databases.&lt;/p&gt;

&lt;p&gt;Each cluster will create its own endpoints.&lt;/p&gt;

&lt;p&gt;For the primary cluster, &lt;strong&gt;both endpoints will work well.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For the other cluster, &lt;strong&gt;only the read-only endpoint will work.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It's normal, it's to be sure to have only one main entry point and replicate the update on all the other databases.&lt;/p&gt;

&lt;p&gt;But a region is not accessible anymore or if you manually do a failover to change the primary region, the read/write endpoint of the first region will be disabled and the one of the new primary region will be enabled.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#how-to-do-manually-a-failover"&gt;
  &lt;/a&gt;
  How to do manually a failover?
&lt;/h3&gt;

&lt;p&gt;In the AWS web console, select your global database and in the actions you will have &lt;strong&gt;Fail over global database&lt;/strong&gt;. Click on it and the switch will go on!&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#links"&gt;
  &lt;/a&gt;
  Links
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Terraform documentation : &lt;a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/rds_global_cluster"&gt;https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/rds_global_cluster&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;AWS failover global database documentation : &lt;a href="https://aws.amazon.com/blogs/database/managed-planned-failovers-with-amazon-aurora-global-database/"&gt;https://aws.amazon.com/blogs/database/managed-planned-failovers-with-amazon-aurora-global-database/&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;




&lt;p&gt;I hope it will help you! üç∫&lt;/p&gt;

&lt;p&gt;And see you soon for the next part of this serie. üòÄ&lt;/p&gt;

</description>
      <category>devops</category>
      <category>sre</category>
      <category>aws</category>
      <category>terraform</category>
    </item>
    <item>
      <title>How-to setup a HA/DR database in AWS? [4 - HA Database]</title>
      <author>Maxime Guilbert</author>
      <pubDate>Tue, 18 Jan 2022 17:39:55 +0000</pubDate>
      <link>https://dev.to/adaendra/how-to-setup-a-hadr-database-in-aws-4-ha-database-4kek</link>
      <guid>https://dev.to/adaendra/how-to-setup-a-hadr-database-in-aws-4-ha-database-4kek</guid>
      <description>&lt;p&gt;In this part of the serie, we will see what is the difference between a simple database and an High Available database in AWS, then how to create it with Terraform.&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#differences"&gt;
  &lt;/a&gt;
  Differences
&lt;/h2&gt;

&lt;p&gt;A simple database will be deployed on one &lt;strong&gt;AZ&lt;/strong&gt;. So if this AZ is not accessible for some reasons, the database can't be call anymore.&lt;/p&gt;

&lt;p&gt;So to avoid this kind of problem, we create database clusters. One instance will be the principal with Read/Write rights and all the others will be secondary with Read only rights. And each of these instances will be on multiples AZ.&lt;/p&gt;

&lt;p&gt;With this architecture, if the principal fails, another one will become the principal and the first one can be recreated.&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#creation-of-the-cluster"&gt;
  &lt;/a&gt;
  Creation of the cluster
&lt;/h2&gt;

&lt;h3&gt;
  &lt;a href="#definition-of-a-subnet-group"&gt;
  &lt;/a&gt;
  Definition of a subnet group
&lt;/h3&gt;

&lt;p&gt;To know where we will deploy our multiple instances of the cluster, we will create a &lt;strong&gt;subnet group&lt;/strong&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note : A subnet &lt;em&gt;(which are defined in the VPC module)&lt;/em&gt; can be on multiple AZ in a region. So for best practices, be sure to have one subnet per AZ.&lt;br&gt;
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="k"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"aws_db_subnet_group"&lt;/span&gt; &lt;span class="s2"&gt;"default"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;name&lt;/span&gt;       &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"main"&lt;/span&gt;
  &lt;span class="nx"&gt;subnet_ids&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;aws_subnet&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;frontend&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;aws_subnet&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;backend&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;id&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

  &lt;span class="nx"&gt;tags&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;Name&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"My DB subnet group"&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;This object is really simple, you have a name for your subnet group and the list of the subnet ids.&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#links"&gt;
  &lt;/a&gt;
  Links
&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Terraform doc aws_db_subnet_group : &lt;a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/db_subnet_group"&gt;https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/db_subnet_group&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;




&lt;h3&gt;
  &lt;a href="#definition-of-the-cluster"&gt;
  &lt;/a&gt;
  Definition of the cluster
&lt;/h3&gt;

&lt;p&gt;Now that we know where all the instances can be created, we will create the cluster and then add all the instances we wanted.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="k"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"aws_rds_cluster"&lt;/span&gt; &lt;span class="s2"&gt;"default"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;cluster_identifier&lt;/span&gt;      &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"aurora-cluster-demo"&lt;/span&gt;
  &lt;span class="nx"&gt;engine&lt;/span&gt;                  &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"aurora-mysql"&lt;/span&gt;
  &lt;span class="nx"&gt;engine_version&lt;/span&gt;          &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"5.7.mysql_aurora.2.03.2"&lt;/span&gt;
  &lt;span class="nx"&gt;availability_zones&lt;/span&gt;      &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"us-west-2a"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"us-west-2b"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"us-west-2c"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="nx"&gt;database_name&lt;/span&gt;           &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"mydb"&lt;/span&gt;
  &lt;span class="nx"&gt;master_username&lt;/span&gt;         &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"username"&lt;/span&gt;
  &lt;span class="nx"&gt;master_password&lt;/span&gt;         &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"pwd"&lt;/span&gt;
  &lt;span class="nx"&gt;backup_retention_period&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
  &lt;span class="nx"&gt;preferred_backup_window&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"07:00-09:00"&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;This cluster definition is a light one, and you can retrieve a lot of common parameters from the aws_db_instance.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;/!\ The major point here is the &lt;strong&gt;availability_zones&lt;/strong&gt; parameter, be sure to have in this list only AZ covered by your subnets.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you want to go further for your configuration, you can add a lot of things about the &lt;strong&gt;security&lt;/strong&gt; &lt;em&gt;(port, vpc_security_group_ids, storage_encrypted...)&lt;/em&gt;, the updates &lt;em&gt;(backup_retention_period, preferred_backup_window, preferred_maintenance_window...)&lt;/em&gt; ...&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#security"&gt;
  &lt;/a&gt;
  Security
&lt;/h4&gt;

&lt;p&gt;If you want to have a secure cluster, you must have :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;storage_encrypted&lt;/strong&gt; defined to true to be sure that the data will be encrypted while stored&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;kms_key_id&lt;/strong&gt; should be defined to know which encryption key the database have to use&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;enabled_cloudwatch_logs_exports&lt;/strong&gt; defined to have some logs exported to cloudwatch to be able to do an audit&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;vpc_security_group_ids&lt;/strong&gt; to have a clear definition of who can access to your database&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also another good configuration is to required a TLS transport. To do it, you need to create an &lt;strong&gt;aws_rds_cluster_parameter_group&lt;/strong&gt; and link it to your cluster in the &lt;strong&gt;db_cluster_parameter_group_name&lt;/strong&gt; parameter.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="k"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"aws_rds_cluster_parameter_group"&lt;/span&gt; &lt;span class="s2"&gt;"cluster_param_group"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;name&lt;/span&gt;        &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"cluster_param_group"&lt;/span&gt;
  &lt;span class="nx"&gt;family&lt;/span&gt;      &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"aurora-mysql5.7"&lt;/span&gt;
  &lt;span class="nx"&gt;description&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"cluster_param_group"&lt;/span&gt;

  &lt;span class="nx"&gt;parameter&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;name&lt;/span&gt;  &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"require_secure_transport"&lt;/span&gt;
    &lt;span class="nx"&gt;value&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"ON"&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h4&gt;
  &lt;a href="#links"&gt;
  &lt;/a&gt;
  Links
&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;AWS RDS cluster : &lt;a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/rds_cluster"&gt;https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/rds_cluster&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;aws_rds_cluster_parameter_group : &lt;a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/rds_cluster_parameter_group"&gt;https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/rds_cluster_parameter_group&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;aws_kms_key : &lt;a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/kms_key"&gt;https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/kms_key&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;




&lt;h3&gt;
  &lt;a href="#definition-of-the-instances"&gt;
  &lt;/a&gt;
  Definition of the instances
&lt;/h3&gt;

&lt;p&gt;The declaration of an instance is quite easy and don't have a lot of parameters. &lt;/p&gt;

&lt;p&gt;One of the principal is the reference to the cluster it's linked.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight terraform"&gt;&lt;code&gt;&lt;span class="k"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"aws_rds_cluster_instance"&lt;/span&gt; &lt;span class="s2"&gt;"cluster_instances"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;count&lt;/span&gt;                      &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
  &lt;span class="nx"&gt;identifier&lt;/span&gt;                 &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"aurora-cluster-demo-&lt;/span&gt;&lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nx"&gt;count&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;index&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;
  &lt;span class="nx"&gt;cluster_identifier&lt;/span&gt;         &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;aws_rds_cluster&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;default&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;id&lt;/span&gt;
  &lt;span class="nx"&gt;instance_class&lt;/span&gt;             &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"db.r4.large"&lt;/span&gt;
  &lt;span class="nx"&gt;engine&lt;/span&gt;                     &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;aws_rds_cluster&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;default&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;engine&lt;/span&gt;
  &lt;span class="nx"&gt;engine_version&lt;/span&gt;             &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;aws_rds_cluster&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;default&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;engine_version&lt;/span&gt;
  &lt;span class="nx"&gt;db_subnet_group_name&lt;/span&gt;       &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"db_subnet_group_name"&lt;/span&gt;
  &lt;span class="nx"&gt;db_parameter_group_name&lt;/span&gt;    &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"db_parameter_group"&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;On the example we can see :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the cluster reference in &lt;strong&gt;cluster_identifier&lt;/strong&gt;
&lt;/li&gt;
&lt;li&gt;The engine definition and its version. For a cleaner way to declare it and be sure to have the same than the cluster, you can use : &lt;strong&gt;aws_rds_cluster.default.engine&lt;/strong&gt; and &lt;strong&gt;aws_rds_cluster.default.engine_version&lt;/strong&gt;
&lt;/li&gt;
&lt;li&gt;the instance class for all the calculations and resources&lt;/li&gt;
&lt;li&gt;and the subnet and parameter group names if you have one&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also on this example, you can see a &lt;strong&gt;count&lt;/strong&gt; parameter. It's not a mandatory one, it's a Terraform parameter to create easily multiple instances with the same definition.&lt;/p&gt;

&lt;p&gt;Here, everything is the same except the name, so that's why we include the count index in the name to be sure to have unique names.&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#links"&gt;
  &lt;/a&gt;
  Links
&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Terraform documentation rds_cluster_instance : &lt;a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/rds_cluster_instance"&gt;https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/rds_cluster_instance&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;




&lt;h2&gt;
  &lt;a href="#running-with"&gt;
  &lt;/a&gt;
  Running with
&lt;/h2&gt;

&lt;h3&gt;
  &lt;a href="#how-to-access-to-the-cluster"&gt;
  &lt;/a&gt;
  How to access to the cluster?
&lt;/h3&gt;

&lt;p&gt;With the cluster, 2 endpoints are created :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;one to expose the database with Read/Write rights&lt;/li&gt;
&lt;li&gt;one to expose all the other databases in Read only&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With this endpoints, you can be sure to always have an instance available to answer you if one database/AZ can't be called.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note : If you have 2 instances in your cluster and one instance fails, both endpoints will access to the same database. But you still can't do update from the read only endpoint.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3&gt;
  &lt;a href="#how-to-test-the-high-availability-"&gt;
  &lt;/a&gt;
  How to test the High Availability ?
&lt;/h3&gt;

&lt;p&gt;When you are using a cluster, it's always useful to know how you can test the high availabilty and check if your services goes well in case of a failover.&lt;/p&gt;

&lt;p&gt;So to test it, in the AWS web console, you have an action &lt;strong&gt;Failover&lt;/strong&gt; available for your cluster.&lt;/p&gt;

&lt;p&gt;When you click on it, your Read/Write instance will become a read-only one and a random read-only instance will become the Read/Write one.&lt;/p&gt;




&lt;p&gt;I hope it will help you! üç∫&lt;/p&gt;

&lt;p&gt;And see you soon for the next part of this serie. üòÄ&lt;/p&gt;

</description>
      <category>sre</category>
      <category>aws</category>
      <category>devops</category>
      <category>terraform</category>
    </item>
    <item>
      <title>How-to setup a HA/DR database in AWS? [2 - Definitions]</title>
      <author>Maxime Guilbert</author>
      <pubDate>Tue, 18 Jan 2022 17:39:38 +0000</pubDate>
      <link>https://dev.to/adaendra/how-to-setup-a-hadr-database-in-aws-2-definitions-93p</link>
      <guid>https://dev.to/adaendra/how-to-setup-a-hadr-database-in-aws-2-definitions-93p</guid>
      <description>&lt;p&gt;Welcome to the new post of the serie about &lt;strong&gt;"How to set up a HA/DR database in AWS?"&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Today, we will look at some definitions of AWS terms that will be used in the following posts. So if you are already aware of AWS and its tools, you can ignore that post and go to the next one.&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#regions"&gt;
  &lt;/a&gt;
  Regions
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;AWS has the concept of a Region, which is a physical location around the world where we cluster data centers. We call each group of logical data centers an Availability Zone. Each AWS Region consists of multiple, isolated, and physically separate AZs within a geographic area.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Maps of current AWS Regions&lt;/em&gt;&lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--tSsi4N_S--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/7buromr4c9pxvy5bh0qe.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--tSsi4N_S--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/7buromr4c9pxvy5bh0qe.png" alt="Maps of AWS Regions" width="880" height="480"&gt;&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&lt;em&gt;Definition from AWS : &lt;a href="https://aws.amazon.com/about-aws/global-infrastructure/regions_az/?nc1=h_ls"&gt;https://aws.amazon.com/about-aws/global-infrastructure/regions_az/?nc1=h_ls&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#availability-zone"&gt;
  &lt;/a&gt;
  Availability Zone
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;An Availability Zone (AZ) is one or more discrete data centers with redundant power, networking, and connectivity in an AWS Region. AZs give customers the ability to operate production applications and databases that are more highly available, fault-tolerant, and scalable than would be possible from a single data center. All AZs in an AWS Region are interconnected with high-bandwidth, low-latency networking, over fully redundant, dedicated metro fiber providing high-throughput, low-latency networking between AZs. All traffic between AZs is encrypted.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Definition from AWS : &lt;a href="https://aws.amazon.com/about-aws/global-infrastructure/regions_az/?nc1=h_ls"&gt;https://aws.amazon.com/about-aws/global-infrastructure/regions_az/?nc1=h_ls&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#aws-rds"&gt;
  &lt;/a&gt;
  AWS RDS
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks, such as hardware provisioning, database setup, patching, and backups. It frees you to focus on your applications so you can give them the fast performance, high availability, security, and compatibility they need.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Definition from AWS : &lt;a href="https://aws.amazon.com/rds/?nc1=h_ls"&gt;https://aws.amazon.com/rds/?nc1=h_ls&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#aws-global-database"&gt;
  &lt;/a&gt;
  AWS Global Database
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Definition from AWS : &lt;a href="https://aws.amazon.com/rds/aurora/global-database/?nc1=h_ls"&gt;https://aws.amazon.com/rds/aurora/global-database/?nc1=h_ls&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#aws-s3"&gt;
  &lt;/a&gt;
  AWS S3
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Amazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can store and protect any amount of data for virtually any use case, such as data lakes, cloud-native applications, and mobile apps.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Definition from AWS : &lt;a href="https://aws.amazon.com/s3/"&gt;https://aws.amazon.com/s3/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#aws-kms"&gt;
  &lt;/a&gt;
  AWS KMS
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Definition from AWS : &lt;a href="https://aws.amazon.com/kms/?nc1=h_ls"&gt;https://aws.amazon.com/kms/?nc1=h_ls&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#vpc"&gt;
  &lt;/a&gt;
  VPC
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. It is logically isolated from other virtual networks in the AWS Cloud. You can launch your AWS resources, such as Amazon EC2 instances, into your VPC.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Definition from AWS : &lt;a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html"&gt;https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#subnet"&gt;
  &lt;/a&gt;
  Subnet
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;A subnet is a range of IP addresses in your VPC. You can launch AWS resources, such as EC2 instances, into a specific subnet. When you create a subnet, you specify the IPv4 CIDR block for the subnet, which is a subset of the VPC CIDR block. Each subnet must reside entirely within one Availability Zone and cannot span zones. By launching instances in separate Availability Zones, you can protect your applications from the failure of a single zone.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Definition from AWS : &lt;a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html"&gt;https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#instances-types"&gt;
  &lt;/a&gt;
  Instances types
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Instance types comprise varying combinations of CPU, memory, storage, and networking capacity and give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Definition from AWS : &lt;a href="https://aws.amazon.com/ec2/instance-types/"&gt;https://aws.amazon.com/ec2/instance-types/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;




&lt;p&gt;I hope it will help you! üç∫&lt;/p&gt;

&lt;p&gt;And see you soon for the next part of this serie. üòÄ&lt;/p&gt;

</description>
      <category>aws</category>
      <category>sre</category>
      <category>devops</category>
      <category>terraform</category>
    </item>
    <item>
      <title>How-to setup a HA/DR database in AWS? [1]</title>
      <author>Maxime Guilbert</author>
      <pubDate>Tue, 18 Jan 2022 17:39:19 +0000</pubDate>
      <link>https://dev.to/adaendra/how-to-setup-a-hadr-database-in-aws-1-1ko7</link>
      <guid>https://dev.to/adaendra/how-to-setup-a-hadr-database-in-aws-1-1ko7</guid>
      <description>&lt;p&gt;&lt;em&gt;In every company, we store some datas. For some of them it's with databases. But how can you be sure to keep your data safely and have the best availability ?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you don't know, you are at the right place.&lt;/p&gt;




&lt;p&gt;In this mini-serie, we will see together how to setup a database, then it's High Availability &lt;em&gt;(HA)&lt;/em&gt; and then it's &lt;strong&gt;DR&lt;/strong&gt; to be sure that you and your enterprise can store your data safely, avoid lose in case of failure and be able to recover your data in case of a disaster.&lt;/p&gt;

&lt;p&gt;Some links will be given to help you to have a better understanding of the solution. And some tips too, to let you know what can be interesting to look at, or how to do some specific things.&lt;/p&gt;

&lt;p&gt;To follow the good practice, we will do it with Terraform to have all our &lt;strong&gt;infra as code&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;But before going further, here are the 2 principals definitions to understand what we will do and why.&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#whats-an-high-available-system"&gt;
  &lt;/a&gt;
  What's an High Available system?
&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;HA = High Available&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When we are talking about infrastructure, especially in the cloud, an high available system is :&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A system which can continue to work even if a server/datacenter is non available.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;An example with AWS : an available system is something deploy in multiple &lt;strong&gt;AZ&lt;/strong&gt; &lt;em&gt;(Availability Zones)&lt;/em&gt;.&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#whats-a-disaster-recovery-system"&gt;
  &lt;/a&gt;
  What's a Disaster Recovery system?
&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;DR = Disaster Recovery&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Disaster recovery is an organization‚Äôs method of regaining access and functionality to its IT infrastructure after events like a natural disaster, cyber attack, or even business disruptions related to the COVID-19 pandemic. A variety of disaster recovery (DR) methods can be part of a disaster recovery plan. DR is one aspect of business continuity.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Definition from VMWare : &lt;a href="https://www.vmware.com/topics/glossary/content/disaster-recovery.html"&gt;https://www.vmware.com/topics/glossary/content/disaster-recovery.html&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#what-is-an-infra-as-code-"&gt;
  &lt;/a&gt;
  What is an "Infra as Code" ?
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Infrastructure as Code (IaC) is the management of infrastructure (networks, virtual machines, load balancers, and connection topology) in a descriptive model, using the same versioning as DevOps team uses for source code. Like the principle that the same source code generates the same binary, an IaC model generates the same environment every time it is applied. IaC is a key DevOps practice and is used in conjunction with continuous delivery.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Definition from Microsoft : &lt;a href="https://docs.microsoft.com/en-us/devops/deliver/what-is-infrastructure-as-code"&gt;https://docs.microsoft.com/en-us/devops/deliver/what-is-infrastructure-as-code&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Explicative video from &lt;strong&gt;Techworld by Nana&lt;/strong&gt;&lt;br&gt;
&lt;iframe width="710" height="399" src="https://www.youtube.com/embed/POPP2WTJ8es"&gt;
&lt;/iframe&gt;
&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#prerequisite"&gt;
  &lt;/a&gt;
  Prerequisite
&lt;/h2&gt;

&lt;p&gt;To understand correctly what's going next, you need to know Terraform &lt;em&gt;(How does it work and how to read it)&lt;/em&gt; and to have a basis on AWS.&lt;/p&gt;

&lt;p&gt;For Terraform, please check the documentation &lt;em&gt;(&lt;a href="https://www.terraform.io/docs"&gt;https://www.terraform.io/docs&lt;/a&gt;)&lt;/em&gt; or the following video from &lt;strong&gt;Techworld by Nana&lt;/strong&gt; before.&lt;/p&gt;

&lt;p&gt;&lt;iframe width="710" height="399" src="https://www.youtube.com/embed/l5k1ai_GBDE"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;For the AWS part, I will give you some definitions and links to help you to understand but I'm not sure it will be enough to understand the whole package. So don't hesitate to read something to introduce you to AWS or the following video from &lt;strong&gt;Simplilearn&lt;/strong&gt; and if after you have some questions, don't hesitate in the comments.&lt;/p&gt;

&lt;p&gt;&lt;iframe width="710" height="399" src="https://www.youtube.com/embed/3XFODda6YXo"&gt;
&lt;/iframe&gt;
&lt;/p&gt;




&lt;h3&gt;
  &lt;a href="#links"&gt;
  &lt;/a&gt;
  Links
&lt;/h3&gt;

&lt;h4&gt;
  &lt;a href="#documentation"&gt;
  &lt;/a&gt;
  Documentation
&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;AWS : &lt;a href="https://aws.amazon.com/"&gt;https://aws.amazon.com/&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Terraform : &lt;a href="https://www.terraform.io/docs"&gt;https://www.terraform.io/docs&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;
  &lt;a href="#youtube-channels"&gt;
  &lt;/a&gt;
  Youtube channels
&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Techworld By Nana : &lt;a href="https://www.youtube.com/channel/UCdngmbVKX1Tgre699-XLlUA"&gt;https://www.youtube.com/channel/UCdngmbVKX1Tgre699-XLlUA&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Simplilearn : &lt;a href="https://www.youtube.com/channel/UCsvqVGtbbyHaMoevxPAq9Fg"&gt;https://www.youtube.com/channel/UCsvqVGtbbyHaMoevxPAq9Fg&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;




&lt;p&gt;I hope it will help you! üç∫&lt;/p&gt;

&lt;p&gt;And see you soon for the next part of this serie. üòÄ&lt;/p&gt;

</description>
      <category>aws</category>
      <category>sre</category>
      <category>devops</category>
      <category>terraform</category>
    </item>
    <item>
      <title>Configuring Git/GitHub</title>
      <author>Lupita Rivera</author>
      <pubDate>Tue, 18 Jan 2022 17:13:41 +0000</pubDate>
      <link>https://dev.to/lupitalee/configuring-gitgithub-c69</link>
      <guid>https://dev.to/lupitalee/configuring-gitgithub-c69</guid>
      <description>&lt;p&gt;This will be a blog in two parts the first one  Configuring Git/GitHub and the second Understanding more about Git/GitHub Commands which will be released soon. Lets gets get started!&lt;/p&gt;

&lt;p&gt;but first you might ask what is the difference between local repository and remote repository?&lt;/p&gt;

&lt;p&gt;The local repository is on your machine (git) and the remote repository is on the cloud (github). okay now lets get started.&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#install-git"&gt;
  &lt;/a&gt;
  Install git
&lt;/h4&gt;

&lt;p&gt;Git generally comes pre-installed with most operating systems, but you can check by running git version in the terminal. If this gives you an error or does not come back with a version number, you'll need to install Git. You can install it using Homebrew.&lt;/p&gt;

&lt;p&gt;1) Open the "Terminal" &lt;br&gt;
2) Type &lt;code&gt;brew install git&lt;/code&gt; and press &lt;code&gt;&amp;lt;Enter&amp;gt;&lt;/code&gt;&lt;br&gt;
3) Close the "Terminal"&lt;br&gt;
4) Reopen the "Terminal" &lt;br&gt;
5) Type &lt;code&gt;git --version&lt;/code&gt; and press &lt;code&gt;&amp;lt;Enter&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you see a message starting with "git version...", continue below.&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#create-a-github-account"&gt;
  &lt;/a&gt;
  Create a GitHub Account
&lt;/h4&gt;

&lt;p&gt;To work on and get credit for for your work, you will need to sign up for a GitHub account if you don‚Äôt already have one.&lt;br&gt;
&lt;a href="https://github.com/join"&gt;Github signup webpage&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#configure-git-and-github"&gt;
  &lt;/a&gt;
  Configure Git and GitHub
&lt;/h4&gt;

&lt;p&gt;Git is the tool that we‚Äôll use to download and upload the work that we do. To use Git without signing in every time, you can create a Secure Shell (SSH) key and associate that to your GitHub account. This step will ask you to do work both in your browser and your terminal.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Open the "Terminal"&lt;/li&gt;
&lt;li&gt;Type &lt;code&gt;git config --global color.ui true&lt;/code&gt; and press &lt;code&gt;&amp;lt;Enter&amp;gt;&lt;/code&gt;
&lt;/li&gt;
&lt;li&gt;Type &lt;code&gt;git config --global user.name&lt;/code&gt; + &lt;code&gt;&amp;lt;Space&amp;gt;&lt;/code&gt; + your name and press &lt;code&gt;&amp;lt;Enter&amp;gt;&lt;/code&gt; (Note: this should be your full name, not your GitHub username, in quotes.)&lt;/li&gt;
&lt;li&gt;Type &lt;code&gt;git config --global user.email&lt;/code&gt; + &lt;code&gt;&amp;lt;Space&amp;gt;&lt;/code&gt; + the email address you used to sign up to GitHub and press &lt;code&gt;&amp;lt;Enter&amp;gt;&lt;/code&gt;
&lt;/li&gt;
&lt;li&gt;Type &lt;code&gt;ssh-keygen&lt;/code&gt; and press &lt;code&gt;&amp;lt;Enter&amp;gt;&lt;/code&gt;
&lt;/li&gt;
&lt;li&gt;For each prompt do not type anything, just continue to press &lt;code&gt;&amp;lt;Enter&amp;gt;&lt;/code&gt;
&lt;/li&gt;
&lt;li&gt;Type &lt;code&gt;cat ~/.ssh/id_rsa.pub | pbcopy&lt;/code&gt; and press &lt;code&gt;&amp;lt;Enter&amp;gt;&lt;/code&gt;. This will copy your SSH key to your clipboard&lt;/li&gt;
&lt;li&gt;Open the &lt;a href="https://github.com/settings/ssh/new"&gt;GitHub New SSH key form&lt;/a&gt;(Note: you need to be logged in to GitHub to access that link.)&lt;/li&gt;
&lt;li&gt;Type "My personal Mac" in the "Title" input field&lt;/li&gt;
&lt;li&gt;Paste what‚Äôs on your clipboard from step seven in the "Key" input field&lt;/li&gt;
&lt;li&gt;Click "Add SSH Key"&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hopefully this post will help some of you get started with Git/Github.&lt;/p&gt;

</description>
      <category>tutorial</category>
      <category>github</category>
      <category>git</category>
      <category>gettingstarted</category>
    </item>
    <item>
      <title>Solving Four Kubernetes Networking Challenges</title>
      <author>Michael Bogan</author>
      <pubDate>Tue, 18 Jan 2022 16:56:50 +0000</pubDate>
      <link>https://dev.to/mbogan/solving-four-kubernetes-networking-challenges-49lp</link>
      <guid>https://dev.to/mbogan/solving-four-kubernetes-networking-challenges-49lp</guid>
      <description>&lt;p&gt;One of the main responsibilities of &lt;a href="https://konghq.com/learning-center/kubernetes/what-is-kubernetes/?utm_source=guest&amp;amp;utm_medium=devspotlight&amp;amp;utm_campaign=community"&gt;Kubernetes&lt;/a&gt; is to share nodes between applications. Networking is a fundamental requirement since those applications need to communicate with one another and the outside world.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--ZKvvnN0g--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/olvmy8t3x5xbxw7l96cl.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--ZKvvnN0g--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/olvmy8t3x5xbxw7l96cl.png" alt="A Kubernetes-hosted Distributed Application Architecture" width="603" height="402"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Requests from outside a Kubernetes cluster usually go through a router or &lt;a href="https://konghq.com/learning-center/api-gateway/?utm_source=guest&amp;amp;utm_medium=devspotlight&amp;amp;utm_campaign=community"&gt;API gateway&lt;/a&gt; responsible for proxying them to the appropriate services. The responsibility of Kubernetes networking is to provide the underlying communication layer, enabling requests to reach their intended destinations.&lt;/p&gt;

&lt;p&gt;Distributed applications spread across many nodes. When there are multiple replicas of each application, Kubernetes handles &lt;a href="https://thenewstack.io/how-does-service-discovery-work-in-kubernetes/#:~:text=The%20services%20model%20in%20Kubernetes,%2C%20aspect%20of%20microservices%3A%20discovery.&amp;amp;text=Kubernetes%20refers%20to%20these%20key,with%20a%20set%20of%20pods."&gt;service discovery&lt;/a&gt; and communication between the &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/"&gt;service&lt;/a&gt; and Pods. Inside a Pod, containers can communicate easily and transparently. Inside a cluster, Pods can connect to other Pods, made possible through a combination of virtual network interfaces, bridges, and routing rules through an &lt;a href="https://en.wikipedia.org/wiki/Overlay_network"&gt;overlay network&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Despite the transparent handling, however, Kubernetes networking is more complex than it seems. &lt;strong&gt;Deploying across multiple clouds, maintaining multiple environments, and ensuring reliable and scalable network policies are significant challenges.&lt;/strong&gt; Not all of these complexities are natively addressed by Kubernetes. In this article, we‚Äôll look at how to tackle those challenges.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#the-basics-of-kubernetes-networking"&gt;
  &lt;/a&gt;
  The Basics of Kubernetes Networking
&lt;/h2&gt;

&lt;p&gt;In Kubernetes, &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/"&gt;Pods&lt;/a&gt; are responsible for handling container-to-container communication. Pods leverage network namespaces with their own network resources (interfaces and routing tables). Inside a Pod, containers share these resources, allowing them to communicate via localhost.&lt;/p&gt;

&lt;p&gt;Pod-to-Pod communication must meet the following Kubernetes requirements:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pods need to communicate without &lt;a href="https://en.wikipedia.org/wiki/Network_address_translation"&gt;network address translation (NAT)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Nodes need to be able to communicate with Pods without NAT.&lt;/li&gt;
&lt;li&gt;The IP address that a Pod can see assigned to itself must match the IP that other Pods see.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;a href="https://github.com/containernetworking/cni"&gt;Container Network Interface (CNI)&lt;/a&gt; includes a specification for writing network plugins to configure network interfaces. This allows you to create &lt;a href="https://en.wikipedia.org/wiki/Overlay_network"&gt;overlay networks&lt;/a&gt; that satisfy Pod-to-Pod communication requirements.&lt;/p&gt;

&lt;p&gt;A &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/"&gt;service&lt;/a&gt; is a Kubernetes abstraction that allows Pods to expose and receive requests. It provides a service discovery mechanism through Pod labels and basic load balancing capabilities. Applications running inside Pods can easily use services to connect to other applications running in the cluster. Requests from outside the cluster can be routed through &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/"&gt;Ingress controllers&lt;/a&gt;. These controllers will use &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/"&gt;Ingress&lt;/a&gt; resources to configure routing rules, usually leveraging &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/"&gt;services&lt;/a&gt; to facilitate routing to the correct applications.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#nontrivial-challenges"&gt;
  &lt;/a&gt;
  Non-Trivial Challenges
&lt;/h2&gt;

&lt;p&gt;While these networking capabilities provide the foundational building blocks for Kubernetes managed workloads, the dynamic and complex nature of &lt;a href="https://konghq.com/blog/cloud-native-infrastructure/?utm_source=guest&amp;amp;utm_medium=devspotlight&amp;amp;utm_campaign=community"&gt;cloud native&lt;/a&gt; systems presents several challenges.&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#servicetoservice-communication-reliability"&gt;
  &lt;/a&gt;
  Service-to-Service Communication Reliability
&lt;/h4&gt;

&lt;p&gt;In distributed systems, business functions are divided into multiple, autonomous services running over a cluster of nodes, Pods, and containers. A microservice architecture introduces the need for services to communicate over the network.&lt;/p&gt;

&lt;p&gt;The volatility and elastic nature of the cloud require constant monitoring of the Kubernetes cluster and rerouting in case of failures. With ephemeral Pods and continual rerouting of resources, reliable service-to-service communication is not a given.&lt;/p&gt;

&lt;p&gt;Efficient load balancing algorithms need to assign traffic to available replicas and isolate the overloaded ones. Similarly, service failure means client requests need to be retried and timed out gracefully. Complex scenarios might need &lt;a href="https://en.wikipedia.org/wiki/Circuit_breaker"&gt;circuit breakers&lt;/a&gt; and load-shedding techniques to handle surges in demand and failures. &lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#elaborate-multicloud-deployments"&gt;
  &lt;/a&gt;
  Elaborate Multi-Cloud Deployments
&lt;/h4&gt;

&lt;p&gt;Complex, large-scale systems are often divided into multiple environments, with different parts deployed to different cloud platforms. These heterogeneous environments need to communicate with one another.&lt;/p&gt;

&lt;p&gt;Even within the same cloud tenancy‚Äîor on-premise‚Äîthe same workload can run in different environments (development, staging, and production). Though separated, these environments sometimes need to communicate with one another. For example, a staging environment may need to emulate the production workload and rigorously test the application before it goes live. With successful testing, both code and data may need to migrate from it. &lt;/p&gt;

&lt;p&gt;A seamless migration can be challenging in such cases. Also, there may be cases where a team simultaneously supports both VM and Kubernetes-hosted services. Or, perhaps a team designs systems that support multi-cloud‚Äîor at least multi-region‚Äîdeployments for reliability, specifying the complex network configurations and elaborate ingress and egress rules.&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#service-discovery"&gt;
  &lt;/a&gt;
  Service Discovery
&lt;/h4&gt;

&lt;p&gt;When running Kubernetes in cloud-native environments, it‚Äôs easy to scale services by spawning several replicas across multiple nodes. These application replicas are ephemeral‚Äîinstantiated and destroyed as Kubernetes deems necessary. It‚Äôs non-trivial for &lt;a href="https://konghq.com/learning-center/microservices/"&gt;microservices&lt;/a&gt; in the application to keep track of all these changes to IP addresses and ports. Nonetheless, these microservices need an efficient way to find service replicas. &lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#network-rules-scalability"&gt;
  &lt;/a&gt;
  Network Rules Scalability
&lt;/h4&gt;

&lt;p&gt;Security best practices and industry regulations like the &lt;a href="https://en.wikipedia.org/wiki/Payment_Card_Industry_Data_Security_Standard"&gt;Payment Card Industry Data Security Standard (PCI DSS)&lt;/a&gt; enforce strict networking rules. These rules dictate strict communication constraints between services.&lt;/p&gt;

&lt;p&gt;Kubernetes has the concept of &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/"&gt;Network Policies&lt;/a&gt;. These allow you to control traffic at the IP address or port level. You can specify the rules that would enable a Pod to communicate with other services using labels and selectors.&lt;/p&gt;

&lt;p&gt;As your system of microservices grows in number, reaching hundreds or thousands of services, network policy management becomes a complex, tedious, and error-prone process. &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#how-kong-ingress-controller-can-help"&gt;
  &lt;/a&gt;
  How Kong Ingress Controller Can Help
&lt;/h3&gt;

&lt;p&gt;The &lt;a href="https://konghq.com/solutions/kubernetes-ingress/?utm_source=guest&amp;amp;utm_medium=devspotlight&amp;amp;utm_campaign=community"&gt;Kubernetes Ingress Controller (KIC)&lt;/a&gt; from Kong is an &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/"&gt;Ingress&lt;/a&gt; implementation for Kubernetes. This Ingress controller, powered by &lt;a href="https://konghq.com/kong/?utm_source=guest&amp;amp;utm_medium=devspotlight&amp;amp;utm_campaign=community"&gt;Kong Gateway&lt;/a&gt;, acts as a cloud native, platform-agnostic, scalable API gateway. It‚Äôs built for hybrid and multi-cloud environments and optimized for microservices and distributed architectures.&lt;/p&gt;

&lt;p&gt;The KIC allows for creating a configuration of routing rules, health checks, and load balancing, and it supports a variety of &lt;a href="https://docs.konghq.com/hub/"&gt;plugins&lt;/a&gt; that provide advanced functionality. This wide range of capabilities can help address the challenges we‚Äôve discussed.&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#reliable-servicetoservice-communication"&gt;
  &lt;/a&gt;
  Reliable Service-to-Service Communication
&lt;/h4&gt;

&lt;p&gt;Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/"&gt;services&lt;/a&gt; provide simple load balancing capabilities (Round Robin). One of the core features of KIC is to load balance between replicas of the same application. It can use algorithms like weighted connections or least connections, or even sophisticated, custom implementations. These algorithms leverage the service registry of KIC to provide efficient routing. &lt;/p&gt;

&lt;p&gt;With KIC, you can easily configure retries when a service is down, sensible timeouts, rerouting requests to healthy service instances, or error handling. You can also implement failure patterns such as circuit breaking and load-shedding to smooth and throttle traffic.&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#simpler-multicloud-environments-deployments"&gt;
  &lt;/a&gt;
  Simpler Multi-Cloud Environments Deployments
&lt;/h4&gt;

&lt;p&gt;Multi-environment and heterogeneous infrastructure deployments demand complex network policies and routing configurations. &lt;a href="https://docs.konghq.com/gateway/"&gt;Kong Gateway&lt;/a&gt;, built into KIC, addresses many of these challenges. &lt;/p&gt;

&lt;p&gt;Kong Gateway allows &lt;a href="https://docs.konghq.com/gateway/2.6.x/get-started/quickstart/configuring-a-service/#main"&gt;service registration&lt;/a&gt; independent of where services are deployed. With a registered service, you‚Äôll be able to &lt;a href="https://docs.konghq.com/gateway/2.6.x/get-started/quickstart/configuring-a-service/#2-add-a-route-for-the-service"&gt;add routes&lt;/a&gt;, and KIC will be ready to proxy requests to your service. Additionally, while complex systems can sometimes communicate with different protocols (REST versus gRPC), you can easily configure KIC to support multiple protocols.&lt;/p&gt;

&lt;p&gt;The &lt;a href="https://docs.konghq.com/hub/plugins/overview/"&gt;plugin system&lt;/a&gt; allows you to extend the functionality of KIC for more complex scenarios. &lt;a href="https://docs.konghq.com/hub/"&gt;Kong Plugin Hub&lt;/a&gt; contains a strong collection of useful and battle-tested plugins, and KIC also enables &lt;a href="https://docs.konghq.com/gateway/2.6.x/plugin-development/"&gt;you to develop&lt;/a&gt; and use whatever plugin best suits your needs.&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#enhanced-service-discovery"&gt;
  &lt;/a&gt;
  Enhanced Service Discovery
&lt;/h4&gt;

&lt;p&gt;As mentioned, KIC tracks available instances through its registry of services. As services integrate with KIC, they can self-register and report their availability. This registration can also be done through third-party registration services. By taking advantage of the service registry, KIC can proxy client requests to the proper backends at any time.&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#scalable-network-rules"&gt;
  &lt;/a&gt;
  Scalable Network Rules
&lt;/h4&gt;

&lt;p&gt;Although enforcing network rules through &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/"&gt;Network Policies&lt;/a&gt; can be complicated, KIC can &lt;a href="https://docs.konghq.com/kubernetes-ingress-controller/2.0.x/guides/getting-started-istio/#main"&gt;easily integrate&lt;/a&gt; with &lt;a href="https://konghq.com/learning-center/service-mesh/what-is-a-service-mesh/?utm_source=guest&amp;amp;utm_medium=devspotlight&amp;amp;utm_campaign=community"&gt;service mesh&lt;/a&gt; implementations like the CNCF‚Äôs &lt;a href="https://kuma.io/"&gt;Kuma&lt;/a&gt; or &lt;a href="https://istio.io/"&gt;Istio&lt;/a&gt; with &lt;a href="https://konghq.com/blog/istio-gateway/?utm_source=guest&amp;amp;utm_medium=devspotlight&amp;amp;utm_campaign=community"&gt;Kong Istio Gateway&lt;/a&gt;, extending the capabilities of &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/"&gt;Network Policies&lt;/a&gt; and guaranteeing additional security. &lt;/p&gt;

&lt;p&gt;With &lt;a href="https://kuma.io/docs/1.4.1/security/certificates/"&gt;authentication and authorization&lt;/a&gt; policies, you‚Äôll be able to enhance network security in a secure, consistent, and automated way. Moreover, you can use &lt;a href="https://kuma.io/docs/1.4.1/policies/general-notes-about-kuma-policies/"&gt;network policies and service mesh policies together&lt;/a&gt; to provide an even better security posture.&lt;/p&gt;

&lt;p&gt;An added benefit of the service mesh integration is that it allows for deployment patterns like canary deployments and blue/green deployments. It also enhances observability with reliable metrics and traces. &lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#conclusion"&gt;
  &lt;/a&gt;
  Conclusion
&lt;/h3&gt;

&lt;p&gt;Kubernetes can handle common networking tasks, making it easier for developers and operators to onboard services. However, with large and complex cloud-native systems, networking concerns are rarely simple. Organizations want to split &lt;a href="https://konghq.com/learning-center/microservices/monolith-vs-microservices/?utm_source=guest&amp;amp;utm_medium=devspotlight&amp;amp;utm_campaign=community"&gt;monoliths&lt;/a&gt; into microservices, but they need to address unique concerns such as efficient load balancing or fault tolerance. Similarly, enabling seamless service migrations and transitions between different environments is not easy. Kubernetes networking capabilities need to be extended to support a wider range of scenarios.&lt;/p&gt;

&lt;p&gt;KIC can efficiently tackle many of these challenges. It offers a broad range of functionality, including advanced routing and load balancing rules, complex ingress and egress rules, and fault tolerance measures. You can greatly improve service discovery with the service registry of KIC, which can track all available instances of each service. The easy integration with KIC and service meshes can help establish strong network security policies and leverage different deployment patterns.&lt;/p&gt;

</description>
      <category>kubernetes</category>
      <category>devops</category>
      <category>webdev</category>
    </item>
    <item>
      <title>Why Load Balancing Is Important</title>
      <author>Ifihan Olusheye</author>
      <pubDate>Tue, 18 Jan 2022 16:52:50 +0000</pubDate>
      <link>https://dev.to/ifihan/why-load-balancing-is-important-50kn</link>
      <guid>https://dev.to/ifihan/why-load-balancing-is-important-50kn</guid>
      <description>&lt;h3&gt;
  &lt;a href="#introduction"&gt;
  &lt;/a&gt;
  INTRODUCTION
&lt;/h3&gt;

&lt;p&gt;Modern high-traffic websites must handle hundreds of thousands, if not millions of the concurrent user or client requests when returning accurate text, images, video, or device data in a timely and consistent manner. As a result of this amount of requests, such websites might lag and eventually crash. The use of load balancers can prevent such events from occurring.&lt;/p&gt;

&lt;p&gt;A load balancer is a system that distributes network or application traffic through several servers by acting as a reverse proxy. With a load balancer,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;User requests or network load are distributed through many servers in an optimal manner.&lt;/li&gt;
&lt;li&gt;Allows you to add or remove servers as needed, depending on demand.&lt;/li&gt;
&lt;li&gt;Sends requests to only online repositories, ensuring high availability and reliability.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;
  &lt;a href="#so-what-is-load-balancing"&gt;
  &lt;/a&gt;
  So, what is Load balancing?
&lt;/h4&gt;

&lt;p&gt;Load balancing is the process of evenly spreading incoming network traffic among a collection of backend servers, often referred to as a server farm or server pool. As the name implies, it is a process of balancing the load on a server, and load in this context refers to requests.&lt;/p&gt;

&lt;p&gt;Some several techniques or algorithms can be utilized depending on their use. Some of them are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Least connection: This approach takes the actual server load into account. The latest request is forwarded to the site that has the fewest pending requests at any given time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hash: Requests are distributed based on a key you choose, such as the client's IP address or the file URL.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IP Hash: The client's IP address is used to decide which server will receive the message.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;
  &lt;a href="#why-is-load-balancing-important"&gt;
  &lt;/a&gt;
  Why is load balancing important?
&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;Load balancing improves service availability and helps prevent downtimes. &lt;/li&gt;
&lt;li&gt;To maximize the use of resources and reduce the response time.&lt;/li&gt;
&lt;li&gt;A load balancer prevents the entire network from failing if a server from a server cluster fails.&lt;/li&gt;
&lt;li&gt;A load balancer helps to manage traffic flow on a website.&lt;/li&gt;
&lt;li&gt;Add flexibility to the network. For example, a site can be upgraded without downtime while using load balancers.&lt;/li&gt;
&lt;li&gt;It improves scalability.&lt;/li&gt;
&lt;li&gt;Load balancers improve the capacity of existing servers.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In conclusion, it is good practice to use and advisable to use load balancers if you plan on having a high-traffic website or application.&lt;/p&gt;

</description>
      <category>devops</category>
      <category>programming</category>
      <category>backend</category>
    </item>
  </channel>
</rss>
