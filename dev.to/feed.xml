<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>DEV Community</title>
    <author>DEV Community</author>
    <description>A constructive and inclusive social network for software developers. With you every step of your journey.</description>
    <link>https://dev.to</link>
    <language>en</language>
    <item>
      <title>How to fix "Property 'ethereum' does not exist on type 'Window &amp; typeof globalThis'.ts(2339)"</title>
      <author>Nmurgor</author>
      <pubDate>Sat, 18 Dec 2021 14:50:31 +0000</pubDate>
      <link>https://dev.to/nmurgor/how-to-fix-property-ethereum-does-not-exist-on-type-window-typeof-globalthists2339-42dh</link>
      <guid>https://dev.to/nmurgor/how-to-fix-property-ethereum-does-not-exist-on-type-window-typeof-globalthists2339-42dh</guid>
      <description>&lt;h2&gt;
  &lt;a href="#introduction"&gt;
  &lt;/a&gt;
  Introduction
&lt;/h2&gt;

&lt;p&gt;When working with MetaMask APIs, you might encounter this warning: "Property 'ethereum' does not exist on type 'Window &amp;amp; typeof globalThis'.ts(2339)"  when accessing &lt;code&gt;window.ethereum&lt;/code&gt; in a TypeScript project. MetaMask injects a global API into websites visited by its users at &lt;code&gt;window.ethereum&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--_rl5z7lp--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/http://naftalimurgor.netflify.app/static/img/error-ethereum-does-not-exist.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--_rl5z7lp--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/http://naftalimurgor.netflify.app/static/img/error-ethereum-does-not-exist.png" alt="a code snippet screenshot" width="" height=""&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To fix this warning, and corresponding compile error, add this declaration at the top level of your file, after imports.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight typescript"&gt;&lt;code&gt;&lt;span class="kr"&gt;declare&lt;/span&gt; &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nb"&gt;window&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kr"&gt;any&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Note, this is not the best fix as this temporarily fixes the warning by telling the &lt;code&gt;TypeScript&lt;/code&gt; compiler to treat &lt;code&gt;window&lt;/code&gt; as of type &lt;code&gt;any&lt;/code&gt; hence ignore any warnings. &lt;/p&gt;

&lt;p&gt;This sacrifices also any  IntelliSense and auto-completion provided by code editor on the &lt;code&gt;window&lt;/code&gt; object.&lt;/p&gt;




&lt;p&gt;This article was originally published at &lt;a href="https://naftalimurgor.netlify.com"&gt;https://naftalimurgor.netlify.com&lt;/a&gt;&lt;/p&gt;

</description>
      <category>javascript</category>
      <category>tutorial</category>
      <category>web3</category>
    </item>
    <item>
      <title>How to send e-mail using Python</title>
      <author>Raghav Mrituanjaya</author>
      <pubDate>Sat, 18 Dec 2021 14:31:54 +0000</pubDate>
      <link>https://dev.to/raghavmri/how-to-send-e-mail-using-python-kmk</link>
      <guid>https://dev.to/raghavmri/how-to-send-e-mail-using-python-kmk</guid>
      <description>&lt;p&gt;&lt;iframe width="710" height="399" src="https://www.youtube.com/embed/1UrV0edj1TI"&gt;
&lt;/iframe&gt;
&lt;br&gt;
So, Have you guys ever wondered how do companies like Amazon, Walmart, eBay send emails to your inbox immediately after your purchase. Do you think it's an email sent by an human. I know most of you must have guessed it's an Automated E-mail but how do they do that? &lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--EFiLK-8F--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/0xkgw54r81a2w13x5mct.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--EFiLK-8F--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/0xkgw54r81a2w13x5mct.gif" alt="typing-fast" width="452" height="498"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In order to solve the question that was raised we are gonna be trying to send a email using simple Python Code. Feel free to customize the code according to your needs.&lt;/p&gt;
&lt;h3&gt;
  &lt;a href="#a-simple-program"&gt;
  &lt;/a&gt;
  A Simple Program
&lt;/h3&gt;


&lt;div class="ltag_gist-liquid-tag"&gt;
  
&lt;/div&gt;


&lt;h3&gt;
  &lt;a href="#points-to-note"&gt;
  &lt;/a&gt;
  Points To Note
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Do let me know where I can improve in writing posts in the comment section below as I am too learning new things :)&lt;/li&gt;
&lt;li&gt;Thanks For Reading ♥!&lt;/li&gt;
&lt;/ul&gt;

</description>
      <category>python</category>
      <category>smtp</category>
      <category>webdev</category>
      <category>tutorial</category>
    </item>
    <item>
      <title>Ruby 3 Mindset, what's new ?</title>
      <author>Shayan Holakouee</author>
      <pubDate>Sat, 18 Dec 2021 14:17:09 +0000</pubDate>
      <link>https://dev.to/shayanholakouee/ruby-3-mindset-whats-new--1kh9</link>
      <guid>https://dev.to/shayanholakouee/ruby-3-mindset-whats-new--1kh9</guid>
      <description>&lt;p&gt;The number 3 is very significant in the Ruby 3 release. Be it release version number, making performance 3x faster, or the trio of core contributors(Matz, TenderLove, Koichi). &lt;/p&gt;

&lt;p&gt;Similarly, there were 3 major goals of Ruby 3: being faster, having better concurrency, and ensuring correctness.&lt;/p&gt;

&lt;p&gt;Ruby 3’s main selling point is that it’s 3 times faster than Ruby 2. Matz and the team have been trying to bring in a lot of features in the latest version of Ruby, but their foremost priority has been to ensure that performance is improved drastically, and that backward compatibility is maintained. &lt;/p&gt;

&lt;p&gt;There are a whole bunch of new additions like type checking, ractor, and scheduler, along with improvements to the currently existing paradigms, like performance, fibers, memory, concurrency, and static analysis. We’ll take a deeper look at each one of them one-by-one.&lt;br&gt;
One of the most promising updates for Ruby CPU performance is the new Ruby JIT.&lt;/p&gt;

&lt;p&gt;Another new feature that might significantly improve the programming language’s performance is the Medium Internal Representation JIT compiler, which is lightweight and convenient to work with.&lt;/p&gt;

&lt;p&gt;We hope that this Ruby updates will actually be included in Ruby 3.0 since it might solve Ruby’s issues and result in significant performance improvement beyond the current JIT.&lt;/p&gt;

&lt;p&gt;The point of a new big number is that it breaks stuff from previous releases. &lt;/p&gt;

&lt;p&gt;One final note: In order to manage projects developed on different versions of Ruby, you’ll want a version manager to allow you to easily switch back and forth. I’ve used rbenv; I’m also aware of chruby and RVM that are frequently recommended and accomplish the same thing.&lt;/p&gt;

</description>
      <category>ruby</category>
      <category>rails</category>
      <category>webdev</category>
    </item>
    <item>
      <title>Storage Best Practices for Data and Analytics Applications | AWS White Paper Summary</title>
      <author>Adit Modi</author>
      <pubDate>Sat, 18 Dec 2021 14:10:14 +0000</pubDate>
      <link>https://dev.to/awsmenacommunity/storage-best-practices-for-data-and-analytics-applications-aws-white-paper-summary-3cmd</link>
      <guid>https://dev.to/awsmenacommunity/storage-best-practices-for-data-and-analytics-applications-aws-white-paper-summary-3cmd</guid>
      <description>&lt;h1&gt;
  &lt;a href="#introduction"&gt;
  &lt;/a&gt;
  Introduction
&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Amazon Simple Storage Service (Amazon S3) and Amazon Simple Storage Service Glacier (Amazon S3 Glacier) provide ideal storage solutions for data lakes. &lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data lakes, powered by Amazon S3, provide you with unmatched availability, agility, and flexibility required to combine different types of data and analytics approaches to gain deeper insights, in ways that traditional data silos and data warehouses cannot. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In addition, data lakes built on Amazon S3 integrate with other analytical services for ingestion, inventory, transformation, and security of your data in the data lake. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This guide explains each of these options and provides best practices for building, securing, managing, and scaling a data lake built on Amazon S3.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Because organizations are collecting and analyzing increasing amounts of data, traditional on-premises solutions for data storage, data management, and analytics can no longer keep pace. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data silos that aren’t built to work well together make it difficult to consolidate the storage for more comprehensive and efficient analytics. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This, in turn, limits an organization’s agility and ability to derive more insights and value from its data. In addition, this reduces the capability to seamlessly adopt more sophisticated analytics tools and processes, because it necessitates upskilling of the workforce.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A data lake is an architectural approach that allows you to store all your data in a centralized repository, so that it can be categorized, catalogued, secured, and analyzed by a diverse set of users and tools. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In a data lake you can ingest and store structured, semi-structured, and unstructured data, and transform these raw data assets as needed. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using a cloud-based data lake you can easily decouple the compute from storage, and scale each component independently, which is a huge advantage over an on-premises or Hadoop-based data lake. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can use a complete portfolio of data exploration, analytics, machine learning, reporting, and visualization tools on the data. A data lake makes data and the optimal analytics tools available to more users, across more lines of business, enabling them to get all of the business insights they need, whenever they need them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;More organizations are building data lakes for various use cases. To guide customers in their journey, Amazon Web Services (AWS) has developed a data lake architecture that allows you to build scalable, secure data lake solutions cost-effectively using Amazon S3 and other AWS services.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using the data lake built on Amazon S3 architecture capabilities you can:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ingest and store data from a wide variety of sources into a centralized platform.&lt;/li&gt;
&lt;li&gt;Build a comprehensive data catalog to find and use data assets stored in the data lake.&lt;/li&gt;
&lt;li&gt;Secure, protect, and manage all of the data stored in the data lake.&lt;/li&gt;
&lt;li&gt;Use tools and policies to monitor, analyze, and optimize infrastructure and data.&lt;/li&gt;
&lt;li&gt;Transform raw data assets in place into optimized usable formats.&lt;/li&gt;
&lt;li&gt;Query data assets in place.&lt;/li&gt;
&lt;li&gt;Integrate the unstructured data assets from Amazon S3 with structured data assets in a data warehouse solution to gather valuable business insights.&lt;/li&gt;
&lt;li&gt;Store the data assets into separate buckets as the data goes through extraction, transformation, and load process.&lt;/li&gt;
&lt;li&gt;Use a broad and deep portfolio of data analytics, data science, machine learning, and visualization tools.&lt;/li&gt;
&lt;li&gt;Quickly integrate current and future third-party data-processing tools.&lt;/li&gt;
&lt;li&gt;Securely share processed datasets and results.&lt;/li&gt;
&lt;li&gt;Scale virtually to unlimited capacity.&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The remainder of this paper provides more information about each of these capabilities. The following figure illustrates a sample AWS data lake platform.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--xAgUfusx--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ffk2d7awot0jm2l1rcxk.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--xAgUfusx--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ffk2d7awot0jm2l1rcxk.png" alt="Image description" width="880" height="453"&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;High-level AWS data lake technical reference architecture&lt;/em&gt; &lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#central-storage-amazon-s3-as-the-data-lake-storage-platform"&gt;
  &lt;/a&gt;
  Central storage: Amazon S3 as the data lake storage platform
&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;A data lake built on AWS uses Amazon S3 as its primary storage platform. Amazon S3 provides an optimal foundation for a data lake because of its virtually unlimited scalability and high durability. &lt;/li&gt;
&lt;li&gt;You can seamlessly and non-disruptively increase storage from gigabytes to petabytes of content, paying only for what you use. Amazon S3 is designed to provide 99.999999999% durability. &lt;/li&gt;
&lt;li&gt;&lt;p&gt;It has scalable performance, ease-of-use features, native encryption, and access control capabilities. Amazon S3 integrates with a broad portfolio of AWS and third-party ISV tools for data ingestion, data processing, and data security.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Key data lake-enabling features of Amazon S3 include the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;Decoupling of storage from compute and data processing&lt;/strong&gt; – In traditional Hadoop and data warehouse solutions, storage and compute are tightly coupled, making it difficult to optimize costs and data processing workflows. With Amazon S3, you can cost-effectively store all data types in their native formats. You can then launch as many or as few virtual servers as you need using Amazon Elastic Compute Cloud (Amazon EC2) to run analytical tools, and use services in AWS analytics portfolio, such as Amazon Athena, AWS Lambda, Amazon EMR, and Amazon QuickSight, to process your data.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Centralized data architecture&lt;/strong&gt; – Amazon S3 makes it easy to build a multi-tenant environment, where multiple users can run different analytical tools against the same copy of the data. This improves both cost and data governance over that of traditional solutions, which require multiple copies of data to be distributed across multiple processing platforms.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;S3 Cross-Region Replication:&lt;/strong&gt; – You can use Cross-Region Replication to copy your objects across S3 buckets within the same account or even with a different account. Cross-Region Replication is particularly useful in meeting compliance requirements, minimizing latency by storing the objects closer to the user location, and improving operational efficiency.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Integration with clusterless and serverless AWS services&lt;/strong&gt; – You can use Amazon S3 with Athena, Amazon Redshift Spectrum, and AWS Glue to query and process data. Amazon S3 also integrates with AWS Lambda serverless computing to run code without provisioning or managing servers. You can process event notifications from S3 through AWS Lambda, such as when an object is created or deleted from a bucket. With all of these capabilities, you only pay for the actual amounts of data you process or for the compute time that you consume. For machine learning use cases, you need to store the model training data and the model artifacts generated during model training. Amazon SageMaker integrates seamlessly with Amazon S3, so you can store the model training data and model artifacts on a single or different S3 bucket.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Standardized APIs&lt;/strong&gt; – Amazon S3 RESTful application programming interfaces (APIs) are simple, easy to use, and supported by most major third-party independent software vendors (ISVs), including leading Apache Hadoop and analytics tool vendors. This allows customers to bring the tools they are most comfortable with and knowledgeable about to help them perform analytics on data in Amazon S3.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Secure by default&lt;/strong&gt; – Amazon S3 is secure by default. Amazon S3 supports user authentication to control access to data. It provides access control mechanisms such as bucket policies and access-control lists to provide fine-grained access to data stored in S3 buckets to specific users and groups of users. You can also manage the access to shared data within Amazon S3 using S3 Access Points. More details about S3 Access Points are included in the Securing, protecting, and managing data section. You can also securely access data stored in S3 through SSL endpoints using HTTPS protocol. An additional layer of security can be implemented by encrypting the data-in-transit and data-at-rest using server-side encryption (SSE).&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Amazon S3 for storage of raw and iterative data sets&lt;/strong&gt; – When working with a data lake, the data undergoes various transformations. With extract, transform, load (ETL) processes and analytical operations, various versions of the same data sets are created or required for advanced processing. You can create different layers for storing the data based on the stage of the pipeline, such as raw, transformed, curated, and logs. Within these layers you can also create additional tiers based on the sensitivity of the data.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Storage classes for cost savings, durability, and availability&lt;/strong&gt; – Amazon S3 provides a range of storage classes for various use cases.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;S3 Standard&lt;/strong&gt; – General purpose storage for frequently accessed data.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;S3 Standard Infrequent Access (S3 Standard-IA) and S3 One Zone Infrequent Access (S3 One Zone – IA)&lt;/strong&gt; – Infrequently accessed, long lived data.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;S3 Glacier and S3 Glacier Deep Archive&lt;/strong&gt; – Long-term archival of data.
Using S3 Lifecycle policy, you can move the data across different storage classes for compliance and cost optimization.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Scalability and support for structured, semi-structured, and unstructured data&lt;/strong&gt; – Amazon S3 is a petabyte scale object store which provides virtually unlimited scalability to store any type of data. You can store structured data (such as relational data), semi-structured data (such as JSON, XML, and CSV files), and unstructured data (such as images or media files). This feature makes Amazon S3 the appropriate storage solution for your cloud data lake.&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;
  &lt;a href="#data-ingestion-methods"&gt;
  &lt;/a&gt;
  Data ingestion methods
&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A core capability of a data lake architecture is the ability to quickly and easily ingest multiple types of data:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Real-time streaming data and bulk data assets, from on-premises storage platforms.&lt;/li&gt;
&lt;li&gt;Structured data generated and processed by legacy on-premises platforms - mainframes and data warehouses.&lt;/li&gt;
&lt;li&gt;Unstructured and semi-structured data – images, text files, audio and video, and graphs).&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;li&gt;&lt;p&gt;AWS provides services and capabilities to ingest different types of data into your data lake built on Amazon S3 depending on your use case. This section provides an overview of various ingestion services.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#amazon-kinesis-data-firehose"&gt;
  &lt;/a&gt;
  Amazon Kinesis Data Firehose
&lt;/h2&gt;

&lt;p&gt;Amazon Kinesis Data Firehose is part of the Kinesis family of services that makes it easy to collect, process, and analyze real-time streaming data at any scale. Kinesis Data Firehose is a fully managed service for delivering real-time streaming data directly to data lakes (Amazon S3), data stores, and analytical services for further processing. Kinesis Data Firehose automatically scales to match the volume and throughput of streaming data, and requires no ongoing administration. Kinesis Data Firehose can also be configured to transform streaming data before it’s stored in a data lake built on Amazon S3. Its transformation capabilities include compression, encryption, data batching, and Lambda functions. Kinesis Data Firehose integrates with Amazon Kinesis Data Streams and Amazon Managed Streaming for Apache Kafka to deliver the streaming data into destinations, such as Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and third-party solutions such as Splunk.&lt;/p&gt;

&lt;p&gt;Kinesis Data Firehose can convert your input JSON data to Apache Parquet and Apache ORC before storing the data into your data lake built on Amazon S3. Parquet and Orc being columnar data formats, help save space and allow faster queries on the stored data compared to row-based formats such as JSON. Kinesis Data Firehose can compress data before it’s stored in Amazon S3. It currently supports GZIP, ZIP, and SNAPPY compression formats. GZIP is the preferred format because it can be used by Amazon Athena, Amazon EMR, and Amazon Redshift.&lt;/p&gt;

&lt;p&gt;Kinesis Data Firehose also allows you to invoke Lambda functions to perform transformations on the input data. Using Lambda blueprints, you can transform the input comma-separated values (CSV), structured text, such as Apache Log and Syslog formats, into JSON first. You can optionally store the source data to another S3 bucket. The following figure illustrates the data flow between Kinesis Data Firehose and different destinations.&lt;/p&gt;

&lt;p&gt;Kinesis Data Firehose also provides the ability to group and partition the target files using custom prefixes such as dates for S3 objects. This facilitates faster querying by the use of the partitioning and incremental processing further with the same feature.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--l19Yd49H--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8kved0are5bnt9m9prmm.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--l19Yd49H--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8kved0are5bnt9m9prmm.png" alt="Image description" width="880" height="534"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Delivering real-time streaming data with Kinesis Data Firehose to different destinations with optional backup&lt;/p&gt;

&lt;p&gt;Kinesis Data Firehose also natively integrates with Amazon Kinesis Data Analytics which provides you with an efficient way to analyze and transform streaming data using Apache Flink and SQL applications. Apache Flink is an open-source framework and engine for processing streaming data using Java and Scala. Using Kinesis Data Analytics, you can develop applications to perform time series analytics, feed real-time dashboards, and create real-time metrics. You can also use Kinesis Data Analytics for transforming the incoming stream and create a new data stream that can be written back into Kinesis Data Firehose before it is delivered to a destination.&lt;/p&gt;

&lt;p&gt;Finally, Kinesis Data Firehose encryption supports Amazon S3 server-side encryption with AWS Key Management Service (AWS KMS) for encrypting delivered data in your data lake built on Amazon S3. You can choose not to encrypt the data or to encrypt the data with a key from the list of AWS KMS keys that you own (refer to the Data encryption with Amazon S3 and AWS KMS section of this document). Kinesis Data Firehose can concatenate multiple incoming records, and then deliver them to Amazon S3 as a single S3 object.&lt;/p&gt;

&lt;p&gt;This is an important capability because it reduces the load of Amazon S3 transaction costs and transactions per second. You can grant your application access to send data to Kinesis Data Firehose using AWS Identity and Access Management (IAM). Using IAM, you can also grant Kinesis Data Firehose access to S3 buckets, Amazon Redshift cluster, or Amazon OpenSearch Service cluster. You can also use Kinesis Data Firehose with virtual private cloud (VPC) endpoints (AWS PrivateLink). AWS PrivateLink is an AWS technology that enables private communication between AWS services using an elastic network interface with private IPs in your Amazon VPC.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#aws-snow-family"&gt;
  &lt;/a&gt;
  AWS Snow Family
&lt;/h2&gt;

&lt;p&gt;AWS Snow Family, comprised of AWS Snowcone, AWS Snowball, and AWS Snowmobile, offers hardware devices of varying capacities for movement of data from on-premises locations to AWS. The devices also offer cloud computing capabilities at the edge for the applications that need to perform computations closer to the source of the data. Using Snowcone you can transfer data generated continuously from sensors, IoT devices, and machines, to the AWS Cloud. Snowcone features 8 TB of storage. Snowball and Snowmobile are used to transfer massive amounts of data up to 100 PB.&lt;/p&gt;

&lt;p&gt;Snowball moves terabytes of data into your data lake built on Amazon S3. You can use it to transfer databases, backups, archives, healthcare records, analytics datasets, historic logs, IoT sensor data, and media content, especially in situations where network conditions hinder transfer of large amounts of data both into and out of AWS.&lt;/p&gt;

&lt;p&gt;AWS Snow Family uses physical storage devices to transfer large amounts of data between your on-premises data centers and your data lake built on Amazon S3. You can use AWS Storage Optimized Snowball to securely and efficiently migrate bulk data from on-premises storage platforms and Hadoop clusters. Snowball supports encryption and uses AES-256-bit encryption. Encryption keys are never shipped with the Snowball device, so the data transfer process is highly secure.&lt;/p&gt;

&lt;p&gt;Data is transferred from the Snowball device to your data lake built on Amazon S3 and stored as S3 objects in their original or native format. Snowball also has a Hadoop Distributed File System (HDFS) client, so data may be migrated directly from Hadoop clusters into an S3 bucket in its native format. Snowball devices can be particularly useful for migrating terabytes of data from data centers and locations with intermittent internet access.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#aws-glue"&gt;
  &lt;/a&gt;
  AWS Glue
&lt;/h2&gt;

&lt;p&gt;AWS Glue is a fully managed serverless ETL service that makes it easier to categorize, clean, transform, and reliably transfer data between different data stores in a simple and cost-effective way. The core components of AWS Glue consists of a central metadata repository known as AWS Glue Data Catalog which is a drop-in replacement for an Apache Hive metastore (refer to the Catalog and search section of this document for more information) and an ETL job system that automatically generates Python and Scala code and manages ETL jobs. The following figure depicts the high-level architecture of an AWS Glue environment.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--UUlQrS2p--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/w6o2oqe6c5kb59yulsu0.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--UUlQrS2p--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/w6o2oqe6c5kb59yulsu0.png" alt="Image description" width="880" height="634"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Architecture of an AWS Glue environment&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To ETL the data from source to target, you create a job in AWS Glue, which involves the following steps:&lt;/p&gt;

&lt;p&gt;Before you can run an ETL job, define a crawler and point it to the data source to identify the table definition and the metadata required to run the ETL job. The metadata and the table definitions are stored in the Data Catalog. The data source can be an AWS service, such as Amazon RDS, Amazon S3, Amazon DynamoDB, or Kinesis Data Streams, as well as a third-party JDBC-accessible database. Similarly, a data target can be an AWS service, such as Amazon S3, Amazon RDS, and Amazon DocumentDB (with MongoDB compatibility), as well as a third-party JDBC-accessible database.&lt;/p&gt;

&lt;p&gt;Either provide a script to perform the ETL job, or AWS Glue can generate the script automatically.&lt;/p&gt;

&lt;p&gt;Run the job on-demand or use the scheduler component that helps in initiating the job in response to an event and schedule at a defined time.&lt;/p&gt;

&lt;p&gt;When the job is run, the script extracts the data from the source, transforms the data, and finally loads the data into the data target.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#aws-datasync"&gt;
  &lt;/a&gt;
  AWS DataSync
&lt;/h2&gt;

&lt;p&gt;AWS DataSync is an online data transfer service that helps in moving data between on-premises storage systems and AWS storage services, as well as between different AWS storage services. You can automate the data movement between on-premises Network File Systems (NFS), Server Message Block (SMB), or a self-managed object store to your data lake built on Amazon S3. DataSync allows data encryption and data integrity validation to ensure safe and secure transfer of data. DataSync also has support for an HDFS connector to read directly from on-premises Hadoop clusters and replicate your data to your data lake built on Amazon S3.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#aws-transfer-family"&gt;
  &lt;/a&gt;
  AWS Transfer Family
&lt;/h2&gt;

&lt;p&gt;AWS Transfer Family is a fully managed and secure transfer service that helps you to move files into and out of AWS storage services (for example, your data lake built on Amazon S3 storage and Amazon Elastic File System (Amazon EFS) Network File System (NFS)). AWS Transfer Family supports Secure Shell (SSH) File Transfer Protocol (FTP), FTP Secure (FTPS), and FTP. You can use AWS Transfer Family to ingest data into your data lakes built on Amazon S3 from third parties, such as vendors and partners, to perform an internal transfer within the organization, and distribute subscription-based data to customers.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#storage-gateway"&gt;
  &lt;/a&gt;
  Storage Gateway
&lt;/h2&gt;

&lt;p&gt;Storage Gateway can be used to integrate legacy on-premises data processing platforms with a data lake built on Amazon S3. The File Gateway configuration of Storage Gateway offers on-premises devices and applications a network file share through an NFS connection. Files written to this mount point are converted to objects stored in Amazon S3 in their original format without any proprietary modification. This means that you can integrate applications and platforms that don’t have native Amazon S3 capabilities—such as on-premises lab equipment, mainframe computers, databases, and data warehouses—with S3 buckets, and then use analytical tools such as Amazon EMR or Amazon Athena to process this data.&lt;/p&gt;

&lt;p&gt;Apache Hadoop distributed copy command&lt;br&gt;
Amazon S3 natively supports distributed copy (DistCp), which is a standard Apache Hadoop data transfer mechanism. This allows you to run DistCp jobs to transfer data from an on-premises Hadoop cluster to an S3 bucket. The command to transfer data is similar to the following:&lt;/p&gt;

&lt;p&gt;hadoop distcp hdfs://source-folder s3a://destination-bucket&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#aws-direct-connect"&gt;
  &lt;/a&gt;
  AWS Direct Connect
&lt;/h2&gt;

&lt;p&gt;AWS Direct Connect establishes a dedicated network connection between your on-premises internal network and AWS. AWS Direct Connect links the internal network to an AWS Direct Connect location over a standard Ethernet fiber optic cable. Using the dedicated connection, you can create virtual interface directly with Amazon S3, which can be used to securely transfer data from on-premises into a data lake built on Amazon S3 for analysis.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#aws-database-migration-service"&gt;
  &lt;/a&gt;
  AWS Database Migration Service
&lt;/h2&gt;

&lt;p&gt;AWS Database Migration Service (AWS DMS) facilitates the movement of data from various data stores such as relational databases, NoSQL databases, data warehouses, and other data stores into AWS. AWS DMS allows one-time migration and ongoing replication (change data capture) to keep the source and target data stores in sync. Using AWS DMS, you can use Amazon S3 as a target for the supported database sources. AWS DMS task for Amazon S3 writes both full load migration and change data capture (CDC) in a comma separated value (CSV) format by default.&lt;/p&gt;

&lt;p&gt;You can also write the data into Apache Parquet format (parquet) for more compact storage and faster query options. Both CSV and parquet formats are favorable for in-place querying using services such as Amazon Athena and Amazon Redshift Spectrum (refer to the In-place querying section of this document for more information). As mentioned earlier, Parquet format is recommended for analytical querying. It is useful to use AWS DMS to migrate databases from on-premises to or across different AWS accounts to your data lake built on Amazon S3 during initial data transfer or on a regular basis.&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#data-lake-foundation"&gt;
  &lt;/a&gt;
  Data lake foundation
&lt;/h1&gt;

&lt;p&gt;Amazon S3 provides the foundation for building a data lake, along with integration to other services that can be tailored to your business needs. A common challenge faced by users when building a data lake is the categorization of data and maintaining data across different stages as it goes through the transformation process.&lt;/p&gt;

&lt;p&gt;The following figure depicts a sample data lake and the transformation journey data goes through in its lifecycle.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--wK_Z23UI--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/te77tojtxewpsownb1ba.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--wK_Z23UI--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/te77tojtxewpsownb1ba.png" alt="Image description" width="880" height="520"&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;Sample data lake transformation journey&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This section provides a recommended bucket strategy for building a data lake foundation.&lt;/p&gt;

&lt;p&gt;A data lake can be broadly categorized across four distinct buckets:&lt;/p&gt;

&lt;p&gt;Raw data – Data ingested from the data sources in the raw data format, which is the immutable copy of the data. This can include structured, semi structured, and unstructured data objects such as databases, backups, archives, JSON, CSV, XML, text files, or images.&lt;/p&gt;

&lt;p&gt;Transformed – This bucket consists of transformed data normalized to a specific use case for performance improvement and cost reduction. In this stage, data can be transformed into columnar data formats, such as Apache Parquet and Apache ORC, which can be used by Amazon Athena.&lt;/p&gt;

&lt;p&gt;Curated –The transformed data can be further enriched by blending it with other data sets to provide additional insights. This layer typically contains S3 objects which are optimized for analytics, reporting using Amazon Athena, Amazon Redshift Spectrum, and loading into massively parallel processing data warehouses such as Amazon Redshift.&lt;/p&gt;

&lt;p&gt;Logs – This bucket stores process logs for Amazon S3, and other services in the data lake architecture. The logs can include S3 access logs, CloudWatch logs, or CloudTrail logs.&lt;/p&gt;

&lt;p&gt;The following table shows a recommended sample folder structure for your data lake per environment. Each environment can have the same folder structure with tags to segregate across each environment.&lt;/p&gt;

&lt;p&gt;Recommended sample folder structure for your data lake per environment&lt;/p&gt;

&lt;p&gt;It is recommended you follow best practices when defining your bucket strategy for your data lake built on Amazon S3:&lt;/p&gt;

&lt;p&gt;Buckets names must be unique within a partition. Currently there are three partitions (aws – Standard Regions, aws-cn – China Regions, aws-us-gov – AWS GoveCloud US Regions); however, names can be reused after the buckets are deleted (with several exceptions).&lt;/p&gt;

&lt;p&gt;Deleted bucket names are not available to reuse immediately; hence, if users want to use a particular bucket name, they should not delete the bucket.&lt;/p&gt;

&lt;p&gt;All bucket names across all AWS Regions should comply with DNS naming conventions.&lt;/p&gt;

&lt;p&gt;Buckets can store an unlimited number of objects in a bucket without impacting performance. The objects can be stored in a single bucket or across multiple buckets; however, you cannot create a bucket from within another bucket.&lt;/p&gt;

&lt;p&gt;Production S3 buckets should be hosted under a different AWS account, separate from non-production workloads.&lt;/p&gt;

&lt;p&gt;Build an automatic ingestion mechanism to catalog and create the multiple layers of data storage including Raw, Transformed, and Curated.&lt;/p&gt;

&lt;p&gt;Consider building automatic data classification rules based on schema and data.&lt;/p&gt;

&lt;p&gt;Consider additional folders within the data lakes, such as reports, downstream applications, or user folders.&lt;/p&gt;

&lt;p&gt;Enable versioning, if needed, for protection from accidental deletes.&lt;/p&gt;

&lt;p&gt;Use separate buckets for S3 data which needs to be replicated.&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#catalog-and-search"&gt;
  &lt;/a&gt;
  Catalog and search
&lt;/h1&gt;

&lt;p&gt;The earliest challenges that inhibited building a data lake were keeping track of all of the raw assets as they were loaded into the data lake, and then tracking all of the new data assets and versions that were created by data transformation, data processing, and analytics. Thus, an essential component of a data lake built on Amazon S3 is the Data Catalog. The Data Catalog provides an interface to query all assets stored in data lake S3 buckets. The Data Catalog is designed to provide a single source of truth about the contents of the data lake.&lt;/p&gt;

&lt;p&gt;AWS Glue&lt;br&gt;
AWS Glue is a fully managed ETL service that makes it easier to categorize, clean, transform, and reliably transfer data between different data stores. The AWS Glue Data Catalog, acomponent of AWS Glue, provides a unified metadata repository for performing analytical operations across various data sources, such as Amazon EMR, Amazon Athena, Amazon Redshift, and Amazon Redshift Spectrum, and any application that is compatible with a Hive metastore.&lt;/p&gt;

&lt;p&gt;To create a Data Catalog, use AWS Glue crawlers that crawl the data from the data sources as registered with AWS Glue in the AWS Management Console. The data source can be an S3 bucket, a table in Amazon RDS, Amazon DynamoDB, or Amazon Redshift, or any external database that supports JDBC connectivity.&lt;/p&gt;

&lt;p&gt;You can define custom classifiers or use the built-in classifiers provided by AWS Glue to classify the data. AWS Glue provides classifiers for common file types, such as CSV, JSON, AVRO, or XML. AWS Glue also provides classifiers for common relational database management systems using a JDBC connection. You can create a custom classifier using a grok pattern, an XML tag, JavaScript Object Notation (JSON), or comma-separated values (CSV). The following figure depicts the working of AWS Glue in building a Data Catalog.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--N_Z_I8z6--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/b6kxtmjtg4n1vubd8aqj.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--N_Z_I8z6--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/b6kxtmjtg4n1vubd8aqj.png" alt="Image description" width="880" height="612"&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;AWS Glue Data Catalog&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Data Catalog is a database that stores metadata in tables consisting of data schema, data location, and runtime metrics. Data Catalog is also Apache Hive metastore compatible that can be used as a central repository for storing structural and operational metadata. AWS Glue also provides out-of-box integration with Amazon EMR that allows you to use Data Catalog as an external Hive metastore. Data Catalog is recommended, especially when you need a persistent metastore or a metastore shared between different applications, services, clusters, or AWS accounts. Data Catalog can also be used to create an external table for Athena or Amazon Redshift.&lt;/p&gt;

&lt;p&gt;AWS Lake Formation&lt;br&gt;
AWS Lake Formation helps to easily build, secure, and manage data lakes. Lake Formation provides centralized governance and access control for the data in a data lake built on S3, and controls access to the data through various services, such as AWS Glue, Athena, Amazon Redshift Spectrum, Amazon QuickSight, and Amazon EMR. AWS Lake Formation can connect to an S3 bucket and orchestrate a dataflow that can ingest, clean, transform, and organize the raw data.&lt;/p&gt;

&lt;p&gt;Lake Formation uses AWS Glue Data Catalog to automatically classify data in data lakes, data sources, transforms, and targets. Apart from the metadata, the Data Catalog also stores information consisting of resource links to shared databases and tables in external accounts for cross account access to the data in a data lake built on S3.&lt;/p&gt;

&lt;p&gt;Lake Formation provides you with a grant/revoke permission model to control access to Data Catalog resources (consisting of database and metadata tables), S3 buckets and underlying data in these buckets. Lake Formation permissions along with IAM policies provide granular access to the data stored in data lakes built on S3. These permissions can be used to share Data Catalog resources with external AWS accounts. The users from these accounts can run jobs and queries by combining data from multiple data catalogs across multiple accounts.&lt;/p&gt;

&lt;p&gt;Comprehensive data catalog&lt;br&gt;
You have the flexibility to create a comprehensive data catalog using standard AWS services such as AWS Lambda, DynamoDB, and Amazon OpenSearch Service. At a high level, AWS Lambda triggers are used to populate DynamoDB tables with object names and metadata when those objects are put into S3; Amazon OpenSearch Service is used to search for specific assets, related metadata, and data classifications. The following figure shows a high-level architectural overview of this solution.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--qkJPKVmt--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/uedb6o0nsurt6htytboc.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--qkJPKVmt--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/uedb6o0nsurt6htytboc.png" alt="Image description" width="880" height="281"&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;Comprehensive data catalog using AWS Lambda, DynamoDB, and Amazon OpenSearch Service&lt;/em&gt;&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#transforming-data-assets"&gt;
  &lt;/a&gt;
  Transforming data assets
&lt;/h1&gt;

&lt;p&gt;A core value of a data lake is that it is the collection point and repository for all of an organization’s data assets, regardless of their native formats. This allows quick ingestion, elimination of data duplication and data sprawl, and centralized governance and management. After the data assets are collected, they need to be transformed into normalized formats to be used by a variety of data analytics and processing tools.&lt;/p&gt;

&lt;p&gt;The key to ‘democratizing’ the data and making the data lake available to the widest number of users of varying skill sets and responsibilities is to transform data assets into a format that allows for efficient one-time SQL querying. Data transformation or ETL process prepares the data to be consumed by downstream systems for advanced analytics, visualizations and business reporting, or machine learning.&lt;/p&gt;

&lt;p&gt;The first step involves extracting information from the different data sources which was discussed in the Data ingestion methods section. The data is stored in a raw bucket (refer to the Data lake foundation section of this document). After you have the data, you need to transform it. Transformations might involve aggregating the data from different sources or changing the file format of the data received, such as from CSV to Parquet format to reduce the file size and optimize the Athena query.&lt;/p&gt;

&lt;p&gt;You can compress the files or convert the data format from string to double, or number to date format. After the transformation is complete, the data can be written into transformed bucket (refer to the Data lake foundation section of this document) which is then further used by the downstream systems.&lt;/p&gt;

&lt;p&gt;There are multiple ways to transform data assets, and the “best” way often comes down to the nature of the analytics application, individual preference, skill sets, and the tools available. When a data lake is built on AWS, there is a wide variety of tools and services available for data transformation, so you can pick the methods and tools that best suits your purpose. Because the data lake is inherently multi-tenant, multiple data transformation jobs using different tools can run concurrently.&lt;/p&gt;

&lt;p&gt;There are two common and straightforward methods to transform data assets into Parquet in a data lake built on S3 using Amazon EMR clusters. The first method involves creating an Amazon EMR cluster with Hive installed using the raw data assets in Amazon S3 as input, transforming those data assets into Hive tables, and then writing those Hive tables back out to Amazon S3 in Parquet format. The second method is to use Spark on Amazon EMR. With this method, a typical transformation can be achieved with only 20 lines of PySpark code.&lt;/p&gt;

&lt;p&gt;A third data transformation method on a data lake built on S3 is to use AWS Glue. AWS Glue simplifies and automates difficult and time-consuming data discovery, conversion, mapping, and job scheduling tasks. AWS Glue guides you through the process of transforming and moving your data assets with an easy-to-use console that helps you understand your data sources, transform and prepare these data assets for analytics, and load them reliably from S3 data sources back into S3 destinations.&lt;/p&gt;

&lt;p&gt;AWS Glue automatically crawls raw data assets in your data lake’s S3 buckets, identifies data formats, and then suggests schemas and transformations so that you don’t have to spend time hand coding data flows. You can then edit these transformations, if necessary, using the tools and technologies you already know, such as Python, Spark, Git, and your favorite integrated developer environment, and then share them with other AWS Glue users of the data lake. AWS Glue’s flexible job scheduler can be set up to run data transformation flows on a recurring basis, in response to triggers, or even in response to AWS Lambda events.&lt;/p&gt;

&lt;p&gt;AWS Glue automatically and transparently provisions hardware resources, and distributes ETL jobs on Apache Spark nodes so that ETL run times remain consistent as data volume grows. AWS Glue coordinates the implementation of data lake jobs in the right sequence, and automatically retries failed jobs. With AWS Glue, there are no servers or clusters to manage, and you pay only for the resources consumed by your ETL jobs.&lt;/p&gt;

&lt;p&gt;AWS Lambda functions can also be used for transforming the data stored in your data lake built on Amazon S3. Lambda functions can respond to event notifications from Amazon S3 when an object is created or deleted. You can configure a Lambda function to perform an action asynchronously, based on the event.&lt;/p&gt;

&lt;p&gt;When data is stored in a data lake built on S3, you can share the data with multiple applications. These applications can vary in nature and purpose (for example, ecommerce applications, analytics applications, and marketing applications) and necessitate a different view for each application. S3 Object Lambda can be used to add your own code to process the data retrieved before returning it to an application (for example, masking the personally identifiable information (PII) data or masking credit card information before transferring it to the application).&lt;/p&gt;

&lt;p&gt;S3 Object Lambda uses a Lambda function to automatically process and transform the data as it is being retrieved from an S3 bucket. The following figure shows an example of an S3 Object Lambda.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--WI2BEgEM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/n8o27xcdqehxsx3thlse.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--WI2BEgEM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/n8o27xcdqehxsx3thlse.png" alt="Image description" width="880" height="497"&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;Using S3 Object Lambda to transform data before retrieval by applications&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;S3 Object Lambda can be very useful in redacting PII data for analytics applications, format conversion, enriching data from other data sources, resizing objects, or even implementing custom authorization rules to access the data stored in your data lake built on Amazon S3.&lt;/p&gt;

&lt;p&gt;Another transformation feature that S3 supports natively is batch operations. Using S3 Batch Operations, you can define an operation to be performed on the exabytes of objects stored in your bucket. S3 Batch Operations manages the entire lifecycle of the batch operation by tracking the progress, sending notifications, and storing a detailed completion report for all the operations performed on the objects.&lt;/p&gt;

&lt;p&gt;By using S3 Batch Operations, you can copy objects from a bucket into another bucket in the same or different Region, or invoke a Lambda function to transform the object. S3 Batch Operation can also be used to manage the tags defined for the objects and restoring a large number of objects from S3 Glacier. S3 Batch Operation, along with S3 Object Lock, can be used to manage retention dates and legal holds to apply compliance and governance rules for multiple objects at the same time. S3 Object Lock works on the write once read many (WORM) model that can help in prevention of accidental deletion of objects. It can also help in adhering to regulatory controls.&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#inplace-querying"&gt;
  &lt;/a&gt;
  In-place querying
&lt;/h1&gt;

&lt;p&gt;One of the most important capabilities of a data lake on AWS is the ability to perform in-place transformation and querying of data assets. This allows users to run sophisticated analytical queries directly on their data stored in S3, without having to copy and load data into separate analytics platforms or data warehouses.&lt;/p&gt;

&lt;p&gt;There are various tools to perform in-place querying for data stored in a data lake, such as Presto on Amazon EMR and various partner tools. This section provides an overview of serverless services that not only helps perform in-place querying, but also avoids the procurement and management of servers.&lt;/p&gt;

&lt;p&gt;Users can query S3 data without any additional infrastructure, and only pay for the queries that they run. This makes the ability to analyze vast amounts of unstructured data accessible to any data lake user who can use SQL, and makes it far more cost effective than the traditional method of performing an ETL process, creating a Hadoop cluster or data warehouse, loading the transformed data into these environments, and then running query jobs. AWS Glue, as described in the previous sections, provides the data discovery and ETL capabilities, and Amazon Athena and Amazon Redshift Spectrum provides the serverless in-place querying capabilities.&lt;/p&gt;

&lt;p&gt;In addition to in-place querying using Athena and Redshift Spectrum, S3 also provides capabilities to retrieve subset of your data through S3 Select and S3 Glacier Select, that improves the performance of accessing large amounts of data from your data lake built on S3. Using S3 Select, users can run SQL statements to filter and retrieve only a subset of data stored in their data lake. S3 Select operates on objects stored in CSV, JSON, or Apache Parquet format, and other compression formats such as GZIP or BZIP2. Users can also delimit the result set, thus, reducing latency to retrieve the data and optimizing cost.&lt;/p&gt;

&lt;p&gt;Amazon Athena&lt;br&gt;
Amazon Athena is an interactive query service that makes it easier for you to analyze data directly in S3 using standard SQL. With a few actions in the AWS Management Console, you can use Athena directly against data assets stored in the data lake built on S3 and begin using standard SQL to run one-time queries and get results in a matter of seconds.&lt;/p&gt;

&lt;p&gt;Athena is serverless, so there is no infrastructure to set up or manage, and you only pay for the volume of data assets scanned during the queries you run.&lt;/p&gt;

&lt;p&gt;Athena scales automatically—running queries in parallel—so results are fast, even with large datasets and complex queries. You can use Athena to process unstructured, semi-structured, and structured data sets. Supported data asset formats include CSV, JSON, or columnar data formats such as Apache Parquet and Apache ORC. You can also use Athena to run one-time queries using ANSI SQL without first aggregating or loading the data into Athena. Athena integrates with Amazon QuickSight for easy data visualization. It can also be used with third-party reporting and business intelligence tools by connecting these tools to Athena with a JDBC driver.&lt;/p&gt;

&lt;p&gt;Athena also natively integrates with AWS Glue Data Catalog which provides a persistent metadata store for the data stored in S3. You can create the table and use Athena to query the data based on a metadata store that integrates with the ETL and data discovery features of AWS Glue.&lt;/p&gt;

&lt;p&gt;When querying an existing table, Athena uses Presto under the hood, a distributed SQL engine. Athena can also be used to query S3 inventory using standard SQL. Amazon S3 Inventory is an S3 tool to help manage storage. You can use the tool to audit and report on the replication and encryption status of the objects for business, compliance, and regulatory needs. Athena supports querying S3 inventory files in ORC, Parquet, or CSV format. AWS recommends using ORC-formatted or Parquet-formatted inventory files because these formats provide faster query performance and lower query costs. ORC and Parquet formats are columnar formats that allows the reader to read, decompress, and process the columns that are only required for the current query.&lt;/p&gt;

&lt;p&gt;Amazon Redshift Spectrum&lt;br&gt;
Another way to perform in-place querying of data assets in a data lake built on Amazon S3 is to use Amazon Redshift Spectrum. Amazon Redshift is a large-scale, managed data warehouse service that supports massive parallel processing. By contrast, Amazon Redshift allows you to run Redshift SQL queries directly against massive amounts of data—up to exabytes—stored in a data lake built on S3. Amazon Redshift Spectrum applies sophisticated query optimization, scaling processing across thousands of nodes, so results are fast—even with large data sets and complex queries. Amazon Redshift Spectrum can directly query a wide variety of data assets stored in the data lake, including CSV, TSV, Parquet, Sequence, and RCFile. Because Amazon Redshift Spectrum supports the SQL syntax of Amazon Redshift, you can run sophisticated queries using the same business intelligence tools that you use today. You also have the flexibility to run queries that span frequently accessed data assets that are stored locally in Amazon Redshift, and your full data sets stored in S3.&lt;/p&gt;

&lt;p&gt;Because Amazon Athena and Amazon Redshift share a common data catalog and common data formats, you can use both Athena and Amazon Redshift Spectrum against the same data assets. You would typically use Athena for one-time data discovery and SQL querying, and then use Amazon Redshift Spectrum for more complex queries and scenarios where a large number of data lake users want to run concurrent business intelligence and reporting workloads.&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#the-broader-analytics-portfolio"&gt;
  &lt;/a&gt;
  The broader analytics portfolio
&lt;/h1&gt;

&lt;p&gt;With a data lake built on AWS, data assets get ingested and stored in one massively scalable, low cost, performant platform. Data discovery, transformation, and SQL querying can all be done in place using innovative AWS services such as AWS Glue, Amazon Athena, and Amazon Redshift Spectrum. In addition, there are a wide variety of other AWS services that can be directly integrated with S3 to create any number of sophisticated analytics, machine learning, and artificial intelligence data processing pipelines.&lt;/p&gt;

&lt;p&gt;This allows you to quickly solve a wide range of analytics business challenges on a single platform, against common data assets, without having to worry about provisioning hardware and installing and configuring complex software packages before loading data and performing analytics. Plus, you only pay for what you consume. Some of the most common AWS services that can be used with data assets in a data lake built on S3 are describedin this section.&lt;/p&gt;

&lt;p&gt;Amazon EMR&lt;br&gt;
Amazon EMR is a highly distributed computing framework used to quickly and easily process data in a cost-effective manner. Amazon EMR uses Apache Hadoop, an open-source framework, to distribute data and processing across an elastically resizable cluster of EC2 instances and allows you to use all the common Hadoop tools such as Hive, Pig, Spark, Flink, Hudi, Hue, Livy, MXNet, Presto, TensorFlow, HBase, or Zeppelin. Amazon EMR does all the heavily lifting involved with provisioning, managing, and maintaining the infrastructure and software of a Hadoop cluster, and is integrated directly with S3.&lt;/p&gt;

&lt;p&gt;With Amazon EMR, you can launch a persistent cluster that stays up indefinitely or a temporary cluster that ends after the analysis is complete. In either scenario, you only pay for the hours the cluster is up. Amazon EMR supports a variety of EC2 instance types encompassing general purpose, compute, memory, storage I/O optimized, and GPU instances (for example, M6g, C5, R5, Z1, I3, D2, G4, and P3), and all Amazon EC2 pricing options (On-Demand, Reserved, and Spot). The latest version of EMR 6.1.0 (at the time of writing this document) also supports ARM based instance types.&lt;/p&gt;

&lt;p&gt;AWS Graviton processors are custom built by Amazon Web Services and use 64-bit Arm Neoverse cores to deliver the best price performance. When you launch an Amazon EMR cluster (called a job flow), you choose how many and what type of EC2 instances to provision. Organizations with many different lines of business and a large number of users can build a single data lake solution, store their data assets in S3, and then spin up multiple EMR clusters to share data assets in a multi-tenant fashion.&lt;/p&gt;

&lt;p&gt;Amazon EMR integrates with S3 for input, output, and intermediate data storage. You can choose the HDFS which runs on a cluster consisting of master and core nodes for processing data that does not need to persist after the lifecycle of the cluster. You can use Amazon EMR file system (EMRFS) using S3 as a data layer for applications running on your cluster. The data can be stored on EMRFS to persist even after the cluster lifecycle. This mechanism helps separate compute and storage, thus enabling you to independently scale the compute and storage depending on the workload. You can scale your compute needs by resizing your cluster, and you can scale your storage needs by using S3.&lt;/p&gt;

&lt;p&gt;Amazon SageMaker&lt;br&gt;
Machine learning is another important data lake use case. Amazon SageMaker is a fully managed machine learning service that provides a set of features to build, train, and deploy machine learning models. SageMaker provides a suite of built-in algorithms and mechanisms to build custom algorithms based on user requirements. SageMaker also provides an integrated Jupyter notebook instance to prepare and transform the data, develop code for model training, and subsequently deploy, test, and validate the models.&lt;/p&gt;

&lt;p&gt;SageMaker provides visualization tools and wizards to guide you through the process of creating machine learning models without having to learn complex algorithms and technology. SageMaker can create machine learning models based on datasets stored in your data lake built on S3. SageMaker greatly enhances machine learning capabilities by combining with your data lake built on S3. For training machine learning models, a large amount of data is required, and a data lake backed by S3 and Amazon S3 Glacier provides cost-effective storage.&lt;/p&gt;

&lt;p&gt;Amazon QuickSight&lt;br&gt;
Amazon QuickSight is a fast business analytics service that makes it easy for you to build visualizations, perform targeted analysis, and quickly get business insights from your data assets stored in the data lake, any time, on any device. You can use Amazon QuickSight to seamlessly discover AWS data sources such as Amazon Redshift, Amazon RDS, Amazon Aurora, Amazon Athena, and Amazon S3, connect to any of these data sources and data assets, and get insights from this data in minutes. Amazon QuickSight allows organizations using the data lake to seamlessly scale their business analytics capabilities to hundreds of thousands of users. It delivers fast and responsive query performance by using a Super-fast, Parallel, In-memory Calculation Engine (SPICE).&lt;/p&gt;

&lt;p&gt;Amazon OpenSearch Service&lt;br&gt;
Amazon OpenSearch Service is a managed service where you can deploy and manage OpenSearch clusters at scale. Common use cases include log analytics, real-time application monitoring, and clickstream analytics. Amazon OpenSearch Service can be used to add a search box to a website, analyze metrics, security event data, application and infrastructure logs, or store data to automate business workflows.&lt;/p&gt;

&lt;p&gt;Amazon OpenSearch Service manages the infrastructure provisioning and maintenance, thus reducing the overhead of managing the Amazon OpenSearch Service cluster yourself. Amazon OpenSearch Service integrates with various open source and native AWS services to provide a seamless experience. Amazon OpenSearch Service integrates with open source Logstash to conveniently upload bulk data into Amazon OpenSearch Service domain and with open source Kibana for visualization.&lt;/p&gt;

&lt;p&gt;Amazon OpenSearch Service also runs with Kinesis Data Firehose for ingestion of streaming data, AWS CloudTrail, and Amazon CloudWatch for logging and monitoring, respectively. You can also use your data lake built on Amazon S3 to store the streaming data before it is ingested into the Amazon OpenSearch Service domain. Ultrawarm storage and Cold storage, types of Amazon OpenSearch Service storage, use Amazon S3 for storing infrequently-accessed read-only data in a cost-efficient manner.&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#securing-protecting-and-managing-data"&gt;
  &lt;/a&gt;
  Securing, protecting, and managing data
&lt;/h1&gt;

&lt;h1&gt;
  &lt;a href="#monitoring-and-optimizing-the-data-lake-environment"&gt;
  &lt;/a&gt;
  Monitoring and optimizing the data lake environment
&lt;/h1&gt;

&lt;p&gt;Beyond the efforts required to architect and build a data lake, your organization must also consider the operational aspects of a data lake, and how to cost-effectively and efficiently operate a production data lake at large scale. Key elements you must consider are monitoring the operations of the data lake, making sure that it meets performance expectations and service-level agreements, analyzing utilization patterns, and using this information to optimize the cost and performance of your data lake. AWS provides multiple features and services to help optimize a data lake that is built on AWS, including S3 storage analytics, Amazon CloudWatch metrics, AWS CloudTrail, and Amazon S3 Glacier.&lt;/p&gt;

&lt;p&gt;Data lake monitoring&lt;br&gt;
A key aspect of operating a data lake environment is understanding how all of the components that comprise the data lake are operating and performing, and generating notifications when issues occur or operational performance falls below predefined thresholds.&lt;/p&gt;

&lt;p&gt;Amazon CloudWatch&lt;br&gt;
As an administrator, you need to look at the complete data lake environment holistically. This can be achieved using Amazon CloudWatch. CloudWatch is a monitoring service for AWS Cloud resources and the applications that run on AWS. You can use CloudWatch to collect and track metrics, collect and monitor log files, set thresholds, and initiate alarms. This allows you to automatically react to changes in your data lake built on S3.&lt;/p&gt;

&lt;p&gt;You can also use CloudWatch metrics to understand and improve the performance of applications that are using Amazon S3. You can use CloudWatch for generating daily storage metrics for your data lake built on CloudWatch by collecting and processing storage data for your S3 buckets. You can also monitor the requests to your data lake built on S3 and identify and act upon operational issues quickly. In addition, you can also monitor the S3 APIs that are pending replication, total size, and the maximum time required for replication to the destination Region.&lt;/p&gt;

&lt;p&gt;Amazon Macie&lt;br&gt;
Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover, monitor, and protect your sensitive data stored in your data lake. Macie can be used to scan your data lakes and discover sensitive information such as PII or financial data, and identify and report overly permissive or unencrypted buckets.&lt;/p&gt;

&lt;p&gt;Using Macie, you can run a sensitive data discovery job to identify sensitive information using built-in criteria and techniques, such as machine learning and pattern matching called as managed data identifiers, to analyze objects in your data lake. You can also define your own criteria called as custom data identifiers, using regular expressions defining a text pattern to match and, optionally, character sequences and a proximity rule that refines the results.&lt;/p&gt;

&lt;p&gt;Additionally, after enabled, Macie maintains a complete inventory of your S3 buckets and evaluates and monitors the bucket for security and access control. If Macie detects a potential issue, the service creates a policy finding. Policy findings are generated when policies or settings for S3 bucket are changed in a way that reduces the security of the bucket and its objects. Macie integrates with other AWS services, such as Amazon EventBridge, which is a serverless event bus service that can send findings data to services, such as AWS Lambda and Amazon Simple Notification Service (Amazon SNS), to act on the policy findings.&lt;/p&gt;

&lt;p&gt;AWS CloudTrail&lt;br&gt;
An operational data lake has many users and multiple administrators, and may be subject to compliance and audit requirements, so it’s important to have a complete audit trail of actions taken and who has performed these actions. AWS CloudTrail is an AWS service that enables governance, compliance, operational auditing, and risk auditing of AWS accounts.&lt;/p&gt;

&lt;p&gt;CloudTrail continuously monitors and retains events related to API calls across AWS services that comprise a data lake. CloudTrail provides a history of AWS API calls for an account, including API calls made through the AWS Management Console, AWS SDKs, command line tools, and most data lakes built on S3 services. You can identify which users and accounts made requests or took actions against AWS services that support CloudTrail, the source IP address the actions were made from, and when the actions occurred.&lt;/p&gt;

&lt;p&gt;CloudTrail can be used to simplify data lake compliance audits by automatically recording and storing activity logs for actions made within AWS accounts.&lt;/p&gt;

&lt;p&gt;Integration with Amazon CloudWatch logs provides a convenient way to search through log data, identify out-of-compliance events, accelerate incident investigations, and expedite responses to auditor requests. CloudTrail logs are stored in a separate logs bucket within your data lake (refer to the Data lake foundation section of this document) for durability and deeper analysis.&lt;/p&gt;

&lt;p&gt;Data lake optimization&lt;br&gt;
Optimizing a data lake environment includes minimizing operational costs. By building a data lake on S3, you only pay for the data storage and data processing services that you actually use, as you use them. You can reduce costs by optimizing how you use these services. Data asset storage is often a significant portion of the costs associated with a data lake. Fortunately, AWS has several features that can be used to optimize and reduce costs. These include: S3 Lifecycle management, S3 storage class analysis, S3 Intelligent-Tiering, S3 Storage Lens, and Amazon S3 Glacier storage class.&lt;/p&gt;

&lt;p&gt;Amazon S3 Lifecycle management&lt;br&gt;
Amazon S3 Lifecycle management allows you to create lifecycle rules, which can be used to automatically migrate data assets to a lower cost tier of storage—such as S3 Standard-IA storage class or Amazon S3 Glacier storage class—or let them expire when they are no longer needed. A lifecycle configuration, which consists of an XML file, comprises a set of rules with predefined actions that you want Amazon S3 to perform on data assets during their lifetime. Lifecycle configurations can perform actions based on data asset age and data asset names, but can also be combined with S3 object tagging to perform very granular management of data assets. If the access pattern for your S3 buckets is constantly changing or is evolving, you can use S3 Intelligent-Tiering storage class for automatic cost savings.&lt;/p&gt;

&lt;p&gt;Amazon S3 Storage class analysis&lt;br&gt;
One of the challenges of developing and configuring lifecycle rules for the data lake is gaining an understanding of how data assets are accessed over time. It only makes economic sense to transition data assets to a more cost-effective storage or archive tier if those objects are infrequently accessed. Otherwise, data access charges associated with these more cost-effective storage classes can negate any potential savings. Amazon S3 provides storage class analysis to help you understand how data lake data assets are used. Storage class analysis uses machine learning algorithms on collected access data to help you develop lifecycle rules that will optimize costs.&lt;/p&gt;

&lt;p&gt;Seamlessly tiering to lower cost storage tiers is an important capability of a data lake, particularly as its users plan for, and move to, more advanced analytics and machine learning capabilities. Data lake users will typically ingest raw data assets from many sources,and transform those assets into harmonized formats that they can use for one-time querying and on-going business intelligence querying through SQL. However, users also want to perform more advanced analytics using streaming analytics, machine learning,and artificial intelligence. These more advanced analytics capabilities consist of building data models, validating these data models with data assets, and then training and refining these models with historical data.&lt;/p&gt;

&lt;p&gt;Keeping more historical data assets, particularly raw data assets, allows for better training and refinement of models. Additionally, as your organization’s analytics sophistication grows, you may want to go back and reprocess historical data to look for new insights and value. These historical data assets are infrequently accessed and consume a lot of capacity, so they are often well suited to be stored on an archival storagelayer.&lt;/p&gt;

&lt;p&gt;Another long-term data storage need for the data lake is to keep processed data assets and results for long-term retention for compliance and audit purposes, to be accessed by auditors when needed. Both of these use cases are well served by Amazon S3 Glacier storage class, which is an S3 storage class optimized for infrequently used cold data, and for storing write once read many (WORM) data. You can also use S3 Object Lock to adhere to regulatory compliance.&lt;/p&gt;

&lt;p&gt;S3 Intelligent-Tiering&lt;br&gt;
S3 Intelligent-Tiering is designed to optimize your storage cost by automatically moving the data within your buckets by monitoring your access patterns. S3 Intelligent-Tiers makes use of two low latency high throughout access tiers: one tier for frequently accessed data and another tier for infrequently accessed data. S3 Intelligent Tiering monitors access patterns for the data for a period of 30 months and automatically moves the data to one of the access tiers without operational overhead or performance impact.&lt;/p&gt;

&lt;p&gt;S3 Intelligent-Tiering is recommended for data that has unpredictable access patterns regardless of object type, size and retention period such as data lakes, data analytics applications or new applications. If the data from infrequently accessed tier is accessed, the data will be moved to frequently accessed tier. Additionally, you can also configure S3 Intelligent-Tiering to automatically move data from infrequently accessed tier that has not been accessed for consecutive 90 days to archive access tier and if not accessed for consecutive 180 days to deep archive access tier.&lt;/p&gt;

&lt;p&gt;You can also configure the last access time for archiving up to a maximum of 730 days for both Archive access tier and Deep Archive access tier. S3 Intelligent-Tiering Archive access tier provides same performance as Amazon S3 Glacier storage class and S3 Intelligent Tiering Deep Archive access tier provides same performance as Amazon S3 Glacier Deep Archive storage class.&lt;/p&gt;

&lt;p&gt;S3 Storage Lens&lt;br&gt;
As your data lake becomes more popular and you start expanding to accommodate data from multiple applications across multiple accounts and S3 buckets. It can become increasingly complicated to understand usage of the data across the organization, optimize cost and understand the security posture. S3 Storage Lens gives you the visibility into your object storage across your organization with point-in-time metrics, trend lines and actionable insights.&lt;/p&gt;

&lt;p&gt;You can generate insights at organization, account, region, bucket, and prefix level. S3 aggregates your usage and metrics across all the accounts and provides an account snapshot on the S3 console (Bucket) home page. You can use the Storage Lens dashboard to visualize insights and trends, identify outliers, receive recommendations for storage cost optimization, and so on.&lt;/p&gt;

&lt;p&gt;You can use Storage Lens dashboard to identify your largest buckets and take necessary actions to optimize the cost since the rate charged depends on the object size, duration of storage and storage class. In case you are uploading objects using multi-part upload, there might be a case when the uploads fail or not completed. The incomplete uploads remain in your buckets and are chargeable.&lt;/p&gt;

&lt;p&gt;You can identify these incomplete multipart uploads using Storage Lens dashboard. Additionally, storage lens can also help identify multiple versions of the objects. Finally, you can also use Storage Lens to uncover cold buckets from your account. Cold buckets are the buckets which are not accessed for a long period of time. All these insights can be accessed from the Storage Lens dashboard.&lt;/p&gt;

&lt;p&gt;Amazon S3 Glacier&lt;br&gt;
Amazon S3 Glacier is a low-cost Amazon S3 storage class that provides durable storage with security features for data archiving and backup. S3 Glacier has the same data durability (99.999999999%) and supports lifecycle management on data assets stored in S3, so that data assets can seamlessly migrate from Amazon S3 to S3 Glacier. S3 Glacier storage class is a great storage choice when low storage cost is essential, data assets are rarely retrieved, and retrieval latency of several minutes to several hours is acceptable.&lt;/p&gt;

&lt;p&gt;Different types of data lake assets may have different retrieval needs. For example, compliance data may be infrequently accessed and be relatively small in size but needs to be made available in minutes when auditors request data, whereas historical raw data assets may be very large but can be retrieved in bulk over the course of a day when needed.&lt;/p&gt;

&lt;p&gt;S3 Glacier allows data lake users to specify retrieval times when the data retrieval request is created, with longer retrieval times leading to lower retrieval costs. For processed data and records that need to be securely retained, Amazon S3 Glacier Vault Lock allows data lake administrators to deploy and enforce compliance controls on individual S3 Glacier vaults by a lockable policy.&lt;/p&gt;

&lt;p&gt;Administrators can specify controls such as WORM in a Vault Lock policy and lock the policy from future edits. After locked, the policy becomes immutable and S3 Glacier will enforce the prescribed controls to help achieve your compliance objectives, and provide an audit trail for these assets using AWS CloudTrail.&lt;/p&gt;

&lt;p&gt;Cost and performance optimization&lt;br&gt;
You can optimize your data lake using cost and performance. Amazon S3 provides a very performant foundation for a data lake because its enormous scale provides virtually limitless throughput and extremely high transaction rates. Using S3 best practices for data asset naming ensures high levels of performance. These best practices can be found in the Amazon Simple Storage Service Developer Guide.&lt;/p&gt;

&lt;p&gt;Another area of optimization is to use optimal data formats when transforming raw data assets into normalized formats, in preparation for querying and analytics. These optimal data formats can compress data and reduce data capacities needed for storage, and also substantially increase query performance by common data lake built on S3 analytic services.&lt;/p&gt;

&lt;p&gt;Data lake environments are designed to ingest and process many types of data, and store raw data assets for future archival and reprocessing purposes, as well as store processed and normalized data assets for active querying, analytics, and reporting. A key best practice to reduce storage and analytics processing costs, and improve analytics querying performance, is to use an optimized data format, particularly a format like Apache Parquet.&lt;/p&gt;

&lt;p&gt;Parquet is a columnar compressed storage file format that is designed for querying large amounts of data, regardless of the data processing framework, data model, or programming language. Compared to common raw data log formats such as CSV, JSON, or TXT format, Parquet can reduce the required storage footprint, improve query performance significantly, and greatly reduce querying costs for AWS services, which charge by amount of data scanned.&lt;/p&gt;

&lt;p&gt;Amazon tests comparing CSV and Parquet formats using one TB of log data stored in CSV format to Parquet format showed the following:&lt;/p&gt;

&lt;p&gt;Space savings of 87% with Parquet (1 TB of log data stored in CSV format compressed to 130 GB with Parquet)&lt;/p&gt;

&lt;p&gt;A query time for a representative Athena query was 34x faster with Parquet (237 seconds for CSV versus 5.13 seconds for Parquet), and the amount of data scanned for that Athena query was 99% less (1.15TB scanned for CSV versus 2.69GB for Parquet)&lt;/p&gt;

&lt;p&gt;The cost to run that Athena query was 99.7% less ($5.75 for CSV versus $0.013 for Parquet)&lt;/p&gt;

&lt;p&gt;Parquet has the additional benefit of being an open data format that can be used by multiple querying and analytics tools in a data lake built on Amazon S3, particularly Amazon Athena, Amazon EMR, Amazon Redshift, and Amazon Redshift Spectrum.&lt;/p&gt;

&lt;p&gt;Additional options for performance optimization include rightsizing of the S3 objects to 128 MB, partitioning based on business dates which are typically used while querying.&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#future-proofing-the-data-lake"&gt;
  &lt;/a&gt;
  Future proofing the data lake
&lt;/h1&gt;

&lt;p&gt;A data lake built on AWS can immediately solve a broad range of business analytics challenges and quickly provide value to your business. However, business needs are constantly evolving, AWS and the analytics partner ecosystem are rapidly evolving and adding new services and capabilities, as businesses and their data lake users achieve more experience and analytics sophistication over time. Therefore, it’s important that the data lake can seamlessly and non-disruptively evolve as needed.&lt;/p&gt;

&lt;p&gt;AWS future proofs your data lake with a standardized storage solution that grows with your organization by ingesting and storing all of your business’s data assets on a platform with virtually unlimited scalability and well-defined APIs, and integrates with a wide variety of data processing tools. This allows you to add new capabilities to your data lake as you need them without infrastructure limitations or barriers. Additionally, you can perform agile analytics experiments against data lake assets to quickly explore new processing methods and tools, and then scale the promising ones into production without the need to build new infrastructure, duplicate and/or migrate data, and have users migrate to a new platform.&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#conclusion"&gt;
  &lt;/a&gt;
  Conclusion
&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Amazon S3 is a scalable, highly durable, and reliable service to build and manage a secure data lake at scale. You can ingest and store structured, semi-structured and unstructured data from wide variety of sources into a centralized platform. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With a data lake built on S3, you can use native AWS services to run big data analytics, artificial intelligence (AI) and machine learning (ML) applications to gain insights from your unstructured data sets. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You also have the flexibility to use your preferred analytics, AI and ML solutions from the Amazon Partner Network (APN). S3 provides a wide variety of service features to empower IT managers, storage administrators, and data scientists to enforce access policies, manage objects at scale, audit activities and secure data across their data lake built on S3.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In closing, a data lake built on AWS allows you to evolve your business around your data assets, and to use these data assets to quickly and agilely drive more business value and competitive differentiation without limits.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;
  &lt;a href="#reference"&gt;
  &lt;/a&gt;
  Reference
&lt;/h1&gt;

&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/building-data-lake-aws.html?did=wp_card&amp;amp;trk=wp_card"&gt;Original paper&lt;/a&gt;&lt;/p&gt;

</description>
      <category>aws</category>
      <category>cloud</category>
      <category>datascience</category>
      <category>beginners</category>
    </item>
    <item>
      <title>Play Blur onblur</title>
      <author>Mads Stoumann</author>
      <pubDate>Sat, 18 Dec 2021 13:59:44 +0000</pubDate>
      <link>https://dev.to/madsstoumann/play-blur-onblur-4f8</link>
      <guid>https://dev.to/madsstoumann/play-blur-onblur-4f8</guid>
      <description>&lt;p&gt;Yesterday, a colleague asked me what we should do &lt;code&gt;onfocusout&lt;/code&gt; on a task, we're working on. I told him it's called &lt;code&gt;onblur&lt;/code&gt; in JavaScript (we were both right, more on this later!) &lt;/p&gt;

&lt;p&gt;To this, &lt;em&gt;another&lt;/em&gt; colleague said:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“Is this a reference to the band? Otherwise, it's a shitty name”.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;(if you're not familiar with the band &lt;a href="https://en.wikipedia.org/wiki/Blur_(band)"&gt;Blur&lt;/a&gt; or the &lt;code&gt;onblur&lt;/code&gt;-event, this probably isn't funny at all) &lt;/p&gt;

&lt;p&gt;… maybe it &lt;strong&gt;is&lt;/strong&gt; a shitty name, but what if we could &lt;strong&gt;play&lt;/strong&gt; Blur &lt;code&gt;onblur&lt;/code&gt;?&lt;/p&gt;

&lt;p&gt;First, we need to add an audio-file:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight html"&gt;&lt;code&gt;&lt;span class="nt"&gt;&amp;lt;audio&lt;/span&gt; &lt;span class="na"&gt;src=&lt;/span&gt;&lt;span class="s"&gt;"woohoo.mp3"&lt;/span&gt; &lt;span class="na"&gt;hidden&lt;/span&gt; &lt;span class="na"&gt;id=&lt;/span&gt;&lt;span class="s"&gt;"woohoo"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&amp;lt;/audio&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;We'll give it an id, &lt;code&gt;woohoo&lt;/code&gt;, so we can easily reach it from code.&lt;/p&gt;

&lt;p&gt;Then, we'll add a fieldset, with some inputs:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight html"&gt;&lt;code&gt;&lt;span class="nt"&gt;&amp;lt;fieldset&lt;/span&gt; &lt;span class="na"&gt;id=&lt;/span&gt;&lt;span class="s"&gt;"app"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;legend&amp;gt;&lt;/span&gt;Play Blur onblur&lt;span class="nt"&gt;&amp;lt;/legend&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;label&amp;gt;&lt;/span&gt;♪ Woohoo&lt;span class="nt"&gt;&amp;lt;input&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;"text"&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&amp;lt;/label&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;label&amp;gt;&lt;/span&gt;♫ Woohoo&lt;span class="nt"&gt;&amp;lt;input&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;"text"&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&amp;lt;/label&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;label&amp;gt;&lt;/span&gt;♫ Woohoo&lt;span class="nt"&gt;&amp;lt;input&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;"text"&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&amp;lt;/label&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/fieldset&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;And finally, a little snippet of JavaScript, where we iterate the &lt;code&gt;elements&lt;/code&gt;-collection of the fieldset, and add an &lt;code&gt;onblur&lt;/code&gt;-eventListener to each input:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="p"&gt;[...&lt;/span&gt;&lt;span class="nx"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;elements&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="nx"&gt;forEach&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;input&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;addEventListener&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="s1"&gt;blur&lt;/span&gt;&lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;woohoo&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;play&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="p"&gt;}))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;And that's it! Now focus on a field and leave it ;-)&lt;/p&gt;

&lt;p&gt;&lt;iframe height="600" src="https://codepen.io/stoumann/embed/ExwmqOv?height=600&amp;amp;default-tab=result&amp;amp;embed-version=2"&gt;
&lt;/iframe&gt;
&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#real-usecases"&gt;
  &lt;/a&gt;
  Real use-cases
&lt;/h2&gt;

&lt;p&gt;Okay, so admittedly this is just plain &lt;em&gt;stupid&lt;/em&gt;, but maybe we could actually use this technique in conjunction with the &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/Constraint_validation"&gt;Constraint validation API&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;If a form-field is &lt;strong&gt;valid&lt;/strong&gt;, it could play a tiny "ding!"-sound, and if it's &lt;strong&gt;invalid&lt;/strong&gt;, a tiny "buzz"-sound.&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#-raw-focusout-endraw-"&gt;
  &lt;/a&gt;
  &lt;code&gt;focusout&lt;/code&gt;
&lt;/h2&gt;

&lt;p&gt;Turns out my colleague was right: there &lt;strong&gt;is&lt;/strong&gt; an event called &lt;code&gt;focusout&lt;/code&gt; (and it's counter-part: &lt;code&gt;onfocusin&lt;/code&gt;) – I'm just so old-school, that I've used &lt;code&gt;focus&lt;/code&gt; and &lt;code&gt;blur&lt;/code&gt; for ages! &lt;/p&gt;

&lt;p&gt;Unlike &lt;code&gt;blur&lt;/code&gt;, &lt;code&gt;onfocusout&lt;/code&gt; actually &lt;em&gt;bubbles up&lt;/em&gt;, meaning we can add a single event to the fieldset instead:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight javascript"&gt;&lt;code&gt;&lt;span class="nx"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;addEventListener&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="s1"&gt;focusout&lt;/span&gt;&lt;span class="dl"&gt;'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;woohoo&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;play&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;– but it's not funny at all to play Blur &lt;code&gt;onfocusout&lt;/code&gt;, is it? 😂&lt;/p&gt;

&lt;p&gt;In conslusion: Thank you to my colleagues for giving me an excuse to code something stupid &lt;strong&gt;and&lt;/strong&gt; learning about the &lt;code&gt;onfocusout&lt;/code&gt;-event!&lt;/p&gt;

</description>
      <category>html</category>
      <category>javascript</category>
      <category>webdev</category>
      <category>showdev</category>
    </item>
    <item>
      <title>Secure our website  using JWT (JSON Web Token) in nodeJS or expressJS</title>
      <author>Deepak</author>
      <pubDate>Sat, 18 Dec 2021 13:14:55 +0000</pubDate>
      <link>https://dev.to/deepakjaiswal/secure-our-website-using-jwt-json-web-token-in-nodejs-or-expressjs-5a7d</link>
      <guid>https://dev.to/deepakjaiswal/secure-our-website-using-jwt-json-web-token-in-nodejs-or-expressjs-5a7d</guid>
      <description>&lt;h2&gt;
  &lt;a href="#here-we-are-using-jwt-to-secure-our-application-or-website-from-unauthenticated-user-they-try-to-access-our-data"&gt;
  &lt;/a&gt;
  here we are using JWT to secure our application or website from unauthenticated user they try to access our data.
&lt;/h2&gt;

&lt;p&gt;In npmjs a library named is &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;jsonwebtoken &lt;/p&gt;

&lt;p&gt;npm i jsonwebtoken&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;if we only check user isAuthenticated or not we simply pass the middleware in between request and response&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#authjs"&gt;
  &lt;/a&gt;
  auth.js
&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;export default function getTokenFromUser(req: Request) {&lt;br&gt;
  const authorization = req.headers.token;&lt;br&gt;
  var decoded = jwt.verify(authorization, 'secret');&lt;br&gt;
  if (!decoded) {&lt;br&gt;
    throw new TokenError("No Authorization Header");&lt;br&gt;
  }&lt;br&gt;
  try {&lt;br&gt;
    const token = decoded?.split("User data ")[1];&lt;br&gt;
    return token;&lt;br&gt;
  } catch {&lt;br&gt;
    throw new TokenError("Invalid Token Format");&lt;br&gt;
  }&lt;br&gt;
}&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;we simple pass this auth of in between req,res&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;app.post('/api/post',auth,(req,res)=&amp;gt;{
//if some operation on code we use middleware
const token=jwt.sign({
  data: 'your data to store as token'
}, 'secret', { expiresIn: '1h' });

res.header('token',token).send("success")
});
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;we ensure that you can save your secret key in your config file.&lt;/p&gt;

</description>
    </item>
    <item>
      <title>9 Open Source Python Projects to join in 2022! </title>
      <author>Code_Jedi</author>
      <pubDate>Sat, 18 Dec 2021 12:16:45 +0000</pubDate>
      <link>https://dev.to/code_jedi/9-open-source-python-projects-to-join-in-2022-3c8o</link>
      <guid>https://dev.to/code_jedi/9-open-source-python-projects-to-join-in-2022-3c8o</guid>
      <description>&lt;p&gt;Contributing to open source projects is great for your reputation, skill development and knowledge as a developer.&lt;br&gt;
In this article, I will be going through 9 open source Python projects that you can join today!&lt;/p&gt;


&lt;h1&gt;
  &lt;a href="#9-django"&gt;
  &lt;/a&gt;
  9. Django
&lt;/h1&gt;

&lt;p&gt;Ah yes, the famous web development framework made for Python. It has more than 60k stars on Github and is used by millions of Python developers around the world.&lt;br&gt;
&lt;/p&gt;
&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--566lAguM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-5a155e1f9a670af7944dd5e12375bc76ed542ea80224905ecaf878b9157cdefc.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/django"&gt;
        django
      &lt;/a&gt; / &lt;a href="https://github.com/django/django"&gt;
        django
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      The Web framework for perfectionists with deadlines.
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="rst"&gt;
&lt;h1&gt;
Django&lt;/h1&gt;
&lt;p&gt;Django is a high-level Python web framework that encourages rapid development
and clean, pragmatic design. Thanks for checking it out.&lt;/p&gt;
&lt;p&gt;All documentation is in the "&lt;code&gt;docs&lt;/code&gt;" directory and online at
&lt;a href="https://docs.djangoproject.com/en/stable/" rel="nofollow"&gt;https://docs.djangoproject.com/en/stable/&lt;/a&gt;. If you're just getting started
here's how we recommend you read the docs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, read &lt;code&gt;docs/intro/install.txt&lt;/code&gt; for instructions on installing Django.&lt;/li&gt;
&lt;li&gt;Next, work through the tutorials in order (&lt;code&gt;docs/intro/tutorial01.txt&lt;/code&gt;
&lt;code&gt;docs/intro/tutorial02.txt&lt;/code&gt;, etc.).&lt;/li&gt;
&lt;li&gt;If you want to set up an actual deployment server, read
&lt;code&gt;docs/howto/deployment/index.txt&lt;/code&gt; for instructions.&lt;/li&gt;
&lt;li&gt;You'll probably want to read through the topical guides (in &lt;code&gt;docs/topics&lt;/code&gt;)
next; from there you can jump to the HOWTOs (in &lt;code&gt;docs/howto&lt;/code&gt;) for specific
problems, and check out the reference (&lt;code&gt;docs/ref&lt;/code&gt;) for gory details.&lt;/li&gt;
&lt;li&gt;See &lt;code&gt;docs/README&lt;/code&gt; for instructions on building an HTML version of the docs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Docs are updated rigorously. If you find any problems in the docs, or think
they should be…&lt;/p&gt;
&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/django/django"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;If you have experience with web development in Python and are looking to join an open source project, Django is the project for you!&lt;br&gt;
Start contributing to Django &lt;a href="https://docs.djangoproject.com/en/dev/internals/contributing/#:~:text=Contributing%20to%20Django%20Django%20is%20a%20community%20that,the%20framework%20itself%20or%20in%20the%20wider%20ecosystem."&gt;here&lt;/a&gt;.&lt;/p&gt;


&lt;h1&gt;
  &lt;a href="#8-scrapy"&gt;
  &lt;/a&gt;
  8. Scrapy
&lt;/h1&gt;

&lt;p&gt;Scrapy is the most popular Python web scraping library with over 40k stars on github.&lt;br&gt;
&lt;/p&gt;
&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--566lAguM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-5a155e1f9a670af7944dd5e12375bc76ed542ea80224905ecaf878b9157cdefc.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/scrapy"&gt;
        scrapy
      &lt;/a&gt; / &lt;a href="https://github.com/scrapy/scrapy"&gt;
        scrapy
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      Scrapy, a fast high-level web crawling &amp;amp; scraping framework for Python.
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="rst"&gt;
&lt;p&gt;&lt;a rel="noopener noreferrer" href="https://github.com/scrapy/scrapy/artwork/scrapy-logo.jpg"&gt;&lt;img alt="/artwork/scrapy-logo.jpg" src="https://res.cloudinary.com/practicaldev/image/fetch/s--RyQONOX8--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/scrapy/scrapy/artwork/scrapy-logo.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;
Scrapy&lt;/h2&gt;
&lt;a href="https://pypi.python.org/pypi/Scrapy" rel="nofollow"&gt;&lt;img alt="PyPI Version" src="https://camo.githubusercontent.com/6e6da7d9936802efc8790230859776a579f5d783bc33d45ad76acaeff07748e8/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f5363726170792e737667"&gt;
&lt;/a&gt;
&lt;a href="https://pypi.python.org/pypi/Scrapy" rel="nofollow"&gt;&lt;img alt="Supported Python Versions" src="https://camo.githubusercontent.com/a0ace508aac035c0728a9ccfacf3e1ee4e35064dfe46c0b106945a55f2c2bed6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f5363726170792e737667"&gt;
&lt;/a&gt;
&lt;a href="https://github.com/scrapy/scrapy/actions?query=workflow%3AUbuntu"&gt;&lt;img alt="Ubuntu" src="https://res.cloudinary.com/practicaldev/image/fetch/s--4UA9znDq--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/scrapy/scrapy/workflows/Ubuntu/badge.svg"&gt;
&lt;/a&gt;
&lt;a href="https://github.com/scrapy/scrapy/actions?query=workflow%3AmacOS"&gt;&lt;img alt="macOS" src="https://res.cloudinary.com/practicaldev/image/fetch/s--UQFgCtQK--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/scrapy/scrapy/workflows/macOS/badge.svg"&gt;
&lt;/a&gt;
&lt;a href="https://github.com/scrapy/scrapy/actions?query=workflow%3AWindows"&gt;&lt;img alt="Windows" src="https://res.cloudinary.com/practicaldev/image/fetch/s--QDYBLGMm--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/scrapy/scrapy/workflows/Windows/badge.svg"&gt;
&lt;/a&gt;
&lt;a href="https://pypi.python.org/pypi/Scrapy" rel="nofollow"&gt;&lt;img alt="Wheel Status" src="https://camo.githubusercontent.com/ef40cf44e0798077b1588fd1043671d41ceb8c2718f66e08e83e39bce7fa994f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f776865656c2d7965732d627269676874677265656e2e737667"&gt;
&lt;/a&gt;
&lt;a href="https://codecov.io/github/scrapy/scrapy?branch=master" rel="nofollow"&gt;&lt;img alt="Coverage report" src="https://camo.githubusercontent.com/ab60318bd0f92f54d96ce365912c21cd459821f8e939c014484ce213ee751601/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f7363726170792f7363726170792f6d61737465722e737667"&gt;
&lt;/a&gt;
&lt;a href="https://anaconda.org/conda-forge/scrapy" rel="nofollow"&gt;&lt;img alt="Conda Version" src="https://camo.githubusercontent.com/39dd0624e0dd8d9b71b9831bf4225ff67aaab3ee36427834f9c97e9baf5a3d73/68747470733a2f2f616e61636f6e64612e6f72672f636f6e64612d666f7267652f7363726170792f6261646765732f76657273696f6e2e737667"&gt;
&lt;/a&gt;
&lt;h3&gt;
Overview&lt;/h3&gt;
&lt;p&gt;Scrapy is a fast high-level web crawling and web scraping framework, used to
crawl websites and extract structured data from their pages. It can be used for
a wide range of purposes, from data mining to monitoring and automated testing.&lt;/p&gt;
&lt;p&gt;Scrapy is maintained by &lt;a href="https://www.zyte.com/" rel="nofollow"&gt;Zyte&lt;/a&gt; (formerly Scrapinghub) and &lt;a href="https://github.com/scrapy/scrapy/graphs/contributors"&gt;many other
contributors&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Check the Scrapy homepage at &lt;a href="https://scrapy.org" rel="nofollow"&gt;https://scrapy.org&lt;/a&gt; for more information
including a list of features.&lt;/p&gt;

&lt;h3&gt;
Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Python 3.6+&lt;/li&gt;
&lt;li&gt;Works on Linux, Windows, macOS, BSD&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
Install&lt;/h3&gt;
&lt;p&gt;The quick way:&lt;/p&gt;
&lt;pre&gt;pip install scrapy
&lt;/pre&gt;
&lt;p&gt;See the install section in the documentation at
&lt;a href="https://docs.scrapy.org/en/latest/intro/install.html" rel="nofollow"&gt;https://docs.scrapy.org/en/latest/intro/install.html&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h3&gt;
Documentation&lt;/h3&gt;
&lt;p&gt;Documentation is available online at &lt;a href="https://docs.scrapy.org/" rel="nofollow"&gt;https://docs.scrapy.org/&lt;/a&gt; and in the &lt;code&gt;docs&lt;/code&gt;
directory.&lt;/p&gt;

&lt;h3&gt;
Releases&lt;/h3&gt;
&lt;p&gt;You can check &lt;a href="https://docs.scrapy.org/en/latest/news.html" rel="nofollow"&gt;https://docs.scrapy.org/en/latest/news.html&lt;/a&gt; for the release notes.&lt;/p&gt;

&lt;h3&gt;
Community (blog, twitter, mail list, IRC)&lt;/h3&gt;
&lt;p&gt;See &lt;a href="https://scrapy.org/community/" rel="nofollow"&gt;https://scrapy.org/community/&lt;/a&gt; for details.&lt;/p&gt;

&lt;h3&gt;
Contributing&lt;/h3&gt;
&lt;p&gt;See &lt;a href="https://docs.scrapy.org/en/master/contributing.html" rel="nofollow"&gt;https://docs.scrapy.org/en/master/contributing.html&lt;/a&gt; for details.&lt;/p&gt;

&lt;h4&gt;
Code of Conduct&lt;/h4&gt;
&lt;p&gt;Please note that this project is released with a Contributor Code of Conduct
(see &lt;a href="https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md"&gt;https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md&lt;/a&gt;…&lt;/p&gt;
&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/scrapy/scrapy"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;If you're into web scraping with Python and want to work on improving the web scraping library used by thousands of Python developers, start contributing to Scrapy through &lt;a href="https://docs.scrapy.org/en/latest/contributing.html"&gt;this page&lt;/a&gt;.&lt;/p&gt;




&lt;h1&gt;
  &lt;a href="#7-scikitlearn"&gt;
  &lt;/a&gt;
  7. Scikit-Learn
&lt;/h1&gt;

&lt;p&gt;If you've been involved in machine learning with Python for some time, you've probably come across this library.&lt;br&gt;
&lt;/p&gt;
&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--566lAguM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-5a155e1f9a670af7944dd5e12375bc76ed542ea80224905ecaf878b9157cdefc.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/scikit-learn"&gt;
        scikit-learn
      &lt;/a&gt; / &lt;a href="https://github.com/scikit-learn/scikit-learn"&gt;
        scikit-learn
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      scikit-learn: machine learning in Python
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="rst"&gt;
&lt;p&gt;&lt;a href="https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&amp;amp;branchName=main" rel="nofollow"&gt;&lt;img alt="Azure" src="https://camo.githubusercontent.com/20eae0ee5a048822868c22e93ae8fb0fdcf46865a8efdd220e331c6d156d1975/68747470733a2f2f6465762e617a7572652e636f6d2f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f5f617069732f6275696c642f7374617475732f7363696b69742d6c6561726e2e7363696b69742d6c6561726e3f6272616e63684e616d653d6d61696e"&gt;&lt;/a&gt; &lt;a href="https://app.travis-ci.com/github/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="Travis" src="https://camo.githubusercontent.com/2bcce572d6e2cc6efb45f0257d1c407f6ce921e7fc87b808d4286b37eb6eecf8/68747470733a2f2f6170692e7472617669732d63692e636f6d2f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2e7376673f6272616e63683d6d61696e"&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="Codecov" src="https://camo.githubusercontent.com/4218349860fdfb6262472f8bb560a803676b400086465d55a6419d7c04f8697d/68747470733a2f2f636f6465636f762e696f2f67682f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f6272616e63682f6d61696e2f67726170682f62616467652e7376673f746f6b656e3d506b3847396767337939"&gt;&lt;/a&gt; &lt;a href="https://circleci.com/gh/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="CircleCI" src="https://camo.githubusercontent.com/76759ef257ed71d5317b754c37e2549d4baeae912a830a1d256c2001575614aa/68747470733a2f2f636972636c6563692e636f6d2f67682f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f747265652f6d61696e2e7376673f7374796c653d736869656c6426636972636c652d746f6b656e3d3a636972636c652d746f6b656e"&gt;&lt;/a&gt; &lt;a href="https://github.com/scikit-learn/scikit-learn/actions?query=workflow%3A%22Wheel+builder%22+event%3Aschedule"&gt;&lt;img alt="Nightly wheels" src="https://res.cloudinary.com/practicaldev/image/fetch/s--b0JLZYPo--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/scikit-learn/scikit-learn/workflows/Wheel%2520builder/badge.svg%3Fevent%3Dschedule"&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img alt="Black" src="https://camo.githubusercontent.com/d91ed7ac7abbd5a6102cbe988dd8e9ac21bde0a73d97be7603b891ad08ce3479/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667"&gt;
&lt;/a&gt; &lt;a href="https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9-blue" rel="nofollow"&gt;&lt;img alt="PythonVersion" src="https://camo.githubusercontent.com/acc13c1b7f5461decb407bb7549ffe06b887fa463a6529b784650aebfffa9886/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e37253230253743253230332e38253230253743253230332e392d626c7565"&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/scikit-learn" rel="nofollow"&gt;&lt;img alt="PyPi" src="https://camo.githubusercontent.com/4d84e5e8df28f93d6758e9a0827080d454cbe8c7779cfde157060184c97a49ac/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7363696b69742d6c6561726e"&gt;&lt;/a&gt; &lt;a href="https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="DOI" src="https://camo.githubusercontent.com/d15c5bb658dcbd6e1c6e889ac6332fcbde13031b233bc49229e36357a9b8f482/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f32313336392f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2e737667"&gt;
&lt;/a&gt; &lt;a href="https://scikit-learn.org/scikit-learn-benchmarks/" rel="nofollow"&gt;&lt;img alt="Benchmark" src="https://camo.githubusercontent.com/5ce113299e92d5264298d3e71c9bb0c44199849a70b57406a97fcc94057488d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f42656e63686d61726b656425323062792d6173762d626c7565"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a href="https://scikit-learn.org/" rel="nofollow"&gt;&lt;img alt="https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/logos/scikit-learn-logo.png" src="https://res.cloudinary.com/practicaldev/image/fetch/s--8pGzwEZ---/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/logos/scikit-learn-logo.png"&gt;&lt;/a&gt;
&lt;p&gt;&lt;strong&gt;scikit-learn&lt;/strong&gt; is a Python module for machine learning built on top of
SciPy and is distributed under the 3-Clause BSD license.&lt;/p&gt;
&lt;p&gt;The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the &lt;a href="https://scikit-learn.org/dev/about.html#authors" rel="nofollow"&gt;About us&lt;/a&gt; page
for a list of core contributors.&lt;/p&gt;
&lt;p&gt;It is currently maintained by a team of volunteers.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href="https://scikit-learn.org" rel="nofollow"&gt;https://scikit-learn.org&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;
Installation&lt;/h2&gt;
&lt;h3&gt;
Dependencies&lt;/h3&gt;
&lt;p&gt;scikit-learn requires:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python (&amp;gt;= 3.7)&lt;/li&gt;
&lt;li&gt;NumPy (&amp;gt;= 1.14.6)&lt;/li&gt;
&lt;li&gt;SciPy (&amp;gt;= 1.1.0)&lt;/li&gt;
&lt;li&gt;joblib (&amp;gt;= 0.11)&lt;/li&gt;
&lt;li&gt;threadpoolctl (&amp;gt;= 2.0.0)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Scikit-learn 0.20 was the last version to support Python 2.7 and Python 3.4.&lt;/strong&gt;
scikit-learn 0.23 and later require Python 3.6 or newer
scikit-learn 1.0 and later require Python 3.7 or newer.&lt;/p&gt;
&lt;p&gt;Scikit-learn plotting capabilities (i.e., functions start with &lt;code&gt;plot_&lt;/code&gt; and
classes end with "Display") require Matplotlib (&amp;gt;= 2.2.3)
For running the examples Matplotlib &amp;gt;= 2.2.3 is required.
A few examples require scikit-image &amp;gt;= 0.14.5, a…&lt;/p&gt;
&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/scikit-learn/scikit-learn"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;If you have experience with machine learning and data visualization with Python and want to contribute to one of the most popular Python machine learning libraries, start contributing to scikit-learn &lt;a href="https://scikit-learn.org/stable/developers/contributing.html"&gt;here&lt;/a&gt;.&lt;/p&gt;




&lt;h1&gt;
  &lt;a href="#6-pandas"&gt;
  &lt;/a&gt;
  6. Pandas
&lt;/h1&gt;

&lt;p&gt;Pandas is the most popular data analysis/manipulation library for Python.&lt;br&gt;
&lt;/p&gt;
&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--566lAguM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-5a155e1f9a670af7944dd5e12375bc76ed542ea80224905ecaf878b9157cdefc.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/pandas-dev"&gt;
        pandas-dev
      &lt;/a&gt; / &lt;a href="https://github.com/pandas-dev/pandas"&gt;
        pandas
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;div&gt;
  &lt;a rel="noopener noreferrer" href="https://camo.githubusercontent.com/981d48e57e23a4907cebc4eb481799b5882595ea978261f22a3e131dcd6ebee6/68747470733a2f2f70616e6461732e7079646174612e6f72672f7374617469632f696d672f70616e6461732e737667"&gt;&lt;img src="https://camo.githubusercontent.com/981d48e57e23a4907cebc4eb481799b5882595ea978261f22a3e131dcd6ebee6/68747470733a2f2f70616e6461732e7079646174612e6f72672f7374617469632f696d672f70616e6461732e737667"&gt;&lt;/a&gt;&lt;br&gt;
&lt;/div&gt;

&lt;h1&gt;
pandas: powerful Python data analysis toolkit&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://pypi.org/project/pandas/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/74cb3c88c43d4266705ae6ec7fddc1bbf603eb6d15bf2202ceb3416cd26b7c0d/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f70616e6461732e737667" alt="PyPI Latest Release"&gt;&lt;/a&gt;
&lt;a href="https://anaconda.org/anaconda/pandas/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3946e75037eae5c757c1f87dca16d9308da064caedba9966f56622d7426a0176/68747470733a2f2f616e61636f6e64612e6f72672f636f6e64612d666f7267652f70616e6461732f6261646765732f76657273696f6e2e737667" alt="Conda Latest Release"&gt;&lt;/a&gt;
&lt;a href="https://doi.org/10.5281/zenodo.3509134" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/dc1862109b52a1a02893ed309db67a17104c54b66e050dee3d23c858c2514140/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333530393133342e737667" alt="DOI"&gt;&lt;/a&gt;
&lt;a href="https://pypi.org/project/pandas/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/caf1bfd611737461f1d62e150d6753e05602727131be954051dd3a41dc901101/68747470733a2f2f696d672e736869656c64732e696f2f707970692f7374617475732f70616e6461732e737667" alt="Package Status"&gt;&lt;/a&gt;
&lt;a href="https://github.com/pandas-dev/pandas/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/810dee2f0ccde96614200a43630598d77394709e8699d531d6c6f7bbaaf53841/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f70616e6461732e737667" alt="License"&gt;&lt;/a&gt;
&lt;a href="https://dev.azure.com/pandas-dev/pandas/_build/latest?definitionId=1&amp;amp;branch=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ea3fd2ce7f1812de38f19234b73bf9058233f5f4e28b1967196121463ad4f49c/68747470733a2f2f6465762e617a7572652e636f6d2f70616e6461732d6465762f70616e6461732f5f617069732f6275696c642f7374617475732f70616e6461732d6465762e70616e6461733f6272616e63683d6d6173746572" alt="Azure Build Status"&gt;&lt;/a&gt;
&lt;a href="https://codecov.io/gh/pandas-dev/pandas" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a47a452ac4b46e7072ea605773a8175580826ef69da550de1f409f3d642429dc/68747470733a2f2f636f6465636f762e696f2f6769746875622f70616e6461732d6465762f70616e6461732f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="Coverage"&gt;&lt;/a&gt;
&lt;a href="https://pepy.tech/project/pandas" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/5b1c7be9058d78c455dc07ff2d2da7f4042ddafb85d1061a88ff11be195dcd23/68747470733a2f2f7374617469632e706570792e746563682f706572736f6e616c697a65642d62616467652f70616e6461733f706572696f643d6d6f6e746826756e6974733d696e7465726e6174696f6e616c5f73797374656d266c6566745f636f6c6f723d626c61636b2672696768745f636f6c6f723d6f72616e6765266c6566745f746578743d50795049253230646f776e6c6f6164732532307065722532306d6f6e7468" alt="Downloads"&gt;&lt;/a&gt;
&lt;a href="https://gitter.im/pydata/pandas" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/5dbac0213da25c445bd11f168587c11a200ba153ef3014e8408e462e410169b3/68747470733a2f2f6261646765732e6769747465722e696d2f4a6f696e253230436861742e737667" alt="Gitter"&gt;&lt;/a&gt;
&lt;a href="https://numfocus.org" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/42ca0da5f59b4fa9dd410434fa62cf8942c437d06669273fa7783c15d1be9cee/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706f776572656425323062792d4e756d464f4355532d6f72616e67652e7376673f7374796c653d666c617426636f6c6f72413d45313532334426636f6c6f72423d303037443841" alt="Powered by NumFOCUS"&gt;&lt;/a&gt;
&lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://camo.githubusercontent.com/d91ed7ac7abbd5a6102cbe988dd8e9ac21bde0a73d97be7603b891ad08ce3479/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" alt="Code style: black"&gt;&lt;/a&gt;
&lt;a href="https://pycqa.github.io/isort/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/fe4a658dd745f746410f961ae45d44355db1cc0e4c09c7877d265c1380248943/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f253230696d706f7274732d69736f72742d2532333136373462313f7374796c653d666c6174266c6162656c436f6c6f723d656638333336" alt="Imports: isort"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;
What is it?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;pandas&lt;/strong&gt; is a Python package that provides fast, flexible, and expressive data
structures designed to make working with "relational" or "labeled" data both
easy and intuitive. It aims to be the fundamental high-level building block for
doing practical, &lt;strong&gt;real world&lt;/strong&gt; data analysis in Python. Additionally, it has
the broader goal of becoming &lt;strong&gt;the most powerful and flexible open source data
analysis / manipulation tool available in any language&lt;/strong&gt;. It is already well on
its way towards this goal.&lt;/p&gt;
&lt;h2&gt;
Main Features&lt;/h2&gt;
&lt;p&gt;Here are just a few of the things that pandas does well:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Easy handling of &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html" rel="nofollow"&gt;&lt;strong&gt;missing data&lt;/strong&gt;&lt;/a&gt; (represented as
&lt;code&gt;NaN&lt;/code&gt;, &lt;code&gt;NA&lt;/code&gt;, or &lt;code&gt;NaT&lt;/code&gt;) in floating point as well as non-floating point data&lt;/li&gt;
&lt;li&gt;Size mutability: columns can be &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#column-selection-addition-deletion" rel="nofollow"&gt;&lt;strong&gt;inserted and
deleted&lt;/strong&gt;&lt;/a&gt; from DataFrame and higher dimensional
objects&lt;/li&gt;
&lt;li&gt;Automatic and explicit &lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html?highlight=alignment#intro-to-data-structures" rel="nofollow"&gt;&lt;strong&gt;data alignment&lt;/strong&gt;&lt;/a&gt;: objects can
be explicitly aligned…&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/pandas-dev/pandas"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;If you know how to work with data in Python and want to help build the future of data analysis/manipulation in Python, start contributing to pandas &lt;a href="https://pandas.pydata.org/pandas-docs/stable/development/contributing.html"&gt;here&lt;/a&gt;.&lt;/p&gt;




&lt;h1&gt;
  &lt;a href="#5-flask"&gt;
  &lt;/a&gt;
  5. Flask
&lt;/h1&gt;

&lt;p&gt;Flask is another popular Python web development library with over 50k stars on Github.&lt;br&gt;
&lt;/p&gt;
&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--566lAguM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-5a155e1f9a670af7944dd5e12375bc76ed542ea80224905ecaf878b9157cdefc.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/pallets"&gt;
        pallets
      &lt;/a&gt; / &lt;a href="https://github.com/pallets/flask"&gt;
        flask
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      The Python micro framework for building web applications.
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="rst"&gt;
&lt;h1&gt;
Flask&lt;/h1&gt;
&lt;p&gt;Flask is a lightweight &lt;a href="https://wsgi.readthedocs.io/" rel="nofollow"&gt;WSGI&lt;/a&gt; web application framework. It is designed
to make getting started quick and easy, with the ability to scale up to
complex applications. It began as a simple wrapper around &lt;a href="https://werkzeug.palletsprojects.com/" rel="nofollow"&gt;Werkzeug&lt;/a&gt;
and &lt;a href="https://jinja.palletsprojects.com/" rel="nofollow"&gt;Jinja&lt;/a&gt; and has become one of the most popular Python web
application frameworks.&lt;/p&gt;
&lt;p&gt;Flask offers suggestions, but doesn't enforce any dependencies or
project layout. It is up to the developer to choose the tools and
libraries they want to use. There are many extensions provided by the
community that make adding new functionality easy.&lt;/p&gt;
&lt;h2&gt;
Installing&lt;/h2&gt;
&lt;p&gt;Install and update using &lt;a href="https://pip.pypa.io/en/stable/getting-started/" rel="nofollow"&gt;pip&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;$ pip install -U Flask
&lt;/pre&gt;
&lt;h2&gt;
A Simple Example&lt;/h2&gt;
&lt;div class="highlight highlight-source-python position-relative overflow-auto js-code-highlight"&gt;
&lt;pre&gt;&lt;span class="pl-c"&gt;# save this as app.py&lt;/span&gt;
&lt;span class="pl-k"&gt;from&lt;/span&gt; &lt;span class="pl-s1"&gt;flask&lt;/span&gt; &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-v"&gt;Flask&lt;/span&gt;

&lt;span class="pl-s1"&gt;app&lt;/span&gt; &lt;span class="pl-c1"&gt;=&lt;/span&gt; &lt;span class="pl-v"&gt;Flask&lt;/span&gt;(&lt;span class="pl-s1"&gt;__name__&lt;/span&gt;)

&lt;span class="pl-en"&gt;@&lt;span class="pl-s1"&gt;app&lt;/span&gt;.&lt;span class="pl-en"&gt;route&lt;/span&gt;(&lt;span class="pl-s"&gt;"/"&lt;/span&gt;)&lt;/span&gt;
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;hello&lt;/span&gt;():
    &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-s"&gt;"Hello, World!"&lt;/span&gt;&lt;/pre&gt;

&lt;/div&gt;
&lt;pre&gt;$ flask run
  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
&lt;/pre&gt;

&lt;h2&gt;
Contributing&lt;/h2&gt;
&lt;p&gt;For guidance on setting…&lt;/p&gt;
&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/pallets/flask"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;If you're looking to help build the future of web development with Python, start contributing to flask &lt;a href="https://azcv.readthedocs.io/en/latest/contributing.html"&gt;here&lt;/a&gt;.&lt;/p&gt;




&lt;h1&gt;
  &lt;a href="#4-requests"&gt;
  &lt;/a&gt;
  4. Requests
&lt;/h1&gt;

&lt;p&gt;Requests, the OG library used by millions that is used for making HTTP requests with Python. This might be pretty underwhelming, but you see, the requests library is used to connect to API endpoints, authenticate web connections, scrape data from the web, test web endpoints and more!&lt;br&gt;
Without the requests library, Python wouldn't be where it is today.&lt;br&gt;
&lt;/p&gt;
&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--566lAguM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-5a155e1f9a670af7944dd5e12375bc76ed542ea80224905ecaf878b9157cdefc.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/psf"&gt;
        psf
      &lt;/a&gt; / &lt;a href="https://github.com/psf/requests"&gt;
        requests
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      A simple, yet elegant, HTTP library.
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;h1&gt;
Requests&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Requests&lt;/strong&gt; is a simple, yet elegant, HTTP library.&lt;/p&gt;
&lt;div class="highlight highlight-source-python position-relative overflow-auto js-code-highlight"&gt;
&lt;pre&gt;&lt;span class="pl-c1"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-s1"&gt;requests&lt;/span&gt;
&lt;span class="pl-c1"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-s1"&gt;r&lt;/span&gt; &lt;span class="pl-c1"&gt;=&lt;/span&gt; &lt;span class="pl-s1"&gt;requests&lt;/span&gt;.&lt;span class="pl-en"&gt;get&lt;/span&gt;(&lt;span class="pl-s"&gt;'https://api.github.com/user'&lt;/span&gt;, &lt;span class="pl-s1"&gt;auth&lt;/span&gt;&lt;span class="pl-c1"&gt;=&lt;/span&gt;(&lt;span class="pl-s"&gt;'user'&lt;/span&gt;, &lt;span class="pl-s"&gt;'pass'&lt;/span&gt;))
&lt;span class="pl-c1"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-s1"&gt;r&lt;/span&gt;.&lt;span class="pl-s1"&gt;status_code&lt;/span&gt;
&lt;span class="pl-c1"&gt;200&lt;/span&gt;
&lt;span class="pl-c1"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-s1"&gt;r&lt;/span&gt;.&lt;span class="pl-s1"&gt;headers&lt;/span&gt;[&lt;span class="pl-s"&gt;'content-type'&lt;/span&gt;]
&lt;span class="pl-s"&gt;'application/json; charset=utf8'&lt;/span&gt;
&lt;span class="pl-c1"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-s1"&gt;r&lt;/span&gt;.&lt;span class="pl-s1"&gt;encoding&lt;/span&gt;
&lt;span class="pl-s"&gt;'utf-8'&lt;/span&gt;
&lt;span class="pl-c1"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-s1"&gt;r&lt;/span&gt;.&lt;span class="pl-s1"&gt;text&lt;/span&gt;
&lt;span class="pl-s"&gt;'{"type":"User"...'&lt;/span&gt;
&lt;span class="pl-c1"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-s1"&gt;r&lt;/span&gt;.&lt;span class="pl-en"&gt;json&lt;/span&gt;()
{&lt;span class="pl-s"&gt;'disk_usage'&lt;/span&gt;: &lt;span class="pl-c1"&gt;368627&lt;/span&gt;, &lt;span class="pl-s"&gt;'private_gists'&lt;/span&gt;: &lt;span class="pl-c1"&gt;484&lt;/span&gt;, ...}&lt;/pre&gt;

&lt;/div&gt;
&lt;p&gt;Requests allows you to send HTTP/1.1 requests extremely easily. There’s no need to manually add query strings to your URLs, or to form-encode your &lt;code&gt;PUT&lt;/code&gt; &amp;amp; &lt;code&gt;POST&lt;/code&gt; data — but nowadays, just use the &lt;code&gt;json&lt;/code&gt; method!&lt;/p&gt;
&lt;p&gt;Requests is one of the most downloaded Python packages today, pulling in around &lt;code&gt;30M downloads / week&lt;/code&gt;— according to GitHub, Requests is currently &lt;a href="https://github.com/psf/requests/network/dependents?package_id=UGFja2FnZS01NzA4OTExNg%3D%3D"&gt;depended upon&lt;/a&gt; by &lt;code&gt;500,000+&lt;/code&gt; repositories. You may certainly put your trust in this code.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pepy.tech/project/requests" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d4f511b33402ca109f324ccbd2a1106288cbeec458cd8073730503441b2ab936/68747470733a2f2f706570792e746563682f62616467652f72657175657374732f6d6f6e7468" alt="Downloads"&gt;&lt;/a&gt;
&lt;a href="https://pypi.org/project/requests" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6749e2352acc2dd44be9bcf1cfede548894131b6798c76cedb647c5448df5433/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f72657175657374732e737667" alt="Supported Versions"&gt;&lt;/a&gt;
&lt;a href="https://github.com/psf/requests/graphs/contributors"&gt;&lt;img src="https://camo.githubusercontent.com/546be78b159c18e98376d504e9ea6a7450c1899f1c39b8102ac4494b567cd1ae/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f7073662f72657175657374732e737667" alt="Contributors"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;
Installing Requests&lt;/h2&gt;…&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/psf/requests"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;Start contributing to requests &lt;a href="https://docs.python-requests.org/en/latest/dev/contributing/"&gt;here&lt;/a&gt;.&lt;/p&gt;




&lt;h1&gt;
  &lt;a href="#3-matplotlib"&gt;
  &lt;/a&gt;
  3. Matplotlib
&lt;/h1&gt;

&lt;p&gt;Matplotlib is the most popular data visualization library for Python. &lt;br&gt;
&lt;/p&gt;
&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--566lAguM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-5a155e1f9a670af7944dd5e12375bc76ed542ea80224905ecaf878b9157cdefc.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/matplotlib"&gt;
        matplotlib
      &lt;/a&gt; / &lt;a href="https://github.com/matplotlib/matplotlib"&gt;
        matplotlib
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      matplotlib: plotting with Python
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="rst"&gt;
&lt;p&gt;&lt;a href="https://badge.fury.io/py/matplotlib" rel="nofollow"&gt;&lt;img alt="PyPi" src="https://camo.githubusercontent.com/14cb8dc3674d1dc61b3251b07b0cffd1d17fc7edf8171dceca934826201f4891/68747470733a2f2f62616467652e667572792e696f2f70792f6d6174706c6f746c69622e737667"&gt;
&lt;/a&gt; &lt;a href="https://pepy.tech/project/matplotlib" rel="nofollow"&gt;&lt;img alt="Downloads" src="https://camo.githubusercontent.com/cb6bfebe68a6eb430e0c953877bb8cf23be68337aec6dcab6bd65ae0a71c7bee/68747470733a2f2f706570792e746563682f62616467652f6d6174706c6f746c69622f6d6f6e7468"&gt;&lt;/a&gt; &lt;a href="https://numfocus.org" rel="nofollow"&gt;&lt;img alt="NUMFocus" src="https://camo.githubusercontent.com/42ca0da5f59b4fa9dd410434fa62cf8942c437d06669273fa7783c15d1be9cee/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706f776572656425323062792d4e756d464f4355532d6f72616e67652e7376673f7374796c653d666c617426636f6c6f72413d45313532334426636f6c6f72423d303037443841"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://discourse.matplotlib.org" rel="nofollow"&gt;&lt;img alt="DiscourseBadge" src="https://camo.githubusercontent.com/37a2ae902beb5bcbdd5898b67f91662067241d07e35d269d6fe9de0aec60fca7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f68656c705f666f72756d2d646973636f757273652d626c75652e737667"&gt;
&lt;/a&gt; &lt;a href="https://gitter.im/matplotlib/matplotlib" rel="nofollow"&gt;&lt;img alt="Gitter" src="https://camo.githubusercontent.com/d80272f79ab7aeed2ea3bfbb2a2728c5914681414a7f91ccad955a622d42961e/68747470733a2f2f6261646765732e6769747465722e696d2f6d6174706c6f746c69622f6d6174706c6f746c69622e737667"&gt;
&lt;/a&gt; &lt;a href="https://github.com/matplotlib/matplotlib/issues"&gt;&lt;img alt="GitHubIssues" src="https://camo.githubusercontent.com/5e3863bec2a88b0ad9cdfc503064abc3c85bc3bd0e93a2550b50ae11efcb9917/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f69737375655f747261636b696e672d6769746875622d626c75652e737667"&gt;
&lt;/a&gt; &lt;a href="https://git-scm.com/book/en/v2/GitHub-Contributing-to-a-Project" rel="nofollow"&gt;&lt;img alt="GitTutorial" src="https://camo.githubusercontent.com/37e48b3ca9130628708aa78490bea23af8a3de7c42d66a54d8b433ca4c70a3c1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50522d57656c636f6d652d2532334646383330302e7376673f"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/matplotlib/matplotlib/actions?query=workflow%3ATests"&gt;&lt;img alt="GitHubActions" src="https://res.cloudinary.com/practicaldev/image/fetch/s--kYJNaURl--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://github.com/matplotlib/matplotlib/workflows/Tests/badge.svg"&gt;
&lt;/a&gt; &lt;a href="https://dev.azure.com/matplotlib/matplotlib/_build/latest?definitionId=1&amp;amp;branchName=main" rel="nofollow"&gt;&lt;img alt="AzurePipelines" src="https://camo.githubusercontent.com/ab26ffde10bfb68297abef2f1d46abf6271cee9d27ba35005dcfb1a2b60135bc/68747470733a2f2f6465762e617a7572652e636f6d2f6d6174706c6f746c69622f6d6174706c6f746c69622f5f617069732f6275696c642f7374617475732f6d6174706c6f746c69622e6d6174706c6f746c69623f6272616e63684e616d653d6d61696e"&gt;&lt;/a&gt; &lt;a href="https://ci.appveyor.com/project/matplotlib/matplotlib" rel="nofollow"&gt;&lt;img alt="AppVeyor" src="https://camo.githubusercontent.com/39a7add7af7ed5aab363c06749bac4047c8b65726c9b66053039ba6de4795d6c/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f6769746875622f6d6174706c6f746c69622f6d6174706c6f746c69623f6272616e63683d6d61696e267376673d74727565"&gt;&lt;/a&gt; &lt;a href="https://codecov.io/github/matplotlib/matplotlib?branch=main" rel="nofollow"&gt;&lt;img alt="Codecov" src="https://camo.githubusercontent.com/c004be0e30cc47e3b87e638d69df0fef792d5eb85648813f860f33450cb7109e/68747470733a2f2f636f6465636f762e696f2f6769746875622f6d6174706c6f746c69622f6d6174706c6f746c69622f62616467652e7376673f6272616e63683d6d61696e26736572766963653d676974687562"&gt;&lt;/a&gt; &lt;a href="https://lgtm.com/projects/g/matplotlib/matplotlib" rel="nofollow"&gt;&lt;img alt="LGTM" src="https://camo.githubusercontent.com/720d31b5d417a59438fef42b3c85cd6c7e15464e0777fcb8c2679519558513f5/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f707974686f6e2f6769746875622f6d6174706c6f746c69622f6d6174706c6f746c69622e7376673f6c6f676f3d6c67746d266c6f676f57696474683d3138"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a rel="noopener noreferrer" href="https://camo.githubusercontent.com/109927a15915074d15313889468aa9aa688de3b9e38cc4359a01f665d351114e/68747470733a2f2f6d6174706c6f746c69622e6f72672f5f7374617469632f6c6f676f322e737667"&gt;&lt;img src="https://camo.githubusercontent.com/109927a15915074d15313889468aa9aa688de3b9e38cc4359a01f665d351114e/68747470733a2f2f6d6174706c6f746c69622e6f72672f5f7374617469632f6c6f676f322e737667"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Matplotlib is a comprehensive library for creating static, animated, and
interactive visualizations in Python.&lt;/p&gt;
&lt;p&gt;Check out our &lt;a href="https://matplotlib.org/" rel="nofollow"&gt;home page&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;&lt;a rel="noopener noreferrer" href="https://camo.githubusercontent.com/daba81876385ae6bbe523f69083f914dcae4c3faaee87bdd1274197c1c9551c1/68747470733a2f2f6d6174706c6f746c69622e6f72672f5f7374617469632f726561646d655f707265766965772e706e67"&gt;&lt;img alt="https://matplotlib.org/_static/readme_preview.png" src="https://camo.githubusercontent.com/daba81876385ae6bbe523f69083f914dcae4c3faaee87bdd1274197c1c9551c1/68747470733a2f2f6d6174706c6f746c69622e6f72672f5f7374617469632f726561646d655f707265766965772e706e67"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Matplotlib produces publication-quality figures in a variety of hardcopy
formats and interactive environments across platforms. Matplotlib can be used
in Python scripts, the Python and IPython shell, web application servers, and
various graphical user interface toolkits.&lt;/p&gt;
&lt;h2&gt;
Install&lt;/h2&gt;
&lt;p&gt;For installation instructions and requirements, see the &lt;a href="https://matplotlib.org/stable/users/installing/index.html" rel="nofollow"&gt;install documentation&lt;/a&gt; or
&lt;a href="https://github.com/matplotlib/matplotlibdoc/users/installing/index.rst"&gt;installing.rst&lt;/a&gt; in the source.&lt;/p&gt;
&lt;h2&gt;
Contribute&lt;/h2&gt;
&lt;p&gt;You've discovered a bug or something else you want to change - excellent!&lt;/p&gt;
&lt;p&gt;You've worked out a way to fix it – even better!&lt;/p&gt;
&lt;p&gt;You want to tell us about it – best of all!&lt;/p&gt;
&lt;p&gt;Start at the &lt;a href="https://matplotlib.org/devdocs/devel/contributing.html" rel="nofollow"&gt;contributing guide&lt;/a&gt;!&lt;/p&gt;
&lt;h2&gt;
Contact&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://discourse.matplotlib.org/" rel="nofollow"&gt;Discourse&lt;/a&gt; is the discussion forum for
general questions and discussions and our recommended starting point.&lt;/p&gt;
&lt;p&gt;Our active mailing lists (which are mirrored on Discourse) are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href="https://mail.python.org/mailman/listinfo/matplotlib-users" rel="nofollow"&gt;Users&lt;/a&gt; mailing
list: &lt;a href="https://github.com/matplotlib/matplotlibmailto:matplotlib-users@python.org"&gt;matplotlib-users@python.org&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://mail.python.org/mailman/listinfo/matplotlib-announce" rel="nofollow"&gt;Announcement&lt;/a&gt; mailing
list: &lt;a href="https://github.com/matplotlib/matplotlibmailto:matplotlib-announce@python.org"&gt;matplotlib-announce@python.org&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://mail.python.org/mailman/listinfo/matplotlib-devel" rel="nofollow"&gt;Development&lt;/a&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/matplotlib/matplotlib"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;If you're involved with data visualization with Python and want to contribute to the most used and versatile data visualization library in Python, start contributing to Matplotlib &lt;a href="https://matplotlib.org/stable/devel/contributing.html"&gt;here&lt;/a&gt;.&lt;/p&gt;




&lt;h1&gt;
  &lt;a href="#2-keras"&gt;
  &lt;/a&gt;
  2. Keras
&lt;/h1&gt;

&lt;p&gt;With over 50k stars on Github, Keras is a simple, versatile and robust library for building neural networks with Python.&lt;br&gt;
&lt;/p&gt;
&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--566lAguM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-5a155e1f9a670af7944dd5e12375bc76ed542ea80224905ecaf878b9157cdefc.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/keras-team"&gt;
        keras-team
      &lt;/a&gt; / &lt;a href="https://github.com/keras-team/keras"&gt;
        keras
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      Deep Learning for humans
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;h1&gt;
Keras: Deep Learning for humans&lt;/h1&gt;
&lt;p&gt;&lt;a rel="noopener noreferrer" href="https://camo.githubusercontent.com/906e661107a3bc03104ca5d88336d1f4b0e80fdcac65efaf7904041d371c747f/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6b657261732e696f2f696d672f6b657261732d6c6f676f2d323031382d6c617267652d313230302e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/906e661107a3bc03104ca5d88336d1f4b0e80fdcac65efaf7904041d371c747f/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6b657261732e696f2f696d672f6b657261732d6c6f676f2d323031382d6c617267652d313230302e706e67" alt="Keras logo"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repository hosts the development of the Keras library
Read the documentation at &lt;a href="https://keras.io/" rel="nofollow"&gt;keras.io&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;
About Keras&lt;/h2&gt;
&lt;p&gt;Keras is a deep learning API written in Python
running on top of the machine learning platform &lt;a href="https://github.com/tensorflow/tensorflow"&gt;TensorFlow&lt;/a&gt;
It was developed with a focus on enabling fast experimentation.
&lt;em&gt;Being able to go from idea to result as fast as possible is key to doing good research.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Keras is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;Simple&lt;/strong&gt; -- but not simplistic. Keras reduces developer &lt;em&gt;cognitive load&lt;/em&gt;
to free you to focus on the parts of the problem that really matter.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Flexible&lt;/strong&gt; -- Keras adopts the principle of &lt;em&gt;progressive disclosure of complexity&lt;/em&gt;
simple workflows should be quick and easy, while arbitrarily advanced workflows
should be &lt;em&gt;possible&lt;/em&gt; via a clear path that builds upon what you've already learned.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Powerful&lt;/strong&gt; -- Keras provides industry-strength performance and scalability
it is used by organizations and companies including NASA, YouTube,…&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/keras-team/keras"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;Start contributing to Keras &lt;a href="https://github.com/keras-team/keras-contrib"&gt;here&lt;/a&gt;.&lt;/p&gt;




&lt;h1&gt;
  &lt;a href="#1-tensorflow"&gt;
  &lt;/a&gt;
  1. TensorFlow
&lt;/h1&gt;

&lt;p&gt;TensorFlow is a sophisticated Python neural network, deep learning and machine learning library used by millions with over 160k stars on Github. &lt;br&gt;
&lt;/p&gt;
&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--566lAguM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-5a155e1f9a670af7944dd5e12375bc76ed542ea80224905ecaf878b9157cdefc.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/tensorflow"&gt;
        tensorflow
      &lt;/a&gt; / &lt;a href="https://github.com/tensorflow/tensorflow"&gt;
        tensorflow
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      An Open Source Machine Learning Framework for Everyone
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;div&gt;
  &lt;a rel="noopener noreferrer" href="https://camo.githubusercontent.com/aeb4f612bd9b40d81c62fcbebd6db44a5d4344b8b962be0138817e18c9c06963/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74665f6c6f676f5f686f72697a6f6e74616c2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/aeb4f612bd9b40d81c62fcbebd6db44a5d4344b8b962be0138817e18c9c06963/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74665f6c6f676f5f686f72697a6f6e74616c2e706e67"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href="https://badge.fury.io/py/tensorflow" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a7b5b417de938c1faf3602c7f48f26fde8761a977be85390fd6c0d191e210ba8/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f74656e736f72666c6f772e7376673f7374796c653d706c6173746963" alt="Python"&gt;&lt;/a&gt;
&lt;a href="https://badge.fury.io/py/tensorflow" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52c9f14cae5a90816da6b63cc5c6b57c20fbe2788e643cf0ab8160d3cd9a9ecf/68747470733a2f2f62616467652e667572792e696f2f70792f74656e736f72666c6f772e737667" alt="PyPI"&gt;&lt;/a&gt;
&lt;a href="https://doi.org/10.5281/zenodo.4724125" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/cbb1b583e9445f1dc96b629d833b9f51c1b32971f0def04f0bf4be181d08bff1/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e343732343132352e737667" alt="DOI"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class="table-wrapper-paragraph"&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;&lt;code&gt;Documentation&lt;/code&gt;&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.tensorflow.org/api_docs/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/5fee71a94d467d0fa33c4469ad6e6ef356042a8ca784a0c0eae6a04796b77d38/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6170692d7265666572656e63652d626c75652e737667" alt="Documentation"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href="https://www.tensorflow.org/" rel="nofollow"&gt;TensorFlow&lt;/a&gt; is an end-to-end open source platform
for machine learning. It has a comprehensive, flexible ecosystem of
&lt;a href="https://www.tensorflow.org/resources/tools" rel="nofollow"&gt;tools&lt;/a&gt;
&lt;a href="https://www.tensorflow.org/resources/libraries-extensions" rel="nofollow"&gt;libraries&lt;/a&gt;, and
&lt;a href="https://www.tensorflow.org/community" rel="nofollow"&gt;community&lt;/a&gt; resources that lets
researchers push the state-of-the-art in ML and developers easily build and
deploy ML-powered applications.&lt;/p&gt;
&lt;p&gt;TensorFlow was originally developed by researchers and engineers working on the
Google Brain team within Google's Machine Intelligence Research organization to
conduct machine learning and deep neural networks research. The system is
general enough to be applicable in a wide variety of other domains, as well.&lt;/p&gt;
&lt;p&gt;TensorFlow provides stable &lt;a href="https://www.tensorflow.org/api_docs/python" rel="nofollow"&gt;Python&lt;/a&gt;
and &lt;a href="https://www.tensorflow.org/api_docs/cc" rel="nofollow"&gt;C++&lt;/a&gt; APIs, as well as
non-guaranteed backward compatible API for
&lt;a href="https://www.tensorflow.org/api_docs" rel="nofollow"&gt;other languages&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Keep up-to-date with release announcements and security updates by subscribing
to
&lt;a href="https://groups.google.com/a/tensorflow.org/forum/#!forum/announce" rel="nofollow"&gt;announce@tensorflow.org&lt;/a&gt;
See all the &lt;a href="https://www.tensorflow.org/community/forums" rel="nofollow"&gt;mailing lists&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;
Install&lt;/h2&gt;
&lt;p&gt;See the &lt;a href="https://www.tensorflow.org/install" rel="nofollow"&gt;TensorFlow install guide&lt;/a&gt; for the
&lt;a href="https://www.tensorflow.org/install/pip" rel="nofollow"&gt;pip package&lt;/a&gt;, to
&lt;a href="https://www.tensorflow.org/install/gpu" rel="nofollow"&gt;enable GPU support&lt;/a&gt;, use a
&lt;a href="https://www.tensorflow.org/install/docker" rel="nofollow"&gt;Docker container&lt;/a&gt;, and
&lt;a href="https://www.tensorflow.org/install/source" rel="nofollow"&gt;build from source&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;…&lt;/p&gt;
&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/tensorflow/tensorflow"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;Start contributing to TensorFlow &lt;a href="https://www.tensorflow.org/community/contribute"&gt;here&lt;/a&gt;.&lt;/p&gt;




&lt;h1&gt;
  &lt;a href="#conclusion"&gt;
  &lt;/a&gt;
  Conclusion
&lt;/h1&gt;

&lt;p&gt;I hope that in this article, you've found the open source project that you would like to contribute to, and help build the future of Python.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#educative"&gt;
  &lt;/a&gt;
  Educative
&lt;/h2&gt;

&lt;p&gt;Before I end this article, I'd like to recommend &lt;a href="https://bit.ly/3rVIDoN"&gt;Educative&lt;/a&gt; for developers looking to learn.&lt;br&gt;
&lt;strong&gt;Why Educative?&lt;/strong&gt;&lt;br&gt;
It is home to hundreds of development courses, hands on tutorials, guides and demonstrations to help you stay ahead of the curve in your development journey.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;You can get started with Educative &lt;a href="https://bit.ly/3EeDfAi"&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Byeeee👋&lt;/p&gt;

</description>
      <category>python</category>
      <category>programming</category>
      <category>opensource</category>
      <category>github</category>
    </item>
    <item>
      <title>Let's Install Heroku CLI On Termux an android app.</title>
      <author>Shrikant Dhayje</author>
      <pubDate>Sat, 18 Dec 2021 11:53:05 +0000</pubDate>
      <link>https://dev.to/shriekdj/lets-install-heroku-cli-on-termux-an-android-app-19hb</link>
      <guid>https://dev.to/shriekdj/lets-install-heroku-cli-on-termux-an-android-app-19hb</guid>
      <description>&lt;p&gt;Today I Tried To Install Heroku CLI On Termux But It Does Not Worked i Tried To Install Nodejs First For Building The Heroku CLI From Built but It Does Not Work At All.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;apt install heroku&lt;/code&gt;&lt;br&gt;
does not work said heroku not available&lt;br&gt;
snapd package also unable to install on termux.&lt;br&gt;
and also when tried same error&lt;br&gt;
&lt;code&gt;npm install heroku&lt;/code&gt;&lt;br&gt;
it shows error &lt;em&gt;sudo&lt;/em&gt; permissions need&lt;/p&gt;

&lt;p&gt;At Last This Last Commands Worked&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;apt-get &lt;span class="nb"&gt;install &lt;/span&gt;proot-distro

proot-distro &lt;span class="nb"&gt;install &lt;/span&gt;ubuntu

proot-distro login ubuntu

apt-get &lt;span class="nb"&gt;install &lt;/span&gt;curl

curl https://cli-assets.heroku.com/install-ubuntu.sh | sh
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;You Can Run Ubuntu and Also Other Distro Like ArchLinux, Fedora, OpenSuse, etc.,&lt;/p&gt;

&lt;p&gt;I Know It Is Not A Good Practice to Install these Things On Android Mobile But termux also works on Android Television.&lt;/p&gt;

&lt;p&gt;You Can Use Your Android TV As Backup Linux Machine think About It.&lt;/p&gt;

</description>
      <category>termux</category>
      <category>ubuntu</category>
      <category>heroku</category>
      <category>android</category>
    </item>
    <item>
      <title>How Can You Get Noticed Without a Github Account?</title>
      <author>Fernando Doglio</author>
      <pubDate>Sat, 18 Dec 2021 11:47:14 +0000</pubDate>
      <link>https://dev.to/deleteman123/how-can-you-get-noticed-without-a-github-account-2h1h</link>
      <guid>https://dev.to/deleteman123/how-can-you-get-noticed-without-a-github-account-2h1h</guid>
      <description>&lt;p&gt;I’ve had conversations with many developers trying to understand how to get their first job without any experience. And my go-to solution is: create a Github account and start uploading code projects you’ve worked on. Either that, or create new ones and publish them there.&lt;/p&gt;

&lt;p&gt;Sadly, not everyone has access to Github, given how the platform is part of Microsoft and given the latter is a US company, it needs to comply with the US export law. Thus developers from countries such as Iran, Syria, Crimea and others are blocked by the platform.&lt;/p&gt;

&lt;p&gt;This puts a harsh roadblock in their chances of getting their first job, especially if they’re applying for a remote position or a freelance gig. Either way, the main code repository for developers is out of their reach, so how can they find ways to show what they know how to do?&lt;/p&gt;

&lt;p&gt;Let’s take a look at some options.&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#build-a-wordpress-site"&gt;
  &lt;/a&gt;
  Build a WordPress site
&lt;/h2&gt;

&lt;p&gt;Granted, this is not a code repository, but we’re going barebone here.&lt;/p&gt;

&lt;p&gt;Other public code repositories like Bitbucket are also blocked in many countries, so we’re looking for alternatives.&lt;/p&gt;

&lt;p&gt;Wordress.com allows you to set up a blog for free, where you can add whatever content you want. So let’s take a look at this and consider the opportunity it represents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You’re free to show our code however we want.&lt;/li&gt;
&lt;li&gt;You’re able to provide samples of code through direct download links, or through directly showing what it looks like.&lt;/li&gt;
&lt;li&gt;You can also write content around that code. Are you showing a project you built? Create a series of articles around that project and link it to the code.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can also take advantage of WordPress plugins such as &lt;a href="https://wordpress.org/plugins/code-prettify/"&gt;code-prettify&lt;/a&gt; which lets you highlight code on your posts. That way you generate a similar look and feel users would get by reading code directly on Github.&lt;/p&gt;

&lt;p&gt;If you have access to hosting and a domain name (ideally something like your name .com) you can even download the free version of WordPress from Wordpress.org and install it yourself. This would give you a lot more control over how your site works.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#launchpad"&gt;
  &lt;/a&gt;
  LaunchPad
&lt;/h2&gt;

&lt;p&gt;If you’re looking for something similar to Github but that it’s not blocked, you might want to give &lt;a href="https://launchpad.net/"&gt;LaunchPad&lt;/a&gt; a try.&lt;/p&gt;

&lt;p&gt;It’s similar to Github in the sense that you can publish your project’s code, track bugs and issues, perform code reviews and more. And while the tutorials are written following the use of &lt;a href="http://doc.bazaar.canonical.com/bzr.dev/en/mini-tutorial/index.html"&gt;Bazaar&lt;/a&gt; instead of Git, they claim that the latter is also an option.&lt;/p&gt;

&lt;p&gt;Projects such as &lt;a href="https://launchpad.net/inkscape"&gt;Inkscape&lt;/a&gt;, the Photoshop open-source alternative or &lt;a href="https://launchpad.net/ubuntu-mate"&gt;Ubuntu MATE&lt;/a&gt;, the Linux distro, are hosted there and you can see the type of information that you could potentially show to others.&lt;/p&gt;

&lt;p&gt;Granted, the visual aspect is not the highlight of LaunchPad, considering they don’t allow you to show a lot, in fact, there seems to not be a place for a Readme file. However, you can definitely add a link to an external website, so that way you can provide extra information about the project if you want, and focus on giving access to your code through this platform.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#writing"&gt;
  &lt;/a&gt;
  Writing!
&lt;/h2&gt;

&lt;p&gt;Writing is an alternative to showing projects done. It’s definitely not the same, however, as part of technical articles you can definitely show some aspects of your coding skills.&lt;/p&gt;

&lt;p&gt;And that is what we’re after here, isn’t it? Trying to show potential employees that you know what you’re talking about and that the list of skills on your resume is definitely true.&lt;/p&gt;

&lt;p&gt;Writing in a lot of places is free and public, so you don’t need to spend any money if you don’t want to. And if you’re doing this for a resume, I would strongly suggest you write about the subjects you claim to know. Maybe not about all of them, but the most relevant at least.&lt;/p&gt;

&lt;p&gt;If you’re lucky and some of those articles get some traction, they can even be picked up by newsletters such as &lt;a href="https://javascriptweekly.com/"&gt;JavaScriptWeekly&lt;/a&gt;. That’s a nice greeting card, especially if your interviewer is already a subscriber.&lt;/p&gt;

&lt;p&gt;But even if you’re just getting started, spend some time honing your writing skills, their return of investment is huge considering how much they can affect your communication skills.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#go-to-meetups-conferences-and-speak"&gt;
  &lt;/a&gt;
  Go to meet-ups / conferences and speak
&lt;/h2&gt;

&lt;p&gt;Granted, this one might be a little bit extreme, but what better way to get in front of everyone you want to be hired by than assisting the same social events they do.&lt;/p&gt;

&lt;p&gt;Some of the biggest (and sometimes not so big) software companies normally either sponsor some meet-ups or the big conferences, and they tend to send some of their employees as well. It’s a great opportunity for you to be there, speak about something relevant to your experience and network!&lt;/p&gt;

&lt;p&gt;I understand that it’s not easy for everyone. Especially conferences require travel and not all of them are willing to pay for your plane ticket and hotel room. But at least do the research, there is a ton of information online about the different conferences you could be speaking at and they normally tell you if they have some travel budget or not.&lt;/p&gt;

&lt;p&gt;Think about it, it might sound like a lot, but you’re also putting your name out there. The video of your talk will surely be on YouTube a little bit after, and anybody interested in you will only have to google your name. This is definitely a powerful, yet scary, opportunity.&lt;/p&gt;

&lt;p&gt;Do not discard it until you’ve done the research.&lt;/p&gt;




&lt;p&gt;Getting noticed is not all about code, but it definitely helps. The thing is that when I say “code”, you usually go straight into “side-project”, and from there you go right into Github, and that is where the plan collapses. No Github access, no side-project, no new job. Done.&lt;/p&gt;

&lt;p&gt;But that’s not the case, find alternative ways of showing what you know how to do, either by finding less popular (or user-friendly) alternatives, or by doing other things around your projects (like writing or speaking about it).&lt;/p&gt;

&lt;p&gt;What else would you suggest to a new dev looking for their first job as an alternative to having a portfolio on Github?&lt;/p&gt;




&lt;p&gt;&lt;em&gt;If you liked what you read, consider joining my Free newsletter to get insight into the software development career!&lt;/em&gt; &lt;a href="https://fernandodoglio.substack.com"&gt;https://fernandodoglio.substack.com&lt;/a&gt;&lt;/p&gt;

</description>
      <category>github</category>
      <category>beginners</category>
      <category>productivity</category>
    </item>
    <item>
      <title>Running while(true) in JS</title>
      <author>Abhishek Raj</author>
      <pubDate>Sat, 18 Dec 2021 11:37:53 +0000</pubDate>
      <link>https://dev.to/abhishekraj272/running-whiletrue-in-js-1aoh</link>
      <guid>https://dev.to/abhishekraj272/running-whiletrue-in-js-1aoh</guid>
      <description>&lt;p&gt;You must be wondering this guy is MAD, we can't run while/for loop &lt;strong&gt;synchronously&lt;/strong&gt; client side in &lt;em&gt;javascript&lt;/em&gt;, it will block the main thread and will FREEZE the page.&lt;/p&gt;

&lt;p&gt;Well, it is possible to run it but not a good way to do it and might introduce some bugs in your program.&lt;/p&gt;

&lt;h4&gt;
  &lt;a href="#web-worker"&gt;
  &lt;/a&gt;
  Web Worker
&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Web Worker runs in a single thread isolated from the main thread, so you can run any synchronous operation without blocking the thread.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You can run the loop in Web Worker and send the send the data from Web Worker to Main Thread using &lt;em&gt;postMessage&lt;/em&gt; method. It will run as good as main thread.&lt;/p&gt;

&lt;p&gt;You can try it in the below code snippet&lt;/p&gt;

&lt;p&gt;&lt;iframe height="600" src="https://codepen.io/abhishekraj272/embed/WNZjWMe?height=600&amp;amp;default-tab=result,js&amp;amp;embed-version=2"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: Running synchronous loop is not advised in browser, unless there is a such requirement.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Connect Me @ &lt;a href="https://www.linkedin.com/in/abhishekraj272/"&gt;Linkedin&lt;/a&gt;, &lt;a href="https://github.com/abhishekraj272"&gt;Github&lt;/a&gt;, &lt;a href="https://twitter.com/abhishekraj272"&gt;Twitter&lt;/a&gt;, &lt;a href="https://www.youtube.com/channel/UCvHn2T8DSJzEWzYDdK3Dt8A"&gt;Youtube&lt;/a&gt; 😇&lt;/p&gt;

</description>
      <category>webdev</category>
      <category>javascript</category>
      <category>productivity</category>
      <category>programming</category>
    </item>
    <item>
      <title>Logistic Regression</title>
      <author>Swayam Singh</author>
      <pubDate>Sat, 18 Dec 2021 11:36:11 +0000</pubDate>
      <link>https://dev.to/_s_w_a_y_a_m_/logistic-regression-li9</link>
      <guid>https://dev.to/_s_w_a_y_a_m_/logistic-regression-li9</guid>
      <description>&lt;h2&gt;
  &lt;a href="#overview"&gt;
  &lt;/a&gt;
  Overview
&lt;/h2&gt;

&lt;p&gt;Welcome to the 3&lt;sup&gt;rd&lt;/sup&gt; article of the &lt;a href="https://swayam-blog.hashnode.dev/series/demystifying-ml"&gt;&lt;strong&gt;Demystifying Machine Learning&lt;/strong&gt;&lt;/a&gt; series. In this article, we are going to discuss a supervised classification algorithm &lt;strong&gt;Logistic Regression&lt;/strong&gt;, how it works, why it's important, mathematics behind the scenes, linear and non-linear separation and Regularization.&lt;/p&gt;

&lt;p&gt;A good grasp of Linear Regression is needed to understand this algorithm and luckily we already covered it, for reference you can read it from &lt;a href="https://swayam-blog.hashnode.dev/linear-regression-using-gradient-descent"&gt;here&lt;/a&gt;. &lt;strong&gt;Logistic Regression&lt;/strong&gt; builds the base of &lt;strong&gt;Neural Networks&lt;/strong&gt;,  so it gets very important to understand the terms and working of this algorithm.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Logistic Regression&lt;/strong&gt; is not a regression algorithm, its name does consist of the word "Regression" but it's a classification algorithm.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2&gt;
  &lt;a href="#what-is-logistic-regression"&gt;
  &lt;/a&gt;
  What is Logistic Regression?
&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Logistic Regression&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;also known as Perceptron algorithm is a supervised classification algorithm i.e. we teach our hypothesis with categorical labelled data and it predicts the classes (or categories) with some certain probability&lt;/em&gt;&lt;/strong&gt;.  The reason this algorithm is called Logistic Regression is maybe that it's working is pretty similar to that of Linear Regression and the term "Logistic" is because we use a Logistic function in this algorithm (&lt;em&gt;we'll going to see it later&lt;/em&gt;). The difference is that with Linear Regression we intend to predict the continuous values but with Logistic Regression we want to predict the categorical value. Like whether the student gets enrolled in a university or not, if the picture contains a cat or not, etc.&lt;/p&gt;

&lt;p&gt;Here's a representation of how Logistic Regression classifies the data points.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--Kef-WziO--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639752060348/CzQh-pR6T.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Kef-WziO--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639752060348/CzQh-pR6T.png" alt="1.png" width="640" height="480"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We can see that the &lt;em&gt;blue points&lt;/em&gt; are separated from the &lt;em&gt;orange points&lt;/em&gt; through a line and we call this line a &lt;strong&gt;decision boundary&lt;/strong&gt;. Basically, with Logistic Regression we separate the classes (or categories) with the help of decision boundaries, they can be linear or non-linear. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#working-of-logistic-regression"&gt;
  &lt;/a&gt;
  Working of Logistic Regression
&lt;/h2&gt;

&lt;p&gt;Let's revise what we learnt in &lt;a href="https://swayam-blog.hashnode.dev/linear-regression-using-gradient-descent"&gt;Linear Regression&lt;/a&gt;, we initialise the parameters with all 0s, then with the help of Gradient Descent calculate the optimal parameters by reducing the cost function and lastly draw the hypothesis to predict the continuous-valued target. &lt;/p&gt;

&lt;p&gt;But here we don't need continuous values, instead, we want to output the probability that lies between [0,1]. So the question arises, &lt;strong&gt;how we can get probability or a number between [0,1] from the continuous values of Linear Regression?&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#sigmoid-function"&gt;
  &lt;/a&gt;
  Sigmoid function
&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Sigmoid function&lt;/strong&gt; is a type of &lt;strong&gt;&lt;em&gt;logistic function&lt;/em&gt;&lt;/strong&gt; which takes a real number as input and gives out a real number between [0,1] as output. &lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--TPac8HFh--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756326745/bI7AUi4gX.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--TPac8HFh--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756326745/bI7AUi4gX.png" alt="1.png" width="880" height="90"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--c6-acedH--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639752080151/XSs-lCUma.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--c6-acedH--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639752080151/XSs-lCUma.png" alt="2.png" width="880" height="527"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So basically we'll generate the continuous values using Linear Regression and convert those continuous values into probability i.e. between [0,1] by passing through &lt;strong&gt;sigmoid function&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--pmGjnM5B--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756341261/wJM47_tPB.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--pmGjnM5B--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756341261/wJM47_tPB.png" alt="2.png" width="880" height="106"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So in the end our final hypothesis will look like this:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--qf10WZCQ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756529024/Hsdngqks9.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--qf10WZCQ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756529024/Hsdngqks9.png" alt="3.png" width="880" height="118"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This hypothesis is different from the hypothesis of Linear Regression. Yeah looks fair enough, let me give you a visualisation about overall how Logistic Regression works.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--vCX7SFsd--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639752100559/4t7OW4zk5.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--vCX7SFsd--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639752100559/4t7OW4zk5.png" alt="13.png" width="880" height="660"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note:- X0 is basically 1, we will it later why?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So it's very clear from the above representation that the part behind the &lt;code&gt;sigmoid&lt;/code&gt; function is very similar to that of Linear Regression. Let's now move ahead and define the cost function for Logistic Regression.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#cost-function"&gt;
  &lt;/a&gt;
  Cost function
&lt;/h3&gt;

&lt;p&gt;Our hypothesis is different from that of Linear Regression, so we need to define a new cost function. We already learnt in our &lt;a href="https://swayam-blog.hashnode.dev/linear-regression-using-gradient-descent"&gt;2nd article&lt;/a&gt; about what is cost function is and how to define one for our algorithm. Let's use those concepts and define one for Logistic Regression.&lt;/p&gt;

&lt;p&gt;For simplicity let's consider the case of binary classification which means our target value will be either 1(True) or 0(False). For example, the image contains a cat (1, True) or the image does not contain a cat (0, False). This means that our predicted values will also be either 0 or 1.&lt;/p&gt;

&lt;p&gt;Let me first show you what is the cost function for Logistic Regression and then we'll try to understand its derivation.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--bw9jlVEV--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756571823/yAX5pAFqL.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--bw9jlVEV--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756571823/yAX5pAFqL.png" alt="4.png" width="880" height="100"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Combining both the conditions and taking their mean for &lt;strong&gt;m&lt;/strong&gt; samples in a dataset:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--wySl1UBN--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756596078/c2xN6n22Z.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--wySl1UBN--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756596078/c2xN6n22Z.png" alt="5.png" width="880" height="100"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The equation shown above is of the cost function for Logistic Regression and it looks very different than that of Linear Regression, let's break it down and understand how we came up with the above cost function? Get ready, probability class is about to begin...&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--RsjKG7ZM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756622846/xcBIW7ru-.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--RsjKG7ZM--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756622846/xcBIW7ru-.png" alt="6.png" width="880" height="272"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;There's a negative sign in the original cost function because when training the algorithm we want probabilities to be large but here we are representing it to minimize the cost.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;minimise loss =&amp;gt; max log probability&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Okay, that's a lot of maths, but it's all basics. Focus on the general form in the above equation and that's more than enough to understand how we came up with such a complex looking cost function. Let's see how to calculate the cost for &lt;strong&gt;m&lt;/strong&gt; examples for some datasets:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--LLFIrlby--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756647909/EFS95vPVG.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--LLFIrlby--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756647909/EFS95vPVG.png" alt="7.png" width="880" height="347"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#gradient-descent"&gt;
  &lt;/a&gt;
  Gradient Descent
&lt;/h3&gt;

&lt;p&gt;We already covered the working of gradient descent in our &lt;a href="https://swayam-blog.hashnode.dev/linear-regression-using-gradient-descent"&gt;2nd article&lt;/a&gt;, you can refer to it for revision. In this section, we'll be looking at formulas of gradients and updating the parameters.&lt;/p&gt;

&lt;p&gt;The gradient of the cost is a vector of the same length as θ where the jth parameter (for j=0,1,⋯,n) is defined as follows:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s---kAGMKI_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756666029/ZE6goQy68.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s---kAGMKI_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756666029/ZE6goQy68.png" alt="8.png" width="880" height="170"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Calculation of gradients from cost function is demonstrated in &lt;a href="https://swayam-blog.hashnode.dev/linear-regression-using-gradient-descent"&gt;2nd Article&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As we can see that the formula for calculating gradients is pretty similar to that of Linear Regression but note that the values for hθ(x&lt;sup&gt;i&lt;/sup&gt;) are different due to the use of &lt;code&gt;sigmoid function&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;After calculating gradients we can simultaneously update our parameter θ as :&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--JK2SrwSs--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756688657/HRoDBUBKs.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--JK2SrwSs--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756688657/HRoDBUBKs.png" alt="9.png" width="880" height="145"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Great now we have all the ingredients for writing our own Logistic Regression from scratch, Let's get started with it in the next section. Till now have a break for 15 minutes cause you just studied a hell of lot of maths by now.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#code-implementation"&gt;
  &lt;/a&gt;
  Code Implementation
&lt;/h2&gt;

&lt;p&gt;In this section, we'll be writing our &lt;code&gt;LogisticRegression&lt;/code&gt; class using Python.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note: You can find all the codes for this article from &lt;a href="https://swayam-blog.hashnode.dev/linear-regression-using-gradient-descent"&gt;here&lt;/a&gt;. It's highly recommended to follow the Jupyter notebook while going through this section.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let's begin 🙂&lt;/p&gt;

&lt;p&gt;Let me give you a basic overall working of this class. Firstly it'll take your &lt;em&gt;feature&lt;/em&gt; and &lt;em&gt;target&lt;/em&gt; arrays as input then it'll normalize the features around mean (if you want to) and add an extra column of all 1s to your &lt;em&gt;feature&lt;/em&gt; array for the bias term, as we know from Linear Regression that &lt;code&gt;y=wx+b&lt;/code&gt;. So this &lt;code&gt;b&lt;/code&gt; gets handled by this extra column of 1s with matrix multiplication of &lt;em&gt;features&lt;/em&gt; and &lt;em&gt;parameters&lt;/em&gt; arrays.&lt;/p&gt;

&lt;p&gt;for example:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--tcu0H1WV--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756741748/e70XrL7kZ.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--tcu0H1WV--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756741748/e70XrL7kZ.png" alt="10.png" width="880" height="478"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Then it initializes the parameter array with all 0s after that training loop starts till the epoch count and it calculates the cost and gradient for certain parameters and simultaneously keep updating the parameters with a certain learning rate.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cost_history&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; 

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;calculate_cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="s"&gt;"""
        Returns the cost and gradients.
        parameters: None

        Returns:
            cost : Caculated loss (scalar).
            gradients: array containing the gradients w.r.t each parameter

        """&lt;/span&gt;

        &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;y_hat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_hat&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;y_hat&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

        &lt;span class="n"&gt;gradients&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
            &lt;span class="n"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y_hat&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;gradients&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt;

        &lt;span class="c1"&gt;# Vectorized form
&lt;/span&gt;        &lt;span class="c1"&gt;#  gradients = np.dot(self.X.T, error)/m 
&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gradients&lt;/span&gt;


    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;init_parameters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="s"&gt;"""
        Initialize the parameters as array of 0s
        parameters: None

        Returns: None

        """&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;feature_normalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="s"&gt;"""
        Normalize the samples.
        parameters: 
            X : input/feature matrix

        Returns:
            X_norm : Normalized X.

        """&lt;/span&gt;
        &lt;span class="n"&gt;X_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
            &lt;span class="n"&gt;X_norm&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_norm&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;X_norm&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;is_normalize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="s"&gt;"""
        Iterates and find the optimal parameters for input dataset
        parameters: 
            x : input/feature matrix
            y : target matrix
            learning_rate: between 0 and 1 (default is 0.01)
            epochs: number of iterations (default is 500)
            is_normalize: boolean, for normalizing features (default is True)
            verbose: iterations after to print cost

        Returns:
            parameters : Array of optimal value of weights.

        """&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cost_history&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndim&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;# adding extra dimension, if X is a 1-D array
&lt;/span&gt;            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;is_normalize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;is_normalize&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_normalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_parameters&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gradients&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;calculate_cost&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cost_history&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;gradients&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;"Cost after &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; epochs: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;


    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;is_normalize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="s"&gt;"""
        Returns the predictions after fitting.
        parameters: 
            x : input/feature matrix

        Returns:
            predictions: Array of predicted target values.

        """&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# converting list to numpy array
&lt;/span&gt;        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndim&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;is_normalize&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
                &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;threshold&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;This code looks pretty similar to that of Linear Regression using Gradient Descent. If you are following this series you'll be pretty familiar with this implementation. Still, I like to point out a few methods of this class:-&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;&lt;code&gt;sigmoid&lt;/code&gt;&lt;/strong&gt;: We added this new method to calculate the sigmoid of the continuous values generated from the linear hypothesis i.e. from θ&lt;sup&gt;T&lt;/sup&gt;X to get the probabilities.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;&lt;code&gt;calculate_cost&lt;/code&gt;&lt;/strong&gt;: We change the definition of this function because our cost function is changed too, it's not confusing if you are well aware of the formulas I gave and the &lt;code&gt;numpy&lt;/code&gt; library then it won't be difficult for you to understand.&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/strong&gt;: This function takes the input and returns the array of predicted values 0 and 1. There's an extra parameter &lt;code&gt;threshold&lt;/code&gt; which had a default value of 0.5, if the predicted value &amp;gt; 0.5 then it'll predict 1 otherwise 0 for the predicted array. You can change this &lt;code&gt;threshold&lt;/code&gt; according to your confidence level.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#trying-it-out-on-a-dataset"&gt;
  &lt;/a&gt;
  Trying it out on a dataset
&lt;/h3&gt;

&lt;p&gt;In this sub-section, we will use our class on the dataset to check how it's working.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;All the codes and implementations are provided in this &lt;a href="https://github.com/practice404/demystifying_machine_learning/blob/master/logistic_regression/notebook.ipynb"&gt;jupyter notebook&lt;/a&gt;, follow it for better understanding in this section.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For the dataset, we have records of students' marks for some Exam1 and Exam2 and the target column represents whether they get admitted into the university or not. Let's visualize it using a plot:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--uuLiJiJC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639752142703/GNP0R62nW.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--uuLiJiJC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639752142703/GNP0R62nW.png" alt="3.png" width="778" height="528"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So what we basically want from Logistic Regression is to tell us whether a certain student with some scores of Exam1 and Exam2 is admitted or not. Let's create an instance of the &lt;code&gt;LogisticRegression&lt;/code&gt; class and try it out.&lt;/p&gt;

&lt;p&gt;Firstly I'm going to find the optimal parameters for this dataset and I'm going to show you two ways of doing it.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Using our custom class&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--C_GbZVHn--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639755806863/-skmJvfk3.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--C_GbZVHn--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639755806863/-skmJvfk3.png" alt="4.png" width="880" height="355"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Using Scipy's optimize module&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sometimes using gradient descent takes a lot of time so for time-saving, I'll show you how you can easily find the parameters by just using &lt;code&gt;scipy.optimize.minimize&lt;/code&gt; function bypassing the cost function into it.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--g7V2reys--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639755824131/E1q3mdC80.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--g7V2reys--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639755824131/E1q3mdC80.png" alt="5.png" width="880" height="592"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Firstly I appended an extra column of 1s for bias term then pass my &lt;code&gt;costFunction&lt;/code&gt;, &lt;code&gt;initial_theta&lt;/code&gt;  (initially 0s) and my    &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; as arguments. It easily calculated the optimal parameters in 0.3 seconds much faster than gradient descent which took about 6.5 seconds.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note: &lt;code&gt;costFunction&lt;/code&gt; is similar to what we have in our class method as &lt;code&gt;calculate_cost&lt;/code&gt;, I just put it outside to show you the work of &lt;code&gt;scipy.optimize.minimize&lt;/code&gt; function.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Great now let's see how well it's performed by printing out its accuracy on the training set.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--kQIrEIUC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639755839852/VjGQjf-gg.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--kQIrEIUC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639755839852/VjGQjf-gg.png" alt="6.png" width="880" height="295"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Hmmm, around 89%, it seems good although there are a few algorithms that we'll be covering in future that can perform way much better than this. Now let me show you its decision boundary, as we can see that we didn't perform any polynomial transformation (for more refer to &lt;a href="https://swayam-blog.hashnode.dev/linear-regression-using-gradient-descent"&gt;article 2&lt;/a&gt;) on our input features so the decision boundary is going to be a straight line.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--NeicB5ZD--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639755907860/N8MYuSOL9.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--NeicB5ZD--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639755907860/N8MYuSOL9.png" alt="7.png" width="880" height="579"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;That's so great we just implemented our &lt;code&gt;LogisticRegression&lt;/code&gt; class on the student's dataset. Let's move ahead and understand the problem of overfitting in the next section. Till then have a short 5-minute break.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#problem-of-overfitting"&gt;
  &lt;/a&gt;
  Problem of Overfitting
&lt;/h2&gt;

&lt;p&gt;In order to understand overfitting in Logistic Regression, I'll show you an implementation of this algorithm on another dataset where we need to fit a non-linear decision boundary. Let's visualize our 2nd dataset on the graph:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--Wk-WIH5l--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639755934992/5-pXUnqBv.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Wk-WIH5l--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639755934992/5-pXUnqBv.png" alt="8.png" width="880" height="569"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As we can see, it's not linearly separable data so we need to fit a non-linear decision boundary. If you went through the &lt;a href="https://swayam-blog.hashnode.dev/linear-regression-using-gradient-descent"&gt;2nd article&lt;/a&gt; of this series then you probably know how we do this, but in brief, we take our original features and apply polynomial transformations on them, like squaring, cubing or multiplying with each other to obtain new features and then training our algorithm on those new features results in non-linear classification.&lt;/p&gt;

&lt;p&gt;In the &lt;a href="https://github.com/practice404/demystifying_machine_learning/blob/master/logistic_regression/notebook.ipynb"&gt;notebook&lt;/a&gt; you'll find a function &lt;code&gt;mapFeature&lt;/code&gt; that take individual features as input and return new transformed features. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If you wanna know how it's working then consider referring to the &lt;a href="https://github.com/practice404/demystifying_machine_learning/blob/master/logistic_regression/notebook.ipynb"&gt;notebook&lt;/a&gt; and it's recommended to follow it while reading this article.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;After getting the new transformed features and following the exact steps we followed in the above section, you'll be able to print out its decision boundary that will look something like this:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--5iyAUtOB--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639755949600/JSXj2eNSn.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--5iyAUtOB--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639755949600/JSXj2eNSn.png" alt="9.png" width="880" height="568"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After seeing it, you probably may say that "WOW!, it performed so well almost classified all the training points". Well, it does seem good but in reality, it's worst. Our hypothesis fitted so well on the training set that it loses the generality that means if we provide a new set of points that is not in the training set then our hypothesis will not be able to classify it clearly.&lt;/p&gt;

&lt;p&gt;In short, it's necessary to maintain the generality in our hypothesis so that it can perform well on the data it is never seen. &lt;strong&gt;Regularization&lt;/strong&gt; is the way to achieve it. Let's see how to maintain generality using Regularization in the next section.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#regularization"&gt;
  &lt;/a&gt;
  Regularization
&lt;/h2&gt;

&lt;p&gt;In this section, we'll be discussing how to implement regularization. &lt;strong&gt;&lt;em&gt;Overfitting occurs when the algorithm provides heavy parameters to some features according to the training dataset and hyperparameters. This makes those features dominant in the overall hypothesis and lead to a nice fit in the training set but not so good on the samples outside the training set.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;The plan is to add the square of parameters by multiplying them with some big number (λ) to the cost function because our algorithms' main motive is to decrease the cost function, so in this way, the algorithm will end up giving the small parameters just to cancel the effect addition of parameters by multiplying with a large number (&amp;amp;&lt;/em&gt;&lt;/strong&gt;lambda;). So our final cost function gets modified to:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--QhWM5yOy--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756780412/aW2HU1RZv.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--QhWM5yOy--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756780412/aW2HU1RZv.png" alt="11.png" width="880" height="95"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note: We denote the bias term as θ0 and it's not needed to regularized the bias term that's why we are only considering only θ1 to θn parameters.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Since our cost function is changed that's why our formulas for gradients were also get affected. The new formula for the gradient are:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--Vg6YuANb--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756795932/Yh788isRU.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Vg6YuANb--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756795932/Yh788isRU.png" alt="12.png" width="880" height="167"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The new formulas for gradients can be easily derived by partially differentiating the new cost function J(θ) w.r.t to some θj. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Calculating of gradients from cost function is demonstrated in &lt;a href="https://swayam-blog.hashnode.dev/linear-regression-using-gradient-descent"&gt;2nd Article&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;λ is known as a regularization parameter and it should be greater than 0. &lt;strong&gt;&lt;em&gt;Large value of λ leads to underfitting and very small values lead to overfitting&lt;/em&gt;&lt;/strong&gt;, so you need to pick the right one for your dataset through iterating on some sample values.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#implementing-regularization-on-raw-logisticregression-endraw-class"&gt;
  &lt;/a&gt;
  Implementing Regularization on &lt;code&gt;LogisticRegression&lt;/code&gt; class
&lt;/h3&gt;

&lt;p&gt;We only need to modify the &lt;code&gt;calculate_cost&lt;/code&gt; method because only this method is responsible for calculating both cost and gradients. The modified version is shown below:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;RegLogisticRegression&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cost_history&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; 

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sigmoid_derivative&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;derivative&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;derivative&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;calculate_cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lambda_&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="s"&gt;"""
        Returns the cost and gradients.
        parameters: None

        Returns:
            cost : Caculated loss (scalar).
            gradients: array containing the gradients w.r.t each parameter

        """&lt;/span&gt;

        &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;y_hat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_hat&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;y_hat&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;lambda_&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;gradients&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y_hat&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y_hat&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;lambda_&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;
            &lt;span class="n"&gt;gradients&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt;

        &lt;span class="c1"&gt;# gradients = np.dot(self.X.T, error)/m
&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gradients&lt;/span&gt;


    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;init_parameters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="s"&gt;"""
        Initialize the parameters as array of 0s
        parameters: None

        Returns:None

        """&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;feature_normalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="s"&gt;"""
        Normalize the samples.
        parameters: 
            X : input/feature matrix

        Returns:
            X_norm : Normalized X.

        """&lt;/span&gt;
        &lt;span class="n"&gt;X_norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
            &lt;span class="n"&gt;X_norm&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_norm&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;X_norm&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lambda_&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;is_normalize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="s"&gt;"""
        Iterates and find the optimal parameters for input dataset
        parameters: 
            x : input/feature matrix
            y : target matrix
            learning_rate: between 0 and 1 (default is 0.01)
            epochs: number of iterations (default is 500)
            is_normalize: boolean, for normalizing features (default is True)
            verbose: iterations after to print cost

        Returns:
            parameters : Array of optimal value of weights.

        """&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cost_history&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndim&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;# adding extra dimension, if X is a 1-D array
&lt;/span&gt;            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;is_normalize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;is_normalize&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_normalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_parameters&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gradients&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;calculate_cost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lambda_&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;lambda_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cost_history&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;gradients&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s"&gt;"Cost after &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt; epochs: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;cost&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;


    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;is_normalize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="s"&gt;"""
        Returns the predictions after fitting.
        parameters: 
            x : input/feature matrix

        Returns:
            predictions : Array of predicted target values.

        """&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# converting list to numpy array
&lt;/span&gt;        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndim&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;is_normalize&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
                &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;threshold&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Now we have our regularized version of &lt;code&gt;RegLogisticRegression&lt;/code&gt; class. Let's address the previous problem of overfitting on polynomial regression by using a set of values for λ to pick the right one.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--yhcsvsQb--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639755979522/0MXdt0T4B.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--yhcsvsQb--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639755979522/0MXdt0T4B.png" alt="10.png" width="880" height="251"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--RUtUIEmm--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756009035/JPqLNcv2r.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--RUtUIEmm--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756009035/JPqLNcv2r.png" alt="11.png" width="880" height="217"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I can say that λ=1 and λ=10 looks pretty good and they both are able to maintain the generality in hypothesis, the curve is more smooth and less wiggling type. But we can see that as we keep increasing the value if λ the more our hypothesis starts to &lt;strong&gt;underfit&lt;/strong&gt; the data. It basically means that it starts to perform even worst on the training set. Let's visualise the underfitting by plotting cost functions for each λ&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--rRue2bZ_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756025955/gX8nPqCOC.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--rRue2bZ_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn.hashnode.com/res/hashnode/image/upload/v1639756025955/gX8nPqCOC.png" alt="12.png" width="880" height="259"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We can see that as λ increases cost also increases. So it's advised to select the value for λ carefully according to your custom dataset.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#conclusion"&gt;
  &lt;/a&gt;
  Conclusion
&lt;/h2&gt;

&lt;p&gt;Great work everyone, we successfully learnt and implemented Logistic Regression. Most people don't write their Machine Learning algorithm from scratch instead they use libraries like Scikit-Learn. Scikit-Learn contains wrappers for many Machine Learning algorithms and it's really flexible and easy to use. But it's not harmful to know about the algorithm you're going to use and the best way of doing it is to understand the underlying mathematics and implement it from scratch.&lt;/p&gt;

&lt;p&gt;So in the next article, we'll be making a classification project using the Scikit-Learn library and you'll see how easy it is to use for making some really nice creative projects.&lt;/p&gt;

&lt;p&gt;I hope you have learnt something new, for more updates on upcoming articles get connected with me through &lt;a href="https://twitter.com/_s_w_a_y_a_m_"&gt;Twitter&lt;/a&gt; and stay tuned for more. Till then enjoy your day and keep learning.&lt;/p&gt;

</description>
      <category>machinelearning</category>
      <category>python</category>
      <category>tutorial</category>
    </item>
    <item>
      <title>Backup my way</title>
      <author>Young Yoshie</author>
      <pubDate>Sat, 18 Dec 2021 11:11:56 +0000</pubDate>
      <link>https://dev.to/youngyoshie/backup-my-way-b78</link>
      <guid>https://dev.to/youngyoshie/backup-my-way-b78</guid>
      <description>&lt;p&gt;First thing first, I want to list my own devices, which I have through the years:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Laptop Samsung NP300E4Z-S06VN (Old laptop which I give to my mom)&lt;/li&gt;
&lt;li&gt;Laptop Dell Inspiron 15 3567 (My mom bought it for me when I go to college, I give it to my sister afterward)&lt;/li&gt;
&lt;li&gt;Laptop Acer Nitro AN515-45 (Gaming laptop which I buy for gaming, of course)&lt;/li&gt;
&lt;li&gt;MacBook Pro M1 2020 (My company laptop)&lt;/li&gt;
&lt;li&gt;Phone LG G3 d851 (Kinda broken, the phone I used a long time ago)&lt;/li&gt;
&lt;li&gt;Phone Xiaomi Poco X3 NFC (Primary phone which I use daily)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;App/Service I use daily:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://bitwarden.com/"&gt;Bitwarden&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://getaegis.app/"&gt;Aegis Authenticator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rclone.org/"&gt;Rclone&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tailscale.com/"&gt;Tailscale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Google Keep&lt;/li&gt;
&lt;li&gt;Google Drive (I use 200GB plan)&lt;/li&gt;
&lt;li&gt;GitHub&lt;/li&gt;
&lt;li&gt;GitLab&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The purpose is that I want my data to be safe, secure, and can be easily recovered if I lost some devices;&lt;br&gt;
or in the worst situation, I lost all.&lt;br&gt;
Because you know, it is hard to guess what is waiting for us in the future.&lt;/p&gt;

&lt;p&gt;There are 2 sections which I want to share, the first is &lt;strong&gt;How to backup&lt;/strong&gt;, the second is &lt;strong&gt;Recover strategy&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;
  &lt;a href="#how-to-backup"&gt;
  &lt;/a&gt;
  How to backup
&lt;/h2&gt;

&lt;p&gt;Before I talk about backup, I want to talk about data.&lt;br&gt;
In specifically, which data should I backup?&lt;/p&gt;

&lt;p&gt;I use Arch Linux and macOS, primarily work in the terminal so I have too many dotfiles, for example, &lt;code&gt;~/.config/nvim/init.config&lt;/code&gt;.&lt;br&gt;
Each time I reinstall Arch Linux (I like it a lot), I need to reconfigure all the settings, and it is time-consuming.&lt;/p&gt;

&lt;p&gt;So for the DE and UI settings, I keep it as default as possible, unless it's getting in my way, I leave the default setting there and forget about it.&lt;br&gt;
The others are dotfiles, which I write my own &lt;a href="https://github.com/haunt98/dotfiles"&gt;dotfiles tool&lt;/a&gt; to backup and reconfigure easily and quickly.&lt;br&gt;
Also, I know that installing Arch Linux is not easy, despite I install it too many times (Like thousand times since I was in high school).&lt;br&gt;
Not because it is hard, but as life goes on, the &lt;a href="https://wiki.archlinux.org/title/installation_guide"&gt;official install guide&lt;/a&gt; keeps getting new update and covering too many cases for my own personal use, so I write my own &lt;a href="https://github.com/haunt98/til/blob/main/install-archlinux.md"&gt;guide&lt;/a&gt; to quickly capture what I need to do.&lt;br&gt;
I back up all my dotfiles inside my dotfiles tool in GitHub and GitLab as I trust them both.&lt;/p&gt;

&lt;p&gt;So that is my dotfiles, for my regular data, like Wallpaper or Books, Images, I use Google Drive (Actually I pay for it).&lt;br&gt;
But the step: open the webpage, click the upload button and choose files seems boring and time-consuming.&lt;br&gt;
So I use Rclone, it supports Google Drive, One Drive and many providers but I only use Google Drive for now.&lt;br&gt;
The commands are simple:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;&lt;span class="c"&gt;# Sync from local to remote&lt;/span&gt;
rclone &lt;span class="nb"&gt;sync &lt;/span&gt;MyBooks remote:MyBooks &lt;span class="nt"&gt;-P&lt;/span&gt; &lt;span class="nt"&gt;--exclude&lt;/span&gt; .DS_Store

&lt;span class="c"&gt;# Sync from remote to local&lt;/span&gt;
rclone &lt;span class="nb"&gt;sync &lt;/span&gt;remote:MyBooks MyBooks &lt;span class="nt"&gt;-P&lt;/span&gt; &lt;span class="nt"&gt;--exclude&lt;/span&gt; .DS_Store
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Before you use Rclone to sync to Google Drive, you should read &lt;a href="https://rclone.org/drive/"&gt;Google Drive rclone configuration&lt;/a&gt; first.&lt;/p&gt;

&lt;p&gt;The next data is my passwords and my OTPs.&lt;br&gt;
These are the things which I'm scare to lose the most.&lt;br&gt;
First thing first, I enable 2-Step Verification for all of my important accounts, should use both OTP and phone method.&lt;/p&gt;

&lt;p&gt;I use Bitwarden for passwords (That is a long story, coming from Google Password manager to Firefox Lockwise and then settle down with Bitwarden) and Aegis for OTPs.&lt;br&gt;
The reason I choose Aegis, not Authy (I use Authy for so long but Aegis is definitely better) is because Aegis allows me to extract all the OTPs to a single file (Can be encrypted), which I use to transfer or backup easily.&lt;/p&gt;

&lt;p&gt;As long as Bitwarden provides free passwords stored, I use all of its apps, extensions so that I can easily sync passwords between laptops and phones.&lt;br&gt;
The thing I need to remember is the master password of Bitwarden in my head.&lt;/p&gt;

&lt;p&gt;With Aegis, I export the data, then sync it to Google Drive in my main phone, then sync it back to my backup phone.&lt;br&gt;
For safety, I also store Aegis data locally on all of my laptops (Encrypted of course).&lt;br&gt;
The rule is you always need 2 phones for OTPs, one for carrying around, one always stays at home.&lt;/p&gt;

&lt;p&gt;The main problem here is the OTP, I can not store all of my OTPs in the cloud completely.&lt;br&gt;
Because if I want to access my OTPs in the cloud, I should log in, and then input my OTP, this is a circle, my friends.&lt;br&gt;
The easiest answer is the old phone, which is safe at home.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#recovery-strategy"&gt;
  &lt;/a&gt;
  Recovery strategy
&lt;/h2&gt;

&lt;p&gt;There are many strategies that I process to react as if something strange is happening to my devices.&lt;/p&gt;

&lt;p&gt;If I lost my laptops, single laptop or all, do not panic as long as I have my phones.&lt;br&gt;
The OTPs are in there, the passwords are in Bitwarden cloud, other data is in Google Drive so nothing is lost here.&lt;/p&gt;

&lt;p&gt;If I lost my main phone, but not my laptop, I use the OTPs which are stored locally in my laptops.&lt;/p&gt;

&lt;p&gt;If I lost my main phone, and my laptops, I use the OTPs in my old phone.&lt;br&gt;
Then I go to the nearest SIM store to recover my SIM, as I register my ID with it.&lt;/p&gt;

&lt;p&gt;In the worst situation, I lost everything, my laptops, my phones (Main phone and old).&lt;br&gt;
The first step is to recover my SIM, then log in to Google account using the password and SMS OTP.&lt;br&gt;
After that, log in to Bitwarden account using the master password and OTP from Gmail, which I open previously.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#the-end"&gt;
  &lt;/a&gt;
  The end
&lt;/h2&gt;

&lt;p&gt;This guide will be updated regularly I promise.&lt;/p&gt;

</description>
      <category>backup</category>
      <category>linux</category>
    </item>
  </channel>
</rss>
