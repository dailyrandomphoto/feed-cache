<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>DEV Community</title>
    <author>DEV Community</author>
    <description>A constructive and inclusive social network for software developers. With you every step of your journey.</description>
    <link>https://dev.to</link>
    <language>en</language>
    <item>
      <title>AWS Certified Solutions Architect Professional SAP-C01 Exam Questions Part 3</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 14:20:03 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-3-3leg</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-3-3leg</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;For AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has an on-premises monitoring solution using a PostgreSQL database for persistence of events. The database is unable to scale due to heavy ingestion and it frequently runs out of storage. The company wants to create a hybrid solution and has already set up a VPN connection between its network and AWS. The solution should include the following attributes:&lt;br&gt;
– Managed AWS services to minimize operational complexity.&lt;br&gt;
– A buffer that automatically scales to match the throughput of data and requires no ongoing administration.&lt;br&gt;
– A visualization tool to create dashboards to observe events in near-real time. Support for semi-structured JSON data and dynamic schemas.&lt;br&gt;
Which combination of components will enable the company to create a monitoring solution that will satisfy these requirements? (Choose two.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events.
       B. Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events.
       C. Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.
       D. Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards.
       E. Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A life sciences company is using a combination of open source tools to manage data analysis workflows and Docker containers running on servers in its on-premises data center to process genomics data. Sequencing data is generated and stored on a local storage area network (SAN), and then the data is processed. The research and development teams are running into capacity issues and have decided to re-architect their genomics analysis platform on AWS to scale based on workload demands and reduce the turnaround time from weeks to days. The company has a high-speed AWS Direct Connect connection. Sequencers will generate around 200 GB of data for each genome, and individual jobs can take several hours to process the data with ideal compute capacity. The end result will be stored in Amazon S3. The company is expecting 10-15 job requests each day. Which solution meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use regularly scheduled AWS Snowball Edge devices to transfer the sequencing data into AWS. When AWS receives the Snowball Edge device and the data is loaded into Amazon S3, use S3 events to trigger an AWS Lambda function to process the data.
       B. Use AWS Data Pipeline to transfer the sequencing data to Amazon S3. Use S3 events to trigger an Amazon EC2 Auto Scaling group to launch custom-AMI EC2 instances running the Docker containers to process the data.
       C. Use AWS DataSync to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow. Store the Docker images in Amazon Elastic Container Registry (Amazon ECR) and trigger AWS Batch to run the container and process the sequencing data.
       D. Use an AWS Storage Gateway file gateway to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Batch job that executes on Amazon EC2 instances running the Docker containers to process the data.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has five physical data centers in specific locations around the world. Each data center has hundreds of physical servers with a mix of Windows and Linux-based applications and database services. Each data center also has an AWS Direct Connect connection of 10 Gbps to AWS with a company-approved VPN solution to ensure that data transfer is secure. The company needs to shut down the existing data centers as quickly as possible and migrate the servers and applications to AWS. Which solution meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Install the AWS Server Migration Service (AWS SMS) connector onto each physical machine. Use the AWS Management Console to select the servers from the server catalog, and start the replication. Once the replication is complete, launch the Amazon EC2 instances created by the service.
       B. Install the AWS DataSync agent onto each physical machine. Use the AWS Management Console to configure the destination to be an AMI, and start the replication. Once the replication is complete, launch the Amazon EC2 instances created by the service.
       C. Install the CloudEndure Migration agent onto each physical machine. Create a migration blueprint, and start the replication. Once the replication is complete, launch the Amazon EC2 instances in cutover mode.
       D. Install the AWS Application Discovery Service agent onto each physical machine. Use the AWS Migration Hub import option to start the replication. Once the replication is complete, launch the Amazon EC2 instances created by the service.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A security engineer determined that an existing application retrieves credentials to an Amazon RDS for MySQL database from an encrypted file in Amazon S3. For the next version of the application, the security engineer wants to implement the following application design changes to improve security:&lt;br&gt;
– The database must use strong, randomly generated passwords stored in a secure AWS managed service.&lt;br&gt;
– The application resources must be deployed through AWS CloudFormation.&lt;br&gt;
– The application must rotate credentials for the database every 90 days.&lt;br&gt;
A solutions architect will generate a CloudFormation template to deploy the application. Which resources specified in the CloudFormation template will meet the security engineer’s requirements with the LEAST amount of operational overhead?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Specify a Secrets Manager RotationSchedule resource to rotate the database password every 90 days.
       B. Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Create an AWS Lambda function resource to rotate the database password. Specify a Parameter Store RotationSchedule resource to rotate the database password every 90 days.
       C. Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Create an Amazon EventBridge scheduled rule resource to trigger the Lambda function password rotation every 90 days.
       D. Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Specify an AWS AppSync DataSource resource to automatically rotate the database password every 90 days.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has a three-tier application running on AWS with a web server, an application server, and an Amazon RDS MySQL DB instance. A solutions architect is designing a disaster recovery (DR) solution with an RPO of 5 minutes. Which solution will meet the company’s requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure AWS Backup to perform cross-Region backups of all servers every 5 minutes. Reprovision the three tiers in the DR Region from the backups using AWS CloudFormation in the event of a disaster.
       B. Maintain another running copy of the web and application server stack in the DR Region using AWS CloudFormation drift detection. Configure cross-Region snapshots of the DB instance to the DR Region every 5 minutes. In the event of a disaster, restore the DB instance using the snapshot in the DR Region.
       C. Use Amazon EC2 Image Builder to create and copy AMIs of the web and application server to both the primary and DR Regions. Create a cross-Region read replica of the DB instance in the DR Region. In the event of a disaster, promote the read replica to become the master and reprovision the servers with AWS CloudFormation using the AMIs.
       D. Create AMIs of the web and application servers in the DR Region. Use scheduled AWS Glue jobs to synchronize the DB instance with another DB instance in the DR Region. In the event of a disaster, switch to the DB instance in the DR Region and reprovision the servers with AWS CloudFormation using the AMIs.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to migrate its corporate data center from on premises to the AWS Cloud. The data center includes physical servers and VMs that use VMware and Hyper-V. An administrator needs to select the correct services to collect data for the initial migration discovery process. The data format should be supported by AWS Migration Hub. The company also needs the ability to generate reports from the data. Which solution meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use the AWS Agentless Discovery Connector for data collection on physical servers and all VMs. Store the collected data in Amazon S3. Query the data with S3 Select. Generate reports by using Kibana hosted on Amazon EC2.
       B. Use the AWS Application Discovery Service agent for data collection on physical servers and all VMs. Store the collected data in Amazon Elastic File System (Amazon EFS). Query the data and generate reports with Amazon Athena.
       C. Use the AWS Application Discovery Service agent for data collection on physical servers and Hyper-V. Use the AWS Agentless Discovery Connector for data collection on VMware. Store the collected data in Amazon S3. Query the data with Amazon Athena. Generate reports by using Amazon QuickSight.
       D. Use the AWS Systems Manager agent for data collection on physical servers. Use the AWS Agentless Discovery Connector for data collection on all VMs. Store, query, and generate reports from the collected data by using Amazon Redshift.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is using Amazon Aurora MySQL for a customer relationship management (CRM) application. The application requires frequent maintenance on the database and the Amazon EC2 instances on which the application runs. For AWS Management Console access, the system administrators authenticate against AWS Identity and Access Management (IAM) using an internal identity provider. For database access, each system administrator has a user name and password that have previously been configured within the database. A recent security audit revealed that the database passwords are not frequently rotated. The company wants to replace the passwords with temporary credentials using the company’s existing AWS access controls. Which set of options will meet the company’s requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a new AWS Systems Manager Parameter Store entry for each database password. Enable parameter expiration to invoke an AWS Lambda function to perform password rotation by updating the parameter value. Create an IAM policy allowing each system administrator to retrieve their current password from the Parameter Store. Use the AWS CLI to retrieve credentials when connecting to the database.
       B. Create a new AWS Secrets Manager entry for each database password. Configure password rotation for each secret using an AWS Lambda function in the same VPC as the database cluster. Create an IAM policy allowing each system administrator to retrieve their current password. Use the AWS CLI to retrieve credentials when connecting to the database.
       C. Enable IAM database authentication on the database. Attach an IAM policy to each system administrator’s role to map the role to the database user name. Install the Amazon Aurora SSL certificate bundle to the system administrators’ certificate trust store. Use the AWS CLI to generate an authentication token used when connecting to the database.
       D. Enable IAM database authentication on the database. Configure the database to use the IAM identity provider to map the administrator roles to the database user. Install the Amazon Aurora SSL certificate bundle to the system administrators’ certificate trust store. Use the AWS CLI to generate an authentication token used when connecting to the database.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company’s AWS architecture currently uses access keys and secret access keys stored on each instance to access AWS services. Database credentials are hard-coded on each instance. SSH keys for command-line remote access are stored in a secured Amazon S3 bucket. The company has asked its solutions architect to improve the security posture of the architecture without adding operational complexity. Which combination of steps should the solutions architect take to accomplish this? (Choose three.)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use Amazon EC2 instance profiles with an IAM role
       B. Use AWS Secrets Manager to store access keys and secret access keys
       C. Use AWS Systems Manager Parameter Store to store database credentials
       D. Use a secure fleet of Amazon EC2 bastion hosts for remote access
       E. Use AWS KMS to store database credentials                
       F. Use AWS Systems Manager Session Manager for remote access
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A B D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to change its internal cloud billing strategy for each of its business units. Currently, the cloud governance team shares reports for overall cloud spending with the head of each business unit. The company uses AWS Organizations to manage the separate AWS accounts for each business unit. The existing tagging standard in Organizations includes the application, environment, and owner. The cloud governance team wants a centralized solution so each business unit receives monthly reports on its cloud spending. The solution should also send notifications for any cloud spending that exceeds a set threshold. Which solution is the MOST cost-effective way to meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in each account to create monthly reports for each business unit.
       B. Configure AWS Budgets in the organization’s master account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in the organization’s master account to create monthly reports for each business unit.
       C. Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use the AWS Billing and Cost Management dashboard in each account to create monthly reports for each business unit.
       D. Enable AWS Cost and Usage Reports in the organization’s master account and configure reports grouped by application, environment, and owner. Create an AWS Lambda function that processes AWS Cost and Usage Reports, sends budget alerts, and sends monthly reports to each business unit’s email list
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is configuring connectivity to a multi-account AWS environment to support application workloads that serve users in a single geographic region. The workloads depend on a highly available, on-premises legacy system deployed across two locations. It is critical for the AWS workloads to maintain connectivity to the legacy system, and a minimum of 5 Gbps of bandwidth is required. All application workloads within AWS must have connectivity with one another. Which solution will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from a DX partner for each on–premises location. Create private virtual interfaces on each connection for each AWS account VPC. Associate the private virtual interface with a virtual private gateway attached to each VPC.
       B. Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from two DX partners for each on-premises location. Create and attach a virtual private gateway for each AWS account VPC. Create a DX gateway in a central network account and associate it with the virtual private gateways. Create a public virtual interface on each DX connection and associate the interface with the DX gateway.
       C. Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from two DX partners for each on-premises location. Create a transit gateway and a DX gateway in a central network account. Create a transit virtual interface for each DX interface and associate them with the DX gateway. Create a gateway association between the DX gateway and the transit gateway.
       D. Configure multiple AWS Direct Connect (DX) 10 Gbps dedicated connections from a DX partner for each on-premises location. Create and attach a virtual private gateway for each AWS account VPC. Create a transit gateway in a central network account and associate it with the virtual private gateways. Create a transit virtual interface on each DX connection and attach the interface to the transit gateway.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A financial company needs to create a separate AWS account for a new digital wallet application. The company uses AWS Organizations to manage its accounts. A solutions architect uses the IAM user Support1 from the master account to create a new member account with &lt;a href="mailto:finance1@example.com"&gt;finance1@example.com&lt;/a&gt; as the email address. What should the solutions architect do to create IAM users in the new member account?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Sign in to the AWS Management Console with AWS account root user credentials by using the 64- character password from the initial AWS Organizations email sent to finance1@example.com. Set up the IAM users as required.
       B. From the master account, switch roles to assume the OrganizationAccountAccessRole role with the account ID of the new member account. Set up the IAM users as required.
       C. Go to the AWS Management Console sign-in page. Choose “Sign in using root account credentials.” Sign in by using the email address finance1@example.com and the master account’s root password. Set up the IAM users as required.
       D. Go to the AWS Management Console sign-in page. Sign in by using the account ID of the new member account and the Support1 IAM credentials. Set up the IAM users as required.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A financial company needs to create a separate AWS account for a new digital wallet application. The company uses AWS Organizations to manage its accounts. A solutions architect uses the IAM user Support1 from the master account to create a new member account with &lt;a href="mailto:finance1@example.com"&gt;finance1@example.com&lt;/a&gt; as the email address. What should the solutions architect do to create IAM users in the new member account?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Sign in to the AWS Management Console with AWS account root user credentials by using the 64- character password from the initial AWS Organizations email sent to finance1@example.com. Set up the IAM users as required.
       B. From the master account, switch roles to assume the OrganizationAccountAccessRole role with the account ID of the new member account. Set up the IAM users as required.
       C. Go to the AWS Management Console sign-in page. Choose “Sign in using root account credentials.” Sign in by using the email address finance1@example.com and the master account’s root password. Set up the IAM users as required.
       D. Go to the AWS Management Console sign-in page. Sign in by using the account ID of the new member account and the Support1 IAM credentials. Set up the IAM users as required.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is designing a data processing platform to process a large number of files in an Amazon S3 bucket and store the results in Amazon DynamoDB. These files will be processed once and must be retained for 1 year. The company wants to ensure that the original files and resulting data are highly available in multiple AWS Regions. Which solution will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an S3 CreateObject event notification to copy the file to Amazon Elastic Block Store (Amazon EBS). Use AWS DataSync to sync the files between EBS volumes in multiple Regions. Use an Amazon EC2 Auto Scaling group in multiple Regions to attach the EBS volumes. Process the files and store the results in a DynamoDB global table in multiple Regions. Configure the S3 bucket with an S3 Lifecycle policy to move the files to S3 Glacier after 1 year.
       B. Create an S3 CreateObject event notification to copy the file to Amazon Elastic File System (Amazon EFS). Use AWS DataSync to sync the files between EFS volumes in multiple Regions. Use an AWS Lambda function to process the EFS files and store the results in a DynamoDB global table in multiple Regions. Configure the S3 buckets with an S3 Lifecycle policy to move the files to S3 Glacier after 1 year.
       C. Copy the files to an S3 bucket in another Region by using cross-Region replication. Create an S3 CreateObject event notification on the original bucket to push S3 file paths into Amazon EventBridge (Amazon CloudWatch Events). Use an AWS Lambda function to poll EventBridge (CloudWatch Events) to process each file and store the results in a DynamoDB table in each Region. Configure both S3 buckets to use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class and an S3 Lifecycle policy to delete the files after 1 year.
       D. Copy the files to an S3 bucket in another Region by using cross-Region replication. Create an S3 CreateObject event notification on the original bucket to execute an AWS Lambda function to process each file and store the results in a DynamoDB global table in multiple Regions. Configure both S3 buckets to use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class and an S3 Lifecycle policy to delete the files after 1 year.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is running an Apache Hadoop cluster on Amazon EC2 instances. The Hadoop cluster stores approximately 100 TB of data for weekly operational reports and allows occasional access for data scientists to retrieve data. The company needs to reduce the cost and operational complexity for storing and serving this data. Which solution meets these requirements in the MOST cost-effective manner?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Move the Hadoop cluster from EC2 instances to Amazon EMR. Allow data access patterns to remain the same.
       B. Write a script that resizes the EC2 instances to a smaller instance type during downtime and resizes the instances to a larger instance type before the reports are created.
       C. Move the data to Amazon S3 and use Amazon Athena to query the data for reports. Allow the data scientists to access the data directly in Amazon S3.
       D. Migrate the data to Amazon DynamoDB and modify the reports to fetch data from DynamoDB. Allow the data scientists to access the data directly in DynamoDB.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is building a sensor data collection pipeline in which thousands of sensors write data to an Amazon Simple Queue Service (Amazon SQS) queue every minute. The queue is processed by an AWS Lambda function that extracts a standard set of metrics from the sensor data. The company wants to send the data to Amazon CloudWatch. The solution should allow for viewing individual and aggregate sensor metrics and interactively querying the sensor log data using CloudWatch Logs Insights. What is the MOST cost-effective solution that meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Write the processed data to CloudWatch Logs in the CloudWatch embedded metric format.
       B. Write the processed data to CloudWatch Logs. Then write the data to CloudWatch by using the PutMetricData API call.
       C. Write the processed data to CloudWatch Logs in a structured format. Create a CloudWatch metric filter to parse the logs and publish the metrics to CloudWatch with dimensions to uniquely identify a sensor.
       D. Configure the CloudWatch Logs agent for AWS Lambda. Output the metrics for each sensor in statsd format with tags to uniquely identify a sensor. Write the processed data to CloudWatch Logs.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A car rental company has built a serverless REST API to provide data to its mobile app. The app consists of an Amazon API Gateway API with a Regional endpoint, AWS Lambda functions, and an Amazon Aurora MySQL Serverless DB cluster. The company recently opened the API to mobile apps of partners. A significant increase in the number of requests resulted, causing sporadic database memory errors. Analysis of the API traffic indicates that clients are making multiple HTTP GET requests for the same queries in a short period of time. Traffic is concentrated during business hours, with spikes around holidays and other events. The company needs to improve its ability to support the additional usage while minimizing the increase in costs associated with the solution. Which strategy meets these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Convert the API Gateway Regional endpoint to an edge-optimized endpoint. Enable caching in the production stage.
       B. Implement an Amazon ElastiCache for Redis cache to store the results of the database calls. Modify the Lambda functions to use the cache.
       C. Modify the Aurora Serverless DB cluster configuration to increase the maximum amount of available memory.
       D. Enable throttling in the API Gateway production stage. Set the rate and burst values to limit the incoming calls.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company has application services that have been containerized and deployed on multiple Amazon EC2 instances with public IPs. An Apache Kafka cluster has been deployed to the EC2 instances. A PostgreSQL database has been migrated to Amazon RDS for PostgreSQL. The company expects a significant increase of orders on its platform when a new version of its flagship product is released. What changes to the current architecture will reduce operational overhead and support the product release?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an EC2 Auto Scaling group behind an Application Load Balancer. Create additional read replicas for the DB instance. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3.
       B. Create an EC2 Auto Scaling group behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3.
       C. Deploy the application on a Kubernetes cluster created on the EC2 instances behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution.
       D. Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate and enable auto scaling behind an Application Load Balancer. Create additional read replicas for the DB instance. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company recently completed a large-scale migration to AWS. Development teams that support various business units have their own accounts in AWS Organizations. A central cloud team is responsible for controlling which services and resources can be accessed, and for creating operational strategies for all teams within the company. Some teams are approaching their account service quotas. The cloud team needs to create an automated and operationally efficient solution to proactively monitor service quotas. Monitoring should occur every 15 minutes and send alerts when a team exceeds 80% utilization. Which solution will meet these requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a scheduled AWS Config rule to trigger an AWS Lambda function to call the GetServiceQuota API. If any service utilization is above 80%, publish a message to an Amazon Simple Notification Service (Amazon SNS) topic to alert the cloud team. Create an AWS CloudFormation template and deploy the necessary resources to each account.
       B. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that triggers an AWS Lambda function to refresh the AWS Trusted Advisor service limits checks and retrieve the most current utilization and service limit data. If the current utilization is above 80%, publish a message to an Amazon Simple Notification Service (Amazon SNS) topic to alert the cloud team. Create AWS CloudFormation StackSets that deploy the necessary resources to all Organizations accounts.
       C. Create an Amazon CloudWatch alarm that triggers an AWS Lambda function to call the Amazon CloudWatch GetInsightRuleReport API to retrieve the most current utilization and service limit data. If the current utilization is above 80%, publish an Amazon Simple Email Service (Amazon SES) notification to alert the cloud team. Create AWS CloudFormation StackSets that deploy the necessary resources to all Organizations accounts.
       D. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that triggers an AWS Lambda function to refresh the AWS Trusted Advisor service limits checks and retrieve the most current utilization and service limit data. If the current utilization is above 80%, use Amazon Pinpoint to send an alert to the cloud team. Create an AWS CloudFormation template and deploy the necessary resources to each account.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An AWS customer has a web application that runs on premises. The web application fetches data from a third-party API that is behind a firewall. The third party accepts only one public CIDR block in each client’s allow list. The customer wants to migrate their web application to the AWS Cloud. The application will be hosted on a set of Amazon EC2 instances behind an Application Load Balancer (ALB) in a VPC. The ALB is located in public subnets. The EC2 instances are located in private subnets. NAT gateways provide internet access to the private subnets. How should a solutions architect ensure that the web application can continue to call the third party API after the migration?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Associate a block of customer-owned public IP addresses to the VPC. Enable public IP addressing for public subnets in the VPC.
       B. Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC.
       C. Create Elastic IP addresses from the block of customer-owned IP addresses. Assign the static Elastic IP addresses to the ALB.
       D. Register a block of customer-owned public IP addresses in the AWS account. Set up AWS Global Accelerator to use Elastic IP addresses from the address block. Set the ALB as the accelerator endpoint.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company is using AWS Organizations to manage multiple AWS accounts. For security purposes, the company requires the creation of an Amazon Simple Notification Service (Amazon SNS) topic that enables integration with a third-party alerting system in all the Organizations member accounts. A solutions architect used an AWS CloudFormation template to create the SNS topic and stack sets to automate the deployment of CloudFormation stacks. Trusted access has been enabled in Organizations. What should the solutions architect do to deploy the CloudFormation StackSets in all AWS accounts?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a stack set in the Organizations member accounts. Use service-managed permissions. Set deployment options to deploy to an organization. Use CloudFormation StackSets drift detection.
       B. Create stacks in the Organizations member accounts. Use self-service permissions. Set deployment options to deploy to an organization. Enable the CloudFormation StackSets automatic deployment.
       C. Create a stack set in the Organizations master account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets automatic deployment.
       D. Create stacks in the Organizations master account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets drift detection.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A company wants to provide a desktop as a service (DaaS) to a number of employees using Amazon WorkSpaces. WorkSpaces will need to access files and services hosted on premises with authorization based on the company’s Active Directory. Network connectivity will be provided through an existing AWS Direct Connect connection. The solution has the following requirements: Credentials from Active Directory should be used to access on-premises files and services. Credentials from Active Directory should not be stored outside the company. End users should have a single sign-on (SSO) to on-premises files and services once connected to WorkSpaces. Which strategy should the solutions architect use for end user authentication?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) directory within the WorkSpaces VPC. Use the Active Directory Migration Tool (ADMT) with the Password Export Server to copy users from the on-premises Active Directory to AWS Managed Microsoft AD. Set up a one-way trust allowing users from AWS Managed Microsoft AD to access resources in the on-premises Active Directory. Use AWS Managed Microsoft AD as the directory for WorkSpaces.
       B. Create a service account in the on-premises Active Directory with the required permissions. Create an AD Connector in AWS Directory Service to be deployed on premises using the service account to communicate with the on-premises Active Directory. Ensure the required TCP ports are open from the WorkSpaces VPC to the on-premises AD Connector. Use the AD Connector as the directory for WorkSpaces.
       C. Create a service account in the on-premises Active Directory with the required permissions. Create an AD Connector in AWS Directory Service within the WorkSpaces VPC using the service account to communicate with the on-premises Active Directory. Use the AD Connector as the directory for WorkSpaces.
       D. Create an AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) directory in the AWS Directory Service within the WorkSpaces VPC. Set up a one-way trust allowing users from the on-premises Active Directory to access resources in the AWS Managed Microsoft AD. Use AWS Managed Microsoft AD as the directory for WorkSpaces. Create an identity provider with AWS Identity and Access Management (IAM) from an on-premises ADFS server. Allow users from this identity provider to assume a role with a policy allowing them to run WorkSpaces.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hint Answer: C&lt;/p&gt;

</description>
      <category>awslagi</category>
      <category>aws</category>
      <category>googlecloud</category>
    </item>
    <item>
      <title>AWS Certified Solutions Architect Professional SAP-C01 Exam Questions Part 1</title>
      <author>awslagi.com</author>
      <pubDate>Sun, 22 Aug 2021 14:15:25 +0000</pubDate>
      <link>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-1-3jk7</link>
      <guid>https://dev.to/iam_awslagi/aws-certified-solutions-architect-professional-sap-c01-exam-questions-part-1-3jk7</guid>
      <description>&lt;p&gt;Source:&lt;/p&gt;

&lt;p&gt;For AWS: &lt;a href="https://www.awslagi.com"&gt;https://www.awslagi.com&lt;/a&gt;&lt;br&gt;&lt;br&gt;
For GCP: &lt;a href="https://www.gcp-examquestions.com"&gt;https://www.gcp-examquestions.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;1.Company A has hired you to assist with the migration of an interactive website that allows registered users to rate local restaurants. Updates to the ratings are displayed on the home page, and ratings are updated in real time. Althoughthe website is not very popular today, the company anticipates that it willgrow rapidly over the next few weeks. Theywant the site to be highly available. The current architecture consists of a single Windows Server 2008 R2 web server and a MySQL database running on Linux. Both reside inside an on-premises hypervisor. What would be the most efficient way to transfer the application to AWS, ensuring performance and high-availability?&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;           A. Use AWS VM Import/Export to create an Amazon Elastic Compute Cloud (EC2) Amazon Machine Image (AMI) of the web server. Configure Auto Scaling to launch two web servers in us-west-1a and two in us-est-1b. Launch a Multi-AZ MySQL Amazon Relational Database Service (RDS) instance in us-west-1b. Import the data into Amazon RDS from the latest MySQL backup. Use Amazon Route_53 to create a hosted zone and point an A record to the elastic load balancer
           B. Export web files to an Amazon S3 bucket in us-west-1. Run the website directly out of Amazon S3. Launch a multi-AZ MySQL Amazon RDS instance in us-west-1a. Import the data into Amazon RDS from the latest MySQL backup. Use Route 53 and create an alias record pointing to the elastic load balancer
           C. Use AWS VM Import/Export to create an Amazon EC2 AMI of the web server. Configure auto-scaling to launch two web servers in us-west-1a and two in us-west-1b. Launch a multi-AZ MySQL Amazon RDS instance in us-west-1a. Import the data into Amazon RDS from the latest MySQL backup. Create an elastic load balancer to front your web servers. Use Amazon Route 53 and create an A record pointing to the elastic load balancer
           D. Launch two Windows Server 2008 R2 instances in us-west-1b and two in Us-west-1a. Copy the web files from on premises web server to each Amazon EC2 web server, using Amazon S3 as the repository. Launch a multi-AZ MySQL Amazon RDS instance in us-west-2a. Import the data into Amazon RDS from the latest MySQL backup. Create an elastic load balancer to front your web servers. Use Route 53 and create an alias record pointing to the elastic load balancer.
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A marketing research company has developed a tracking system that collects user behavior during web marketing campaigns on behalf of their customers all over the world. The tracking system consists of an auto-scaled group of Amazon Elastic Compute Cloud (EC2) instances behind an elastic load balancer (ELB), and the collected data is stored in Amazon DynamoDB. After the campaign is terminated, the tracking system is torn down and the data is moved to Amazon Redshift, where it is aggregated, analyzed and used to generate detailed reports. The company wants to be able to instantiate new tracking systems in any region without any manual intervention and therefore adopted AWS CloudFormation. What needs to be done to make sure that the AWS CloudFormation template works in every AWS region? (Choose 2 answers)&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Avoid using DeletionPolicies for EBS snapshots
       B. The names of the Amazon DynamoDB tables must be different in every target region
       C. Use the built-in Mappings and FindInMap functions of AWS CloudFormation to refer to the AMI ID set in the ImageId attribute of the Auto Scaling::LaunchConfiguration resource
       D. IAM users with the right to start AWS CloudFormation stacks must be defined for every target region.
       E. Use the built-in function of AWS CloudFormation to set the AvailabilityZone attribute of the ELB resource
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A development team that is currently doing a nightly six-hour build which is lengthening over time on-premises with a large and mostly underutilized server would like to transition to a continuous integration model of development on AWS with multiple builds triggered within the same day. However, they areare concerned about cost, security, and how to integrate with existing on-premises applications such as their LDAP and email servers which cannot move off-premises. The development environment needs a source code repository, a project management system with a MySQL database, resources for performing the builds, and a storage location for QA to pick up builds from. What AWS services combination would you recommend to meet the development team’s requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. A Bastion host Amazon Elastic Compute Cloud (EC2) instance running a VPN server for access from on-premises, Amazon EC2 for the source code repository with attached Amazon Elastic Block Store (EBS) volumes, Amazon EC2 and Amazon Relational Database Service (RDS) MySQL for the project management system, EIPs for the source code repository and project management system, Amazon Simple Queue Service (SQS) for a build queue, An Auto Scaling group of Amazon EC2 instances for performing builds, and Amazon Simple Email Service for sending the build output
       B. An AWS Storage Gateway for connecting on-premises software applications with cloud-based storage securely, Amazon EC2 for the source code repository with attached Amazon EBS volumes, Amazon EC2 and Amazon RDS MySQL for the project management system, EIPs for the source code repository and project management system, Amazon Simple Notification Service (SNS) for a notification-initiated build, An Auto Scaling group of Amazon EC2 instances for performing builds, and Amazon S3 for the build output.
       C. An AWS Storage Gateway for connecting on-premises software applications with cloud-based storage securely, Amazon EC2 for the source code repository with attached Amazon EBS volumes, Amazon EC2 and Amazon RDS MySQL for the project management system, EIPs for the source code repository and project management system, Amazon SQS for a build queue, An Amazon Elastic MapReduce (EMR) cluster of Amazon EC2 instances for performing builds, and Amazon CloudFront for the build output.
       D. A VPC with a VPN Gateway back to their on-premises servers, Amazon EC2 for the source-code repository with attached Amazon EBS volumes, Amazon EC2 and Amazon RDS MySQL for the project management system, EIPs for the source code repository and project management system, SQS for a build queue, An Auto Scaling group of EC2 instances for performing builds, and S3 for the build output
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A large enterprise wants to adopt CloudFormation to automate administrative tasks and implement the security principles of least priviledge and separation of duties. They have identified the following roles with the corresponding tasks in the company:&lt;/li&gt;
&lt;li&gt; Network administrators: create, modify and delete VPCs, subnets, NACLs, routing tables, and security groups application operators: deploy complete application stacks (ELB, Auto-Scaling groups, RDS) whereas all resources must be deployed in the VPCs managed by the network administrators.&lt;/li&gt;
&lt;li&gt;Both groups must maintain their own CloudFormation templates and should be able to create, update and delete only their own CloudFormation stacks. &lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The company has followed your advice to create two IAM groups, one for applications and one for - networks. Both IAM groups are attached to IAM policies that grant rights to perform the necessary task of each group as well as the creation, update and deletion of CloudFormation stacks. &lt;br&gt;
Given setup and requirements, which statements represent valid design considerations? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Network stack updates will fail upon attempts to delete a subnet with EC2 instances
       B. Restricting the launch of EC2 instances into VPCs requires resource level permissions in the IAM policy of the application group
       C. Nesting network stacks within application stacks simplifies management and debugging, but requires resource level permissions in the IAM policy of the network group
       D. Unless resource level permissions are used on the cloudformation:DeleteStack action, network administrators could tear down application stacks
       E. The application stack cannot be deleted before all network stacks are deleted
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To enable end-to-end HTTPS connections from the userˈs browser to the origin via CloudFront, which of the following options would be valid? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use a self signed certificate in the origin and CloudFront default certificate in CloudFront
       B. Use the CloudFront default certificate in both the origin and CloudFront
       C. Use third-party CA certificate in the origin and CloudFront default certificate in CloudFront
       D. Use third-party CA certificate in both the origin and CloudFront
       E. Use a self signed certificate in both the origin and CloudFront
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A customer is runningan application in US-West (Northern California) region and wants to setup disaster recovery failover to the Asian Pacific (Singapore) region.The customer isinterested in achieving a low Recovery Point Objective (RPO) foran Amazon Relational DatabaseService(RDS) multi-AZ MySQL database instance. Which approach is best suited to this need?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Synchronous replication
       B. Asynchronous replication
       C. Route53 health checks
       D. Copying of RDS incremental snapshots
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A document storage company is deploying their application to AWS and changing their business model to support both Free Tier and Premium Tier users. The Premium Tier &lt;br&gt;
users will be allowed to store up to 200GB of data and Free Tier customers will be allowed to store only 5GB. The customer expects that billions of files will be stored. All users need to be alerted when approaching 75 percent quota utilization and again at 90 percent quota use. To support the Free Tier and Premium Tier users, how should they architect their application?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. The company should utilize an Amazon Simple Workflow Service activity worker that updates the userˈs used data counter in Amazon DynamoDB. The Activity Worker will use Simple Email Service to send an email if the counter increases above the appropriate thresholds.
       B. The company should deploy an Amazon Relational Database Service (RDS) relational database with a stored objects table that has a row for each stored object along with the size of each object. The upload server will query the aggregate consumption of the user in question (by first determining the files stored by the user, and then querying the stored objects table for respective file sizes) and send an email via Amazon Simple Email Service if the thresholds are breached.
       C. The company should write both the content length and the username of the files owner as S3 metadata for the object. They should then create a a file watcher to iterate over each object and aggregate the size for each user and send a notification via Amazon Simple Queue Service to an emailing service if the storage threshold is exceeded
       D. The company should create two separate Amazon Simple Storage Service buckets, one for data storage for Free Tier Users, and another for data storage for Premium Tier users. An Amazon Simple Workflow Service activity worker will query all objects for a given user based on the bucket the data is stored in and aggregate storage. The activity worker will notify the user via Amazon Simple Notification Service when necessary.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A public archives organization is about to move a pilot application they are running on AWS into production. You have been hired to analyze their application architecture and give cost-saving recommendations. The application displays scanned historical documents. Each document is split into individual image tiles at multiple zoom levels to improve responsiveness and ease of use for the end users. At maximum zoom level the average document will be 8000x 6000 pixels in size, split into multiple 40pxx 40px image tiles. The tiles are batch processed by Amazon Elastic Compute Cloud (EC2) instances, and put into an Amazon Simple Storage Service(S3) bucket.A browser-based JavaScript viewer fetches tiles from the Amazon (S3) bucket and displays them to users as they zoom and pan around each document. The average storage size of all zoom levels for a document is approximately 30MB of JPEG tiles. Originals of each document are archived in Amazon Glacier. The company expects to process and host over 500,000 scanned documents in the first year. What are your recommendations? Choose 3 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Deploy an Amazon CloudFront distribution in front of the Amazon S3 tiles bucket
       B. Increase the size (width/height) of the individual tiles at the maximum zoom level
       C. Store the maximum zoom level in the low cost Amazon S3 Glacier option and only retrieve the most frequently access tiles as they are requested by users.
       D. Use Amazon S3 Reduced Redundancy Storage for each zoom level.
       E. Decrease the size (width/height) of the individual tiles at the maximum zoom level.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A B D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Your multi-national customer wants to rewrite a website portal to “take advantage of AWS best practices”. Other information that you have for this large Enterprise customer is as follows:&lt;/li&gt;
&lt;li&gt;Part of the portal is an employee-only section, and authentication must be against the corporate Active Directory.
• You used a web analytics website to discover that on average there were 140,000 visitors per month over the past year, a peak of 187,000 unique visitors last month, and a minimum of 109,000 unique visitors two months ago. You have no information about what percentage of these visitors represents employees who signed into the portal.&lt;/li&gt;
&lt;li&gt; The web analytics website also revealed that traffic breakdown is 40 percent South America, 50 percent North America, and 10 percent other.&lt;/li&gt;
&lt;li&gt;The customer’s primary data center is located in Sao Paulo Brazil.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Their chief technology officer believes that response time for logging in to the employee portal is a primary metric, because employees complain that the current website is too slow in this regard.&lt;br&gt;
When you present your proposed application architecture to the customer, which of the following should you propose as part of the architecture? Choose 3 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. A three-subnet VPC, with an AD controller in the AWS region. The AWS AD controller will be part of the primary AD controller’s forest, and will synchronize with the corporate controller over a dedicated pipe to the corporate data center
       B. Do not use Amazon CloudFront, because the employees who log in to the portal have unique (private) session data that should not be cached in a content delivery network.
       C. A three-subnet VPC, with all AD calls traversing a dedicated pipe to the corporate data center
       D. Establish the AWS presence in the US-EAST region, with a dedicated pipe to the corporate data center.
       E. Establish the AWS presence in multiple regions: SA-EAST, and also US-EAST, with a dedicated pipe from both SA-EAST and US-EAST to the corporate data center – and also a dedicated connection between regions. Replicate data as needed between the regions. Use a geo load balancer to determine which region is primary for a given user.
       F. Use Amazon CloudFront to cache pages for users at the nearest edge location.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A D F&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;For a 3-tier, customer facing, inclement weather site utilizing a MySQL database running in a Region which has two AZs (Availability Zone), which architecture provides fault tolerance within the Region for the application that minimally requires 6 web tier servers and 6 application tier servers running in the web and application tiers and one MySQL database?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. A web tier deployed in 2 AZs with 6 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (Elastic Load Balancer), and an application tier deployed in 2 AZs with 6 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB, and a Multi-AZ RDS (Relational Database Service) deployment
       B. A web tier deployed in 2 AZs with 3 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (Elastic Load Balancer), and an application tier deployed in 2 AZs with 3 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB, and a Multi-AZ RDS (Relational Database Service) deployment.
       C. A web tier deployed in 2 AZs with 3 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (Elastic Load Balancer), and an application tier deployed in 2 AZs with 6 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB, and one RDS (Relational Database Service) instance deployed with read replicas in the other AZ.
       D. A web tier deployed in 1 AZ with 6 EC2 (Elastic Compute Cloud) instances inside an Auto Scaling Group behind an ELB (Elastic Load Balancer), and an application tier deployed in the same AZ with 6 EC2 instances inside an Auto Scaling Group behind an ELB, and a Multi-AZ RDS (Relational Database Service) deployment, with 6 stopped web tier EC2 instances and 6 stopped application tier EC2 instances all in the other AZ ready to be started if any of the running instances in the first AZ fails.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A gaming company adopted AWS Cloud Formation to automate load-testing of their games. They have created an AWS Cloud Formation template for each gaming environment and one for the load-testing stack. The load-testing stack creates an Amazon Relational Database Service (RDS) Postgres database and two web servers running on Amazon Elastic Compute Cloud (EC2) that send HTTP requests, measure response times, and write the results into the database. A test run usually takes between 15 and 30 minutes. Once the tests are done, the AWS CloudFormation stacks are torn down immediately. The test results written to the Amazon RDS database must remain accessible for visualization and analysis. Select possible solutions that allow access to the test results after the AWS Cloud Formation load-testing stack is deleted. Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Define an update policy to prevent deletion of the Amazon RDS database after the AWS CloudFormation stack is deleted.
       B. Define a deletion policy of type Snapshot for the Amazon RDS resource to assure that the RDS database can be restored after the AWS CloudFormation stack is deleted.
       C. Define automated backups with a backup retention period of 30 days for the Amazon RDS database and perform point-in-time recovery of the database after the AWS CloudFormation stack is deleted.
       D. Define an Amazon RDS Read-Replica in the load-testing AWS CloudFormation stack and define a dependency relation between master and replica via the DependsOn attribute
       E. Define a deletion policy of type Retain for the Amazon RDS resource to assure that the RDS database is not deleted with the AWS CloudFormation stack.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B E&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You are an architect for a news-sharing mobile application. Anywhere in the world, your users can see local news on topics they choose. They can post pictures and videos from inside the application. Since the application is being used on a mobile phone, connection stability is required for uploading content, and delivery should be quick. Content is accessed a lot in the first minutes after it has been posted, but is quickly replaced by new content before disappearing. The local nature of the news means that 90 percent of the uploaded content is then read locally (less than a hundred kilometers from where it was posted). What solution will optimize the user experience when users upload and view content (by minimizing page load times and minimizing upload times)?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Upload and store the content in a central Amazon Simple Storage Service (S3) bucket, and use an Amazon CloudFront Distribution for content delivery.
       B. Upload and store the content in an Amazon Simple Storage Service (S3) bucket in the region closest to the user, and use multiple Amazon CloudFront distributions for content delivery
       C. Upload the content to an Amazon Elastic Compute Cloud (EC2) instance in the region closest to the user, send the content to a central Amazon Simple Storage Service (S3) bucket, and use an Amazon CloudFront distribution for content delivery.
       D. Use an Amazon CloudFront distribution for uploading the content to a central Amazon Simple Storage Service (S3) bucket and for content delivery.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A customer is deploying an SSL enabled Web application to AWS and would like to implement a separation of roles between the EC2 service administrators that are entited to login to Instances as well as making API calls and the security officers who will maintain and have exclusive access to the applicationˈs X.509 certificate that contains the private key. Which configuration option could satisfy the above requirement?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure the web servers to retrieve the certificate upon boot from an CloudHSM that is managed by the security officers.
       B. Configure system permissions on the web servers to restrict access to the certificate only to the authorized security officers.
       C. Configure IAM policies authorizing access to the certificate store only to the security officers and terminate SSL on an ELB.
       D. Upload the certificate on an S3 bucket owned by the security officers and accessible only by the EC2 Role of the web servers.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You are designing security inside your VPC. You are considering the options for establishing separate security zones, and enforcing network traffic rules across the different zones to limit which instances can communicate. How would you accomplish these requirements? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure multiple subnets in your VPC, one for each zone. Configure routing within your VPC in such a way that each subnet only has routes to other subnets with which it needs to communicate, and doesnˈt have routes to subnets with which it shouldnˈt be able to communicate.
       B. Configure your instances to use pre-set IP addresses with an IP address range for every security zone. Configure NACLs to explicitly allow or deny communication between the different IP address ranges, as required for interzone communication.
       C. Configure a security group for every zone. Configure a default allow all rule. Configure explicit deny rules for the zones that shouldnˈt be able to communicate with one another
       D. Configure a security group for every zone. Configure allow rules only between zones that need to be able to communicate with one another. Use the implicit deny all rule to block any other traffic.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your company currently has a highly available web application running in production. The application’s web front-end utilizes an Elastic Load Balancerand Auto Scaling across three Availability Zones.During peak load, your web servers operate at 90% utilization and leverage a combination of Heavy Utilization Reserved Instances for steady state load and On-Demand and Spot Instances for peak load. You are tasked with designing a cost effective architecture to allow the application to recover quickly in the event that an Availability Zoneis unavailable during peak load. Which option provides the most cost effective high availability architectural design for this application?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Continue to run your web front-end at 90% utilization, but leverage a high bid price strategy to cover the loss of any of the other Availability Zones during peak load.
       B. Increase use of spot instances to cost effectively scale the web front-end across all Availability Zones to lower aggregate utilization levels that will allow an Availability Zone to fail during peak load without affecting the application’s availability.
       C. Increase Auto Scaling capacity and scaling thresholds to allow the web front-end to cost effectively scale across all Availability Zones to lower aggregate utilization levels that will allow an Availability Zone to fail during peak load without affecting the application’s availability.
       D. Continue to run your web front-end at 90% utilization, but purchase an appropriate number of light utilization RIs in each Availability Zone to cover the loss of any of the other Availability Zones during peak load.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An Enterprise customer is starting their migration to the cloud, their main reason for migrating is agility, and they want to make their internal Microsoft Active Directory available to any applications running on AWS; this is so internal users only have to remember one set of credentials and as a central point of user control for leavers and joiners. How could they make their Active Directory secure, and highly available, with minimal on-premises infrastructure changes, in the most cost and time-efficient way? Choose the most appropriate:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Using Amazon Elastic Compute Cloud (EC2), they could create a DMZ using a security group; within the security group they could provision two smaller Amazon EC2 instances that are running Openswan for resilient IPSEC tunnels, and two larger instances that are domain controllers; they would use multiple Availability Zones
       B. Using VPC, they could create an extension to their data center and make use of resilient hardware IPSEC tunnels; they could then have two domain controller instances that are joined to their existing domain and reside within different subnets, in different Availability Zones.
       C. Within the customerˈs existing infrastructure, they could provision new hardware to run Active Directory Federation Services; this would present Active Directory as a SAML2 endpoint on the internet; any new application on AWS could be written to authenticate using SAML2.
       D. The customer could create a stand-alone VPC with its own Active Directory Domain Controllers; two domain controller instances could be configured, one in each Availability Zone; new applications would authenticate with those domain controllers.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;An AWS customer is deploying a web application that is composed of a front-end running on Amazon EC2 and confidential data that is stored on Amazon S3. The customers security policy requires that the all access operations to this sensitive data must be authenticated and authorized by a centralized access management system that is operated by a separate security team. In addition, the web application team that owns and administers the EC2 web front-end instances is prohibited from having any ability to access the data that circumvents this centralized access management system. Which of the following configurations will support these requirements:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure the web application to authenticate end-users against the centralized access management system. Have the web application provision trusted users STS tokens entitling the download of approved data directly from Amazon S3.
       B. Encrypt the data on Amazon S3 using a CloudHSM that is operated by the separate security team. Configure the web application to integrate with the CloudHSM for decrypting approved data access operations for trusted end-users.
       C. Configure the web application to authenticate end-users against the centralized access management system using SAML. Have the end-users authenticate to IAM using their SAML token and download the approved data directly from Amazon S3.
       D. Have the separate security team create an IAM Role that is entitled to access the data on Amazon S3. Have the web application team provision their instances with this Role while denying their IAM users access to the data on Amazon S3.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You have been asked to design network connectivity between your existing data centers and AWS. Your application’s EC2 instances must be able to connect to existing backend resources located in your data center. Network traffic between AWS and your data centers will start small, but ramp up to 10s of GB per second over the course of several months. The success of your application is dependent upon getting to market quickly. Which of the following design options will allow you to meet your objectives?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Quickly submit a DirectConnect request to provision a 1 Gbps cross connect between your data center and VPC, then increase the number or size of your DirectConnect connections as needed
       B. Quickly create an internal ELB for your backend applications, submit a DirectConnect request to provision a 1 Gbps cross connect between your data center and VPC, then increase the number or size of your DirectConnect connections as needed
       C. Allocate EIPs and an Internet Gateway for your VPC instances to use for quick, temporary access to your backend applications, then provision a VPN connection between a VPC and existing on-premises equipment.
       D. Provision a VPN connection between a VPC and existing on-premises equipment, submit a DirectConnect partner request to provision cross connects between your data center and the DirectConnect location, then cut over from the VPN connection to one or more DirectConnect connections as needed.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You have an application running on an EC2 instance which will allow users to download files from a private S3 bucket using a pre-signed URL. Before generating the URL, the application should verify the existence of the file in S3. How should the application use AWS credentials to access the S3 bucket securely?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create an IAM user for the application with permissions that allow list access to the S3 bucket; launch the instance as the IAM user, and retrieve the IAM user’s credentials from the EC2 instance user data.
       B. Create an IAM role for EC2 that allows list access to objects in the S3 bucket; launch the instance with the role, and retrieve the role’s credentials from the EC2 instance metadata
       C. Use the AWS account access keys; the application retrieves the credentials from the source code of the application.
       D. Create an IAM user for the application with permissions that allow list access to the S3 bucket; the application retrieves the IAM user credentials from a temporary directory with permissions that allow read access only to the application user.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A startup deploys its photo-sharing site in a VPC. An elastic load balancer distributes web traffic across two subnets. The load balancer session stickiness is configured to use the AWS-generated session cookie, with a session TTL of 5 minutes. The web server Auto Scaling group is configured as min-size=4, max-size=4. The startup is preparing for a public launch, by running load-testing software installed on a single Amazon Elastic Compute Cloud (EC2) instance running in us-west-2a. After 60 minutes of load-testing, the web server logs show the following:&lt;br&gt;
+———————-+————————-+&lt;br&gt;
| # of HTTP requests | # of HTTP requests |&lt;br&gt;
WEBSERVER LOGS | from load-tester | from private beta users |&lt;br&gt;
+—————————————|———————-|————————-+&lt;br&gt;
| webserver #1 (subnet in us-west-2a): | 19,210 | 434 |&lt;br&gt;
| webserver #2 (subnet in us-west-2a): | 21,790 | 490 |&lt;br&gt;
| webserver #3 (subnet in us-west-2b): | 0 | 410 |&lt;br&gt;
| webserver #4 (subnet in us-west-2b): | 0 | 428 |&lt;br&gt;
+—————————————+———————-+————————-+&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Which recommendations can help ensure that load-testing HTTP requests are evenly distributed across the four webservers?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Choose 2 answers&lt;br&gt;
           B. Launch and run the load-tester Amazon EC2 instance from us-east-1 instead.&lt;br&gt;
           C. Use a third-party load-testing service which offers globally distributed test clients.&lt;br&gt;
           D. Configure Elastic Load Balancing and Auto Scaling to distribute across us-west-2a and us-west-2b.&lt;br&gt;
           E. Configure Elastic Load Balancing session stickiness to use the app-specific session cookie&lt;br&gt;
           F. Re-configure the load-testing software to re-resolve DNS for each web request.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C F&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To meet regulatory requirements, a pharmaceuticals company needs to archive data after a drug trial test is concluded. Each drug trial test may generate up to several thousands of files, with compressed file sizes ranging from 1 byte to 100MB. Once archived, data rarely needs to be restored, and on the rare occasion when restoration is needed, the company has 24 hours to restore specific files that match certain metadata. Searches must be possible by numeric file ID, drug name, participant names, date ranges, and other metadata. Which is the most cost-effective architectural approach that can meet the requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Store individual compressed files and search metadata in Amazon Simple Storage Service (S3). Create a lifecycle rule to move the data to Amazon Glacier, after a certain number of days. When restoring data, query the Amazon S3 bucket for files matching the search criteria, and retrieve the file to S3 reduced redundancy in order to move it back to S3 Standard class.
       B. Store individual files in Amazon Glacier, using the file ID as the archive name. When restoring data, query the Amazon Glacier vault for files matching the search criteria.
       C. First, compress and then concatenate all files for a completed drug trial test into a single Amazon Glacier archive. Store the associated byte ranges for the compressed files along with other search metadata in an Amazon RDS database with regular snapshotting. When restoring data, query the RDS database for files that match the search criteria, and create restored files from the retrieved byte ranges
       D. Store individual files in Amazon S3, and store search metadata in an Amazon Relational Database Service (RDS) multi-AZ database. Create a lifecycle rule to move the data to Amazon Glacier after a certain number of days. When restoring data, query the Amazon RDS database for files matching the search criteria, and move the files matching the search criteria back to S3 Standard class.
       E. Store individual files in Amazon Glacier, and store the search metadata in an Amazon RDS multi-AZ database. When restoring data, query the Amazon RDS database for files matching the search criteria, and retrieve the archive name that matches the file ID returned from the database query.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You have been asked to virtually extend two existing data centers into AWS to support a highly available application that depends on existing, on-premises resources located in multiple data centers and static content that is served from an Amazon Simple Storage Service (S3) bucket. Your design currently includes a dual-tunnel VPN connection between your CGW and VGW. Which component of your architecture represents a potential single point of failure that you should consider changing to make the solution more highly available?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Add another CGW in a different data center and create another dual-tunnel VPN connection
       B. Add a second VGW in a different Availability Zone, and a CGW in a different data center, and create another dual-tunnel.
       C. No changes are necessary: the network architecture is currently highly available
       D. Add another VGW in a different Availability Zone and create another dual-tunnel VPN connection
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your company has recently extended its datacenter into a VPC on AWS to add burst computing capacity as needed. Members of your Network Operations Center need to be able to go to the AWS Management Console and administer Amazon EC2 instances as necessary. You donˈt want to create new IAM users for each NOC member and make those users sign in again to the AWS Management Console. Which option below will meet the needs for your NOC members&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use Web Identity Federation to retrieve AWS temporary security credentials to enable your NOC members to sign in to the AWS Management Console.
       B. Use your on-premises SAML 2.0-compliant identity provider (IdP) to retrieve temporary security credentials to enable NOC members to sign in to the AWS Management Console.
       C. Use OAuth 2.0 to retrieve temporary AWS security credentials to enable your NOC members to sign in to the AWS Management Console.
       D. Use your on-premises SAML 2.0-compliant identity provider (IdP) to grant the NOC members federated access to the AWS Management Console via the AWS single sign-on (SSO) endpoint.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A media production company wants to deliver high-definition raw video material for preproduction and dubbing to customers all around the world. They would like to use Amazon CloudFront for their scenario, and they require the ability to limit downloads per customer and video file to a configurable number. A CloudFront download distribution with TTL = 0 was already setup to make sure all client HTTP requests hit an authentication backend on Amazon Elastic Compute Cloud (EC2)/Amazon Relational Database Service (RDS) first, which is responsible for restricting the number of downloads. Content is stored in Amazon Simple Storage Service (S3) and configured to be accessible only via CloudFront. What else needs to be done to achieve an architecture that meets the requirements? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Enable CloudFront logging into an Amazon S3 bucket, leverage Amazon Elastic MapReduce (EMR) to analyze CloudFront logs to determine the number of downloads per customer, and return the content S3 URL unless the download limit is reached.
       B. Enable CloudFront logging into an Amazon S3 bucket, let the authentication backend determine the number of downloads per customer by parsing those logs, and return the content S3 URL unless the download limit is reached
       C. Enable URL parameter forwarding, let the authentication backend count the number of downloads per customer in Amazon RDS, and return the content S3 URL unless the download limit is reached
       D. Configure a list of trusted signers, let the authentication backend count the number of download requests per customer in Amazon RDS, and return a dynamically signed URL unless the download limit is reached.
       E. Enable URL parameter forwarding, let the authentication backend count the number of downloads per customer in Amazon RDS, and invalidate the CloudFront distribution as soon as the download limit is reached.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your customer is implementing a video on-demand streaming platform on AWS. The requirements are; support for multiple devices such as iOS, Android, and PC as client devices, using a standard client player, using streaming technology (not download,) and scalable architecture with cost effectiveness. Which architecture meets the requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Store the video contents to Amazon Simple Storage Service (S3) as an origin server. Configure the Amazon CloudFront distribution with a streaming option to stream the video contents
       B. Store the video contents to Amazon S3 as an origin server. Configure the Amazon CloudFront distribution with a download option to stream the video contents.
       C. Launch a streaming server on Amazon Elastic Compute Cloud (EC2) (for example, Adobe Media Server), and store the video contents as an origin server. Configure the Amazon CloudFront distribution with a download option to stream the video contents.
       D. Launch a streaming server on Amazon EC2(for example, Adobe Media Server), and store the video contents as an origin server. Launch and configure the required amount of streaming servers on Amazon EC2 as an edge server to stream the video contents.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A research scientist is planning for the one-time launch of an Elastic MapReduce cluster and is encouraged by her manager to minimize costs. The cluster is designed to ingest 200TB of genomics data with a total of 100 Amazon Elastic Compute Cloud (EC2) instances and is expected to run for around four hours. The resulting data set must be stored temporarily until archived into an Amazon Relational Database Service (RDS) Oracle instance. Which option will help save the mostmoney while meeting requirements?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Deploy on-demand master, core and task nodes and store ingest and output files in Amazon Simple Storage Service (S3) Reduced Redundancy Storage (RRS).
       B. Store the ingest files in Amazon S3 RRS and store the output files in S3. Deploy Reserved Instances for the master, and core nodes and on-demand for the task nodes.
       C. Store ingest and output files in Amazon S3. Deploy on-demand for the master, and core nodes and spot for the task nodes.
       D. Optimize by deploying a combination of on-demand, RI, and spot-pricing models for the master, core, and task nodes. Store ingest and output files in Amazon S3 with a lifecycle policy that archives them to Amazon Glacier.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your social media monitoring application uses a Python app running on AWS Elastic Beanstalk to inject tweets, Facebook updates and RSS feeds into an Amazon Kinesis stream. A second AWS Elastic Beanstalk app generates key performance indicators into an Amazon DynamoDB table and powers a dashboard application.&lt;br&gt;
What is the most efficient option to prevent any data loss for this application?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Add a second Amazon Kinesis stream in another Availability Zone and use AWS data pipeline to replicate data across Kinesis streams.
       B. Add a third AWS Elastic Beanstalk app that uses the Amazon Kinesis S3 connector to archive data from Amazon Kinesis into Amazon S3.
       C. Use AWS Data Pipeline to replicate your DynamoDB tables into another region.
       D. Use the second AWS Elastic Beanstalk app to store a backup of Kinesis data onto Amazon Elastic Block Store (EBS), and then create snapshots from your Amazon EBS volumes.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You tried to integrate two subsystems (front-end and back-end) with an HTTP interface to one large system. These subsystems don’t store any state inside. All state is stored in an Amazon DynamoDB table. You have launched each of these two subsystems from a separate AMI. Black box testing has shown that these servers have stopped running and are issuing malformed requests that do not meet HTTP specifications from the client. Your developers have discover and fixed this issue, and you deploy the fix to the two subsystems as soon as possible without service disruption. What are the most effective options to deploy the fixes? Choose 3 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Use VPC.
       B. Use AWS OpsWorks auto healing for both the front-end and back-end instance pair
       C. Use Elastic Load Balancing in front of the front-end subsystem and Auto Scaling to keep the specified number of instances
       D. Use Elastic Load Balancing in front of the back-end subsystem and Auto Scaling to keep specified number of instances.
       E. Use Amazon CloudFront which accesses the front-end server when origin fetch
       F. Use Amazon Simple Queue Service SQS between the front-end and back-end subsystems
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B C D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;When deploying a highly available 2-tier web application on AWS, which combination of AWS Services meets the requirements?&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;AWS Direct Connect&lt;br&gt;
2.Amazon Route 53&lt;br&gt;
3.AWS Storage Gateway&lt;br&gt;
4.Elastic Load Balancing&lt;br&gt;
5.Amazon EC2&lt;br&gt;
6.Auto Scaling&lt;br&gt;
7.Amazon VPC&lt;br&gt;
8.AWS Cloud Trail&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. 2,4,5 and 6
       B. 3,4,5 and 8
       C. 1,2,5 and 6
       D. 1 through 8
       E. 1,3,5 and 7
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your customer needs to create an application to allow contractors to upload videos to Amazon Simple Storage Service (S3) so they can be transcoded into a different format. She creates AWS Identity and Access Management (IAM) users for her application developers, and in just one week, they have the application hosted on a fleet of Amazon Elastic Compute Cloud (EC2) instances. The attached IAM role is assigned to the instances. As expected, a contractor who authenticates to the application is given a pre-signed URL that points to the location for video upload. However, contractors are reporting that they cannot upload their videos. Which of the following are valid reasons for this behavior? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. The IAM role does not explicitly grant permission to upload the object
       B. The contractorsˈ accounts have not been granted “write” access to the S3 bucket.
       C. The application is not using valid security credentials to generate the pre-signed URL.
       D. The developers do not have access to upload objects to the S3 bucket
       E. The S3 bucket still has the associated default permissions
       F. The pre-signed URL has expired.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: C F&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your company runs a complex customer relations management system that consists of around 10 different software components all backed by the same Amazon Relational Database Service (RDS) database. You adopted AWS OpsWorks to simplify management and deployment of that application and created an AWS OpsWorks stack with layers for each of the individual components. An internal security policy requires that all instances should run on the latest Amazon Linux AMI and that instances must be replaced within one month after the latest Amazon Linux AMI has been released. AMI replacements should be done without incurring application downtime or capacity problems. You decide to write a script to be run as soon as a new Amazon Linux AMI is released. Which solutions support the security policy and meet your requirements? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Create a new stack and layers with identical configuration, add instances with the latest Amazon Linux AMI specified as a custom AMI to the new layers, switch DNS to the new stack, and tear down the old stack
       B. Identify all Amazon Elastic Compute Cloud (EC2) instances of your AWS OpsWorks stack, stop each instance, replace the AMI ID property with the ID of the latest Amazon Linux AMI ID, and restart the instance. To avoid down time, make sure not more than one instance is stopped at the same time.
       C. Specify the latest Amazon Linux AMI as a custom AMI at the stack level, terminate instances of the stack and let AWS OpsWorks launch new instances with the new AMI.
       D. Add new instances with the latest Amazon Linux AMI specified as a custom AMI to all AWS OpsWorks layers of your stack, and terminate the old ones.
       E. Assign a custom recipe to each layer which replaces the underlying AMI. Use AWS OpsWorks life-cycle events to incrementally execute this custom recipe and update the instances with the new AMI.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A utility company is building an application that stores data coming from more than 10,000 sensors. Each sensor has a unique ID and will send a datapoint (approximately 1 KB) every 10 minutes throughout the day. Each datapoint contains the information coming from the sensor as well as a timestamp. This company would like to query information coming from a particular sensor for the past week very rapidly and would like to delete all data that is older thanfour weeks. Using Amazon DynamoDB for its scalability and rapidity, how would you implement this in the most cost-effective way?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. One table for each week, with a primary key that is the concatenation of the sensor ID and the timestamp
       B. One table for each week, with a primary key that is the sensor ID, and a hash key that is the timestamp
       C. One table, with a primary key that is the concatenation of the sensor ID and the timestamp
       D. One table, with a primary key that is the sensor ID, and a hash key that is the timestamp
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your company sells consumer devices and needs to record the first activation of all sold devices. Devices are not activated until the information is written on a persistent database. Activation data is very important for your company and must be analyzed daily with a MapReduce job. The execution time of the data analysis process must be less than three hours per day. Devices are usually sold evenly during the year, but when a new device model is out, there is a predictable peak in activations, that is, for a few days there are 10 times or even 100 times more activations than in the average day. Which of the following databases and analysis framework would you implement to better optimize costs and performance for this workload?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Amazon Relational Database Service and Amazon Elastic MapReduce with Spot Instances
       B. Amazon DynamoDB and Amazon Elastic MapReduce with Spot Instances
       C. Amazon Relational Database Service and Amazon Elastic MapReduce with Reserved Instances
       D. Amazon DynamoDB and Amazon Elastic MapReduce with Reserved Instances
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You are moving an existing traditional system to AWS, and during the migration discover that there is a master server which is a single point of failure. Having examined the implementation of the master server you realize there is not enough time during migration to re-engineer it to be highly available, though you do discover that it stores its state in a local MySQL database. In order to minimize down-time you select RDS to replace the local database and configure master to use it, what steps would best allow you to create aself-healing architecture:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Replicate the local database into a RDS Read Replica. Place the master node into a multi-AZ auto-scaling group with a minimum of one and a maximum of one with health checks.
       B. Migrate the local database into a multi-AZ RDS database. Place the master node into a Cross-Zone ELB with a minimum of one and a maximum of one with health checks.
       C. Replicate the local database into a RDS Read Replica. Place the master node into a Cross-Zone ELB with a minimum of one and a maximum of one with health checks.
       D. Migrate the local database into a multi-AZ RDS database. Place the master node into a multi-AZ auto-scaling group with a minimum of one and a maximum of one with health checks.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: D&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A customer is in the process of deploying multiple applications to AWS that are owned and operated by different development teams. Each development team maintains the authorization offits users independently from other teams. The customerˈs information security team would like to be able to delegate user authorization to the individual development teams but independently apply restrictions to the users permissions based on factors such as the userˈs device and location . For example, the information security team would like to grant read-only permissions to a user who is defined by the development team as read/write whenever the user is authenticating from outside the corporate network. What steps can the information security team take to implement this capability?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Operate an authentication service that generates AWS Security Token Service (STS) tokens with IAM policies from application-defined IAM roles.
       B. Add additional IAM policies to the application IAM roles that deny user privileges based on information security policy.
       C. Enable federation with the internal LDAP directory and grant the application teams permissions to modify users.
       D. Configure IAM policies that restrict modification of the application IAM roles only to the information security team.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You are designing a file-sharing service. This service will have millions of files in it. Revenue for the service will come from fees based on how much storage a user is using. You also want to store metadata on each file, such as title, description and whether the object is public or private. How do you achieve all of these goals in a way that is economical and can scale to millions of users?&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Store all files in Amazon Simple Storage Service (S3). Create a bucket for each user. Store metadata in the filename of each object, and access it with LIST commands against the S3 API.
       B. Store all files in Amazon S3. Create Amazon DynamoDB tables for the corresponding key-value pairs on the associated metadata, when objects are uploaded.
       C. Create a striped set of 4000 IOPS Elastic Load Balancing volumes to store the data. Use a database running in Amazon Relational Database Service (RDS) to store the metadata.
       D. Create a striped set of 4000 IOPS Elastic Load Balancing volumes to store the data. Create Amazon DynamoDB tables for the corresponding key-value pairs on the associated metadata, when objects are uploaded.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Your company has been contracted to develop and operate a website that tracks NBA basketball statistics. Statistical data to derive reports like “best game-winning shots from the regular season” and more frequently built reports like “top shots of the game” need to be stored durably for repeated lookup. Leveraging social media techniques, NBA fans submit and vote on new report types from the existing data set so the system needs to accommodate variability in data queries and new static reports must be generated and posted daily. Initial research in the design phase indicates that there will be over 3 million report queries on game day by end users and other applications that use this application as a data source. It is expected that this system will gain in popularity over time and reach peaks of 10-15 million report queries of the system on game days. Select the answer that will allow your application to best meet these requirements while minimizing costs.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Launch a multi-AZ MySQL Amazon Relational Database Service (RDS) Read Replica connected to your multi AZ master database and generate reports by querying the Read Replica. Perform a daily table cleanup.
       B. Generate reports from a multi-AZ MySQL Amazon RDS deployment and have an offline task put reports in Amazon Simple Storage Service (S3) and use CloudFront to cache the content. Use a TTL to expire objects daily.
       C. Implement a multi-AZ MySQL RDS deployment and have the application generate reports from Amazon ElastiCache for in-memory performance results. Utilize the default expire parameter for items in the cache.
       D. Query a multi-AZ MySQL RDS instance and store the results in a DynamoDB table. Generate reports from the DynamoDB table. Remove stale tables daily.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Youˈve been tasked with moving an ecommerce web application from a customerˈs datacenter into a VPC. The application must be fault tolerant and well as highly scalable. Moreover, the customer is adamant that service interruptions not affect the user experience. As you near launch, you discover that the application currently uses multicast to share session state between web servers. In order to handle session state within the VPC, you choose to:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Store session state in Amazon ElastiCache for Redis
       B. Enable session stickiness via Elastic Load Balancing
       C. Create a mesh VPN between instances and allow multicast on it.
       D. Store session state in Amazon Relational Database Service
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Your application is leveraging IAM Roles for EC2 for accessing objects stored in S3. Which two of the following IAM policies control access to your S3 objects?&lt;br&gt;
Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. An IAM trust policy allows the EC2 instance to assume an EC2 instance role
       B. An IAM access policy allows the EC2 role to access S3 objects
       C. An IAM bucket policy allows the EC2 role to access S3 objects
       D. An IAM trust policy allows applications running on the EC2 instance to assume an EC2 role
       E. An IAM trust policy allows applications running on the EC2 instance to access S3 objects
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: A B&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You are designing Internet connectivity for your VPC. The Web servers must be available on the Internet. The application must have a highly available architecture. Which alternatives should you consider? Choose 2 answers&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;       A. Configure ELB with an EIP. Place all your Web servers behind ELB. Configure a Route53 A record that points to the EIP.
       B. Place all your Web servers behind ELB. Configure a Route53 CNAME to point to the ELB DNS name.
       C. Configure a CloudFront distribution and configure the origin to point to the private IP addresses of your Web servers. Configure a Route53 CNAME record to your CloudFront distribution.
       D. Configure a NAT instance in your VPC. Create a default route via the NAT instance and associate it with all subnets. Configure a DNS A record that points to the NAT Instance public IP address.
       E. Assign EIPs to all Web servers. Configure a Route53 record set with all EIPs, with health checks and DNS failover.
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answer: B E&lt;/p&gt;

</description>
      <category>awslagi</category>
      <category>aws</category>
      <category>googlecloud</category>
    </item>
    <item>
      <title>Injecting pre-rendered widgets/content</title>
      <author>dave </author>
      <pubDate>Sun, 22 Aug 2021 14:03:16 +0000</pubDate>
      <link>https://dev.to/davethebrit/injecting-pre-rendered-widgets-content-2bjb</link>
      <guid>https://dev.to/davethebrit/injecting-pre-rendered-widgets-content-2bjb</guid>
      <description>&lt;p&gt;Hi! First time member and post. I had a question that I tried to find an answer for, but likely wasn't searching the right terms.&lt;/p&gt;

&lt;p&gt;We have two teams building React experiences. One who is building the core page, and another that owns widget experiences (think carousels that are componentized, have data and specific logic attached). Is there a concept within React of "injecting" these carousels into the body of a page? Does this impact SSR, performance or security?&lt;/p&gt;

&lt;p&gt;Many thanks in advance&lt;/p&gt;

</description>
      <category>react</category>
    </item>
    <item>
      <title>Sharpen your Ruby: Part 1</title>
      <author>Eric The Coder</author>
      <pubDate>Sun, 22 Aug 2021 13:47:49 +0000</pubDate>
      <link>https://dev.to/ericchapman/sharpen-your-ruby-part-1-18f</link>
      <guid>https://dev.to/ericchapman/sharpen-your-ruby-part-1-18f</guid>
      <description>&lt;p&gt;&lt;a href="https://twitter.com/EricTheCoder_?ref_src=twsrc%5Etfw"&gt;Follow me on Twitter @EricTheCoder_&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;


&lt;p&gt;I develop in Javascript, Python, PHP, and Ruby. By far Ruby is my favorite programming language. Together let start a journey and revisit our Ruby foundations. &lt;/p&gt;

&lt;h2&gt;You want to sharpen your Ruby?&lt;/h2&gt; 

&lt;p&gt;In this series, we will start from the beginning and will discover every aspect of Ruby one step at a time. &lt;/p&gt;

&lt;p&gt;Each post will include some theory but also exercise and solution.&lt;/p&gt;

&lt;p&gt;If you have any questions/comments or you are new and need help, you can comment below or send me a message.&lt;/p&gt;

&lt;h2&gt;Run your Ruby code&lt;/h2&gt;

&lt;p&gt;No need to go through a complete install. Just go to this website &lt;a href="https://replit.com/languages/ruby"&gt;https://replit.com/languages/ruby&lt;/a&gt; and start learning right now. You will have plenty of time to figure out the Ruby installation on your local machine later on...&lt;/p&gt;

&lt;h1&gt;Ruby Variables&lt;/h1&gt;

&lt;p&gt;If you’re new to programming, variables are the fundamental building blocks of a programming language as they are used to store different values that you want to process in your code.&lt;/p&gt;

&lt;p&gt;Once the variable is store in program memory, it can be used later on.&lt;/p&gt;

&lt;p&gt;For example, let say you want to store the user name you can use a variable call name and set its value to Mike Taylor.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;span class="nb"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'Mike Taylor'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;In Ruby string is enclosed with quotation marks.&lt;/p&gt;

&lt;p&gt;The variable name we just created is a string variable. In Ruby, we don't have to specify the variable type. &lt;/p&gt;

&lt;p&gt;Ruby is a Just-in-time (JIT) interpreted language. Which automatically recognizes the data type based on what variables are stored.&lt;/p&gt;

&lt;p&gt;Here are some Ruby basic variables types and how to create them&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;span class="c1"&gt;# string&lt;/span&gt;
&lt;span class="n"&gt;full_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'Mike Taylor'&lt;/span&gt;

&lt;span class="c1"&gt;# integer number&lt;/span&gt;
&lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;

&lt;span class="c1"&gt;# float number&lt;/span&gt;
&lt;span class="n"&gt;book_price&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;15.80&lt;/span&gt;

&lt;span class="c1"&gt;# booleans&lt;/span&gt;
&lt;span class="n"&gt;active?&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;true&lt;/span&gt;
&lt;span class="n"&gt;admin_user?&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Ruby also has more advanced variables types like an array, hash, structure, and class. We will cover all of those in detail later.&lt;/p&gt;

&lt;h1&gt;Output&lt;/h1&gt;

&lt;p&gt;In Ruby, it is possible to output information to the console/terminal.&lt;/p&gt;

&lt;p&gt;For example, let's send our name variable to the console&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;span class="nb"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'Mike Taylor'&lt;/span&gt;
&lt;span class="nb"&gt;puts&lt;/span&gt; &lt;span class="nb"&gt;name&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;The puts method will take any value we give him and print it to the console...&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;Mike Taylor
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Others example&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;span class="nb"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'Mike Taylor'&lt;/span&gt;
&lt;span class="nb"&gt;puts&lt;/span&gt; &lt;span class="s1"&gt;'Hello World'&lt;/span&gt;
&lt;span class="nb"&gt;puts&lt;/span&gt; &lt;span class="s1"&gt;'Hello'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;name&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;Hello World
Hello
Mike Taylor
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;As you can see we can send multiple values to puts method and he will display all of them.&lt;/p&gt;

&lt;p&gt;Another Ruby method very similar to puts is the method print. Print can display something to the console but will not send the line break after each print. Example:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;span class="nb"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'Mike Taylor'&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;'Hello '&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;name&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;





&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;Hello Mike Taylor
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h1&gt;Input&lt;/h1&gt;

&lt;p&gt;How about getting info from the user. In Ruby we use the method gets to do just that&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;'Enter user name: '&lt;/span&gt;
&lt;span class="nb"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;gets&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;The console will then wait for user input:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;Enter user name: _
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;The gets method will return everything you type plus a line break characters. If you don't want to read the line break characters use the chomp method to remove that last character&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;'Enter user name: '&lt;/span&gt;
&lt;span class="nb"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;gets&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;chomp&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h2&gt;Everything in Ruby is an object&lt;/h2&gt;

&lt;p&gt;The following concept will be a bit more advance. Beginners could have hard time to fully understand this concept. (and it is ok).&lt;/p&gt;

&lt;p&gt;In Ruby, everything is an object. Event types like integer, string, array are all objects. All Ruby objects inherit properties and methods from their parent object. &lt;/p&gt;

&lt;p&gt;Here an example&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;span class="nb"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'Mike'&lt;/span&gt;
&lt;span class="nb"&gt;puts&lt;/span&gt; &lt;span class="nb"&gt;name&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;upcase&lt;/span&gt; &lt;span class="c1"&gt;# MIKE&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;In this example, we use the method upcase to convert our string to upper case. How is that's possible? The name variable is a string and the string in Ruby is an object. All string objects have already many pre-build methods to help the developer.&lt;/p&gt;

&lt;p&gt;The .upcase method is not the only method provided by the string object. There are many more methods available. We will discover some of those in future posts.&lt;/p&gt;

&lt;p&gt;If you want to know all available methods for string or any other Ruby objects you can consult Ruby official documentation: &lt;a href="https://www.ruby-lang.org/en/documentation/"&gt;https://www.ruby-lang.org/en/documentation/&lt;/a&gt;  &lt;/p&gt;

&lt;p&gt;We will come back later to this concept, but just keep it in your mind: Everything in Ruby is an object.&lt;/p&gt;

&lt;h1&gt;Exercise&lt;/h1&gt;

&lt;p&gt;Create a little program that asks for the user name and age and save the result in the name and age variable. &lt;/p&gt;

&lt;p&gt;Then display name and age variable in the console&lt;/p&gt;

&lt;p&gt;Solution&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight ruby"&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;'Enter user name: '&lt;/span&gt;
&lt;span class="nb"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;gets&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;chomp&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;'Enter user age: '&lt;/span&gt;
&lt;span class="n"&gt;age&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;gets&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;chomp&lt;/span&gt;

&lt;span class="nb"&gt;puts&lt;/span&gt; &lt;span class="s1"&gt;'The user name is: '&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;name&lt;/span&gt;
&lt;span class="nb"&gt;puts&lt;/span&gt; &lt;span class="s1"&gt;'The user age is: '&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;age&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;That's it for today. The journey just started, stay tuned for the next post very soon. &lt;/p&gt;

&lt;p&gt;If you have any comments or questions please do so here or send me a message on Twitter. &lt;/p&gt;

&lt;p&gt;Follow me: &lt;a href="https://twitter.com/EricTheCoder_?ref_src=twsrc%5Etfw"&gt;@EricTheCoder_&lt;/a&gt;&lt;/p&gt;

</description>
      <category>ruby</category>
      <category>rails</category>
      <category>beginners</category>
      <category>tutorial</category>
    </item>
    <item>
      <title>Meetings don't have to suck - Part 1</title>
      <author>Abhinav Pandey</author>
      <pubDate>Sun, 22 Aug 2021 13:42:08 +0000</pubDate>
      <link>https://dev.to/abh1navv/meetings-don-t-have-to-suck-part-1-4lh0</link>
      <guid>https://dev.to/abh1navv/meetings-don-t-have-to-suck-part-1-4lh0</guid>
      <description>&lt;p&gt;We have all been through this phase lately where the number of meetings being added to our calendars has increased. Some of us are still working remotely and having long tiring communications throughout the day. But this is not what meetings are supposed to be.&lt;/p&gt;

&lt;p&gt;Meetings are &lt;strong&gt;an opportunity to have effective communication&lt;/strong&gt; between team members. So where do things go wrong? &lt;/p&gt;

&lt;p&gt;Let's define how an ideal (almost utopian) process should look like to get the most out of our meetings without being mentally drained by them.&lt;/p&gt;

&lt;p&gt;This is Part 1 where we talk about the meeting invite. A good meeting invite is the corporate analogy of &lt;em&gt;well begun is half done&lt;/em&gt;.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#the-set-up"&gt;
  &lt;/a&gt;
  The Set Up
&lt;/h2&gt;

&lt;p&gt;The most common way of setting up meetings is to send an email which blocks time in the calendar and informs the invited people about it. We should take care of a few steps to make this email useful.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#the-objective"&gt;
  &lt;/a&gt;
  The objective
&lt;/h3&gt;

&lt;p&gt;The email must have an objective statement. &lt;br&gt;
What are we trying to achieve through this meeting? &lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--355M4tAp--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://c.tenor.com/Xgwu6c8wwwMAAAAM/meeting-yesterdays-meeting.gif" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--355M4tAp--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://c.tenor.com/Xgwu6c8wwwMAAAAM/meeting-yesterdays-meeting.gif" alt="A meeting about yesterday's meeting"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A few examples of meeting objectives:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;An action plan for the day or to solve a problem.&lt;/li&gt;
&lt;li&gt;Brainstorming for the next feature.&lt;/li&gt;
&lt;li&gt;Backlog prioritization or sprint planning.&lt;/li&gt;
&lt;li&gt;Status updates (sucks the most, by unanimous decision).&lt;/li&gt;
&lt;li&gt;Knowledge transfer.&lt;/li&gt;
&lt;li&gt;Clarification calls, like a screen-sharing session.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When you put an objective statement in your email, it sets the tone for the invitees.  &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It makes them understand what you are trying to achieve and they will know what to bring to the table.&lt;/li&gt;
&lt;li&gt;They may point out that there could be other people you may want to include to achieve this objective.&lt;/li&gt;
&lt;li&gt;They may point out that there are people who could have been left out.(it isn't as bad as it sounds, this is the kind of being left out we need)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;
  &lt;a href="#the-agenda"&gt;
  &lt;/a&gt;
  The Agenda
&lt;/h3&gt;

&lt;p&gt;This is the most important part of the invitation. This is the essential detail which makes your meeting look purposeful and important.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--CJBB7B4z--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://media2.giphy.com/media/ekmhEyFhUhcPRLyUs5/giphy.webp%3Fcid%3D6c09b95289b085baeeee5cbf323dcdbf5701f786cecfb4e6%26rid%3Dgiphy.webp%26ct%3Dg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--CJBB7B4z--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://media2.giphy.com/media/ekmhEyFhUhcPRLyUs5/giphy.webp%3Fcid%3D6c09b95289b085baeeee5cbf323dcdbf5701f786cecfb4e6%26rid%3Dgiphy.webp%26ct%3Dg" alt="Agenda joke"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;What should it include?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The items to be discussed - with a one liner detail about them.&lt;/li&gt;
&lt;li&gt;The priority of the items - it may be ok to leave out low priority items at the end of the meeting for the sake of time or readiness. &lt;strong&gt;The agenda may not be met completely&lt;/strong&gt;
&lt;/li&gt;
&lt;li&gt;The type of each item - 

&lt;ul&gt;
&lt;li&gt;Is it an &lt;strong&gt;information&lt;/strong&gt; being handed out? &lt;/li&gt;
&lt;li&gt;Is it something which will require in depth &lt;strong&gt;discussion&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;Are we just going to &lt;strong&gt;define next steps&lt;/strong&gt; and assignees?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The time allotted for each item - this is quite an expert level process. It won't be easy to reach but it should be aimed for. If this is followed well, it means that most meetings cannot run out of time and do not miss out on agenda items.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Sending out an agenda will give the invitees time to think about each item and will set expectations from them. They will not come to the meeting with a blank mind expecting to be surprised. &lt;/p&gt;

&lt;p&gt;If you cannot define an agenda in written, you are not ready to occupy everyone's time yet.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#the-preparation"&gt;
  &lt;/a&gt;
  The preparation
&lt;/h3&gt;

&lt;p&gt;Apart from setting the context and the agenda, it is important to let people know if there are any preparations they need to do to be ready before the meeting. &lt;br&gt;
For e.g.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We are going to discuss "Feature X" which we have been working on and would like to include perspectives of a few more people who were not aware of "Feature X". To get yourself accustomed to the topic, please go through&lt;br&gt;
    - the attached documents&lt;br&gt;
    - the JIRA ticket&lt;br&gt;
    - the designs located at this URI on our cloud storage&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There could be more examples of this but this is the most common use case I go through regularly. This should give you an idea. Try to have as many people up to date with the knowledge required for your meeting.&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#the-timing"&gt;
  &lt;/a&gt;
  The Timing
&lt;/h3&gt;

&lt;p&gt;Nobody likes sudden meetings. Everyone is busy with their work and is following a plan. Imagine you are playing your favorite sport right now, would you like to help me with an urgent Math problem? More importantly, it does not make them effective in the meeting if they had no time to go over the agenda and develop some feedback or ideas of their own. &lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--PLq93wT9--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://media0.giphy.com/media/QXPdeQwJYXv6wKXy2G/giphy.webp%3Fcid%3D6c09b95275a17b64135c30040b7ed57aae0d7208140e8df3%26rid%3Dgiphy.webp%26ct%3Dg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--PLq93wT9--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://media0.giphy.com/media/QXPdeQwJYXv6wKXy2G/giphy.webp%3Fcid%3D6c09b95275a17b64135c30040b7ed57aae0d7208140e8df3%26rid%3Dgiphy.webp%26ct%3Dg" alt="Everybody gets a meeting"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The least a team should do is plan one day ahead so that when we join in the morning, we are aware of our meeting schedule for the entire day and are able to plan the rest of our time effectively.&lt;/p&gt;

&lt;p&gt;If the situation requires to create an urgent meeting during the day, make sure you give the invitees enough time to think about the topic before joining the meeting.&lt;/p&gt;

&lt;p&gt;There are some other benefits of planning one day ahead:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;People may suggest a better time as per their availability.&lt;/li&gt;
&lt;li&gt;They get time to go through the preparations.&lt;/li&gt;
&lt;li&gt;They may invite other required people.&lt;/li&gt;
&lt;li&gt;They may suggest a change in agenda.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Remember.&lt;/p&gt;

&lt;p&gt;The participants contribute more in the time they get to prepare for the meeting compared to the duration of the meeting. &lt;/p&gt;

&lt;p&gt;Oh! I almost forgot about the duration. It will be proportional to the list of agenda items and the number of people involved. However, you have to consider the attention span of the people involved. &lt;/p&gt;

&lt;p&gt;It really depends on how interesting the meeting is but usually meetings from 30-60 minutes is what should solve problems effectively without being a burden on people. Cut down the agenda or break down work into smaller teams so that nobody has to sit through long durations. Meeting time (just like all time) is money.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--y_LbZA_1--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://encrypted-tbn0.gstatic.com/images%3Fq%3Dtbn:ANd9GcTo-gaRSkjv9fgWlEOrnUZWe5F1AxmLYDkP3w%26usqp%3DCAU" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--y_LbZA_1--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://encrypted-tbn0.gstatic.com/images%3Fq%3Dtbn:ANd9GcTo-gaRSkjv9fgWlEOrnUZWe5F1AxmLYDkP3w%26usqp%3DCAU" alt="Dangerous meeting"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Keep in mind the above points the next time you set up a meeting. Apart from this, as a meeting leader/organizer, you are expected to have a well thought of reason to have the meeting and are responsible for its success.&lt;/p&gt;

&lt;p&gt;In the next part, things get more interesting when we talk about the meeting process itself. &lt;/p&gt;

&lt;p&gt;Connect with me on &lt;a href="https://twitter.com/abh1navv"&gt;Twitter&lt;/a&gt; and/or subscribe to my weekly newsletter (attached to my twitter profile)&lt;/p&gt;

</description>
      <category>productivity</category>
      <category>leadership</category>
      <category>mentalhealth</category>
      <category>learning</category>
    </item>
    <item>
      <title>PWA || Mistakes While making pwa INSTALLABLE</title>
      <author>SHUBHAM </author>
      <pubDate>Sun, 22 Aug 2021 13:09:38 +0000</pubDate>
      <link>https://dev.to/shubham_ingale/pwa-mistakes-while-making-pwa-2le3</link>
      <guid>https://dev.to/shubham_ingale/pwa-mistakes-while-making-pwa-2le3</guid>
      <description>&lt;h2&gt;
  &lt;a href="#the-mistakes-that-make-your-pwa-not-valid-to-install-or-service-and-manifest-does-not-meet-installability"&gt;
  &lt;/a&gt;
  The Mistakes that make your pwa not valid to install. Or service and manifest does not meet installability
&lt;/h2&gt;

&lt;p&gt;Today,I'm writing the mistake that I created while making my pwas and grabbed some knowledge and now I am sharing that knowledge with you.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#m1-icons-purpose-"&gt;
  &lt;/a&gt;
  M1 : Icons purpose .
&lt;/h2&gt;

&lt;p&gt;In webmanifest we have to declare the purpose of our icon in &lt;code&gt;"icons":[{"purpose":"my purpose"}]&lt;/code&gt;&lt;br&gt;
We write purpose &lt;code&gt;maskable&lt;/code&gt; but it is not valid for Installiblity. You have to declare purpose to any .&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight json"&gt;&lt;code&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="nl"&gt;"icons"&lt;/span&gt;&lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="err"&gt;...&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="nl"&gt;"purpose"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;"any"&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;

&lt;h2&gt;
  &lt;a href="#m2-broken-path"&gt;
  &lt;/a&gt;
  M2 : Broken path.
&lt;/h2&gt;

&lt;p&gt;It's more important that you have to write correct path every time.&lt;br&gt;
If you write any broken path it will not meet Installiblity.&lt;br&gt;
&lt;em&gt;Where it's important.&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;While Adding caches(sw.js)&lt;/li&gt;
&lt;li&gt;While writing paths of icon , scope and start_url.(manifest)
## M3 : Path have to smaller case
In my app
I was written 
&lt;a href="https://formal-stack.netlify.app/APP/"&gt;https://formal-stack.netlify.app/APP/&lt;/a&gt;
And my folder was also &lt;code&gt;APP&lt;/code&gt; but it's important that start_url and scope in manifest have to match with the scope of registration of service worker (with case sensitivity).
I replaced &lt;code&gt;APP&lt;/code&gt; with &lt;code&gt;app&lt;/code&gt; and my app became installable.
## source code
If you want to check pwa example.
Check
&lt;a href="https://notableapp.github.io"&gt;https://notableapp.github.io&lt;/a&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href="https://formal-stack.netlify.app"&gt;https://formal-stack.netlify.app&lt;/a&gt;&lt;/p&gt;


&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--i3JOwpme--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-ba8488d21cd8ee1fee097b8410db9deaa41d0ca30b004c0c63de0a479114156f.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/NotableAPP"&gt;
        NotableAPP
      &lt;/a&gt; / &lt;a href="https://github.com/NotableAPP/Formal-stack-pdfs"&gt;
        Formal-stack-pdfs
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      Make pdf from image , markdown and more is coming...
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;h1&gt;
Formal-stack-pdfs&lt;/h1&gt;
&lt;p&gt;Hey there this is app where you can create pdfs with jsPDF library and our ui/ux this will help you to convert the images into pdf&lt;/p&gt;
&lt;p&gt;App running at - &lt;a href="https://formal-stack.netlify.app" rel="nofollow"&gt;https://formal-stack.netlify.app&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Currently under development.....&lt;/p&gt;
&lt;/div&gt;

  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/NotableAPP/Formal-stack-pdfs"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;



&lt;div class="ltag-github-readme-tag"&gt;
  &lt;div class="readme-overview"&gt;
    &lt;h2&gt;
      &lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--i3JOwpme--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev.to/assets/github-logo-ba8488d21cd8ee1fee097b8410db9deaa41d0ca30b004c0c63de0a479114156f.svg" alt="GitHub logo"&gt;
      &lt;a href="https://github.com/NotableAPP"&gt;
        NotableAPP
      &lt;/a&gt; / &lt;a href="https://github.com/NotableAPP/NotableAPP.github.io"&gt;
        NotableAPP.github.io
      &lt;/a&gt;
    &lt;/h2&gt;
    &lt;h3&gt;
      The noting app
    &lt;/h3&gt;
  &lt;/div&gt;
  &lt;div class="ltag-github-body"&gt;
    
&lt;div id="readme" class="md"&gt;
&lt;h1&gt;
BOOSTED&lt;/h1&gt;
&lt;/div&gt;

  &lt;/div&gt;
  &lt;div class="gh-btn-container"&gt;&lt;a class="gh-btn" href="https://github.com/NotableAPP/NotableAPP.github.io"&gt;View on GitHub&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;



</description>
      <category>javascript</category>
      <category>webdev</category>
      <category>webmanifest</category>
      <category>pwa</category>
    </item>
    <item>
      <title>How are client hints really useful for web performance</title>
      <author>Hargunbeer Singh</author>
      <pubDate>Sun, 22 Aug 2021 13:01:30 +0000</pubDate>
      <link>https://dev.to/hamiecod/how-are-client-hints-really-useful-for-web-performance-4491</link>
      <guid>https://dev.to/hamiecod/how-are-client-hints-really-useful-for-web-performance-4491</guid>
      <description>&lt;h2&gt;
  &lt;a href="#introduction"&gt;
  &lt;/a&gt;
  Introduction
&lt;/h2&gt;

&lt;p&gt;Client Hints are HTTP request header fields that a server can request from a client in order to get information about the client's device, network, etc. The server can then determine the type of data to be displayed according to the client information received.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#overview"&gt;
  &lt;/a&gt;
  Overview
&lt;/h2&gt;

&lt;p&gt;A server must tell the client that it supports client hints, this can be done using the &lt;code&gt;Accept-CH&lt;/code&gt; header. When a client that supports client hints receives the &lt;code&gt;Accept-CH&lt;/code&gt; header it can append client hint headers that match the advertised field-values to subsequent requests. For example: on receiving &lt;code&gt;Accept-CH&lt;/code&gt; as a header from the server, the client would append &lt;code&gt;Width&lt;/code&gt; headers to all subsequent requests, which sends some information about the client to the server in this format:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;Accept-CH: Width
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Client hints can also be specified in HTML using the &lt;code&gt;&amp;lt;meta&amp;gt;&lt;/code&gt; tag with &lt;code&gt;http-equiv&lt;/code&gt; attribute.&lt;br&gt;&lt;br&gt;
The data from the client is not sent in the initial request to the server, in the initial request the server returns &lt;code&gt;Accept-CH&lt;/code&gt; in the headers which informs the client that it takes data, in the future request the client would append the header with the corresponding data.&lt;br&gt;&lt;br&gt;&lt;br&gt;
Before client hints were a thing &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#vary-header"&gt;
  &lt;/a&gt;
  Vary Header
&lt;/h2&gt;

&lt;p&gt;The client hints which determine which resources are sent in responses should be included in the &lt;strong&gt;Vary&lt;/strong&gt; header. This ensures that a different resource is cached for every different value of the hint header. Usually, client hints like &lt;code&gt;Width&lt;/code&gt; and &lt;code&gt;DPR&lt;/code&gt; are specified in the Vary header.&lt;br&gt;&lt;br&gt;&lt;br&gt;
You may omit using Vary header if the values of the client hints change frequently, causing a lot of cache enteries, thus costing more money for maintaining the caches.&lt;br&gt;&lt;br&gt;&lt;br&gt;
Example: You would specify &lt;code&gt;Width&lt;/code&gt; in Vary header generally as you want the response to be cached because the width of the device does not frequently change. You would not specify the network speed in the Vary header as it changes frequently and you would not like to store multiple resources for different network speeds unless you want to burn your money spinning up multiple caches.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#analogy"&gt;
  &lt;/a&gt;
  Analogy
&lt;/h2&gt;

&lt;p&gt;Suppose you want to dine at a restaurant, so you look up the restaurant on google maps and it says that it has table reservations and you can specify the food beforehand to save you time. You like specifying the food beforehand, so you call up the restaurant and tell the staff the time you wanna dine and the food you will eat(suppose chicken soup and kebab). So when you go to the restaurant, they give you the already specified food saving you time.&lt;br&gt;&lt;br&gt;&lt;br&gt;
In this analogy, the restaurant specifying on google maps that it accepts orders beforehand is &lt;code&gt;Accept-CH&lt;/code&gt; in client hints as the server specifies that it would take client data. You calling up the restaurant and specifying the food beforehand is the data you send, like your operating system, device width or even browser dark or light mode. The restaurant staff keeping the food ready beforehand is related to the server returning specific data corresponding to the data it received, like it would show different colors if the client has dark mode and it would make the website responsive if the width of the client's device is shorter.&lt;br&gt;&lt;br&gt;&lt;br&gt;
While departing from the restaurant, you say that you really like the food and would come to the restaurant again to eat the same food, the restaurant staff remembers that, when you call them up next time and book a table and specify that you need the same food, they would not need to ask you the specific food items as they remember that you ate chicken soup and kebab last time and they would serve the same, thus saving you time.&lt;br&gt;&lt;br&gt; &lt;br&gt;
Here, you telling the staff at your first visit that you would eat the same food is the client hint in the Vary header, the staff remembering it is related to caching the response for the request. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#example"&gt;
  &lt;/a&gt;
  Example
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;A client hint would require to know your operating system information and then show linux download files if it is a linux operating system or windows download files if it is a windows operating system.&lt;/li&gt;
&lt;li&gt;A client sends the width and height of his device to the server via client hints. This helps make the website responsive, the width received by client hints is then used in media queries.&lt;/li&gt;
&lt;li&gt;It is used for image delivery optimization, if you want to send an image to the client, you would usually send the same image for various device widths and sizes, and then optimize them to suit the width of the client's device in the client side code, this is really inefficient as suppose you are sending a 4k image to all the devices, the image is about 10mb in size, the image would be rendered nicely on a 4k monitor, but for other devices, suppose tablets and mobiles, you would need to decrease the height and width of the image to suit it to tablets and mobiles. Using client hints, you would just send the required resolution image(suppose a 300x300 image for a mobile phone), and you would not have to send the huge 4k image to every device and then suit it to the device with client-side code. Here, client-side code is not the problem but bandwidth is, you are sending huge images to devices which do not even need them, hence lowering the performance of your website&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#more-info"&gt;
  &lt;/a&gt;
  More Info
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.keycdn.com/blog/client-hints"&gt;KeyCDN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developers.google.com/web/updates/2015/09/automating-resource-selection-with-client-hints"&gt;Google Developers blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://web.dev/user-agent-client-hints/"&gt;Web.dev blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <category>webdev</category>
      <category>performance</category>
    </item>
    <item>
      <title>Free courses this month (limited time)</title>
      <author>PythonBasics</author>
      <pubDate>Sun, 22 Aug 2021 12:15:06 +0000</pubDate>
      <link>https://dev.to/basicspython/free-courses-this-month-limited-time-17e1</link>
      <guid>https://dev.to/basicspython/free-courses-this-month-limited-time-17e1</guid>
      <description>&lt;p&gt;Some of my courses are 100% free this month. If you want to learn more about ethical hacking or programming don't hesitate, it's a limited time offer 😄&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ethical hacking&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://www.udemy.com/course/ethical-hacking-password-cracking-with-python/"&gt;Password cracking with Python&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://www.udemy.com/course/sql-injection-tutorial/"&gt;SQL Injection tutorial&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://www.udemy.com/course/ethical-hacking-introduction-to-exploits/"&gt;Introduction to Exploits&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Programming&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://www.udemy.com/course/c-programming-exercises-for-beginners/"&gt;C programming exercises&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://www.udemy.com/course/haskell-exercises-for-beginners/"&gt;Haskell programming exercises&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;DevOps&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://www.udemy.com/course/redis-database-tutorial/"&gt;Redis Database tutorial&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://www.udemy.com/course/awk-tutorial/"&gt;Linux AWK tutorial&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://www.udemy.com/course/linux-tmux/"&gt;Linux tmux tutorial&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <category>programming</category>
      <category>redis</category>
      <category>linux</category>
      <category>haskell</category>
    </item>
    <item>
      <title>VMC on AWS Overview</title>
      <author>Rimpal Johal</author>
      <pubDate>Sun, 22 Aug 2021 12:13:07 +0000</pubDate>
      <link>https://dev.to/aws-builders/vmc-on-aws-overview-j5g</link>
      <guid>https://dev.to/aws-builders/vmc-on-aws-overview-j5g</guid>
      <description>&lt;p&gt;VMware Cloud on AWS is a cloud service jointly developed by VMware and AWS. This offering brings the same technologies enterprise customers use on premises (e.g., VMware vSphere® ESXi, vSAN, NSX) into the cloud.&lt;/p&gt;

&lt;p&gt;VMware Cloud on AWS (in short VMC) helps customers deploy and accelerate migration of VMware vSphere-based workloads to the cloud while providing robust capabilities and hybrid cloud solutions. VMware Cloud on AWS allows organizations to take immediate advantage of the scalability, availability, security, and global reach of the AWS infrastructure. &lt;/p&gt;

&lt;p&gt;Also this gives customers capability to access native AWS services from the VMWare Cloud on AWS SDDC. The access to AWS native services from SDDC is without incurring any ingress or egress charges.&lt;/p&gt;

&lt;p&gt;A key advantage of VMware Cloud on AWS is that it runs ESXi hosts on bare metal EC2 instances on the AWS infrastructure. This helps end users to establish a VMware Cloud on AWS&lt;br&gt;
SDDC and quickly begin deploying workloads to the cloud.&lt;/p&gt;

&lt;p&gt;Use cases for VMC on AWS are&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Cloud Migration: This helps customers to migrate to cloud without converting or re-architecting their existing application stacks.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data Center Extension: VMC is the most viable solution for customers who are looking for expanding data center capacity in a cost-effective way.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Disaster Recovery: VMC can be considered as a DR solution for a customers who want combine VMWare disaster recovery with AWS Cloud.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Application Modernization: Customers can liverage private access to AWS native services to modernize application with the use of feature rich AWS native services.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first step for VMC on AWS is to - Deploy a SDDC. Below are the steps to deploy a SDDC.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open &lt;a href="https://console.cloud.vmware.com/"&gt;https://console.cloud.vmware.com/&lt;/a&gt; - login with your credentials.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Under my services, click on the VMWare Cloud on AWS tile.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--a4CkH70l--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zcpb4e4a5l73cjgg4b3a.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--a4CkH70l--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zcpb4e4a5l73cjgg4b3a.png" alt="image"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click on Create SDDC.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--xLDSUJdX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ujvz2iheyue74ziw231y.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--xLDSUJdX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ujvz2iheyue74ziw231y.png" alt="image"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The first step when creating a new SDDC is to define properties relating to the AWS region you wish to deploy your SDDC to, the deployment and host types for the SDDC, and to give your SDDC a name. Enter the following parameters and click Next:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Property&lt;/strong&gt;&lt;br&gt;
  Cloud                          &amp;lt;&amp;gt;&lt;br&gt;
  AWS Region                     &amp;lt;&amp;lt; Your AWS Region&amp;gt;&amp;gt;&lt;br&gt;
  Deployment                     &amp;lt;&amp;lt; Mult-Host&amp;gt;&amp;gt;&lt;br&gt;
  Host Type                      &amp;lt;&amp;lt; I3 ( local SSD)&lt;br&gt;
  SDDC Name                      &amp;lt;&amp;lt; Custom Name&amp;gt;&amp;gt;&lt;br&gt;
  Number of Hosts                &amp;lt;&amp;lt; 2 &amp;gt;&amp;gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In this step, connect to the AWS account(Note this is the customer AWS account - defined/named as sidecar account). Select the AWS account and click next&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For VPC and subnet, select the VPC and choose the subnet.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For the management subnet, define the CIDR or use the default.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Review and acknowledge and click deploy SDDC.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Once the SDDC is deployed , you can see your new SDDC in the list of SDDCs.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;That completes the first section of deploying the SDDC.&lt;/p&gt;

</description>
      <category>vmc</category>
      <category>aws</category>
      <category>vmware</category>
      <category>migration</category>
    </item>
    <item>
      <title>Mastering this single technology makes you employable for life</title>
      <author>Simon Barker</author>
      <pubDate>Sun, 22 Aug 2021 11:43:19 +0000</pubDate>
      <link>https://dev.to/allthecode/mastering-this-single-technology-makes-you-employable-for-life-2o9p</link>
      <guid>https://dev.to/allthecode/mastering-this-single-technology-makes-you-employable-for-life-2o9p</guid>
      <description>&lt;p&gt;Technology moves at a frighteningly fast pace and for developers it can feel like we have managed to grab on to a tigers tail and are now barely holding on whilst flying across the open plains of this technological continent we have made.&lt;/p&gt;

&lt;p&gt;From DevOps and Infra-as-code to React and serverless, there is an enormous array of technologies to learn and master, with new ones being released all the time. So, how can I possibly say that there is one to learn that will make you forever employable? &lt;/p&gt;

&lt;p&gt;What magical technology is so foundational and ubiquitous that it could possibly be considered valuable enough for a developer to learn to give employability security for life? This magical technology is old, it's older than Javascript, older than Python and basically the same age as C!&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#is-it-a-bird-is-it-a-plane"&gt;
  &lt;/a&gt;
  Is it a bird? Is it a plane?
&lt;/h2&gt;

&lt;p&gt;Some say it's a programming language, others say it's not, but it meets the modern trend of being declarative and also being cross platform. It powers some of the largest websites in the world whilst also being light enough to run in little phone apps.&lt;/p&gt;

&lt;p&gt;The technology I am talking about is SQL, which stands for Structured Query Language and is a relational database technology that has been, and will continue to be, a core foundational technology of the web. If you are a web developer who doesn't know SQL you need to bump it to the top of your "To Learn" list today.&lt;/p&gt;

&lt;p&gt;It might not get the press that NoSQL does, it might not be as hot as MongoDB or ElasticSearch and it might feel a bit too much like IT and not Dev'ing, but SQL is pervasive and is much better at solving many problems still than NoSQL.&lt;/p&gt;

&lt;p&gt;NoSQL is powerful and amazing. The schema-less design approach allows you to feel very free and unconstrained, however when you want to start managing relationships and keeping duplicated data in sync across many documents you start to feel the trade off for that write time freedom. &lt;/p&gt;

&lt;p&gt;Want to grab related data in SQL? Well it's just a &lt;code&gt;LEFT JOIN&lt;/code&gt; away!&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#where-to-learn-sql"&gt;
  &lt;/a&gt;
  Where to learn SQL
&lt;/h2&gt;

&lt;p&gt;You can start to learn basic SQL concepts and follow along with a &lt;a href="https://www.udemy.com/topic/sql/"&gt;Udemy course&lt;/a&gt; or searching for SQL on &lt;a href="https://www.freecodecamp.org/news/search/?query=sql"&gt;FreeCodeCamp.org&lt;/a&gt; however another good resource is the SQL murder mystery site:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://mystery.knightlab.com/walkthrough.html"&gt;Walkthrough for beginners&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://selectstarsql.com/"&gt;Interactive Book&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mystery.knightlab.com/"&gt;SQL Murder Mystery&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#installing-sql"&gt;
  &lt;/a&gt;
  Installing SQL
&lt;/h2&gt;

&lt;p&gt;There are a number of free flavours of SQL like &lt;a href="https://www.mysql.com/"&gt;MySQL&lt;/a&gt; and &lt;a href="https://www.postgresql.org/download/"&gt;PostgreSQL&lt;/a&gt;, either one is fine — people do have opinions on which is better, at this learning stage it really doesn't matter though.&lt;/p&gt;

&lt;p&gt;You can get MySQL as part of a one click &lt;a href="https://www.mamp.info/en/mamp/mac/"&gt;MAMP&lt;/a&gt;, &lt;a href="https://www.wampserver.com/en/"&gt;WAMP&lt;/a&gt; or &lt;a href="https://www.apachefriends.org/index.html"&gt;XAMPP&lt;/a&gt; installation, you can install them directly onto your system or you can run them in a Docker container if that's your preference.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#getting-data-for-learning-sql"&gt;
  &lt;/a&gt;
  Getting data for learning SQL
&lt;/h2&gt;

&lt;p&gt;Database technologies can be hard to get really good at without some real data. Inserting small amounts of data in a tutorial like songs and albums is good for the basics but when you land in a job with a SQL database with 1,000 tables that is 15 years old you're going to find yourself wishing for more experience that that. Here are a couple:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.imdb.com/interfaces/"&gt;IMDB Database&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ergast.com/mrd/db/"&gt;F1 Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#summary"&gt;
  &lt;/a&gt;
  Summary
&lt;/h2&gt;

&lt;p&gt;SQL might not be the new hotness but it's also not going anywhere, it is a manifestation of relational algebra and has been used to build much of the web. &lt;/p&gt;

&lt;p&gt;The idea that if you write Javascript you should use NoSQL is nonsense. You should use the best database technology for the data needs your project has. Most large systems will use multiple database and storage technologies including both SQL and NoSQL. SQL has been around for 50 years now and its relevance is only growing, especially with the growing importance of Data Science.&lt;/p&gt;

&lt;p&gt;So, when you are burnt out on learning your 15th Javascript frame work and want to work on something stable you will be very happy you have SQL to underpin your career and skills.&lt;/p&gt;

</description>
      <category>career</category>
      <category>webdev</category>
      <category>beginners</category>
      <category>programming</category>
    </item>
    <item>
      <title>Training Machine Learning models in AzureML</title>
      <author>Vivek0712</author>
      <pubDate>Sun, 22 Aug 2021 11:39:05 +0000</pubDate>
      <link>https://dev.to/vivek0712/training-machine-learning-models-in-azureml-118m</link>
      <guid>https://dev.to/vivek0712/training-machine-learning-models-in-azureml-118m</guid>
      <description>&lt;p&gt;Welcome to the next part of my blog series on Azure Machine Learning. Now we are going to jump into one of the most important aspect of the MLOps - Training of Machine Learning Models.&lt;/p&gt;

&lt;p&gt;Usually Machine Learning models require high compute power to train on the dataset. It is quite evident that not everyone has access to high processing and storage capabilities. So, keeping in mind of the cost of procuring the infrastructure for the same, its better option to leverage the cloud resources and pay for the usage. &lt;/p&gt;

&lt;p&gt;Now, coming to training of Machine Learning models in Azure, we are going to follow simple steps to do so. At the end of the article, you will have created a training environment, trained your ML model and stored the artefacts of the models for future use. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#steps"&gt;
  &lt;/a&gt;
  Steps
&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Setup the environment&lt;/li&gt;
&lt;li&gt;Creating your ML Script&lt;/li&gt;
&lt;li&gt;Create Training Environment dependency file&lt;/li&gt;
&lt;li&gt;Training the model&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;
  &lt;a href="#setup-the-environment-for-ml"&gt;
  &lt;/a&gt;
  Setup the Environment for ML
&lt;/h2&gt;

&lt;p&gt;Follow the previous article to create Workspace, Compute and Data.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#creating-your-ml-script"&gt;
  &lt;/a&gt;
  Creating your ML Script
&lt;/h2&gt;

&lt;p&gt;Before creating the ML Script, lets first create the directories for source files&lt;/p&gt;


&lt;div class="ltag_gist-liquid-tag"&gt;
  
&lt;/div&gt;


&lt;p&gt;The ML Script does the following &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Parsess the Argument passed to the script (Input Data and Hyperparameters)&lt;/li&gt;
&lt;li&gt;Starts the run for the experiment&lt;/li&gt;
&lt;li&gt;Does all the ML Training&lt;/li&gt;
&lt;li&gt;Logs the metrics of the run&lt;/li&gt;
&lt;li&gt;Stores the output model&lt;/li&gt;
&lt;/ul&gt;


&lt;div class="ltag_gist-liquid-tag"&gt;
  
&lt;/div&gt;


&lt;h2&gt;
  &lt;a href="#creating-training-environment-dependency-file"&gt;
  &lt;/a&gt;
  Creating Training Environment Dependency File
&lt;/h2&gt;

&lt;p&gt;An Environment is managed and versioned in an Azure Machine Learning Workspace. You can update an existing environment and retrieve a version to reuse. Environments are exclusive to the workspace they are created in and can't be used across different workspaces.&lt;/p&gt;

&lt;p&gt;We will use YAML file to list the dependencies of the environment we are going to use&lt;/p&gt;


&lt;div class="ltag_gist-liquid-tag"&gt;
  
&lt;/div&gt;


&lt;h2&gt;
  &lt;a href="#training-the-machine-learning-model"&gt;
  &lt;/a&gt;
  Training the Machine learning Model
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Create the environment from the dependency file&lt;/li&gt;
&lt;li&gt;Creating Configuration for Python Script&lt;/li&gt;
&lt;li&gt;Submitting the Script to the Experiment&lt;/li&gt;
&lt;li&gt;Register the model along with the RSME Value&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are going to create 10 different runs for 10 different Alpha values for our Ridge Algorithm to be trained on the Diabetes Dataset. The model with less RMSE will be considered as best model.&lt;/p&gt;

&lt;p&gt;A ScriptRunConfig packages together the configuration information needed to submit a run in Azure ML, including the script, compute target, environment, and any distributed job-specific configs.&lt;/p&gt;

&lt;p&gt;A model is the result of a Azure Machine learning training Run or some other model training process outside of Azure.Register a model with the provided workspace, we use &lt;em&gt;register&lt;/em&gt; method.&lt;/p&gt;


&lt;div class="ltag_gist-liquid-tag"&gt;
  
&lt;/div&gt;


&lt;p&gt;We have successfully created and trained our Machine Learning model. In next article, we will see how to deploy our model to production and perform real-time inference.&lt;/p&gt;

</description>
      <category>azure</category>
      <category>microsoft</category>
      <category>machinelearning</category>
      <category>cloud</category>
    </item>
    <item>
      <title>6 CSS Shorthand properties to improve your web application</title>
      <author>Carmine Scarpitta</author>
      <pubDate>Sun, 22 Aug 2021 10:55:25 +0000</pubDate>
      <link>https://dev.to/cscarpitta/6-css-shorthand-properties-to-improve-your-web-application-2dbj</link>
      <guid>https://dev.to/cscarpitta/6-css-shorthand-properties-to-improve-your-web-application-2dbj</guid>
      <description>&lt;h1&gt;
  &lt;a href="#why-should-i-care-shorthand-properties-balloon"&gt;
  &lt;/a&gt;
  Why should I care shorthand properties? 🎈
&lt;/h1&gt;

&lt;p&gt;CSS is a language used to describe how a web page should look like. With CSS we can set position, colors, fonts, layout of each element in an HTML page. 🌈&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--bJh4UVgi--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/suxzb0mwwowa8nbvmy7n.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--bJh4UVgi--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/suxzb0mwwowa8nbvmy7n.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this blog post, I will share some tips to improve your CSS code and the performance of your web app. ⚠️&lt;/p&gt;


&lt;center&gt; 🔥 🌞 🌴 🍄 🔥 &lt;/center&gt;

&lt;p&gt;There are several ways in which a CSS file can be optimized. One of the things you should care when writing your CSS code is to minimize the number of lines. 💥&lt;/p&gt;

&lt;p&gt;There are several reasons to care about the number of code lines 😲:&lt;br&gt;
👉 improve the code readability&lt;br&gt;
👉 improve the loading speed of your web page&lt;br&gt;
👉 improve the ranking in search engines (e.g. Google Search, Bing, ...), because ranking depends on the loading speed and optimization level of your application&lt;/p&gt;


&lt;center&gt;🔥 🔥 🔥 🔥 🔥&lt;/center&gt;

&lt;p&gt;An interesting tip to reduce the number of lines of code is the following:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Use CSS shorthand properties whenever possible&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this blog post I'll show you what shorthand properties are and how they can be used to optimize your CSS code.&lt;/p&gt;

&lt;p&gt;Interested? Read on!!! 😊💻&lt;/p&gt;
&lt;h1&gt;
  &lt;a href="#what-are-shorthand-properties"&gt;
  &lt;/a&gt;
  What are shorthand properties?
&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Shorthand properties&lt;/strong&gt; let you set the values of multiple other CSS properties simultaneously.&lt;/p&gt;

&lt;p&gt;CSS supports a number of shorthand properties. In this blog post we will see the most used ones. ✈️&lt;/p&gt;
&lt;h1&gt;
  &lt;a href="#css-background-shorthand"&gt;
  &lt;/a&gt;
  CSS Background Shorthand
&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Background property&lt;/strong&gt; lets you set different background properties of an HTML element (e.g. a &lt;code&gt;div&lt;/code&gt;) in a single line of CSS.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt; is a shorthand for:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;background-color&lt;/li&gt;
&lt;li&gt;background-image&lt;/li&gt;
&lt;li&gt;background-position&lt;/li&gt;
&lt;li&gt;background-size&lt;/li&gt;
&lt;li&gt;background-repeat&lt;/li&gt;
&lt;li&gt;background-origin&lt;/li&gt;
&lt;li&gt;background-clip&lt;/li&gt;
&lt;li&gt;background-attachment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So instead of having background-color, background-image,  background-position, background-size, background-repeat, background-origin, background-clip, background-attachment defined for an HTML element, we can use a single property. 😍&lt;/p&gt;

&lt;p&gt;Hmmm confused??? 😱 😕 &lt;/p&gt;

&lt;p&gt;A picture is worth a thousand words:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--uVdTk2Z_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zukoe629szz9kl9kng5a.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--uVdTk2Z_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zukoe629szz9kl9kng5a.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;That's the trick. Thanks to &lt;code&gt;background&lt;/code&gt; shorthand property, we have compressed 8 CSS lines into one line.&lt;/p&gt;

&lt;p&gt;Now let's image a complex web application, with dozens of CSS files and thousands of lines per file. 😱&lt;/p&gt;

&lt;p&gt;Definitely, this results in much smaller files, cleaner code and faster loading times for your web application. 😄&lt;/p&gt;
&lt;h1&gt;
  &lt;a href="#css-border-shorthand"&gt;
  &lt;/a&gt;
  CSS Border Shorthand
&lt;/h1&gt;

&lt;p&gt;The second shorthand property that I want to show you is called &lt;strong&gt;Border&lt;/strong&gt;. Border shorthand is used to set the border of an HTML element.&lt;/p&gt;

&lt;p&gt;It is a shorthand for:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;border-width&lt;/li&gt;
&lt;li&gt;border-style&lt;/li&gt;
&lt;li&gt;border-color&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is an example:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--3n5Y0C32--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ua2jaajgkkzyzkujcekc.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--3n5Y0C32--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ua2jaajgkkzyzkujcekc.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Three properties at one go. Not so bad!!! 😏 🔥&lt;/p&gt;
&lt;h1&gt;
  &lt;a href="#css-font-shorthand"&gt;
  &lt;/a&gt;
  CSS Font Shorthand
&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Font&lt;/strong&gt; shorthand is used to set the following font properties:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;font-style&lt;/li&gt;
&lt;li&gt;font-variant&lt;/li&gt;
&lt;li&gt;font-weight&lt;/li&gt;
&lt;li&gt;font-size/line-height&lt;/li&gt;
&lt;li&gt;font-family&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--R1dFEYpk--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/epbng7ypqeahp7wsnvbf.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--R1dFEYpk--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/epbng7ypqeahp7wsnvbf.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;
  &lt;a href="#css-inset-shorthand"&gt;
  &lt;/a&gt;
  CSS Inset Shorthand
&lt;/h1&gt;

&lt;p&gt;The &lt;strong&gt;Inset&lt;/strong&gt; property has to do with the positioning of an HTML element. It is a shorthand for:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;top&lt;/li&gt;
&lt;li&gt;right&lt;/li&gt;
&lt;li&gt;bottom&lt;/li&gt;
&lt;li&gt;left&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--q29d4mVq--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9ppsfx8coso1j5zsmyg9.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--q29d4mVq--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9ppsfx8coso1j5zsmyg9.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Simple and efficient! ☺️ 🌺&lt;/p&gt;
&lt;h1&gt;
  &lt;a href="#css-padding-shorthand"&gt;
  &lt;/a&gt;
  CSS Padding Shorthand
&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Padding&lt;/strong&gt; is a way to add space around an element. More precisely it allows you to add space between the element and its border.&lt;/p&gt;

&lt;p&gt;To completely set the padding of an HTML element, we need to set four values:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;padding-top&lt;/li&gt;
&lt;li&gt;padding-right&lt;/li&gt;
&lt;li&gt;padding-bottom&lt;/li&gt;
&lt;li&gt;padding-left&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The meaning of these values is quite intuitive:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;padding-top&lt;/strong&gt; is the space between the element and the its border&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;padding-right&lt;/strong&gt; is the space between the element and its right border&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;padding-bottom&lt;/strong&gt; is the space between the element and its bottom border&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;padding-left&lt;/strong&gt; is the space between the element and its left border&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All these properties can be specified in a single declaration, using the &lt;strong&gt;padding&lt;/strong&gt; shorthand. 🌻&lt;/p&gt;

&lt;p&gt;The syntax is straightforward:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight css"&gt;&lt;code&gt;&lt;span class="nt"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;padding-top&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;padding-right&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;padding-bottom&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;padding-left&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Here is an example:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--_siOyeqy--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/pi10x5153xgp7lyg8kct.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--_siOyeqy--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/pi10x5153xgp7lyg8kct.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#css-margin-shorthand"&gt;
  &lt;/a&gt;
  CSS Margin Shorthand
&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Margin&lt;/strong&gt; property is similar to padding. Margin is the space around the element, outside its borders.&lt;/p&gt;

&lt;p&gt;To specify a margin you need to provide four different values:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;margin-top&lt;/strong&gt; is the space between the top border of the element and the other elements&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;margin-right&lt;/strong&gt; is the space between the right border of the element and the other elements&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;margin-bottom&lt;/strong&gt; is the space between the bottom border of the element and the other elements&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;margin-left&lt;/strong&gt; is the space between the left border of the element and the other elements&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The syntax is straightforward 🌹:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight css"&gt;&lt;code&gt;&lt;span class="nt"&gt;margin&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;margin-top&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;margin-right&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;margin-bottom&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;margin-left&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Here is an example:&lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--WprVOB20--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ay5vhdzzts34yyv060yh.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--WprVOB20--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ay5vhdzzts34yyv060yh.png" alt="Alt Text"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#conclusion"&gt;
  &lt;/a&gt;
  Conclusion
&lt;/h1&gt;

&lt;p&gt;We have come to an end. 🌼 I want to remark the following tip:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Use CSS shorthand properties whenever possible&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;because they help to reduce the lines of CSS code and improve readability. &lt;/p&gt;

&lt;p&gt;Reducing the CSS file size will also &lt;strong&gt;improve the loading speed of our web pages&lt;/strong&gt;, because the CSS file is &lt;strong&gt;smaller&lt;/strong&gt;. This will also &lt;strong&gt;improve our ranking in search engine&lt;/strong&gt;, because the search engine algorithms tend to reward the optimized web pages. 🌈 🚀&lt;/p&gt;


&lt;center&gt;🔥 🔥 🔥 🔥 🔥&lt;/center&gt;

&lt;p&gt;I hope I convinced you to care about lines of CSS code. 😜&lt;/p&gt;

&lt;p&gt;If you liked this post, consider following me on Twitter &lt;a href="https://twitter.com/cscarpitta94"&gt;@cscarpitta94&lt;/a&gt; and on dev &lt;a href="https://dev.to/cscarpitta"&gt;cscarpitta&lt;/a&gt; 😍&lt;/p&gt;



&lt;center&gt;👋 🔥&lt;/center&gt;

</description>
      <category>webdev</category>
      <category>css</category>
      <category>beginners</category>
      <category>codenewbie</category>
    </item>
  </channel>
</rss>
