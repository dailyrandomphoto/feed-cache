<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>DEV Community</title>
    <author>DEV Community</author>
    <description>A constructive and inclusive social network for software developers. With you every step of your journey.</description>
    <link>https://dev.to</link>
    <language>en</language>
    <item>
      <title>Responsive Navbar Using HTML &amp; CSS</title>
      <author>Nikhil Bobade </author>
      <pubDate>Sun, 21 Nov 2021 13:53:22 +0000</pubDate>
      <link>https://dev.to/nikhil27b/responsive-navbar-using-html-css-3aic</link>
      <guid>https://dev.to/nikhil27b/responsive-navbar-using-html-css-3aic</guid>
      <description>&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;How are you all I am writing this post after so long break. Today I Created a simple responsive header menu using HTML &amp;amp; CSS I hope you like this also comments about your thoughts. also For more content follow me on Instagram  &lt;a href="https://www.instagram.com/developer_nikhil27/"&gt;@developer_nikhil27&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thank you üôÇ!&lt;/p&gt;

&lt;p&gt;&lt;iframe height="600" src="https://codepen.io/NikhilBobade/embed/jOLyypp?height=600&amp;amp;default-tab=result&amp;amp;embed-version=2"&gt;
&lt;/iframe&gt;
&lt;/p&gt;

</description>
      <category>html</category>
      <category>javascript</category>
      <category>webdev</category>
      <category>beginners</category>
    </item>
    <item>
      <title>8 Programmer Life Lessons They Don‚Äôt Teach You At School</title>
      <author>python.land</author>
      <pubDate>Sun, 21 Nov 2021 13:31:37 +0000</pubDate>
      <link>https://dev.to/python_land/8-programmer-life-lessons-they-dont-teach-you-at-school-djp</link>
      <guid>https://dev.to/python_land/8-programmer-life-lessons-they-dont-teach-you-at-school-djp</guid>
      <description>&lt;p&gt;Some things you don‚Äôt learn at school. These lessons come right from the work floor; I learned them the hard way!&lt;/p&gt;

&lt;p&gt;&lt;a href="https://medium.com/pythonland/8-programmer-life-lessons-they-dont-teach-you-at-school-dc8049f3e9e8?sk=73ac18dc9052e8f1cfb85e548b1a8594"&gt;Read the full article for free on Medium using this link&lt;/a&gt;&lt;/p&gt;

</description>
      <category>programming</category>
      <category>beginners</category>
    </item>
    <item>
      <title>Day 44 of 100 Days of Code &amp; Scrum: Deadlines and Pressure</title>
      <author>Rammina</author>
      <pubDate>Sun, 21 Nov 2021 12:54:43 +0000</pubDate>
      <link>https://dev.to/rammina/day-44-of-100-days-of-code-scrum-deadlines-and-pressure-il0</link>
      <guid>https://dev.to/rammina/day-44-of-100-days-of-code-scrum-deadlines-and-pressure-il0</guid>
      <description>&lt;p&gt;Happy Sunday, everyone!&lt;/p&gt;

&lt;p&gt;I decided that I will focus on my website primarily for this week. I must create an output that I can deploy and show at the end of the week. I'm giving myself a deadline so that I'm forced to work the best that I can.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#yesterday"&gt;
  &lt;/a&gt;
  Yesterday
&lt;/h2&gt;

&lt;p&gt;Just like I do every weekend, I did my 6th Weekly Sprint Review and Retrospective, in which I went over what I've managed to do well, what my shortcomings were, and what I could do better in the future.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#today"&gt;
  &lt;/a&gt;
  Today
&lt;/h2&gt;

&lt;p&gt;I set up my weekly Sprint Goals, which will mostly basic concern my &lt;strong&gt;company website's homepage&lt;/strong&gt;. The others are secondary goals, which I will work on for a little bit each day without compromising my main objective.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#weekly-sprint-goals"&gt;
  &lt;/a&gt;
  Weekly Sprint Goals
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;finish my company website's homepage (and avoid getting distracted by more interesting projects).&lt;/li&gt;
&lt;li&gt;continue to learn Next.js and Typescript by using concepts while I build my website.&lt;/li&gt;
&lt;li&gt;continue studying for Professional Scrum Master I (PSM I) certification.&lt;/li&gt;
&lt;li&gt;continue networking, but allocate less time to this (coding is more important).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Have a great week, everyone!&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--9gNNIpK9--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/x9ayfxxxaz2g2hfcqbsk.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--9gNNIpK9--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/x9ayfxxxaz2g2hfcqbsk.png" alt="Thank You Banner" width="880" height="293"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;
  &lt;a href="#resourcesrecommended-readings"&gt;
  &lt;/a&gt;
  Resources/Recommended Readings
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.freecodecamp.org/news/how-to-receive-emails-via-your-sites-contact-us-form-with-aws-ses-lambda-api-gateway/"&gt;How to Receive Emails from Your Site's "Contact Us" form Using AWS SES, Lambda, &amp;amp; API Gateway&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nextjs.org/learn/basics/create-nextjs-app?utm_source=next-site&amp;amp;utm_medium=nav-cta&amp;amp;utm_campaign=next-website"&gt;Official Next.js tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.typescriptlang.org/docs/handbook/intro.html"&gt;The Typescript Handbook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://scrumguides.org/scrum-guide.html"&gt;The 2020 Scrum Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mlapshin.com/index.php/scrum-quizzes/"&gt;Mikhail Lapshin's Scrum Quizzes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#disclaimer"&gt;
  &lt;/a&gt;
  DISCLAIMER
&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;This is not a guide&lt;/strong&gt;, it is just me sharing my experiences and learnings. This post only expresses my thoughts and opinions (based on my limited knowledge) and is in no way a substitute for actual references. If I ever make a mistake or if you disagree, I would appreciate corrections in the comments!&lt;/p&gt;



&lt;h3&gt;
  &lt;a href="#other-media"&gt;
  &lt;/a&gt;
  Other Media
&lt;/h3&gt;

&lt;p&gt;Feel free to check me in other media and reach out to me!&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;a href="https://twitter.com/RamminaR"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--C341ckO3--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://res.cloudinary.com/rammina/image/upload/v1636792959/twitter-logo_laoyfu_pdbagm.png" alt="Twitter logo" width="128" height="50"&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;a href="https://github.com/Rammina"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--u4ZlXeay--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://res.cloudinary.com/rammina/image/upload/v1636795051/GitHub-Emblem2_epcp8r.png" alt="Github logo" width="128" height="50"&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

</description>
      <category>100daysofcode</category>
      <category>beginners</category>
      <category>javascript</category>
      <category>programming</category>
    </item>
    <item>
      <title>Top 8 Docker Best Practices for using Docker in Production ‚úÖ</title>
      <author>Techworld with Nana</author>
      <pubDate>Sun, 21 Nov 2021 12:53:45 +0000</pubDate>
      <link>https://dev.to/techworld_with_nana/top-8-docker-best-practices-for-using-docker-in-production-1m39</link>
      <guid>https://dev.to/techworld_with_nana/top-8-docker-best-practices-for-using-docker-in-production-1m39</guid>
      <description>&lt;p&gt;Docker adoption rises constantly üìà and many are familiar with it, but not everyone is using Docker according to the best practices. üëÄ&lt;/p&gt;




&lt;p&gt;Before moving on, if you don't know what Docker is, you can learn everything you need to get started in this &lt;a href="https://youtu.be/3c-iBn73dDE"&gt;free Docker Crash Course üê≥&lt;/a&gt;&lt;/p&gt;




&lt;h2&gt;
  &lt;a href="#why-using-best-practices"&gt;
  &lt;/a&gt;
  Why using Best Practices? ü§∑‚Äç‚ôÄÔ∏è
&lt;/h2&gt;

&lt;p&gt;So, in my &lt;a href="https://youtu.be/8vXoMqWgbQQ"&gt;new video 'Top 8 Docker Production Best Practices'&lt;/a&gt; I want to show you 8 ways you can use Docker in a right way in your projects to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;‚úÖ &lt;strong&gt;improve security&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;‚úÖ &lt;strong&gt;optimize&lt;/strong&gt; the &lt;strong&gt;image size&lt;/strong&gt;, &lt;/li&gt;
&lt;li&gt;‚úÖ take advantage of some of the useful Docker features &lt;/li&gt;
&lt;li&gt;‚úÖ and also write &lt;strong&gt;cleaner and more maintainable Dockerfiles&lt;/strong&gt;
&lt;/li&gt;
&lt;/ul&gt;




&lt;h2&gt;
  &lt;a href="#best-practice"&gt;
  &lt;/a&gt;
  1Ô∏è‚É£ Best Practice
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Use an official and verified Docker image as a base image, whenever available.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let's say you are developing a Node.js application and want to build and run it as a Docker image. &lt;/p&gt;

&lt;p&gt;Instead of taking a base operating system image and installing node.js, npm and whatever other tools you need for your application, use the official node image for your application.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--Zc-ArYbv--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/spd6rkqkwnitp2riio29.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Zc-ArYbv--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/spd6rkqkwnitp2riio29.png" alt="1st best practice" width="880" height="497"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Improvements:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Cleaner Dockerfile&lt;/li&gt;
&lt;li&gt;Official and verified image, which is already built with the best practices&lt;/li&gt;
&lt;/ul&gt;




&lt;h2&gt;
  &lt;a href="#best-practice"&gt;
  &lt;/a&gt;
  2Ô∏è‚É£ Best Practice
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Use specific Docker image versions&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Okay, so we have selected the base image, but now when we build our applications image from this Dockerfile, it will always use the &lt;code&gt;latest&lt;/code&gt; tag of the node image.&lt;/p&gt;

&lt;p&gt;Now why is this a problem? ü§î&lt;br&gt;
‚ùå - you might get a different image version as in the previous build &lt;br&gt;
‚ùå - the new image version may break stuff&lt;br&gt;
‚ùå - &lt;code&gt;latest&lt;/code&gt; tag is unpredictable, causing unexpected behavior &lt;/p&gt;

&lt;p&gt;So instead of a random latest image tag, you want to fixate the version and just like you deploy your own application with a specific version you want to use the official image with a specific version. &lt;br&gt;
And the rule here is: &lt;strong&gt;the more specific the better&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--vIWh6vcV--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zikxyawtnrwk526xov6z.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--vIWh6vcV--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zikxyawtnrwk526xov6z.png" alt="2nd best practice" width="880" height="426"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Improvements:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Transparency to know exactly what version of the base image you're using&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;
  &lt;a href="#best-practice"&gt;
  &lt;/a&gt;
  3Ô∏è‚É£ Best Practice
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Use Small-Sized Official Images&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;When choosing a Node.js image, you will see there are actually multiple official images. Not only with different version numbers, but also with &lt;strong&gt;different operating system distributions&lt;/strong&gt;:&lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--zXYUDcn---/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/n4szjwytxzttkc4wovxz.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--zXYUDcn---/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/n4szjwytxzttkc4wovxz.png" alt="Node image versions" width="880" height="486"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So the question is: Which one do you choose and why is it important? ü§∑üèª‚Äç‚ôÇÔ∏è&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1) Image Size&lt;/strong&gt;&lt;br&gt;
‚ùå  Well, if the image is based on a &lt;strong&gt;full-blown OS distribution&lt;/strong&gt; like Ubuntu or Centos, you will have a bunch of tools already packaged in the image. So the &lt;strong&gt;image size will be larger&lt;/strong&gt;, but you &lt;strong&gt;don't need most of these tools&lt;/strong&gt; in your application images.&lt;/p&gt;

&lt;p&gt;‚úÖ  In contrast having &lt;strong&gt;smaller images means you need less storage space&lt;/strong&gt; in image repository as well as on a deployment server and of course you can transfer the images faster when pulling or pushing them from the repository. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2) Security Issue&lt;/strong&gt;&lt;br&gt;
‚ùå  In addition to that, with lots of tools installed inside, you need to consider the security aspect. Because such base images usually contain &lt;a href="https://snyk.io/blog/opensourcesecurity-2020survey/"&gt;hundreds of known vulnerabilities&lt;/a&gt; and basically create a &lt;strong&gt;larger attack surface&lt;/strong&gt; to your application image.&lt;/p&gt;

&lt;p&gt;This way you basically end up introducing unnecessary security issues from the beginning to your image! üôâ&lt;/p&gt;

&lt;p&gt;‚úÖ  In comparison by using &lt;strong&gt;smaller images with leaner OS distributions&lt;/strong&gt;, which only bundle the necessary system&lt;br&gt;
tools and libraries, you're also minimizing the attack surface and making sure that you build &lt;strong&gt;more secure images&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;So the best practice here would be to select an image with a specific version based on a leaner OS distribution like alpine for example:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--hNbFYYmk--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/36oou4hn3wq55n14u9dz.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--hNbFYYmk--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/36oou4hn3wq55n14u9dz.png" alt="3rd best practice" width="880" height="460"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alpine has everything you need to start your application in a container, but is much more lightweight. And for most of the images that you look on a Docker Hub, you will see a version tag with alpine distribution inside.&lt;/p&gt;

&lt;p&gt;It is one of the most common and popular base images for Docker containers.&lt;/p&gt;


&lt;h2&gt;
  &lt;a href="#best-practice"&gt;
  &lt;/a&gt;
  4Ô∏è‚É£ Best Practice
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Optimize caching for image layers when building an image&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So what are image layers and what does caching and image layer mean? ü§î&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1) What are Image Layers?&lt;/strong&gt;&lt;br&gt;
A Docker image is built based on a Dockerfile.&lt;br&gt;
And in a Dockerfile &lt;strong&gt;each command or instruction creates an image layer&lt;/strong&gt;:&lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--sxX5q4x3--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dmtb90pbzqygh0h86a10.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--sxX5q4x3--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/dmtb90pbzqygh0h86a10.png" alt="Docker Image Layers" width="880" height="464"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So when we use a base image of node alpine like in the above example it already has layers, because it was already built using its own Dockerfile. Plus, in our Dockerfile on top of that we have a couple of other commands that each will add a new layer to this image.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2) Now what about caching?&lt;/strong&gt;&lt;br&gt;
Each layer will get cached by Docker. üëç&lt;br&gt;
So when you rebuild your image, if your Dockerfile hasn't changed, Docker will just use the cached layers to build the image.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt; of cached image layers:&lt;br&gt;
‚úÖ  - Faster image building&lt;br&gt;
‚úÖ  - Faster pulling and pushing of new image versions: &lt;br&gt;
If I pull a new image version of the same application and let's say 2 new layers have been added in the new version: Only the newly added layers will be downloaded, the rest are already locally cached by Docker.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3) Optimize the Caching&lt;/strong&gt;&lt;br&gt;
So to optimize the caching, you need to know that:&lt;br&gt;
Once a layer changes, &lt;strong&gt;all following or downstream layers have to be re-created as well&lt;/strong&gt;. In other words: when you change the contents of one line in the Dockerfile, caches of all the following lines or layers will be busted and invalidated. üò£&lt;/p&gt;

&lt;p&gt;So the rule here and the best practice is:&lt;br&gt;
&lt;strong&gt;Order your commands&lt;/strong&gt; in the Dockerfile &lt;strong&gt;from the least to the most frequently changing commands&lt;/strong&gt; to take advantage of caching and this way optimize how fast the image gets built. üöÄ&lt;/p&gt;


&lt;h2&gt;
  &lt;a href="#best-practice"&gt;
  &lt;/a&gt;
  5Ô∏è‚É£ Best Practice
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Use &lt;code&gt;.dockerignore&lt;/code&gt; file&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now usually when we build the image, we don't need everything we have in the project to run the application inside. We&lt;br&gt;
&lt;strong&gt;don't need the auto-generated folders&lt;/strong&gt;, like &lt;code&gt;targets&lt;/code&gt; or &lt;code&gt;build&lt;/code&gt; folder, we don't need the &lt;code&gt;readme&lt;/code&gt; file etc.&lt;/p&gt;

&lt;p&gt;So how do we exclude such content from ending up in our application image? ü§î&lt;br&gt;
üëâ Using a &lt;code&gt;.dockerignore&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;It's pretty straightforward. We basically just create this &lt;code&gt;.dockerignore&lt;/code&gt; file and &lt;strong&gt;list all the files and folders that we want to be ignored&lt;/strong&gt; and when building the image, Docker will look at the contents and ignore anything specified inside.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--yAIQkTEs--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9fso63260yg2o7sddcxa.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--yAIQkTEs--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9fso63260yg2o7sddcxa.png" alt="5th best practice" width="880" height="480"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Improvements:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Reduced image size&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;
  &lt;a href="#best-practice"&gt;
  &lt;/a&gt;
  6Ô∏è‚É£ Best Practice
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Make use of Multi-Stage Builds&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;But now let's say there are some contents (like development, testing tools and libraries) in your project that you NEED for building the image - so during the&lt;br&gt;
build process - but you DON'T NEED them in the final image itself to run the application.&lt;/p&gt;

&lt;p&gt;If you keep these artifacts in your final image even though they're absolutely unnecessary for running the application, it will again result in an increased image size and increased attack surface. üßê&lt;/p&gt;

&lt;p&gt;So how do we &lt;strong&gt;separate the build stage from the runtime stage&lt;/strong&gt;.&lt;br&gt;
In other words, how do we exclude the build dependencies from the image, while still having them available while building the image? ü§∑‚Äç‚ôÄÔ∏è&lt;/p&gt;

&lt;p&gt;Well, for that you can use what's called &lt;strong&gt;multi-stage builds&lt;/strong&gt; üí°&lt;/p&gt;

&lt;p&gt;The multi-stage builds feature allows you to use multiple temporary images during the build process, but keep only&lt;br&gt;
the latest image as the final artifact:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--Qr2zxvmx--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/trdfkkdl96x1tkj4ffkh.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Qr2zxvmx--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/trdfkkdl96x1tkj4ffkh.png" alt="6th best practice" width="880" height="475"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So these previous steps (marked "1st" in the above picture) will be discarded.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Improvements:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Separation of Build Tools and Dependencies from what's needed for runtime&lt;/li&gt;
&lt;li&gt;Less dependencies and reduced image size&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;
  &lt;a href="#best-practice"&gt;
  &lt;/a&gt;
  7Ô∏è‚É£ Best Practice
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Use the Least Privileged User&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, when we create this image and eventually run it as a container, &lt;strong&gt;which operating system user&lt;/strong&gt; will be used to start the application inside? ü§î&lt;br&gt;
By default, when a Dockerfile does not specify a user, it uses a &lt;strong&gt;root user&lt;/strong&gt;. üôâ  But in reality there is mostly no reason to run containers with root privileges.&lt;/p&gt;

&lt;p&gt;‚ùå  This basically introduces a security issue, because when container starts on the host it, will potentially have root access on the Docker host.&lt;br&gt;
So running an application inside the container with a root user will make it &lt;strong&gt;easier for an attacker to escalate privileges on the host&lt;/strong&gt; and basically get hold of the underlying host and its processes, not only the container itself ü§Ø  Especially if the application inside the container is vulnerable to exploitation.&lt;/p&gt;

&lt;p&gt;‚úÖ  To avoid this, the best practice is to simply &lt;strong&gt;create a dedicated user&lt;/strong&gt; and a dedicated group in the Docker image to run the application and also &lt;strong&gt;run the application&lt;/strong&gt; inside the container &lt;strong&gt;with that user&lt;/strong&gt;:&lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--AD-IE3a9--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zrjr4m1885u1yqbnsjik.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--AD-IE3a9--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zrjr4m1885u1yqbnsjik.png" alt="7th best practice" width="880" height="479"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can use a directive called &lt;code&gt;USER&lt;/code&gt; with the username and then start the application conveniently.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; Some images already have a &lt;strong&gt;generic user&lt;/strong&gt; bundled in, which you can use. So you don't have to create a new one. For example the node.js image already bundles a generic user called &lt;code&gt;node&lt;/code&gt;, which you can simply use to run the application inside the container. üëç&lt;/p&gt;


&lt;h2&gt;
  &lt;a href="#best-practice"&gt;
  &lt;/a&gt;
  8Ô∏è‚É£ Best Practice
&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Scan your Images for Security Vulnerabilities&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Finally, how do you make sure and validate the image you build has a few or no security vulnerabilities? üßê&lt;/p&gt;

&lt;p&gt;So my final best practice is, once you build the image to &lt;strong&gt;scan it for security vulnerabilities&lt;/strong&gt; using the &lt;code&gt;docker scan&lt;/code&gt; command. üîç&lt;/p&gt;

&lt;p&gt;In the background Docker actually uses a service called snyk to do the vulnerability scanning of the images. The scan uses a database of vulnerabilities, which gets constantly updated. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example output&lt;/strong&gt; of &lt;code&gt;docker scan&lt;/code&gt; command:&lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--kS-pa8yP--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/tmwear5nj98iyfm63rik.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--kS-pa8yP--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/tmwear5nj98iyfm63rik.png" alt="docker scan output" width="879" height="552"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You see: &lt;br&gt;
1) the &lt;strong&gt;type of vulnerability&lt;/strong&gt;, &lt;br&gt;
2) a &lt;strong&gt;URL for more information&lt;/strong&gt; &lt;br&gt;
3) but also what's very useful and interesting you see &lt;strong&gt;which version&lt;/strong&gt; of the relevant library actually &lt;strong&gt;fixes that vulnerability&lt;/strong&gt;. So you can update your libraries to get rid of these issues. üëç&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Automate the scanning üöÄ&lt;/strong&gt;&lt;br&gt;
In addition to scanning your images manually with &lt;code&gt;docker scan&lt;/code&gt; command on a CLI, you can also &lt;strong&gt;configure Docker Hub&lt;/strong&gt; to &lt;strong&gt;scan the images automatically&lt;/strong&gt;, when they get pushed to the repository. And of course you can &lt;strong&gt;integrate&lt;/strong&gt; this check in your &lt;strong&gt;CI/CD pipeline&lt;/strong&gt; when building your Docker images.&lt;/p&gt;



&lt;p&gt;So these are &lt;strong&gt;8 production best practices that you can apply today to make your Docker images leaner and more secure&lt;/strong&gt;! üöÄüòä  Hope it is helpful for some of you! Of course there are many more best practices related to Docker, but I think applying these will already give you great results when using Docker in production.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Do you know some other best practices, which you think are&lt;br&gt;
super important and have to be mentioned? &lt;br&gt;
Please share them in the comments for others üôå üëç&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The full video is available here: ü§ì&lt;br&gt;
&lt;iframe width="710" height="399" src="https://www.youtube.com/embed/8vXoMqWgbQQ"&gt;
&lt;/iframe&gt;
&lt;/p&gt;




&lt;p&gt;&lt;strong&gt;Like, share and follow me&lt;/strong&gt; üòç for more content:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.instagram.com/techworld_with_nana/"&gt;Instagram - Posting many behind-the-scene stuff&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.facebook.com/groups/techworldwithnana"&gt;Private FB group&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="//linkedin.com/in/nana-janashia"&gt;LinkedIn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/Njuchi_"&gt;Twitter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/channel/UCdngmbVKX1Tgre699-XLlUA?sub_confirmation=1"&gt;YouTube&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
      <category>docker</category>
      <category>devops</category>
      <category>tutorial</category>
      <category>beginners</category>
    </item>
    <item>
      <title>How To Efficiently Update React State For Multiple DOM Inputs Using the useReducer() Hook</title>
      <author>Kyle Williams</author>
      <pubDate>Sun, 21 Nov 2021 12:48:41 +0000</pubDate>
      <link>https://dev.to/kylewcode/how-to-efficiently-update-react-state-for-multiple-dom-inputs-using-the-usereducer-hook-2e8d</link>
      <guid>https://dev.to/kylewcode/how-to-efficiently-update-react-state-for-multiple-dom-inputs-using-the-usereducer-hook-2e8d</guid>
      <description>&lt;p&gt;This article assumes some basic familiarity with the &lt;code&gt;useReducer()&lt;/code&gt; hook. Examples are using &lt;code&gt;react-bootstrap&lt;/code&gt; but you don't need to be using it in your own project for this to work.&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#efficient-vs-inefficient"&gt;
  &lt;/a&gt;
  Efficient VS Inefficient
&lt;/h1&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--pli0D1M5--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vkbvqdtwsrbtmmmu0vyp.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--pli0D1M5--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vkbvqdtwsrbtmmmu0vyp.png" alt="HTML form with inputs for name and address" width="880" height="582"&gt;&lt;/a&gt;&lt;br&gt;
Any DOM structure of HTML inputs would do, but let's say for example you have an HTML form such as the one above. You want React to update state for every change of the input by the user.&lt;/p&gt;
&lt;h2&gt;
  &lt;a href="#inefficient"&gt;
  &lt;/a&gt;
  Inefficient
&lt;/h2&gt;

&lt;p&gt;Assuming this state object...&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;  const initState = {
    firstName: "",
    lastName: "",
    street: "",
    aptSuite: "",
    city: "",
    stateName: "",
    zipcode: "",
    date: "",
    title: "",
    status: "fillingOutForm",
  };
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Assuming a form input element structured like this...&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;&amp;lt;Form.Label htmlFor="user-first-name"&amp;gt;First name&amp;lt;/Form.Label&amp;gt;
  &amp;lt;Form.Control
    type="text"
    name="FIRSTNAME" // Used for the action type
    id="user-first-name"
    value={formState.firstName} // formState from useReducer
    required
    onChange={(e) =&amp;gt; {
      const name = e.target.name;
      const value = e.target.value;
      dispatch({type: "CHANGE_" + name, payload: value });
    }}
/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;You could have a separate action type within the reducer function for each DOM input such as...&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;switch (type) {
  case CHANGE_FIRSTNAME:
    // Return modified state.
  case CHANGE_LASTNAME:
    // Return modified state.
  case CHANGE_STREET:
    // Return modified state.
  default:
    return state;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;This is inefficient however.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#efficient"&gt;
  &lt;/a&gt;
  Efficient
&lt;/h2&gt;

&lt;p&gt;The solution to this inefficiency is to abstract outwards in the reducer function.&lt;/p&gt;

&lt;p&gt;Given this &lt;code&gt;onChange&lt;/code&gt; handler...&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;// For example, the DOM input attribute name is 'firstName'
onChange={(e) =&amp;gt; {
  const field = e.target.name;
  const value = e.target.value;

  dispatch({
    type: "CHANGE_INPUT",
    payload: {
      value,
      field,
    },
  });
}}
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;...the reducer function could contain this...&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;  function formReducer(state, action) {
    const { type, payload } = action;

    switch (type) {
      case "CHANGE_INPUT":
        return { ...state, [payload.field]: payload.value };
      default:
        return state;
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;em&gt;Normally one would have more cases in the reducer function but this example is simplified for educational purposes&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In the code above, a computed property name is used to take the attribute name of the element ('firstName') and update state in the right place. In this case...&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;const initState = {
  firstName: "Whatever was type in by user",
  // Rest of state properties...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;h1&gt;
  &lt;a href="#gotchas"&gt;
  &lt;/a&gt;
  Gotchas
&lt;/h1&gt;

&lt;p&gt;Remember how to access the data needed using computed property names. You need to wrap the dot notation object accessor for the action payload object in &lt;strong&gt;brackets&lt;/strong&gt;.&lt;br&gt;
&lt;code&gt;return { ...state, [payload.field]: payload.value };&lt;/code&gt;&lt;/p&gt;
&lt;h1&gt;
  &lt;a href="#further-cleaning"&gt;
  &lt;/a&gt;
  Further Cleaning
&lt;/h1&gt;

&lt;p&gt;Optimization of code length can be achieved by moving code from the &lt;code&gt;onChange()&lt;/code&gt; handler to its own function. It might even be more descriptive to change the name to something like &lt;code&gt;updateStateWithInputValue&lt;/code&gt;.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;const changeDOMInput = (e) =&amp;gt; {
  const field = e.target.name;
  const value = e.target.value;
  dispatch({
    type: "CHANGE_INPUT",
    payload: {
      value,
      field,
    },
  });
};

onChange={(e) =&amp;gt; {
  changeDOMInput(e);
}}
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;I hope this helps!&lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#connect-with-me"&gt;
  &lt;/a&gt;
  Connect With Me!
&lt;/h1&gt;

&lt;p&gt;&lt;a href="https://www.kylewcode.com/"&gt;www.kylewcode.com&lt;/a&gt;&lt;br&gt;
&lt;a href="https://twitter.com/kylewcode"&gt;Twitter&lt;/a&gt;&lt;br&gt;
&lt;a href="https://www.linkedin.com/in/85kylewilliams/"&gt;LinkedIn&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/kylewcode"&gt;GitHub&lt;/a&gt;&lt;/p&gt;

</description>
      <category>react</category>
      <category>javascript</category>
      <category>webdev</category>
    </item>
    <item>
      <title>Backend Skillset Roadmap</title>
      <author>Samarth Gupta</author>
      <pubDate>Sun, 21 Nov 2021 12:43:13 +0000</pubDate>
      <link>https://dev.to/livesamarthgupta/backend-skillset-roadmap-4d5i</link>
      <guid>https://dev.to/livesamarthgupta/backend-skillset-roadmap-4d5i</guid>
      <description>&lt;h1&gt;
  &lt;a href="#hey-there"&gt;
  &lt;/a&gt;
  Hey There,
&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Are you a Java developer, or a Python developer, or maybe Golang developer?&lt;/strong&gt;&lt;br&gt;
Well, whoever you are, we all face the same set of problems in our backend, just our language of choice is different in solving those problems. But one may think how to grow, not just in particular technology but as an engineer in a whole. So I present the &lt;a href="https://whimsical.com/backend-dev-3UYBHbZzFYjmZbPToLxJje"&gt;Backend Skillset Roadmap&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--vfs4lYTp--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2j98mcv95xlnq07wczpm.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--vfs4lYTp--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/2j98mcv95xlnq07wczpm.png" alt="Image description" width="880" height="759"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This roadmap is language agnostic and could be thought of as a checklist instead of a roadmap. You don't need to learn things serially, one can progress through each parallelly.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;P.S: Don't be Jack of all and Master of none.&lt;/p&gt;
&lt;/blockquote&gt;

</description>
      <category>beginners</category>
      <category>bestofdev</category>
      <category>devjournal</category>
      <category>programming</category>
    </item>
    <item>
      <title>The development cycle, and how I follow it</title>
      <author>Piyush Raj</author>
      <pubDate>Sun, 21 Nov 2021 12:42:20 +0000</pubDate>
      <link>https://dev.to/piyush181/the-development-cycle-and-how-i-follow-it-2h5</link>
      <guid>https://dev.to/piyush181/the-development-cycle-and-how-i-follow-it-2h5</guid>
      <description>&lt;p&gt;Hello! I am Piyush Raj and today I will be telling you about the development cycle and how I personally follow it. In this post, I will have the different stages in software development and at the end of each paragraph, I will show you what I do at that particular stage.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#preproduction-where-the-pot-of-ideas-boil"&gt;
  &lt;/a&gt;
  Pre-Production: Where the pot of ideas boil
&lt;/h2&gt;

&lt;p&gt;As the name suggests, pre-production is that stage before creating the final product. This is the stage where the team comes together to plan out how the product will look like through the use of designs, mind maps, etc.&lt;/p&gt;

&lt;p&gt;In this stage, I usually like to make a &lt;a href="https://milanote.com/"&gt;Milanote&lt;/a&gt; board and put design specifications(or a checklist that the final product must/should have) and based on that I(or my team if applicable) make a visual design of the final product using it. I like to use Milanote because it has many tools that help me put my ideas down in a flowing way that anyone can understand and helps me to plan in an efficient way. I also like to use MS Paint and Gimp to create any visual designs.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#production-where-the-ideas-are-cooked-into-finished-products"&gt;
  &lt;/a&gt;
  Production: Where the ideas are cooked into finished products
&lt;/h2&gt;

&lt;p&gt;This is probably the most exciting(and my favourite) part of the development cycle. This is where you(and your team) come together to make the final product based on all of the planning you did in the pre-production stage.&lt;/p&gt;

&lt;p&gt;As I said earlier this is my favourite part of the development cycle. I usually like to use &lt;a href="https://www.github.com"&gt;GitHub&lt;/a&gt; as my source control tool for my projects and to collaborate on projects as well. As for what tools I use, it's probably what you'd expect, &lt;a href="https://code.visualstudio.com"&gt;VS Code&lt;/a&gt; for creating web projects and the &lt;a href="https://unity.com/"&gt;Unity Engine&lt;/a&gt; to create video games.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#postproduction"&gt;
  &lt;/a&gt;
  Post-Production
&lt;/h2&gt;

&lt;p&gt;This is kind of what most people would refer to as the "boring" or not so fun stage. This is the stage where the final product is tested for any issues. This is done mainly through AB testing and other forms of testing. This is also the stage where apps are given constant updates after production. This stage mainly depends on user feedback and results from the AB tests.&lt;/p&gt;

&lt;p&gt;I'll be very honest, I haven't really been part of a post-production stage but I do know that we run beta tests and roll them out to a few users if it's an app and we usually ask for opinions if the final product is a website. We then work accordingly based on the feedbacks we receive.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#conclusion"&gt;
  &lt;/a&gt;
  Conclusion
&lt;/h2&gt;

&lt;p&gt;So, I guess that was the end of this article and I would like to thank you for taking your time to read this article. I hope you enjoyed and hope you learnt something new!&lt;/p&gt;

</description>
      <category>beginners</category>
      <category>discuss</category>
      <category>productivity</category>
      <category>devjournal</category>
    </item>
    <item>
      <title>My Coding Bootcamp Odyssey - Weeks 2-4</title>
      <author>Rich Keyzor</author>
      <pubDate>Sun, 21 Nov 2021 12:39:40 +0000</pubDate>
      <link>https://dev.to/webdevrich/my-coding-bootcamp-odyssey-weeks-2-4-3jej</link>
      <guid>https://dev.to/webdevrich/my-coding-bootcamp-odyssey-weeks-2-4-3jej</guid>
      <description>&lt;p&gt;&lt;strong&gt;Odyssey&lt;/strong&gt; : &lt;em&gt;noun&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;a series of experiences that give knowledge or understanding to someone&lt;/p&gt;

&lt;p&gt;an intellectual or spiritual quest&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Hi folks, thanks for tuning in! This is a kinda catch-up post as I have been a bad boy and not blogged weekly as I intended - blame it on the mind-blowing-bending stuff we have learned ü§Ø and the need to rest my brain!&lt;/p&gt;

&lt;p&gt;I am now calling this series my odyssey because that better encapsulates the path I am on - a bit more epic that a mere functional journey, I think.&lt;/p&gt;

&lt;p&gt;So to orient ourselves this is about weeks 2-4 of my odyssey, following week 1 which was an introduction week. This is called the fundamentals block which covers the building blocks of JavaScript which we need to understand to progress to more higher-level and useful ways of using JavaScript. I'm a big fan of fundamentals-first learning.&lt;/p&gt;

&lt;p&gt;I'm going to list the topics we have covered and give them a rating based on how well I think I grasped the topic:&lt;/p&gt;

&lt;p&gt;ü§Ø I get what it is about but would struggle to teach it or give a good example&lt;/p&gt;

&lt;p&gt;üòê I get it and could probably explain it&lt;/p&gt;

&lt;p&gt;ü§ì Love it! totally makes sense&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Here we go:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Testing with Jest ü§ì&lt;/li&gt;
&lt;li&gt;Array Methods ü§ì&lt;/li&gt;
&lt;li&gt;Higher order functions üòê&lt;/li&gt;
&lt;li&gt;Scope ü§ì&lt;/li&gt;
&lt;li&gt;Function hoisting üòê&lt;/li&gt;
&lt;li&gt;Difference between a variable's value versus its reference ü§ì&lt;/li&gt;
&lt;li&gt;'Pure functions' üòê&lt;/li&gt;
&lt;li&gt;Testing side effects of functions with no return value with jest mock functions ü§Ø&lt;/li&gt;
&lt;li&gt;Closure üòê&lt;/li&gt;
&lt;li&gt;Destructuring ü§ì&lt;/li&gt;
&lt;li&gt;Higher order functions&lt;/li&gt;
&lt;li&gt;Recursion ü§Ø&lt;/li&gt;
&lt;li&gt;Object Orientated Programming ü§ì&lt;/li&gt;
&lt;li&gt;Factory functions and &lt;code&gt;this&lt;/code&gt; keyword ü§ì&lt;/li&gt;
&lt;li&gt;Constructor functions and instance methods ü§ì&lt;/li&gt;
&lt;li&gt;
&lt;code&gt;Class&lt;/code&gt; syntax ü§ì&lt;/li&gt;
&lt;li&gt;Callback functions üòê&lt;/li&gt;
&lt;li&gt;HTTP requests with Axios ü§ì&lt;/li&gt;
&lt;li&gt;Promises and .then() and .catch() üòê&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Wow! what a list - we covered so much in 3 weeks. It was not all plain sailing but we will be getting a lot more practice so I am confident the majority will sink in, in time.&lt;/p&gt;

&lt;p&gt;Bring on the back end!&lt;/p&gt;

</description>
      <category>beginners</category>
      <category>javascript</category>
      <category>webdev</category>
    </item>
    <item>
      <title>How to use celery with flask</title>
      <author>Salem Olorundareüë®‚Äçüíª</author>
      <pubDate>Sun, 21 Nov 2021 12:30:08 +0000</pubDate>
      <link>https://dev.to/itz_salemm/how-to-use-celery-with-flask-2k1m</link>
      <guid>https://dev.to/itz_salemm/how-to-use-celery-with-flask-2k1m</guid>
      <description>&lt;h2&gt;
  &lt;a href="#introduction"&gt;
  &lt;/a&gt;
  Introduction
&lt;/h2&gt;

&lt;p&gt;Have you ever come across programs or task that takes a lot of time to process or give an actual output? Tasks such as sending emails and uploading data over the Internet takes time to process which can slow down your application‚Äôs workflow. These task should be run separately from other tasks.&lt;/p&gt;

&lt;p&gt;Your application should be able to process with these task in the background and continue with other tasks. After these task have processed and the result is ready, then it can be served to the user.&lt;/p&gt;

&lt;p&gt;I will be introducing you to setting up and configuration of celery and Redis in a flask project which handles async function or tasks like this. We would also take a look at the application of celery in real time with build and email sender app. Remember that these task takes time.&lt;/p&gt;

&lt;p&gt;We would be making our email sender run as a background task.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#prerequisites"&gt;
  &lt;/a&gt;
  Prerequisites
&lt;/h2&gt;

&lt;p&gt;This tutorial assumes that you already know about the basics of Python and Flask and also assumes that you have python and Flask framework set up on your system or machine. You must also have the very least and basic understanding of HTML to build out email forms.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#what-is-a-task-queue"&gt;
  &lt;/a&gt;
  What is a Task Queue?
&lt;/h2&gt;

&lt;p&gt;A task queue is a system that distributes task that needs to be complete as a background without interfering with the applications request and response cycle.&lt;/p&gt;

&lt;p&gt;Task Queues makes Assigning work that slows down application processes while running easier. Intensive application can be handle by software applications like this in the background while users still interacts with the website and carry on with other activities.&lt;/p&gt;

&lt;p&gt;This ensures that the user's engagement is consistent, timely, and unaffected by the workload.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#what-is-celery"&gt;
  &lt;/a&gt;
  What is Celery?
&lt;/h2&gt;

&lt;p&gt;Celery is a Python task queue that allows task to run asynchronously with web applications without disturbing the application‚Äôs request response cycle.&lt;/p&gt;

&lt;p&gt;Celery is highly scalable, which is one of the several reasons why it is being used for background work and also allowing new workers to be dispatched on-demand to handle increasing workload or traffic. &lt;/p&gt;

&lt;p&gt;With Celery being a well supported project and also well documented, Celery still has a thriving user community, though it is still under development.&lt;/p&gt;

&lt;p&gt;Celery is also easy to integrate into various web frameworks, with most of them providing libraries to assist with this. Celery can also interface with several other message brokers hence, its thriving community.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#what-is-an-asynchronous-task"&gt;
  &lt;/a&gt;
  What is an Asynchronous Task?
&lt;/h2&gt;

&lt;p&gt;An asynchronous task is simply a function that runs behind every other process running on your app. This kind of function, when called, does not affect the normal flow of the application.&lt;/p&gt;

&lt;p&gt;With asynchronous operations, you can switch to a new task before the previous one is complete. By using asynchronous programming, you can handle many requests at once, accomplishing more in a shorter amount of time. &lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#installation-and-configuration-for-celery-on-flask"&gt;
  &lt;/a&gt;
  Installation and Configuration for Celery on Flask
&lt;/h2&gt;

&lt;p&gt;Running Celery requires the use of a broker. Redis is the most well-known of the brokers.&lt;/p&gt;

&lt;p&gt;For sending and receiving messages, Celery requires the use of message broker, such as&lt;br&gt;
‚¶Å RabbitMQ&lt;br&gt;
‚¶Å Redis&lt;br&gt;
‚¶Å Amazon SQS&lt;/p&gt;

&lt;p&gt;Note: Message brokers are programs built to aids the communication between services when sending information from both ends.&lt;/p&gt;

&lt;p&gt;We would be making use of the Redis server in the tutorial.&lt;/p&gt;
&lt;h2&gt;
  &lt;a href="#creating-a-flask-server"&gt;
  &lt;/a&gt;
  Creating a Flask server
&lt;/h2&gt;

&lt;p&gt;Creating a Flask server is easy. Navigate to the folder where you want your server created. Create a new python file and give it a name, in our case celeryapp.py&lt;br&gt;
And add this simple code to your python script:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;flask&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;
    &lt;span class="n"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;@&lt;/span&gt;&lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;route&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"/"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;home&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;""&lt;/span&gt;&lt;span class="n"&gt;Hello&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;World&lt;/span&gt;&lt;span class="err"&gt;!&lt;/span&gt;&lt;span class="s"&gt;""&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;"__main"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;debug&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Now, let's test our server to make sure it's working.&lt;br&gt;
To start our server, run the following commands in your terminal:&lt;br&gt;
python celeryapp.py&lt;/p&gt;

&lt;p&gt;If you have followed the article correctly, your result should look simple to the image below.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--vj51U99N--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://paper-attachments.dropbox.com/s_DC6382D20790F0CBCB254B1B53649C4EE0725D6299C9DF0C4B476627695C1870_1636643567417_flask_server.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--vj51U99N--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://paper-attachments.dropbox.com/s_DC6382D20790F0CBCB254B1B53649C4EE0725D6299C9DF0C4B476627695C1870_1636643567417_flask_server.png" alt="Running Redis Server" width="804" height="260"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;
  &lt;a href="#communicating-between-celery-and-flask"&gt;
  &lt;/a&gt;
  Communicating between Celery and Flask
&lt;/h2&gt;

&lt;p&gt;Now that we have created our server, we need to connect Celery with our flask application. To do this, update your celeryapp.py file to look like this below.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;    &lt;span class="c1"&gt;#imports
&lt;/span&gt;    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;flask&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;
    &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;celery&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Celery&lt;/span&gt;

    &lt;span class="c1"&gt;#creates a Flask object
&lt;/span&gt;    &lt;span class="n"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;#Configure the redis server
&lt;/span&gt;    &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'CELERY_BROKER_URL'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'redis://localhost:6379/0'&lt;/span&gt;
    &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'CELERY_RESULT_BACKEND'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'redis://localhost:6379/0'&lt;/span&gt;

    &lt;span class="c1"&gt;#creates a Celery object
&lt;/span&gt;    &lt;span class="n"&gt;celery&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Celery&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;broker&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'CELERY_BROKER_URL'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;celery&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Celery is initialized by creating an object of class Celery with the application name and the connection to the message broker URL which is set to CELERY_BROKER_URL as key in the app.config. If you run something other than Redis or have the broker on a different machine, you will need to change the URL accordingly.&lt;/p&gt;

&lt;p&gt;it‚Äôs always best to add additional configuration through celery.conf.update() for Celery. Though it is not a requirement,  the CELERY_RESULT_BACKEND is only necessary to  store status and results from tasks in Celery.&lt;/p&gt;

&lt;p&gt;The function that would run as a background task is just a normal function with has the celery.task decorator. With just this decorator, the function would always run in the back ground. For example:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;    &lt;span class="o"&gt;@&lt;/span&gt;&lt;span class="n"&gt;celery&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;task&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;async_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arg1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arg2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;#Async task
&lt;/span&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Just  like any other function,  to execute our celery task, it needs to be invoke, To invoke the function, add the following code to your celeryapp.py file.&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt; `async_function(10, 30)`
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;
&lt;h2&gt;
  &lt;a href="#sending-an-asynchronous-email-with-celery"&gt;
  &lt;/a&gt;
  Sending an Asynchronous Email with Celery.
&lt;/h2&gt;

&lt;p&gt;Now, let's see how Celery works in the real world. Let's apply Celery in sending an email with our flask application. First, we need to build out our email form to let users send emails. Here is the HTML template to build the form.&lt;br&gt;
&lt;/p&gt;
&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;    &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;html&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;Flask&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;Celery&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="o"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;h2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;Sending&lt;/span&gt; &lt;span class="n"&gt;Asynchronous&lt;/span&gt; &lt;span class="n"&gt;Email&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="n"&gt;h2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;message&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;get_flashed_messages&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"color: red;"&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{{&lt;/span&gt; &lt;span class="n"&gt;message&lt;/span&gt; &lt;span class="p"&gt;}}&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;endfor&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;form&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"POST"&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
          &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;Send&lt;/span&gt; &lt;span class="n"&gt;email&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"text"&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"email"&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"{{ email }}"&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
          &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"submit"&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"submit"&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"Send"&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="o"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="n"&gt;form&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="o"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="o"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="n"&gt;html&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;This is just a regular HTML syntax with the ability to show flask messages from Flask. Hopefully, you should be able to get around with it.&lt;/p&gt;

&lt;p&gt;Note: The code above is the index.html file.&lt;/p&gt;

&lt;p&gt;To send Emails, we would make use of the Flask-Mail extension.&lt;br&gt;
Flask-Mail requires some configuration, including information about the email server that it will use to send emails. &lt;/p&gt;

&lt;p&gt;Add the following code to your celeryapp.py app to configure your Email sender:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;    &lt;span class="n"&gt;Flask&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Mail&lt;/span&gt; &lt;span class="n"&gt;configuration&lt;/span&gt;
    &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'MAIL_SERVER'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'smtp.googlemail.com'&lt;/span&gt;
    &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'MAIL_PORT'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;587&lt;/span&gt;
    &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'MAIL_USE_TLS'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
    &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'MAIL_USERNAME'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'MAIL_USERNAME'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'MAIL_PASSWORD'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'MAIL_PASSWORD'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'MAIL_DEFAULT_SENDER'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;'flask@example.com'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Note: For this to work, make sure you have saved your email and password to an environment variables. For security reasons and easy accessibility my password and email are stored in an environment variables.&lt;/p&gt;

&lt;p&gt;Since we have a single route in this app, we created a route index alone to cater for it. Update your celeryapp.py file with the following code.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;    &lt;span class="o"&gt;@&lt;/span&gt;&lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;route&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'/'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;methods&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'GET'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'POST'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;index&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;method&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;'GET'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;render_template&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'index.html'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;email&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;session&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'email'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;''&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;email&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;form&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'email'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;session&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'email'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;email&lt;/span&gt;
    &lt;span class="c1"&gt;# sends this content
&lt;/span&gt;    &lt;span class="n"&gt;email_msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s"&gt;'subject'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;'Testing Celery with Flask'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s"&gt;'to'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;email&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s"&gt;'body'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;'Testing background task with Celery'&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;form&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'submit'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;'Send'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# sends the email content to the backgraound function
&lt;/span&gt;        &lt;span class="n"&gt;send_email&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;delay&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;email_msg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;flash&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'Sending email to {0}'&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;email&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;flash&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'No Email sent'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;redirect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'index'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;The code above shows a function, which gets the input from our html form and saves it in a session, for easier accessibility. &lt;/p&gt;

&lt;p&gt;This function checks for events on the submit button and  send_email after the submit button is clicked.  The email_msg contains the subject, recipient‚Äôs email address, and the body of the message being sent.&lt;/p&gt;

&lt;p&gt;To allow users know what is going in the background, a flash message is displayed when the email is being submitted.&lt;/p&gt;

&lt;p&gt;Note: We saved the user's value in the text field in the session to remember it after the page reloads.&lt;/p&gt;

&lt;p&gt;The last piece of this application is the asynchronous task that gets the job done when a user submits the email form.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;    &lt;span class="o"&gt;@&lt;/span&gt;&lt;span class="n"&gt;celery&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;task&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;send_email&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;email_msg&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;#Async function to send an email with Flask-Mail
&lt;/span&gt;    &lt;span class="n"&gt;msg_sub&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Message&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;email_msg&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'subject'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;email_sender&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'MAIL_DEFAULT_SENDER'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;recipient&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;email_msg&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'to'&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
    &lt;span class="n"&gt;msg_sub&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;body&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;email_msg&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'body'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;app_context&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;mail&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;send&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg_sub&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;As said earlier, this task is decorated with celery.task to make it run in the background.&lt;/p&gt;

&lt;p&gt;The function creates a Message object from Flask-Mail using the email data dictionary. For Flask-Mail to run, it must build an application context before calling the send() method.&lt;/p&gt;

&lt;h2&gt;
  &lt;a href="#conclusion"&gt;
  &lt;/a&gt;
  Conclusion
&lt;/h2&gt;

&lt;p&gt;Celery more than a few extra steps beyond simply sending a job to a background thread, but the benefits in terms of flexibility and scalability are hard to ignore.&lt;/p&gt;

&lt;p&gt;Sending a scheduled task is easier done using celery than any other means of running scheduled task. Imagine you want to perform a  task daily, In this case, Celery can be used to run programs in the background with any necessarily human triggered.&lt;/p&gt;

&lt;p&gt;Although Celery is used in most cases for long running task, it can also be used to connect to third party APIs. As soon as data is gotten back from the API in your celery task, it is then served to the user. &lt;/p&gt;

</description>
      <category>python</category>
      <category>flask</category>
      <category>webdev</category>
      <category>tutorial</category>
    </item>
    <item>
      <title>SMS Bomber App - Flutter</title>
      <author>Dev Nirwal</author>
      <pubDate>Sun, 21 Nov 2021 12:26:09 +0000</pubDate>
      <link>https://dev.to/devn913/sms-bomber-app-flutter-2lp7</link>
      <guid>https://dev.to/devn913/sms-bomber-app-flutter-2lp7</guid>
      <description>&lt;p&gt;Note:- This app is only for educational purpose only. Please don't abuse the app or the API. Use it for fun not for revenge.&lt;/p&gt;

&lt;p&gt;About this app:-  This is one of my small project to prank your friends which send bulk sms in a short period of time. Based on Flutter and Dart and for back-end I have used &lt;a href="https://github.com/devn913/bomber_api"&gt;https://github.com/devn913/bomber_api&lt;/a&gt; which is hosted on free Heroku tier.&lt;/p&gt;

&lt;p&gt;I am not a Flutter developer so if you have any ideas how to make this app better, I am open to contribution make sure to Fork it star it and follow me if you liked this project.&lt;/p&gt;

&lt;p&gt;If you want me to add more API contact me by mail - &lt;a href="mailto:devnirwal16@gmail.com"&gt;devnirwal16@gmail.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you want to add more API yourself use this repo - &lt;a href="https://github.com/devn913/bomber_api"&gt;https://github.com/devn913/bomber_api&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Project GitHub Link - &lt;a href="https://github.com/Devn913/bomber_app"&gt;https://github.com/Devn913/bomber_app&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;APK link  - &lt;a href="https://github.com/Devn913/bomber_app/blob/main/app-release.apk"&gt;https://github.com/Devn913/bomber_app/blob/main/app-release.apk&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;My GitHub Profile &lt;a href="https://github.com/Devn913"&gt;https://github.com/Devn913&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Virus Total - &lt;a href="https://www.virustotal.com/gui/file/5f1c9a5eddddd14da717d0681d892665e946517f0b18c5a5ac7e6e2da4d6ffc1?nocache=1"&gt;https://www.virustotal.com/gui/file/5f1c9a5eddddd14da717d0681d892665e946517f0b18c5a5ac7e6e2da4d6ffc1?nocache=1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Star it, Follow me if you liked this project.&lt;/p&gt;

&lt;p&gt;Contact me if you are facing any issues.&lt;/p&gt;

</description>
      <category>flutter</category>
      <category>dart</category>
      <category>python</category>
      <category>android</category>
    </item>
    <item>
      <title>AWS Serverless Data Analytics Pipeline | AWS White Paper Summary</title>
      <author>Adit Modi</author>
      <pubDate>Sun, 21 Nov 2021 12:13:22 +0000</pubDate>
      <link>https://dev.to/awsmenacommunity/aws-serverless-data-analytics-pipeline-aws-white-paper-summary-4h3f</link>
      <guid>https://dev.to/awsmenacommunity/aws-serverless-data-analytics-pipeline-aws-white-paper-summary-4h3f</guid>
      <description>&lt;h1&gt;
  &lt;a href="#introduction"&gt;
  &lt;/a&gt;
  Introduction
&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A serverless data lake architecture enables agile and self-service data onboarding and analytics for all data consumer roles across a company. By using AWS serverless technologies as building blocks, you can rapidly and interactively build data lakes and data processing pipelines to ingest, store, transform, and analyze petabytes of structured and unstructured data from batch and streaming sources, without needing to manage any storage or compute infrastructure.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This architecture includes a data lake, data processing pipelines, and a consumption layer that enables several ways to analyze the data in the data lake without moving it, including business intelligence (BI) dashboarding, exploratory interactive SQL, big data processing, predictive analytics, and ML.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;
  &lt;a href="#logical-architecture-of-modern-data-lake-centric-analytics-platforms"&gt;
  &lt;/a&gt;
  Logical architecture of modern data lake centric analytics platforms
&lt;/h1&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--EyoOW-Ni--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ljbs5k29hz0kp8ca8oy8.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--EyoOW-Ni--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ljbs5k29hz0kp8ca8oy8.png" alt="Image description" width="627" height="522"&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;Architecture of a data lake centric analytics platform&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You can think of a data lake centric analytics architecture as a stack of six logical layers, where each layer is composed of multiple components. A layered, component-oriented architecture promotes separation of concerns, decoupling of tasks, and flexibility. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This provides the agility needed to quickly integrate new data sources, support new analytics methods, and add tools required to keep up with the accelerating pace of changes in the analytics landscape. In the following sections, we look at the key responsibilities, capabilities, and integrations of each logical layer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;ingestion layer&lt;/strong&gt; is responsible for bringing data into the data lake. It provides the ability to connect to internal and external data sources over a variety of protocols. It can ingest batch and streaming data into the storage layer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;storage layer&lt;/strong&gt; is responsible for providing durable, scalable, secure, and money- saving components to store vast quantities of data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;cataloging and search layer&lt;/strong&gt; is responsible for storing business and technical metadata about datasets hosted in the storage layer. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;processing layer&lt;/strong&gt; is responsible for transforming data into a consumable state through data validation, cleanup, normalization, transformation, and enrichment. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;consumption layer&lt;/strong&gt; is responsible for providing scalable and performant tools to gain insights from the vast amount of data in the data lake. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;security and governance layer&lt;/strong&gt; is responsible for protecting the data in the storage layer and processing resources in all other layers. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;
  &lt;a href="#serverless-data-lake-centric-analytics-architecture"&gt;
  &lt;/a&gt;
  Serverless data lake centric analytics architecture
&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;To compose the layers described in our logical architecture, AWS introduces a reference architecture that uses AWS serverless and managed services. In this approach, AWS services provide the following capabilities:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Providing and managing scalable, resilient, secure, and cost-effective infrastructural components&lt;/li&gt;
&lt;li&gt;Ensuring infrastructural components natively integrate with each other&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This reference architecture enables you to focus more time on rapidly building data and analytics pipelines. It significantly accelerates new data onboarding and driving insights from your data. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;

&lt;p&gt;The AWS serverless and managed components enable self-service across all data consumer roles by providing the following key benefits:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Easy configuration-driven use&lt;/li&gt;
&lt;li&gt;Freedom from infrastructure management&lt;/li&gt;
&lt;li&gt;Pay-per-use pricing model
The following diagram illustrates this architecture.&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--P54ECb3h--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/jg5npvnxdja05jnkmhul.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--P54ECb3h--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/jg5npvnxdja05jnkmhul.png" alt="Image description" width="625" height="462"&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;AWS Serverless Data Analytics Pipeline Reference Architecture&lt;/em&gt; &lt;/p&gt;

&lt;h1&gt;
  &lt;a href="#ingestion-layer"&gt;
  &lt;/a&gt;
  Ingestion layer
&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The ingestion layer in the presented serverless architecture is composed of a set of purpose-built AWS services to enable data ingestion from a variety of sources. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each of these services enables simple self-service data ingestion into the data lake landing zone and provides integration with other AWS services in the storage and security layers. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Individual purpose-built AWS services match the unique connectivity, data format, data structure, and data velocity requirements of operational database sources, streaming data sources, and file sources.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#gt-operational-database-sources"&gt;
  &lt;/a&gt;
  -&amp;gt; Operational database sources
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Typically, organizations store their operational data in various relational and NoSQL databases. AWS Data Migration Service (AWS DMS) can connect to a variety of operational RDBMS and NoSQL databases and ingest their data into Amazon Simple Storage Service (Amazon S3) buckets in the data lake landing zone. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With AWS DMS, you can first perform a one-time import of the source data into the data lake and replicate ongoing changes happening in the source database. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;AWS DMS encrypts S3 objects using AWS Key Management Service (AWS KMS) keys as it stores them in the data lake. AWS DMS is a fully managed, resilient service and provides a wide choice of instance sizes to host database replication tasks.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;AWS Lake Formation provides a scalable, serverless alternative, called blueprints, to ingest data from AWS native or on-premises database sources into the landing zone in the data lake. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A Lake Formation blueprint is a predefined template that generates a data ingestion AWS Glue workflow based on input parameters such as source database, target Amazon S3 location, target dataset format, target dataset partitioning columns, and schedule. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A blueprint-generated AWS Glue workflow implements an optimized and parallelized data ingestion pipeline consisting of crawlers, multiple parallel jobs, and triggers connecting them based on conditions. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#gt-streaming-data-sources"&gt;
  &lt;/a&gt;
  -&amp;gt; Streaming data sources
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The ingestion layer uses Amazon Kinesis Data Firehose to receive streaming data from internal and external sources. With a few clicks, you can configure a Kinesis Data Firehose API endpoint where sources can send streaming data. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This streaming data can be clickstreams, application and infrastructure logs and monitoring metrics, and IoT data such as devices telemetry and sensor readings. Kinesis Data Firehose does the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Buffers incoming streams&lt;/li&gt;
&lt;li&gt;Batches, compresses, transforms, and encrypts the streams&lt;/li&gt;
&lt;li&gt;Stores the streams as S3 objects in the landing zone in the data lake&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Kinesis Data Firehose natively integrates with the security and storage layers and can deliver data to Amazon S3, Amazon Redshift, and Amazon Elasticsearch Service (Amazon ES) for real-time analytics use cases. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Kinesis Data Firehose is serverless, requires no administration, and has a cost model where you pay only for the volume of data you transmit and process through the service. Kinesis Data Firehose automatically scales to adjust to the volume and throughput of incoming data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#gt-file-sources"&gt;
  &lt;/a&gt;
  -&amp;gt; File sources
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Many applications store structured and unstructured data in files that are hosted on Network Attached Storage (NAS) arrays. Organizations also receive data files from partners and third-party vendors. Analyzing data from these file sources can provide valuable business insights&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#internal-file-shares"&gt;
  &lt;/a&gt;
  Internal file shares
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;AWS DataSync can ingest hundreds of terabytes and millions of files from NFS and SMB enabled NAS devices into the data lake landing zone. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;DataSync automatically handles scripting of copy jobs, scheduling and monitoring transfers, validating data integrity, and optimizing network utilization. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;DataSync can perform one-time file transfers and monitor and sync changed files into the data lake. DataSync is fully managed and can be set up in minutes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#partner-data-files"&gt;
  &lt;/a&gt;
  Partner data files
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;FTP is most common method for exchanging data files with partners. The AWS Transfer Family is a serverless, highly available, and scalable service that supports secure FTP endpoints and natively integrates with Amazon S3&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#gt-data-apis"&gt;
  &lt;/a&gt;
  -&amp;gt; Data APIs
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Organizations today use SaaS and partner applications such as Salesforce, Marketo, and Google Analytics to support their business operations. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Analyzing SaaS and partner data in combination with internal operational application data is critical to gaining 360- degree business insights. Partner and SaaS applications often provide API endpoints to share data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#saas-apis"&gt;
  &lt;/a&gt;
  SaaS APIs
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The ingestion layer uses Amazon AppFlow to easily ingest SaaS applications data into the data lake. With a few clicks, you can set up serverless data ingestion flows in Amazon AppFlow. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Your flows can connect to SaaS applications (such as Salesforce, Marketo, and Google Analytics), ingest data, and store it in the data lake. You can schedule Amazon AppFlow data ingestion flows or trigger them by events in the SaaS application. Ingested data can be validated, filtered, mapped, and masked before storing in the data lake. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Amazon AppFlow natively integrates with authentication, authorization, and encryption services in the security and governance layer.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#partner-apis"&gt;
  &lt;/a&gt;
  Partner APIs
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To ingest data from partner and third-party APIs, organizations build or purchase custom applications that connect to APIs, fetch data, and create S3 objects in the landing zone by using AWS SDKs. These applications and their dependencies can be packaged into Docker containers and hosted on AWS Fargate. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;AWS Glue Python shell jobs also provide serverless alternative to build and schedule data ingestion jobs that can interact with partner APIs by using native, open-source, or partner-provided Python libraries. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;AWS Glue provides out-of-the-box capabilities to schedule singular Python shell jobs or include them as part of a more complex data ingestion workflow built on AWS Glue workflows.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;
  &lt;a href="#thirdparty-data-sources"&gt;
  &lt;/a&gt;
  Third-party data sources
&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Your organization can gain a business edge by combining your internal data with third- party datasets such as historical demographics, weather data, and consumer behavior data. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;AWS Data Exchange provides a serverless way to find, subscribe to, and ingest third-party data directly into Amazon S3 buckets in the data lake landing zone. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;
  &lt;a href="#storage-layer"&gt;
  &lt;/a&gt;
  Storage layer
&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Amazon S3 provides the foundation for the storage layer in our architecture. Amazon S3 provides virtually unlimited scalability at low cost for our serverless data lake. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data is stored as S3 objects organized into raw, cleaned, and curated zone buckets, and prefixes. Amazon S3 encrypts data using keys managed in AWS KMS. IAM policies control granular zone-level and dataset-level access to various users and roles.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Amazon S3 provides 99.99% of availability and 99.999999999% of durability, and charges only for the data it stores. To significantly reduce costs, Amazon S3 provides colder tier storage options called Amazon S3 Glacier &amp;amp; S3 Glacier Deep Archive. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To automate cost optimizations, Amazon S3 provides configurable lifecycle policies and S3 Intelligent-Tiering options to automate moving older data to colder tiers. AWS services in our ingestion, cataloging, processing, and consumption layers can natively read and write S3 objects. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;
  &lt;a href="#cataloging-and-search-layer"&gt;
  &lt;/a&gt;
  Cataloging and search layer
&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A data lake typically hosts many datasets which have evolving schema and new data partitions. A central data catalog that manages metadata for all the datasets in the data lake is crucial to enabling self-service discovery of data in the data lake. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Additionally, separating metadata from data into a central schema enables schema-on-read for the processing and consumption layer components.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the presented architecture, Lake Formation provides the central catalog to store and manage metadata for all datasets hosted in the data lake. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Organizations manage both technical metadata (such as versioned table schemas, partitioning information, physical data location, and update timestamps) and business attributes (such as data owner, data steward, column business definition, and column information sensitivity) of all their datasets in Lake Formation. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Services such as AWS Glue, Amazon EMR, and Amazon Athena natively integrate with Lake Formation and automate discovering and registering dataset metadata into the Lake Formation catalog. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Additionally, Lake Formation provides APIs to enable metadata registration and management using custom scripts and third-party products. AWS Glue crawlers in the processing layer can track evolving schemas and newly added partitions of datasets in the data lake, and add new versions of corresponding metadata in the Lake Formation catalog.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lake Formation provides the data lake administrator a central place to set up granular table and column level permissions for databases and tables hosted in the data lake. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;After Lake Formation permissions are set up, users and groups can access only authorized tables and columns using multiple processing and consumption layer services such as Athena, Amazon EMR, AWS Glue, and Amazon Redshift Spectrum.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;
  &lt;a href="#processing-layer"&gt;
  &lt;/a&gt;
  Processing layer
&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The processing layer in our architecture is composed of two types of components:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Components used to create multi-step data processing pipelines.&lt;/li&gt;
&lt;li&gt;Components to orchestrate data processing pipelines on schedule or in response to event triggers (such as ingestion of new data into the landing zone).&lt;/li&gt;
&lt;/ul&gt;


&lt;/li&gt;
&lt;li&gt;&lt;p&gt;AWS Glue and AWS Step Functions provide serverless components to build, orchestrate, and run pipelines that can easily scale to process large data volumes. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Multi-step workflows built using AWS Glue and Step Functions can catalog, validate, clean, transform, and enrich individual datasets and advance them from raw to cleaned and cleaned to curated zones in the storage layer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;AWS Glue is a serverless, pay-per-use ETL service for building and running Python or Spark jobs (written in Scala or Python) without requiring you to deploy or manage clusters. AWS Glue automatically generates the code to accelerate your data transformations and loading processes. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;AWS Glue ETL builds on top of Apache Spark and provides commonly used out-of-the-box data source connectors, data structures, and ETL transformations to validate, clean, transform, and flatten data stored in many open-source formats such as CSV, JSON, Parquet, and Avro. AWS Glue ETL also provides capabilities to incrementally process partitioned data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Additionally, you can use AWS Glue to define and run crawlers that can crawl folders in the data lake, discover datasets and their partitions, infer schema, and define tables in the Lake Formation catalog. AWS Glue provides more than a dozen built-in classifiers that can parse a variety of data structures stored in open-source formats. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;AWS Glue also provides triggers and workflow capabilities that you can use to build multi-step end- to-end data processing pipelines that include job dependencies and running parallel steps. You can schedule AWS Glue jobs and workflows or run them on demand. AWS Glue natively integrates with AWS services in storage, catalog, and security layers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To make it easy to clean and normalize data, Glue also provides a visual data preparation tool called Glue DataBrew which is an interactive, point-and-click visual interface without requiring to write any code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step Functions is a serverless engine that you can use to build and orchestrate scheduled or event-driven data processing workflows. You use Step Functions to build complex data processing pipelines that involve orchestrating steps implemented by using multiple AWS services such as AWS Glue, AWS Lambda, Amazon Elastic Container Service (Amazon ECS) containers, and more. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;
  &lt;a href="#consumption-layer"&gt;
  &lt;/a&gt;
  Consumption layer
&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;The consumption layer in the presented architecture is composed using fully managed, purpose-built, analytics services that enable interactive SQL, BI dashboarding, batch processing, and ML.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#interactive-sql"&gt;
  &lt;/a&gt;
  Interactive SQL
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Amazon Athena is an interactive query service that enables you to run complex ANSI SQL against terabytes of data stored in Amazon S3 without needing to first load it into a database. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Athena queries can analyze structured, semi-structured, and columnar data stored in open-source formats such as CSV, JSON, XML Avro, Parquet, and ORC. Athena uses table definitions from Lake Formation to apply schema-on-read to data read from Amazon S3.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#data-warehousing-and-batch-analytics"&gt;
  &lt;/a&gt;
  Data warehousing and batch analytics
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Amazon Redshift is a fully managed data warehouse service that can host and process petabytes of data and run thousands highly performant queries in parallel. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Amazon Redshift uses a cluster of compute nodes to run very low-latency queries to power interactive dashboards and high-throughput batch analytics to drive business decisions. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can run Amazon Redshift queries directly on the Amazon Redshift console or submit them using the JDBC/ODBC endpoints provided by Amazon Redshift.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#business-intelligence"&gt;
  &lt;/a&gt;
  Business intelligence
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Amazon QuickSight provides a serverless BI capability to easily create and publish rich, interactive dashboards. QuickSight enriches dashboards and visuals with out-of-the- box, automatically generated ML insights such as forecasting, anomaly detection, and narrative highlights. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;QuickSight natively integrates with Amazon SageMaker to enable additional custom ML model-based insights to your BI dashboards. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#predictive-analytics-and-ml"&gt;
  &lt;/a&gt;
  Predictive analytics and ML
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Amazon SageMaker is a fully managed service that provides components to build, train, and deploy ML models using an interactive development environment (IDE) called Amazon SageMaker Studio. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In Amazon SageMaker Studio, you can upload data, create new notebooks, train and tune models, move back and forth between steps to adjust experiments, compare results, and deploy models to production, all in one place by using a unified visual interface. Amazon SageMaker also provides managed Jupyter notebooks that you can spin up with just a few clicks.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;
  &lt;a href="#security-and-governance-layer"&gt;
  &lt;/a&gt;
  Security and governance layer
&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Components across all layers of the presented architecture protect data, identities, and processing resources by natively using the following capabilities provided by the security and governance layer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#authentication-and-authorization"&gt;
  &lt;/a&gt;
  Authentication and authorization
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;AWS Identity and Access Management (IAM) provides user-, group-, and role-level identity to users and the ability to configure fine-grained access control for resources managed by AWS services in all layers of our architecture. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IAM supports multi-factor authentication and single sign-on through integrations with corporate directories and open identity providers such as Google, Facebook, and Amazon.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#encryption"&gt;
  &lt;/a&gt;
  Encryption
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;AWS KMS provides the capability to create and manage symmetric and asymmetric customer managed encryption keys. AWS services in all layers of our architecture natively integrate with AWS KMS to encrypt data in the data lake. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It supports both creating new keys and importing existing customer keys. Access to the encryption keys is controlled using IAM and is monitored through detailed audit trails in CloudTrail.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#network-protection"&gt;
  &lt;/a&gt;
  Network protection
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Our architecture uses Amazon Virtual Private Cloud (Amazon VPC) to provision a logically isolated section of the AWS Cloud (called VPC) that is isolated from the internet and other AWS customers. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Amazon VPC provides the ability to choose your own IP address range, create subnets, and configure route tables and network gateways. AWS services from other layers in our architecture launch resources in this private VPC to protect all traffic to and from these resources.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;
  &lt;a href="#monitoring-and-logging"&gt;
  &lt;/a&gt;
  Monitoring and logging
&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;AWS services in all layers of our architecture store detailed logs and monitoring metrics in Amazon CloudWatch. CloudWatch provides the ability to analyze logs, visualize monitored metrics, define monitoring thresholds, and send alerts when thresholds are crossed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All AWS services in our architecture also store extensive audit trails of user and service actions in CloudTrail. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This event history simplifies security analysis, resource change tracking, and troubleshooting. In addition, you can use CloudTrail to detect unusual activity in your AWS accounts. These capabilities help simplify operational analysis and troubleshooting.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;
  &lt;a href="#conclusion"&gt;
  &lt;/a&gt;
  Conclusion
&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;With AWS serverless and managed services, you can build a modern, low-cost data lake centric analytics architecture in days. A decoupled, component-driven architecture enables you to start small and quickly add new purpose-built components to one of six architecture layers to address new requirements and data sources.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;
  &lt;a href="#reference"&gt;
  &lt;/a&gt;
  Reference
&lt;/h1&gt;

&lt;p&gt;&lt;a href="https://d1.awsstatic.com/whitepapers/aws-serverless-data-analytics-pipeline.pdf?did=wp_card&amp;amp;trk=wp_card"&gt;Original paper&lt;/a&gt;&lt;/p&gt;

</description>
      <category>aws</category>
      <category>serverless</category>
      <category>datascience</category>
      <category>analytics</category>
    </item>
    <item>
      <title>Access private containers on Amazon ECS using PrivateLink and Terraform</title>
      <author>Lorenz Vanthillo</author>
      <pubDate>Sun, 21 Nov 2021 12:03:19 +0000</pubDate>
      <link>https://dev.to/lvthillo/access-private-containers-on-amazon-ecs-using-privatelink-and-terraform-2odf</link>
      <guid>https://dev.to/lvthillo/access-private-containers-on-amazon-ecs-using-privatelink-and-terraform-2odf</guid>
      <description>&lt;p&gt;Many companies are using container orchestration services (like ECS or EKS) to host their microservice environment. Those microservices can offer APIs which need to be accessible for other customers.&lt;br&gt;
If those customers are also using AWS then the best solution would be to keep all communication privately inside AWS. The services inside the VPC of the &lt;em&gt;customer&lt;/em&gt; should be able to communicate with the containers hosted in the VPC of the &lt;em&gt;provider&lt;/em&gt; which offers the APIs. In this demo I'll use ECS to host the API.&lt;/p&gt;

&lt;p&gt;I'll use one AWS account but two VPCs. One VPC for the API provider and one VPC for the customer (=  API consumer). In reallity they will both have a separate account but that does not make a big difference in this setup (see later). Another important remark is that I'll serve my API over &lt;code&gt;HTTP&lt;/code&gt; to keep things basic. It's recommended to use &lt;code&gt;HTTPS&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The API provider is hosting their containers in ECS using Fargate (serverless) and uses a private Application Load Balancer (ALB) in front of it. To allow a private connection between the VPC of the customer and the VPC of the API provider we need to setup AWS PrivateLink. AWS PrivateLink can provide a private connectivity between VPCs or AWS services. It's not possible to connect PrivateLink with an Application Load Balancer. It requires a Network Load Balancer (NLB). This means our architecture will look like this:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--UpdnJvCV--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ur5b05or5vfw22xupugt.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--UpdnJvCV--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ur5b05or5vfw22xupugt.png" alt="architecture" width="880" height="1173"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The sample API being used in this demo is a task API which can print all tasks and specific tasks based on &lt;code&gt;id&lt;/code&gt;.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;docker run &lt;span class="nt"&gt;-d&lt;/span&gt; &lt;span class="nt"&gt;-p&lt;/span&gt; 8080:8080 lvthillo/python-flask-api
&lt;span class="nv"&gt;$ &lt;/span&gt;curl localhost/api/tasks
&lt;span class="o"&gt;[{&lt;/span&gt;&lt;span class="s2"&gt;"id"&lt;/span&gt;: 1, &lt;span class="s2"&gt;"name"&lt;/span&gt;: &lt;span class="s2"&gt;"task1"&lt;/span&gt;, &lt;span class="s2"&gt;"description"&lt;/span&gt;: &lt;span class="s2"&gt;"This is task 1"&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;, &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"id"&lt;/span&gt;: 2, &lt;span class="s2"&gt;"name"&lt;/span&gt;: &lt;span class="s2"&gt;"task2"&lt;/span&gt;, &lt;span class="s2"&gt;"description"&lt;/span&gt;: &lt;span class="s2"&gt;"This is task 2"&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;, &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"id"&lt;/span&gt;: 3, &lt;span class="s2"&gt;"name"&lt;/span&gt;: &lt;span class="s2"&gt;"task3"&lt;/span&gt;, &lt;span class="s2"&gt;"description"&lt;/span&gt;: &lt;span class="s2"&gt;"This is task 3"&lt;/span&gt;&lt;span class="o"&gt;}]&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;curl localhost/api/task/2
&lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"id"&lt;/span&gt;: 2, &lt;span class="s2"&gt;"name"&lt;/span&gt;: &lt;span class="s2"&gt;"task2"&lt;/span&gt;, &lt;span class="s2"&gt;"description"&lt;/span&gt;: &lt;span class="s2"&gt;"This is task 2"&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;To set up the infrastructure I'm using Terraform. The code is available on &lt;a href="https://github.com/lvthillo/aws-ecs-privatelink"&gt;my GitHub&lt;/a&gt;. We need to create two VPCs. I've created a basic module which will create a VPC which contains four subnets (in two AZs) and deploy a NAT Gateway and an Internet Gateway. Also the route tables are configured properly.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight hcl"&gt;&lt;code&gt;&lt;span class="nx"&gt;module&lt;/span&gt; &lt;span class="s2"&gt;"network"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;source&lt;/span&gt;                 &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"./modules/network"&lt;/span&gt;
  &lt;span class="nx"&gt;vpc_name&lt;/span&gt;               &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"vpc-1"&lt;/span&gt;
  &lt;span class="nx"&gt;vpc_cidr&lt;/span&gt;               &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;var&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;vpc&lt;/span&gt;
  &lt;span class="nx"&gt;subnet_1a_public_cidr&lt;/span&gt;  &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;var&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;pub_sub_1a&lt;/span&gt;
  &lt;span class="nx"&gt;subnet_1b_public_cidr&lt;/span&gt;  &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;var&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;pub_sub_1b&lt;/span&gt;
  &lt;span class="nx"&gt;subnet_1a_private_cidr&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;var&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;priv_sub_1a&lt;/span&gt;
  &lt;span class="nx"&gt;subnet_1b_private_cidr&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;var&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;priv_sub_1b&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Next we need to deploy the ECS cluster, service and task definition inside the VPC of the API provider. The ALB will be deployed in front of it.&lt;br&gt;
The &lt;code&gt;ecs&lt;/code&gt; module will deploy two sample API containers.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight hcl"&gt;&lt;code&gt;&lt;span class="nx"&gt;module&lt;/span&gt; &lt;span class="s2"&gt;"ecs"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;source&lt;/span&gt;               &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"./modules/ecs"&lt;/span&gt;
  &lt;span class="nx"&gt;ecs_subnets&lt;/span&gt;          &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;network&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;private_subnets&lt;/span&gt;
  &lt;span class="nx"&gt;ecs_container_name&lt;/span&gt;   &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"demo"&lt;/span&gt;
  &lt;span class="nx"&gt;ecs_port&lt;/span&gt;             &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;var&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ecs_port&lt;/span&gt; &lt;span class="c1"&gt;# we use networkMode awsvpc so host and container ports should match&lt;/span&gt;
  &lt;span class="nx"&gt;ecs_task_def_name&lt;/span&gt;    &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"demo-task"&lt;/span&gt;
  &lt;span class="nx"&gt;ecs_docker_image&lt;/span&gt;     &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"lvthillo/python-flask-api"&lt;/span&gt;
  &lt;span class="nx"&gt;vpc_id&lt;/span&gt;               &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;network&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;vpc_id&lt;/span&gt;
  &lt;span class="nx"&gt;alb_sg&lt;/span&gt;               &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;alb&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;alb_sg&lt;/span&gt;
  &lt;span class="nx"&gt;alb_target_group_arn&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;alb&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;alb_tg_arn&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nx"&gt;module&lt;/span&gt; &lt;span class="s2"&gt;"alb"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;source&lt;/span&gt;         &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"./modules/alb"&lt;/span&gt;
  &lt;span class="nx"&gt;alb_subnets&lt;/span&gt;    &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;network&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;private_subnets&lt;/span&gt;
  &lt;span class="nx"&gt;vpc_id&lt;/span&gt;         &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;network&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;vpc_id&lt;/span&gt;
  &lt;span class="nx"&gt;ecs_sg&lt;/span&gt;         &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ecs&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ecs_sg&lt;/span&gt;
  &lt;span class="nx"&gt;alb_port&lt;/span&gt;       &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;var&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;alb_port&lt;/span&gt;
  &lt;span class="nx"&gt;ecs_port&lt;/span&gt;       &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;var&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ecs_port&lt;/span&gt;
  &lt;span class="nx"&gt;default_vpc_sg&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;network&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;default_vpc_sg&lt;/span&gt;
  &lt;span class="nx"&gt;vpc_cidr&lt;/span&gt;       &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;var&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;vpc&lt;/span&gt; &lt;span class="c1"&gt;# Allow inbound client traffic via AWS PrivateLink on the load balancer listener port&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;The security group of the ECS service only allows connections from the ALB.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight hcl"&gt;&lt;code&gt;&lt;span class="nx"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"aws_security_group_rule"&lt;/span&gt; &lt;span class="s2"&gt;"ingress"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;type&lt;/span&gt;                     &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"ingress"&lt;/span&gt;
  &lt;span class="nx"&gt;description&lt;/span&gt;              &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"Allow ALB to ECS"&lt;/span&gt;
  &lt;span class="nx"&gt;from_port&lt;/span&gt;                &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;var&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ecs_port&lt;/span&gt;
  &lt;span class="nx"&gt;to_port&lt;/span&gt;                  &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;var&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ecs_port&lt;/span&gt;
  &lt;span class="nx"&gt;protocol&lt;/span&gt;                 &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"tcp"&lt;/span&gt;
  &lt;span class="nx"&gt;security_group_id&lt;/span&gt;        &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;aws_security_group&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ecs_sg&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;id&lt;/span&gt;
  &lt;span class="nx"&gt;source_security_group_id&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;var&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;alb_sg&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;An NLB has no security group so we can not reference it in the security group of the ALB. We need to find another way to restrict ALB access.&lt;br&gt;
The &lt;code&gt;alb&lt;/code&gt; module uses the VPC subnet - in which we will create our NLB - as source in the ALB security group. This means everything inside the API provider its VPC can communicate with the ALB.&lt;br&gt;
In a production environment it can be useful to deploy the NLB in separate NLB subnets and restrict to those subnet CIDR ranges.&lt;br&gt;
Now let's deploy this private NLB in front of our private ALB.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight hcl"&gt;&lt;code&gt;&lt;span class="nx"&gt;module&lt;/span&gt; &lt;span class="s2"&gt;"nlb"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;source&lt;/span&gt;            &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"./modules/nlb"&lt;/span&gt;
  &lt;span class="nx"&gt;nlb_subnets&lt;/span&gt;       &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;network&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;private_subnets&lt;/span&gt;
  &lt;span class="nx"&gt;vpc_id&lt;/span&gt;            &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;network&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;vpc_id&lt;/span&gt;
  &lt;span class="nx"&gt;alb_arn&lt;/span&gt;           &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;alb&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;alb_arn&lt;/span&gt;
  &lt;span class="nx"&gt;nlb_port&lt;/span&gt;          &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;var&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;nlb_port&lt;/span&gt;
  &lt;span class="nx"&gt;alb_listener_port&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;alb&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;alb_listener_port&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Here I'm using some new AWS magic. &lt;a href="https://aws.amazon.com/about-aws/whats-new/2021/09/application-load-balancer-aws-privatelink-static-ip-addresses-network-load-balancer/"&gt;Since September 2021 it's possible to register an ALB as target of an NLB&lt;/a&gt;.&lt;br&gt;
This was not possible in the past. I created some &lt;a href="https://github.com/lvthillo/terraform-nlb-alb-connector"&gt;Terraform module&lt;/a&gt; which deployed a Lambda to monitor the IPs of the ALB and updated the NLB target group when needed. I'm very happy that it's not needed anymore!&lt;br&gt;
Now we can define &lt;code&gt;alb&lt;/code&gt; as &lt;code&gt;target_type&lt;/code&gt; for our NLB target group.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight hcl"&gt;&lt;code&gt;&lt;span class="nx"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"aws_lb_target_group"&lt;/span&gt; &lt;span class="s2"&gt;"nlb_tg"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;name&lt;/span&gt;        &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;var&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;nlb_tg_name&lt;/span&gt;
  &lt;span class="nx"&gt;port&lt;/span&gt;        &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;var&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;alb_listener_port&lt;/span&gt;
  &lt;span class="nx"&gt;protocol&lt;/span&gt;    &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"TCP"&lt;/span&gt;
  &lt;span class="nx"&gt;target_type&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"alb"&lt;/span&gt;
  &lt;span class="nx"&gt;vpc_id&lt;/span&gt;      &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;var&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;vpc_id&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Next we can deploy our AWS PrivateLink-powered service (referred to as an endpoint service). In a real production environment you'll have to add &lt;code&gt;allowed_principals&lt;/code&gt; to whitelist the account ID from the customer.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight hcl"&gt;&lt;code&gt;&lt;span class="nx"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"aws_vpc_endpoint_service"&lt;/span&gt; &lt;span class="s2"&gt;"privatelink"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;acceptance_required&lt;/span&gt;        &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;
  &lt;span class="nx"&gt;network_load_balancer_arns&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;nlb&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;nlb_arn&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="c1"&gt;#checkov:skip=CKV_AWS_123:For this demo I don't need to configure the VPC Endpoint Service for Manual Acceptance&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Everything is ready from API provider perspective. We have deployed the VPC, ECS infrastructure, ALB, NLB and PrivateLink.&lt;br&gt;
The customer its VPC can be deployed using the same &lt;code&gt;network&lt;/code&gt; module. The customer needs a VPC endpoint to connect to PrivateLink.&lt;br&gt;
The VPC endpoint Fully Qualified Domain Name (FQDN) will point to the AWS PrivateLink endpoint FQDN.&lt;br&gt;
This creates an elastic network interface to the VPC endpoint service that the DNS endpoints can access.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight hcl"&gt;&lt;code&gt;&lt;span class="nx"&gt;resource&lt;/span&gt; &lt;span class="s2"&gt;"aws_vpc_endpoint"&lt;/span&gt; &lt;span class="s2"&gt;"vpce"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;vpc_id&lt;/span&gt;             &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;network_added&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;vpc_id&lt;/span&gt;
  &lt;span class="nx"&gt;service_name&lt;/span&gt;       &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;aws_vpc_endpoint_service&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;privatelink&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;service_name&lt;/span&gt;
  &lt;span class="nx"&gt;vpc_endpoint_type&lt;/span&gt;  &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"Interface"&lt;/span&gt;
  &lt;span class="nx"&gt;security_group_ids&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;aws_security_group&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;vpce_sg&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;id&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="nx"&gt;subnet_ids&lt;/span&gt;         &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;network_added&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;public_subnets&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;At last I'm deploying an EC2 in the customer VPC (public subnet) to test the integration. Here for I'm using &lt;a href="https://registry.terraform.io/modules/terraform-aws-modules/ec2-instance/aws/latest"&gt;this&lt;/a&gt; Terraform module.&lt;br&gt;
Update the &lt;code&gt;key&lt;/code&gt;variable in &lt;code&gt;variables.tf&lt;/code&gt; with the name of your existing AWS SSH key.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight hcl"&gt;&lt;code&gt;&lt;span class="nx"&gt;module&lt;/span&gt; &lt;span class="s2"&gt;"ec2_instance_added"&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;source&lt;/span&gt;                      &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"terraform-aws-modules/ec2-instance/aws"&lt;/span&gt;
  &lt;span class="nx"&gt;version&lt;/span&gt;                     &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"~&amp;gt; 3.0"&lt;/span&gt;
  &lt;span class="nx"&gt;name&lt;/span&gt;                        &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"test-instance-vpc-2"&lt;/span&gt;
  &lt;span class="nx"&gt;associate_public_ip_address&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;
  &lt;span class="nx"&gt;ami&lt;/span&gt;                         &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"ami-05cd35b907b4ffe77"&lt;/span&gt; &lt;span class="c1"&gt;# eu-west-1 specific&lt;/span&gt;
  &lt;span class="nx"&gt;instance_type&lt;/span&gt;               &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"t2.micro"&lt;/span&gt;
  &lt;span class="nx"&gt;key_name&lt;/span&gt;                    &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;var&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;key&lt;/span&gt;
  &lt;span class="nx"&gt;vpc_security_group_ids&lt;/span&gt;      &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;aws_security_group&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ssh_sg_added&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;id&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="nx"&gt;subnet_id&lt;/span&gt;                   &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;element&lt;/span&gt;&lt;span class="err"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;network_added&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;public_subnets&lt;/span&gt;&lt;span class="err"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="err"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;Deploy the Terraform stack to test the setup.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;git clone https://github.com/lvthillo/aws-ecs-privatelink.git
&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;cd &lt;/span&gt;aws-ecs-privatelink
&lt;span class="nv"&gt;$ &lt;/span&gt;terraform init
&lt;span class="nv"&gt;$ &lt;/span&gt;terraform apply
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--VV-U2yKX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/eqa03qx04x2ir6o59u3r.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--VV-U2yKX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/eqa03qx04x2ir6o59u3r.png" alt="Port configuration" width="880" height="250"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Check the Terraform outputs and SSH to the EC2 instance deployed in the customer VPC. Then try to &lt;code&gt;curl&lt;/code&gt;the VPC endpoint.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--Ims2c5NE--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/pzfx5gjd0u2c44iv9ohe.png" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--Ims2c5NE--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/pzfx5gjd0u2c44iv9ohe.png" alt="Terraform apply" width="880" height="123"&gt;&lt;/a&gt;&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;ssh &lt;span class="nt"&gt;-i&lt;/span&gt; your-key.pem ec2-user@54.247.23.167
&lt;span class="nv"&gt;$ &lt;/span&gt;curl http://vpce-07715651ecce291f6-x6r0avfj.vpce-svc-0beea6830e4c68cd2.eu-west-1.vpce.amazonaws.com/api/tasks
&lt;span class="o"&gt;[{&lt;/span&gt;&lt;span class="s2"&gt;"id"&lt;/span&gt;: 1, &lt;span class="s2"&gt;"name"&lt;/span&gt;: &lt;span class="s2"&gt;"task1"&lt;/span&gt;, &lt;span class="s2"&gt;"description"&lt;/span&gt;: &lt;span class="s2"&gt;"This is task 1"&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;, &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"id"&lt;/span&gt;: 2, &lt;span class="s2"&gt;"name"&lt;/span&gt;: &lt;span class="s2"&gt;"task2"&lt;/span&gt;, &lt;span class="s2"&gt;"description"&lt;/span&gt;: &lt;span class="s2"&gt;"This is task 2"&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;, &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"id"&lt;/span&gt;: 3, &lt;span class="s2"&gt;"name"&lt;/span&gt;: &lt;span class="s2"&gt;"task3"&lt;/span&gt;, &lt;span class="s2"&gt;"description"&lt;/span&gt;: &lt;span class="s2"&gt;"This is task 3"&lt;/span&gt;&lt;span class="o"&gt;}]&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;curl http://vpce-07715651ecce291f6-x6r0avfj.vpce-svc-0beea6830e4c68cd2.eu-west-1.vpce.amazonaws.com/api/task/2
&lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"id"&lt;/span&gt;: 2, &lt;span class="s2"&gt;"name"&lt;/span&gt;: &lt;span class="s2"&gt;"task2"&lt;/span&gt;, &lt;span class="s2"&gt;"description"&lt;/span&gt;: &lt;span class="s2"&gt;"This is task 2"&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;That was it! Our customer is able to access our API hosted in ECS using PrivateLink. All communication remains inside AWS.&lt;br&gt;
There are some important caveats to remember which are described at the bottom of &lt;a href="https://aws.amazon.com/blogs/networking-and-content-delivery/application-load-balancer-type-target-group-for-network-load-balancer/"&gt;this post&lt;/a&gt;. Don't forget to destroy this demo stack using &lt;code&gt;terraform destroy&lt;/code&gt;!&lt;/p&gt;

&lt;p&gt;I hope you enjoyed it!&lt;/p&gt;

</description>
    </item>
  </channel>
</rss>
