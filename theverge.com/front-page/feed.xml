<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <title>The Verge -  Front Pages</title>
  <icon>https://cdn.vox-cdn.com/community_logos/52801/VER_Logomark_32x32..png</icon>
  <updated>2021-09-29T13:42:17-04:00</updated>
  <id>https://www.theverge.com/rss/front-page/index.xml</id>
  <link type="text/html" href="https://www.theverge.com/" rel="alternate"/>
  <entry>
    <published>2021-09-29T13:42:17-04:00</published>
    <updated>2021-09-29T13:42:17-04:00</updated>
    <title>Google search’s next phase: context is king</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/xHowirLRwCuklvCPudSGDvMWE8w=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69926828/acastro_180427_1777_0003.6.jpg" /&gt;
        &lt;figcaption&gt;Illustration: Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;


  &lt;p&gt;Google says search is still far from solved&lt;/p&gt; &lt;p class="p--has-dropcap" id="R930gy"&gt;At its Search On event today, Google introduced several new features that, taken together, are its strongest attempts yet to get people to do more than type a few words into a search box. By leveraging its new Multitask Unified Model (MUM) machine learning technology in small ways, the company hopes to kick off a virtuous cycle: it will provide more detail and context-rich answers, and in return it hopes users will ask more detailed and context-rich questions. The end result, the company hopes, will be a richer and deeper search experience.&lt;/p&gt;
&lt;p id="lyLN12"&gt;Google SVP Prabhakar Raghavan oversees search alongside Assistant, ads, and other products. He likes to say — and repeated in an interview this past Sunday — that “search is not a solved problem.” That may be true, but the problems he and his team are trying to solve now have less to do with wrangling the web and more to do with adding context to what they find there. &lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="dzvXJE"&gt;&lt;q&gt;AI will help Google explore the questions people are asking&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="75lUXB"&gt;For its part, Google is going to begin flexing its ability to recognize constellations of related topics using machine learning and present them to you in an organized way. A coming redesign to Google search will begin showing “Things to know” boxes that send you off to different subtopics. When there’s a section of a video that’s relevant to the general topic — even when the video as a whole is not — it will send you there. Shopping results will begin to show inventory available in nearby stores, and even clothing in different styles associated with your search.&lt;/p&gt;
&lt;p id="gyV1eY"&gt;For your part, Google is offering — though perhaps “asking” is a better term — new ways to search that go beyond the text box. It’s making an aggressive push to get its image recognition software Google Lens into more places. It will be built into the Google app on iOS and also the Chrome web browser on desktops. And with MUM, Google is hoping to get users to do more than just identify flowers or landmarks, but instead use Lens directly to ask questions and shop.&lt;/p&gt;
&lt;p id="orhHOq"&gt;“It’s a cycle that I think will keep escalating,” Raghavan says. “More technology leads to more user affordance, leads to better expressivity for the user, and will demand more of us, technically.” &lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/BMVe7nxgCDRZMqzn0z4aOkpyvww=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22884248/google_lens_shirt_sock_search_wide_slow.gif"&gt;
      &lt;cite&gt;Image: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Google Lens will let users search using images and refine their query with text. &lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="312ACt"&gt;Those two sides of the search equation are meant to kick off the next stage of Google search, one where its machine learning algorithms become more prominent in the process by organizing and presenting information directly. In this, Google efforts will be helped hugely by recent advances in AI language processing. Thanks to systems known as large language models (MUM is one of these), machine learning has got much better at mapping the connections between words and topics. It’s these skills that the company is leveraging to make search not just more accurate, but more explorative and, it hopes, more helpful.&lt;/p&gt;
&lt;p id="QcnS2Q"&gt;One of Google’s examples is instructive. You may not have the first idea what the parts of your bicycle are called, but if something is broken you’ll need to figure that out. Google Lens can visually identify the derailleur (the gear-changing part hanging near the rear wheel) and rather than just give you the discrete piece of information, it will allow you to ask questions about fixing that thing directly, taking you to the information (in this case, the excellent &lt;a href="https://www.youtube.com/watch?v=BTYPHGWDE6E"&gt;Berm Peak Youtube&lt;/a&gt; channel).&lt;/p&gt;
&lt;div class="c-float-left"&gt;&lt;aside id="qFXrbc"&gt;&lt;q&gt;Multimodal search requires entirely new input from users&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="MLe91V"&gt;The push to get more users to open up Google Lens more often is fascinating on its own merits, but the bigger picture (so to speak) is about Google’s attempt to gather more context about your queries. More complicated, multimodal searches combining text and images demand “an entirely different level of contextualization that we the provider have to have, and so it helps us tremendously to have as much context as we can,” Raghavan says. &lt;/p&gt;
&lt;p id="nkHj6l"&gt;We are very far from the so-called “ten blue links” of search results that Google provides. It has been showing information boxes, image results, and direct answers for a long time now. Today’s announcements are another step, one where the information Google provides is not just a ranking of relevant information but a distillation of what its machines understand by scraping the web. &lt;/p&gt;
&lt;p id="ZderHj"&gt;In some cases — as with shopping — that distillation means you’ll likely be sending Google more page views. As with Lens, that trend is important to keep an eye on: Google searches increasingly push you to Google’s own products. But there’s a bigger danger here, too. The fact that Google is telling you more things directly increases a burden it’s always had: to speak with less bias. &lt;/p&gt;
&lt;p id="NCCmTM"&gt;By that, I mean bias in two different senses. The first is technical: the machine learning models that Google wants to use to improve search have well-documented problems with racial and gender biases. They’re trained by reading large swaths of the web, and, as a result, tend to pick up nasty ways of talking. Google’s troubles with its AI ethics team are also well documented at this point — it &lt;a href="https://www.theverge.com/2021/2/19/22292011/google-second-ethical-ai-researcher-fired"&gt;fired two lead researchers&lt;/a&gt; after they published a paper on this very subject. As Google’s VP of search, Pandu Nayak, told &lt;em&gt;The Verge&lt;/em&gt;’s &lt;a href="https://www.theverge.com/e/22460309"&gt;James Vincent in his article on today’s MUM announcements&lt;/a&gt;, Google knows that all language models have biases, but the company believes it can avoid “putting it out for people to consume directly.”&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/Vo4XOpVcCPykH4Q7FUe55J6_Hgg=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22885162/Untitled__2_.gif"&gt;
      &lt;cite&gt;Image: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;A new feature called “Things to know” will help users explore topics related to their searches. &lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="Fhuquw"&gt;Be that as it may (and to be clear, it may not be), it sidesteps another consequential question and another type of bias. As Google begins telling you more of its own syntheses of information directly, what is the point of view from which it’s speaking? As journalists, we often talk about how the so-called “view from nowhere” is an inadequate way to present our reporting. What is Google’s point of view? This is an issue the company has confronted in the past, sometimes known as the “one true answer” problem. When Google tries to give people short, definitive answers using automated systems, it often ends up &lt;a href="https://searchengineland.com/googles-one-true-answer-problem-featured-snippets-270549"&gt;spreading bad information&lt;/a&gt;. &lt;/p&gt;
&lt;p id="3a0rVn"&gt;Presented with that question, Raghavan responds by pointing to the complexity of modern language models. “Almost all language models, if you look at them, are embeddings in a high dimension space. There are certain parts of these spaces that tend to be more authoritative, certain portions that are less authoritative. We can mechanically assess those things pretty easily,” he explains. Raghavan says the challenge is then how to present some of that complexity to the user without overwhelming them. &lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="bqROVQ"&gt;&lt;q&gt;Can Google remain neutral if it’s delivering answers to users directly?&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="BvW70n"&gt;But I get the sense that the real answer is that, for now at least, Google is doing what it can to avoid facing the question of its search engine’s point of view by avoiding the domains where it could be accused of, as Raghavan puts it, “excessive editorializing.” Often when speaking to Google executives about these problems of bias and trust, they focus on easier-to-define parts of those high-dimension spaces like “authoritativeness.” &lt;/p&gt;
&lt;p id="ps3UWY"&gt;For example, Google’s new “Things to know” boxes won’t appear when somebody searches for things Google has identified as “particularly harmful/sensitive,” though a spokesperson says that Google is not “allowing or disallowing specific curated categories, but our systems are able to scalably understand topics for which these types of features should or should not trigger.”&lt;/p&gt;
&lt;p id="ocHpkr"&gt;Google search, its inputs, outputs, algorithms, and language models have all become almost unimaginably complex. When Google tells us that it is able to understand the contents of videos now, we take for granted that it has the computing chops to pull that off — but the reality is that even just indexing such a massive corpus is a monumental task that dwarfs the original mission of indexing the early web. (Google is only indexing audio transcripts of a subset of YouTube, for the record, though with MUM it aims to do visual indexing and other video platforms in the future). &lt;/p&gt;
&lt;p id="EHV15b"&gt;Often when you’re speaking to computer scientists, the &lt;a href="https://en.wikipedia.org/wiki/Travelling_salesman_problem"&gt;traveling salesman problem&lt;/a&gt; will come up. It’s a famous conundrum where you attempt to calculate the shortest possible route between a given number of cities, but it’s also a rich metaphor for thinking through how computers do their machinations. &lt;/p&gt;
&lt;p id="1SWuVC"&gt;“If you gave me all the machines in the world, I could solve fairly big instances,” Raghavan says. But for search, he says that it is unsolved and perhaps unsolvable by just throwing more computers at it. Instead, Google needs to come up with new approaches, like MUM, that take better advantage of the resources Google can realistically create. “If you gave me all the machines there were, I’m still bounded by human curiosity and cognition.”&lt;/p&gt;
&lt;p class="c-end-para" id="6UQaHl"&gt;Google’s new ways of understanding information are impressive, but the challenge is what it will do with the information and how it will present it. The funny thing about the traveling salesman problem is that nobody seems to stop and ask what exactly is in the case, what is he showing all his customers as he goes door to door?&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22698504/google-search-on-event-ai-mum-google-lens-update-changes"/>
    <id>https://www.theverge.com/2021/9/29/22698504/google-search-on-event-ai-mum-google-lens-update-changes</id>
    <author>
      <name>Dieter Bohn</name>
    </author>
  </entry>
  <entry>
    <published>2021-09-29T13:40:00-04:00</published>
    <updated>2021-09-29T13:40:00-04:00</updated>
    <title>Google Maps is making it easier to see wildfires and tree coverage</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="Google’s Tree Canopy Lab" src="https://cdn.vox-cdn.com/thumbor/kvH5CmxgHi-PBZ_NxZA9Jo617yQ=/79x0:922x562/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69926815/Tree_Canopy_Insights.0.png" /&gt;
        &lt;figcaption&gt;Google’s Tree Canopy Lab | Image: Google&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="VwAh1r"&gt;Google Maps has new features that should make it easier for users to see wildfires, tree canopy, and locations without formal addresses. It’s all aimed at helping communities be “safer, more sustainable, and discoverable,” according to the company.&lt;/p&gt;
&lt;p id="MJRZkH"&gt;A new wildfire layer on Maps will begin rolling out globally this week, Google announced today. It’ll show most major fires, those prompting evacuations, across the world. Red splotches and pins on the layer will indicate where blazes are and how far they’ve spread. By tapping on any single wildfire, users can see more information, like how many acres have burned, what percentage of the fire has been contained, and links to local emergency resources. &lt;/p&gt;
&lt;p id="09agYi"&gt;The map is updated about every hour. It uses data that’s gathered by satellites that are then processed by Google’s geospatial analysis platform, Earth Engine.  &lt;/p&gt;
&lt;p id="NQEPmX"&gt;It builds on a feature &lt;a href="https://www.theverge.com/2020/8/20/21376343/google-maps-search-wildfire-boundaries-california"&gt;Google piloted in California&lt;/a&gt; in 2019 and expanded to the rest of the US in 2020. That pilot allowed people in the US to see the shifting boundaries of a nearby wildfire in near real-time but didn’t allow for users to view more than one fire at a time. That was limiting since multiple blazes can break out simultaneously within the same region during fire season, which is growing longer and more intense in the Western US as a result of climate change. &lt;/p&gt;
&lt;div class="c-float-left"&gt;  &lt;figure class="e-image"&gt;
        &lt;img alt="Google rolled out a new wildfire layer on Maps." data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/rAGjPu9Aqc61IuxBXUN1QgulZBo=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22885246/wildfire_layer_2.gif"&gt;
      &lt;cite&gt;GIF: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Google plans to roll out a new wildfire layer on Maps.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p id="ReVAIE"&gt;Not only will users in the US now have a bird’s-eye view of major fires nearby; they’ll also even be able to see small fires on the map — a feature not yet available in other parts of the world. Australia and some other locations could have that capability “in the coming months,” Google says. &lt;/p&gt;
&lt;p id="pAGnJ2"&gt;There’s still one limitation, however, that Google has yet to hammer out with the wildfire layer. It can only be updated when the user is connected to the internet, which poses problems when wildfires &lt;a href="https://www.sfchronicle.com/california-wildfires/article/Wildfire-threat-Bay-Area-cell-phone-internet-15495592.php"&gt;take out power lines and cell towers&lt;/a&gt;.&lt;/p&gt;
&lt;p id="FZlghR"&gt;The other two updates Google announced are primarily designed to give local governments better planning and mapping tools. Google &lt;a href="https://www.theverge.com/2020/11/18/21573081/google-new-tool-hot-cities-trees-climate-change-temperature"&gt;launched a Tree Canopy Lab&lt;/a&gt; last year in Los Angeles to help the city identify spots in need of more trees. Getting more greenery to those neighborhoods is one way to prevent heat-related illnesses and death in places that are the most vulnerable during heatwaves. Temperatures can actually differ &lt;a href="https://www.theverge.com/2021/7/14/22575481/nyc-urban-heat-island-effect-thermal-camera-surface-temperature"&gt;from neighborhood to neighborhood&lt;/a&gt; based on how much greenery they have, a trend that is &lt;a href="https://www.theverge.com/2021/5/25/22452777/hot-summer-weather-race-urban-heat-island-us-cities"&gt;made worse by a history of racial segregation&lt;/a&gt; in the US. &lt;/p&gt;
&lt;p id="hZNWro"&gt;Google says Tree Canopy will be available in over 100 more cities in 2022, including Guadalajara, London, Sydney, and Toronto. It charts out the density of tree cover using images taken by plane for Google Maps.&lt;/p&gt;
&lt;div class="c-float-right"&gt;  &lt;figure class="e-image"&gt;
        &lt;img alt="Google’s new Address Maker app makes it easier for users to chart routes to locations without a formal address." data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/xp94RbXGKXvm48CyM4MoxomfvVo=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22885339/Address_Maker__1_.gif"&gt;
      &lt;cite&gt;GIF: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Google’s new Address Maker app makes it easier for users to chart routes to locations without a formal address.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p id="pSXyTR"&gt;Using Google Maps has always been more challenging for people who live in places without formal addresses. That’s made it more difficult for packages and emergency services like ambulances to reach many people living in rural areas and developing nations where addresses aren’t commonplace. So Google today launched a new app called Address Maker to make it easier for governments and NGOs to build out their own maps to fill in the blanks. &lt;/p&gt;
&lt;p id="XoJE2V"&gt;Address Maker is an expansion to Google’s existing &lt;a href="https://www.theverge.com/2020/5/28/21272954/google-maps-plus-code-geolocation-digital-address-six-digit"&gt;Plus Codes&lt;/a&gt;, which allow people to share their location using a six-digit code in lieu of an address. (There are also Plus Codes for places with traditional addresses. The Plus Code for Google’s headquarters, for example, is ‘CWC8+R9 Mountain View, California.’)&lt;/p&gt;
&lt;p id="bkReJE"&gt;While a Plus Code for places without addresses will show up on Google Maps, a route to reach it might not. Address Maker aims to make it faster and easier for officials and local advocates to assign Plus Codes to locations and chart previously unmapped roads for an entire community. It uses an open-source algorithm, and Plus Codes are accessible offline.&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22698533/google-maps-wildfires-tree-coverage-tracker"/>
    <id>https://www.theverge.com/2021/9/29/22698533/google-maps-wildfires-tree-coverage-tracker</id>
    <author>
      <name>Justine Calma</name>
    </author>
  </entry>
  <entry>
    <published>2021-09-29T13:31:15-04:00</published>
    <updated>2021-09-29T13:31:15-04:00</updated>
    <title>Google’s Search On fall 2021 event: news and announcements</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/nYDhE-z3bLPg6F5xmQxa1KlxSZw=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69926764/acastro_180427_1777_0001.5.jpg" /&gt;
        &lt;figcaption&gt;Illustration by Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p&gt;All the new features coming to the search giant’s core product&lt;/p&gt; &lt;p id="kXiPpK"&gt;Google is announcing several new updates and features to its core search functions for its &lt;a href="https://searchon.withgoogle.com/"&gt;Search On fall 2021 event&lt;/a&gt;. The new changes to search utilize AI through Google Lens visual searches in new ways; allow for easier window shopping online through local stores, checking what’s in stock at a nearby shop right from a product search; and introduce improvements to tracking wildfires through Google Maps, among other things.&lt;/p&gt;
&lt;p id="RtLxTM"&gt;We have all the info on the latest features to come to Google search and will continue to monitor how these new updates take shape. Stay tuned to check out all the coverage of Search On 2021 and Google’s new announcements.&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22699191/google-search-on-fall-2021-news-announcements"/>
    <id>https://www.theverge.com/2021/9/29/22699191/google-search-on-fall-2021-news-announcements</id>
    <author>
      <name>Dieter Bohn</name>
      <name>Justine Calma</name>
      <name>Antonio G. Di Benedetto</name>
      <name>James Vincent</name>
    </author>
  </entry>
  <entry>
    <published>2021-09-29T13:30:11-04:00</published>
    <updated>2021-09-29T13:30:11-04:00</updated>
    <title>Google expands shopping searches with Lens and in-store inventory checks</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/EWprwDNQAGM1IAQg-57T1fcZMp4=/141x0:1659x1012/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69926747/Search_On_Lens_Desktop_screenshot.0.jpg" /&gt;
        &lt;figcaption&gt;Image: Google&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="woIlY1"&gt;Shopping online isn’t always a convenience. If you enjoy window shopping or browsing curated collections at a brick-and-mortar store for inspiration, finding something online you don’t yet know you want or are unaware of is tricky if you start with a text search. Google is announcing new shopping search tools to try to alleviate this, with features that utilize Google Lens for finding products to buy from pictures online, broader search terms to help you browse clothing, and the ability to check in-store inventory from home. It claims the new tools will help shoppers “find what they’re looking for in a more visual way.” This comes after &lt;a href="https://www.theverge.com/2020/4/21/21228741/google-shopping-free-listing-ads-search-coronavirus-covid"&gt;Google allowed all businesses to create listings on Google Shopping for free&lt;/a&gt; last year. Now, it wants more window shopping to be done right from Google search.&lt;/p&gt;
&lt;p id="vXNJqK"&gt;Google Lens has been around since 2017, &lt;a href="https://www.theverge.com/2018/8/17/17715048/google-goggles-app-update-lens-ai"&gt;replacing Google Goggles&lt;/a&gt; that came before it, with &lt;a href="https://www.theverge.com/2018/5/8/17333154/google-lens-ai-ar-live-demo-hands-on-io-2018"&gt;the ability to use a smartphone camera to conduct visual searches&lt;/a&gt; based on the identification of objects found in the real world. Those image searches have allowed users to learn more about the things around them, even finding the same or similar item to buy without looking for a label or barcode to scan. Now, Google wants to make it possible to shop for any product you see in an image or video on the web with nothing more than the picture itself. Soon, iOS users will have a new dedicated button in the Google app, allowing a Google Lens search of any image on a page to bring up Google Shopping listings for purchase through a visual match. The feature will also be coming to Chrome on the desktop. &lt;/p&gt;
&lt;p id="HJBNhK"&gt;Google did not give specific dates for this feature launching on iOS or desktop, stating that it hopes to roll them out by the end of the year. There was no initial mention of if or when this feature may come to Android, but Google has clarified that it plans to extend this functionality to Android at a later date, after the iOS and desktop versions.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/jV9ZIZu0xgGoDD9LE6cp4IyvlGg=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22885615/Search_On_Lens_Mode.gif"&gt;
      &lt;cite&gt;GIF: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Google Lens search in the Google app on iOS.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/6IMBomtnwiJfFffKLOs5F8VPJ70=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22885621/Search_On_Lens_Desktop.gif"&gt;
      &lt;cite&gt;GIF: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Desktop Google Lens search on Chrome.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="x8M8Tu"&gt;Another shopping-focused feature coming from Google, which has surely been spurred on by the boom in e-commerce since the beginning of the pandemic, is easier browsing of clothing, accessories, and shoes via search results based on general terms. Google says that if you search for a generic article on mobile, for example, “cropped jackets,” you will see a visual feed of that type of clothing in a variety of colors and designs. These visual results will be accompanied by relevant videos, style guides, or local shops that carry those styles. From there, you can filter your search further according to brand, style, or department; check ratings and reviews; or compare prices on the results that appeal to you most.&lt;/p&gt;
&lt;p id="3Gtmdk"&gt;Google calls this window shopping, which is one of the challenges of shopping for clothes online compared to going to a physical store to see what’s on display. It claims the dataset is pulled from over 24 billion product listings. The new feature is available only on mobile and is usable right from a Google search beginning today.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/9pSNZFTXk45QIDYEBu3b3oCDkK8=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22885624/Shoppable_Search.gif"&gt;
      &lt;cite&gt;GIF: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Shoppable search within the Google app.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="MK3OxF"&gt;The third and final Google Shopping update allows users to remotely check in-store inventory directly within a Google search. Shoppers searching for a product are able to filter by “in stock.” This selection should show nearby stores that have the item available. Google claims the new feature can help a small business attract new customers, though it remains to be seen how accurate it might be across a variety of retailers and how one might ensure a product is there for them once they arrive — particularly at small businesses that do not have curbside or in-store pickup. &lt;/p&gt;
&lt;p id="VDIDtP"&gt;Google has indicated that it does rely on data from the retailer to determine stock status and claims it will only indicate an item is in stock when there is high confidence; otherwise, it may show limited stock.&lt;/p&gt;
&lt;p id="NAnntE"&gt;The new “in stock” filter is available today across mobile web browsers and the Google app on both iOS and Android.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/J0k_JYSB5tKG7oj6AYQxQV4YdL0=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22883129/SWIS_in_stock_filter.gif"&gt;
      &lt;cite&gt;GIF: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Google’s in-stock filtering when shopping nearby locations for an item.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;


</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22696646/google-shopping-lens-search-inventory-check-ios-chrome"/>
    <id>https://www.theverge.com/2021/9/29/22696646/google-shopping-lens-search-inventory-check-ios-chrome</id>
    <author>
      <name>Antonio G. Di Benedetto</name>
    </author>
  </entry>
  <entry>
    <published>2021-09-29T13:23:06-04:00</published>
    <updated>2021-09-29T13:23:06-04:00</updated>
    <title>Google Lens will soon search for words and images combined</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/iQI8hANhfsaYapnfkDq449xSta8=/0x0:1800x1200/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69926700/google_lens_socks_with_this_pattern_pic.0.jpg" /&gt;
    &lt;/figure&gt;

  &lt;p id="jEnRgj"&gt;Google is updating its visual search tool Google Lens with new AI-powered language features. The update will let users further narrow searches using text. So, for example, if you snap a photo of a paisley shirt in order to find similar items online using Google Lens, you can add the command “socks with this pattern” to specify the garments you’re looking for. &lt;/p&gt;
&lt;p id="ahaSzW"&gt;Additionally, Google is launching a new “Lens mode” option in its iOS Google app, allowing users to search using any image that appears while searching the web. This will be available “soon,” but it’ll be limited to the US. Google is also launching Google Lens on desktop within the Chrome browser, letting users select any image or video when browsing the web to find visual search results without leaving their tab. This will be available globally “soon.”	&lt;/p&gt;
&lt;p id="wpZIUW"&gt;These updates are part of Google’s latest push to improve its search tools using AI language understanding. The updates to Lens are powered by a machine learning model that the company unveiled at I/O earlier this year named MUM. In addition to these new features, Google is also introducing new AI-powered tools to its web and mobile searches.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/1NsWwk0iw40qIuXHElj4LhkdBG4=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22884223/google_lens_derailleur.gif"&gt;
      &lt;cite&gt;Image: Google&lt;/cite&gt;
      &lt;figcaption&gt;
&lt;em&gt;Using the updated Google Lens to identify a bike’s derailleur&lt;/em&gt;. &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="cUlfhG"&gt;The changes to Google Lens show the company hasn’t lost interest in this feature, which has always shown promise but seemed to appeal more as a novelty. Machine learning techniques have made object and image recognition features relatively easy to launch at a basic level, but, as today’s updates show, they require a little finesse on the part of the users to be properly functional. Enthusiasm may be picking up, though — Snap &lt;a href="https://www.theverge.com/2021/8/26/22642017/snapchat-scan-feature-ar-camera-visual-search"&gt;recently upgraded its own Scan feature&lt;/a&gt;, which functions pretty much identically to Google Lens. &lt;/p&gt;
&lt;p id="JAiBQc"&gt;Google wants these Lens updates to turn its world-scanning AI into a more useful tool. It gives the example of someone trying to fix their bike but not knowing what the mechanism on the rear wheel is called. They snap a picture with Lens, add the search text “how to fix this,” and Google pops up with the results that identify the mechanism as a “derailleur.” &lt;/p&gt;
&lt;p id="URUqLp"&gt;As ever with these demos, the examples Google is offering seem simple and helpful. But we’ll have to try out the updated Lens for ourselves to see if AI language understanding is really making visual search more than just a parlor trick. &lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22698014/google-lens-update-text-search-desktop-chrome"/>
    <id>https://www.theverge.com/2021/9/29/22698014/google-lens-update-text-search-desktop-chrome</id>
    <author>
      <name>James Vincent</name>
    </author>
  </entry>
  <entry>
    <published>2021-09-29T13:22:40-04:00</published>
    <updated>2021-09-29T13:22:40-04:00</updated>
    <title>Google is using AI to help users explore the topics they’re searching for — here’s how</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/OXKtcKb_23TFAF1Du1aHDeGJ6Gw=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69926694/acastro_180508_1777_google_IO_0003.0.jpg" /&gt;
        &lt;figcaption&gt;Illustration by Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="M03k9t"&gt;“Can you get medicine for someone at the pharmacy?”&lt;/p&gt;
&lt;p id="B5vnm2"&gt;It’s a simple enough question for humans to understand, says Pandu Nayak, vice president of search at Google, but such a query represents the cutting-edge of machine comprehension. You and I can see that the questioner is asking if they can fill out a subscription for &lt;em&gt;another person&lt;/em&gt;, Nayak tells &lt;em&gt;The Verge&lt;/em&gt;. But until recently, if you typed this question into Google, it would direct you to websites explaining how to fill out &lt;em&gt;your&lt;/em&gt; prescription. “It missed the subtlety that the prescription was for someone else,” he says.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="xH4SyC"&gt;&lt;q&gt;MUM is Google’s biggest, brightest AI language model&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="rrwnss"&gt;The key to delivering the right answer, says Nayak, is AI, which Google is using today to improve its search results. The prescription query was solved in 2019, when Google integrated a machine learning model &lt;a href="https://www.theverge.com/2019/10/25/20931657/google-bert-search-context-algorithm-change-10-percent-langauge"&gt;called BERT&lt;/a&gt; into search. As part of a new generation of AI language systems known as large language models (the most famous of which is OpenAI’s GPT-3), BERT was able to parse the nuances of our prescription query correctly and return the right results. Now, in 2021, Google is updating its search tools yet again, using another acronymized AI system that’s BERT’s successor: MUM.&lt;/p&gt;
&lt;p id="yjWpoX"&gt;Originally&lt;a href="https://blog.google/products/search/introducing-mum/"&gt; revealed&lt;/a&gt; at Google I/O in May, MUM is at least 1,000 times bigger than BERT, says Nayak; on the same order of magnitude as GPT-3, which has 175 billion parameters. (Parameters being a measure of a model’s size and complexity.) MUM is also multimodal, meaning it processes visual data as well as text. And it’s been trained on 75 languages, which allows the system to “generalize from languages where there’s a lot of data, like English, to languages where there’s less data, like Hindi,” says Nayak. That helps in ensuring that any upgrades it provides are spread across Google’s many markets.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/Vo4XOpVcCPykH4Q7FUe55J6_Hgg=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22885162/Untitled__2_.gif"&gt;
      &lt;cite&gt;Image: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;A new feature rolling out in the coming months named “Things to know” will use AI to help users explore topics related to their searches. &lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="GUJBlL"&gt;Nayak speaks of MUM with pride, as the latest AI wunderkind trained in Google’s labs. But the company is also cautious. Large language models are controversial for a number of reasons. They’re prone to lying, for example — as happy writing fiction as fact. And they’ve been shown time and time again to encode racial and gender biases. This is a problem that Google’s own researchers have highlighted and been shot down for doing so. Notably, Google fired two of its top ethics researchers, &lt;a href="https://www.theverge.com/2020/12/3/22150355/google-fires-timnit-gebru-facial-recognition-ai-ethicist"&gt;Timnit Gebru&lt;/a&gt; and &lt;a href="https://www.theverge.com/2021/2/19/22292011/google-second-ethical-ai-researcher-fired"&gt;Margaret Mitchell&lt;/a&gt;, after &lt;a href="https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/"&gt;they co-authored a paper&lt;/a&gt; highlighting problems with exactly this technology.&lt;/p&gt;
&lt;p id="378AJV"&gt;For these reasons, perhaps, the changes to search that Google is launching are relatively restrained. The company is introducing three new features “in the coming months,” some powered by MUM, each of which is ancillary to its search engine’s primary function — ranking web results. But Nayak says they’re just the tip of the iceberg when it comes to Google’s ambitions to improve its products with AI. “To me, this is just the start,” he says.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="1ANHUv"&gt;&lt;q&gt;“Things to know” will be automatically generated based on users’ searches&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="sZcWPG"&gt;First, though, the features. Number one is called “Things to know” and acts as an advanced snippet function, pulling out answers to predicted questions based on user’s searches. Type in “acrylic painting,” for example, and “Things to know” will automatically generate new queries, like “How do you use household items in acrylic painting.” Nayak says there are certain “sensitive queries” that &lt;em&gt;won’t&lt;/em&gt; trigger this response (like “bomb making”) but that most topics are automatically covered. It will be rolling out in the “coming months.”&lt;/p&gt;
&lt;p id="o6n8Xp"&gt;The second new feature suggests further searches that might help users broaden or refine their queries. So, with the “acrylic painting” search above, Google might now suggest a narrower focus, like “acrylic painting techniques,” or a broader remit, like “different styles of painting.” As Nayak puts it, Google wants to use AI’s ability to recognize “the space of possibilities within [a] topic” and help people explore variants of their own searches. This feature will be available immediately, though it is not powered by MUM. &lt;/p&gt;
&lt;p id="VBAKnC"&gt;The third new feature is more straightforward and based on video transcription. When users are searching for video content, Google will use MUM to suggest new searches based on what it hears within the video. Nayak gives the example of watching a video about Macaroni penguins and Google suggesting a new search of “Macaroni penguin life story.” Again, it’s about suggesting new areas of search for users. This feature will launch on September 29th in English in the US.&lt;/p&gt;
&lt;p id="6YiNuE"&gt;In addition to these AI-based changes, Google is also expanding its “&lt;a href="https://www.theverge.com/2021/7/22/22587197/google-search-about-this-result-feature-context-explanation-terms-language"&gt;About This&lt;/a&gt;” feature in search, which will give new information about the source of results. It’s also bringing its MUM-powered AI smarts to its visual search tech, Google Lens.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/OjI7isUaz_bNGkDo-LY1WErv5Eo=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22885141/Untitled.png"&gt;
      &lt;cite&gt;Image: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Google will give users new option to “refine” or “broaden” their search — using MUM to explore related topics. &lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="8abtQD"&gt;The change to search is definitely the main focus, but what’s interesting is also what Google &lt;em&gt;isn’t&lt;/em&gt; launching. When it demoed MUM at I/O earlier this year, it showed off ambitious features where users could literally talk to the subjects of their searches, like the dwarf planet Pluto, and &lt;a href="https://www.theverge.com/2021/5/18/22442328/google-io-2021-ai-language-model-lamda-pluto"&gt;ask them questions&lt;/a&gt;. In another, users asked expansive questions, like “I just hiked Mt. Adams, I want to hike Mt. Fuji in the fall. What should I do differently?” before being directed to relevant snippets and web pages.&lt;/p&gt;
&lt;p id="ZJDMN8"&gt;It seems these sorts of searches, which are rooted deeply in the functionality of large language models, are too free-form for Google to launch publicly. Most likely, the reason for this is that the language models could easily say the wrong thing. That’s when those bias problems come into play. For example, when GPT-3 is &lt;a href="https://www.vox.com/future-perfect/22672414/ai-artificial-intelligence-gpt-3-bias-muslim"&gt;asked to complete a sentence&lt;/a&gt; like “Audacious is to boldness as Muslim is to …,” nearly a quarter of the time, it finishes the sentence with the word “terrorism.” These aren’t problems that are easy to navigate.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="ZGE2Qo"&gt;&lt;q&gt;“Even if the model has biases, we’re not putting it out for people to consume directly”&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="ASR31v"&gt;When questioned about these difficulties, Nayak reframes the problems. He says it’s obvious that language models suffer from biases but that this isn’t necessarily the challenge for Google. “Even if the model has biases, we’re not putting it out for people to consume directly,” he says. “We’re launching products. And what matters is, are the products serving our users? Are they surfacing undesirable things or not?”&lt;/p&gt;
&lt;p id="grzlvU"&gt;But the company can’t completely stamp out these problems in its finished products either. Google’s Photo app infamously&lt;a href="https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai"&gt; tagged Black people as “gorillas”&lt;/a&gt; in one well-known incident, and the sort of racial and gender-based discrimination present in language AI is often much more subtle and difficult to detect.&lt;/p&gt;
&lt;p id="1XxEC5"&gt;There’s also the problem of what the shift to AI-generated answers might mean for the wider future of Google search. In a&lt;a href="https://arxiv.org/pdf/2105.02274.pdf"&gt; speculative paper&lt;/a&gt; published earlier this year, Google’s researchers considered the question of replacing search altogether with large language models and &lt;a href="https://www.theverge.com/2021/5/26/22454513/google-future-of-search-conversation-ai-mum-lamda"&gt;highlighted a number of difficulties&lt;/a&gt; with the approach. (Nayak is definitive that this is not a serious prospect for the company: “That is absolutely not the plan.”)&lt;/p&gt;
&lt;p id="DKOiYb"&gt;And there’s also the consistent grumbling that Google continues to take up more space in search results with its own product, shunting searches to Google Shopping, Google Maps, and so on. The new MUM-powered “Things to know” feature certainly seems to be part of this trend: filleting out the most informative search results from web pages, and potentially stopping users from clicking through, and therefore sustaining the creator of that data.&lt;/p&gt;
&lt;p id="GJ9cCC"&gt;Nayak’s response to this is that Google delivers more traffic to the web each year and that if it doesn’t “build compelling experiences” for users, then the company “will not be around to send traffic to the web” in the future. It’s not a wholly convincing answer. Google may deliver more traffic each year, but how much of that is just a function of increasing web use? And even if Google does disappear from search, wouldn’t other search engines pick up the slack in sending people traffic?&lt;/p&gt;
&lt;p id="T9mP4x"&gt;Whatever the case, it’s clear that the company is putting AI language understanding at the heart of its search tools — at the heart of Google, indeed. There are many open questions about the challenges of integrating this tech, but for now, Google is happy to continue the search for answers of its own.&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22696268/google-search-on-updates-ai-mum-explained"/>
    <id>https://www.theverge.com/2021/9/29/22696268/google-search-on-updates-ai-mum-explained</id>
    <author>
      <name>James Vincent</name>
    </author>
  </entry>
  <entry>
    <published>2021-09-29T13:16:46-04:00</published>
    <updated>2021-09-29T13:16:46-04:00</updated>
    <title>Ah fuck: YouTube reinstates classic ‘I can’t believe you’ve done this’ meme</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/uSJbf66JxNC5YwGRsrlZ-9sMhfE=/154x0:825x447/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69924627/Screen_Shot_2021_09_29_at_10.19.08_AM.0.png" /&gt;
    &lt;/figure&gt;

  &lt;p id="JP9Z5p"&gt;Whether it wants to be or not, YouTube is a guardian of internet history, with countless classic videos sitting in its archives. That makes it jarring when the company acts like what it is: a multinational corporation with no real understanding of this value. Case in point, this week, YouTube removed the original upload of the “&lt;a href="https://knowyourmeme.com/memes/i-cant-believe-youve-done-this"&gt;Ah fuck, I can’t believe you’ve done this&lt;/a&gt;” meme, initially rejecting an appeal from its creator (and the guy who can’t believe this was done) and claiming that the clip violates the company’s “violent or graphic content policy.”&lt;/p&gt;
&lt;p id="ADM4jv"&gt;In the words of &lt;a href="http://paulweedon.co.uk/"&gt;Paul Weedon&lt;/a&gt; himself, star of and uploader of the original video: Ah, fuck. “It’s completely out of the blue,” Weedon told &lt;em&gt;The Verge&lt;/em&gt; regarding the takedown. “I made the case that it’s been online going on 15 years and is basically part of internet culture.” &lt;/p&gt;
&lt;p id="iGSbp7"&gt;Weedon &lt;a href="https://twitter.com/Twotafkap/status/1442952265552187394"&gt;tweeted&lt;/a&gt; out the news of the takedown and his unsuccessful appeal, noting that the video had been on YouTube for 14 years and racked up 12 million views in that time with “no issues whatsoever.” &lt;/p&gt;
&lt;p id="iIdHJI"&gt;However, &lt;a href="https://www.youtube.com/watch?v=wKbU8B-QVZk"&gt;now the video is available again&lt;/a&gt;. In a &lt;a href="https://twitter.com/TeamYouTube/status/1443247423250444295"&gt;response tweet&lt;/a&gt; from its TeamYouTube account, the video hoster now says “this was a mistake on our end and your video is back up.”&lt;/p&gt;
&lt;div id="lVy7jq"&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p lang="en" dir="ltr"&gt;So this has happened. &lt;a href="https://t.co/O2baPp0ybq"&gt;pic.twitter.com/O2baPp0ybq&lt;/a&gt;&lt;/p&gt;— Paul Weedon (@Twotafkap) &lt;a href="https://twitter.com/Twotafkap/status/1442952265552187394?ref_src=twsrc%5Etfw"&gt;September 28, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id="cypzMC"&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p lang="en" dir="ltr"&gt;Following up: this was a mistake on our end and your video is back up. We’re so sorry about the frustration this caused &amp;amp; have shared feedback with the team to prevent similar mistakes in the future. Look out for an email from us with more soon.&lt;/p&gt;— TeamYouTube (@TeamYouTube) &lt;a href="https://twitter.com/TeamYouTube/status/1443247423250444295?ref_src=twsrc%5Etfw"&gt;September 29, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;p id="fhvPe6"&gt;The clip is a classic: low-res, contextless, and instantly funny. And, of course, &lt;a href="https://youtu.be/O7lRV1VHv1g"&gt;countless re-uploads&lt;/a&gt; still exist (alongside the other, legitimately horrible content that YouTube is happy to leave up). &lt;/p&gt;
&lt;p id="TYDRV4"&gt;The video has been &lt;a href="https://knowyourmeme.com/memes/i-cant-believe-youve-done-this"&gt;remixed and re-memed&lt;/a&gt; in countless ways since it first went viral in the mid-2010s, and Weedon himself has an interesting relationship with the content. In an &lt;a href="https://www.vice.com/en/article/4avnqj/my-life-as-a-meme-i-cant-believe-youve-done-this-revisited"&gt;article for &lt;em&gt;Vice&lt;/em&gt;&lt;/a&gt; published earlier this year, he describes how the clip was part of a series of “stunts” he and his friends filmed in the vein of &lt;em&gt;Jackass&lt;/em&gt; and how he sold the original rights for the video to the now-defunct &lt;a href="http://Break.com"&gt;Break.com&lt;/a&gt; and isn’t now sure who even owns the IP. &lt;/p&gt;
&lt;p id="RmVoXM"&gt;“At the time, going viral wasn’t really comparable to any other experience and it certainly wasn’t something I could discuss in solidarity with my friends,” writes Weedon. “All of a sudden you’re everywhere and it’s out of your control. You either try to fight it and get destroyed or embrace it and try to cash in. After yanking down several other videos on my YouTube channel, I opted for the latter.”&lt;/p&gt;
&lt;p id="ieeNQv"&gt;To add insult to injury, Weedon is currently exploring the possibility of making a documentary about the meme, so the takedown added a little twist to the proceedings. “This takes things in a completely different direction,” he tells &lt;em&gt;The Verge&lt;/em&gt;. “It says a lot about where YouTube is going. On Twitter, people seem to be saying the same thing: that this is a video that represents what YouTube used to be, and they’ve torched it.”&lt;/p&gt;
&lt;p id="VXLG9y"&gt;Weedon says he’s found all the footage from the day the meme was shot and that he and a group of friends are still exploring how to tell the story. “We’re still figuring out what we want to do with it,” he says. “Though, I definitely regret promising the documentary would be coming ‘soon’ in the teaser. I forgot how impatient people on the internet can be.”&lt;/p&gt;
&lt;div id="FUUguD"&gt;&lt;div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"&gt;&lt;iframe src="https://www.youtube.com/embed/o1yfyzT3Q0c?rel=0" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute; border: 0;" allowfullscreen="" scrolling="no" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture;"&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p id="AbL5kb"&gt;&lt;em&gt;&lt;strong&gt;Update, September 29th, 6:13AM ET: &lt;/strong&gt;&lt;/em&gt;&lt;em&gt;Updated the story with comment from Weedon. &lt;/em&gt;&lt;/p&gt;
&lt;p id="1rQDKS"&gt;&lt;em&gt;&lt;strong&gt;Update, September 29th, 1:15PM ET: &lt;/strong&gt;&lt;/em&gt;&lt;em&gt;Updated the story to reflect the video’s reinstatement and added a statement from YouTube.&lt;/em&gt;&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22699955/youtube-takes-down-ah-fuck-cant-believe-youve-done-this-meme"/>
    <id>https://www.theverge.com/2021/9/29/22699955/youtube-takes-down-ah-fuck-cant-believe-youve-done-this-meme</id>
    <author>
      <name>James Vincent</name>
    </author>
  </entry>
  <entry>
    <published>2021-09-29T12:00:00-04:00</published>
    <updated>2021-09-29T12:00:00-04:00</updated>
    <title>The Dart Zone Pro MK-3 might be the best foam blaster Nerf never made</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/ZVD2vT5rCktGf-vaQvFTz8ZtSWw=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69926176/vpavic_210928_4778_0031.0.jpg" /&gt;
        &lt;figcaption&gt;Photo by Vjeran Pavic / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;


  &lt;p&gt;Almost everything Hasbro won’t give you in one package&lt;/p&gt; &lt;p class="p--has-dropcap p-large-text" id="RDxn6n"&gt;Earlier this year, we introduced you to the world of &lt;a href="https://www.theverge.com/22324389/nerf-gun-diy-homemade-blaster-out-of-darts-jupiter-caliburn-captain-slug-hasbro"&gt;Nerf blasters so powerful they leave your childhood in the dust&lt;/a&gt;, designed by a community of makers who no longer need the official Nerf brand to advance the state of the art. But makers and hobbyists aren’t the only ones pushing the foam-flinging sport forward — there’s also Dart Zone, a toymaker whose 2020 Walmart-exclusive &lt;a href="https://goto.walmart.com/c/482924/565706/9383?u=https%3A%2F%2Fwww.walmart.com%2Fip%2FAdventure-Force-Tactical-Strike-Nexus-Pro-Ultimate-Dart-Blaster-shoots-over-125-FT%2F204550182&amp;amp;sharedid=theverge.com" rel="sponsored nofollow noopener" target="_blank"&gt;$50 Nexus Pro&lt;/a&gt; and &lt;a href="https://goto.walmart.com/c/482924/565706/9383?u=https%3A%2F%2Fwww.walmart.com%2Fip%2FAdventure-Force-Tactical-Strike-Aeon-Pro-Ultimate-Dart-Blaster-Shoots-Over-125-FT%2F215999810%3F&amp;amp;sharedid=theverge.com" rel="sponsored nofollow noopener" target="_blank"&gt;$25 Aeon Pro&lt;/a&gt; brought a never-before-seen level of range and accuracy to blasters you could just buy off the shelf. &lt;/p&gt;
&lt;p id="1BHc2F"&gt;For the past several days, I’ve been testing the brand’s new flagship blaster, the hotly anticipated &lt;a href="https://dartzoneblasters.com/dart-zone-pro-series-mk-3/"&gt;$130 Dart Zone Pro MK-3&lt;/a&gt;, which pitches itself as the first fully automatic blaster designed for sporting-grade Nerf, accurately firing at up to 150 feet per second. And while I’ll have to wait for the next local Nerf war to &lt;em&gt;really&lt;/em&gt; tell you if it can compete (I do have one important doubt), this company seems to &lt;em&gt;get &lt;/em&gt;the Nerf community in a way I’ve never seen before. &lt;/p&gt;
&lt;div class="c-wide-block"&gt;  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/i-4HhDGX1CzdzTBOplA7-vY5Uis=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22886337/vpavic_210928_4778_0031.jpg"&gt;
      &lt;cite&gt;Photo by Vjeran Pavic / The Verge&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;The MK-3 weighs in at nearly three pounds, including 15 long darts, eight AAs, and a pair of AAAs in the red dot sight. &lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;div class="c-float-right c-float-hang"&gt;&lt;div id="KlILTQ"&gt;&lt;div data-anthem-component="aside:10779717"&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p id="u5S0E3"&gt;While Hasbro is trying to sell us on &lt;a href="https://www.theverge.com/2021/9/8/22663549/nerf-hyper-rush-40-siege-50-mach-100-review"&gt;a new rubbery little ball&lt;/a&gt; to get any sort of range at all, &lt;a href="https://www.theverge.com/2019/9/23/20880209/nerf-ultra-one-blaster-foam-darts-120-feet-incompatible-ammo-drm-date-price"&gt;the second such attempt at new ammo in recent memory&lt;/a&gt;, the MK-3 manages to deliver modded-blaster grade performance from the standard long and short darts that the community’s been using for years. That 150 feet-per-second promise is on the money: plug in eight regular AA batteries, and it’s nearly as powerful (yet quieter!) than the LiPo-powered &lt;a href="https://www.amazon.com/Nerf-A8494-Elite-2-in-1-Demolisher/dp/B00HX8PSQ8/?tag=theverge02-20" rel="sponsored nofollow noopener" target="_blank"&gt;Demolisher&lt;/a&gt; I modded a few years back. I can easily hit targets 80 feet away, dart after dart after dart, and blast up to 130 feet away when I angle the MK-3 up into the air. &lt;/p&gt;
&lt;p id="y5UFmN"&gt;But wonder of wonders, you don’t &lt;em&gt;need&lt;/em&gt; to use AAs at all. A single Philips-head screw is the only thing keeping you from lifting out the entire battery tray — where you’ll find a genuine XT-60 connector to plug in your very own LiPo battery. Swap in a two-cell (2S) 7.4V LiPo, and you’ll get roughly the same performance while shedding nearly half a pound of weight. Swap in a three-cell (3S) 11.1V LiPo, and your darts can instantly go 20 feet per second faster, &lt;em&gt;plus&lt;/em&gt; I saw my full auto rate-of-fire jump from three-plus darts per second to four-plus darts per second. It also spins up notably faster, meaning you don’t need to hold the rev trigger as long before you can blast! &lt;/p&gt;
&lt;div class="c-wide-block"&gt;  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/yovsFbsUJ02C0-U1K5rEBsXLYuY=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22886346/vpavic_210928_4778_0070.jpg"&gt;
      &lt;cite&gt;Photo by Vjeran Pavic / The Verge&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;There’s roughly 142mm x 61mm x 16mm worth of space for a LiPo, plus a cord channel; 50x40mm worth of that space is 25mm deep, if you’ve got a small but thick battery pack.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p id="VhUdaE"&gt;It’s the easiest mod ever, and it’s completely different from the way Nerf brand owner Hasbro operates. Nerf has frowned on modding for years, presumably for safety reasons, and has even sealed its blasters with epoxy from time to time. And while Dart Zone creative director Bryan Sturtevant tells me his company isn’t exactly &lt;em&gt;encouraging&lt;/em&gt; you to mod this one — “if you put the wrong LiPo in it, you can blow up the blaster” — the company’s also not remotely worried about three-cell battery packs. The specially chosen 36,000RPM motors and larger-gauge wire are designed for 12 volts, and they’re LiPo-tested and ready.&lt;/p&gt;
&lt;p id="CfSENa"&gt;I’m a little less sure about the health of your darts while using it. For the MK-3, Dart Zone’s using serrated flywheels that appear to be slightly shredding every dart I fire out the end. I’ve been firing off magazine after magazine in my backyard and a nearby park, and I’ve never seen quite so much foam dust settle in my barrel. Heck, I had a hard time measuring dart speed with one of my ballistic chronographs because tiny specks of foam were shooting out the end. Almost every dart I’ve fired shows signs of wear, some have missing chunks of foam, and I have five completely busted darts because this blaster yanked off their tips. And that’s with AAs, not an upgraded LiPo battery. &lt;/p&gt;
&lt;div class="c-wide-block"&gt;  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/_XEWG-gvzk9ABaPUTQCWpKSwHsc=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22886966/darts_2200.jpg"&gt;
      &lt;cite&gt;Photo by Sean Hollister / The Verge&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Every one of these darts is getting worn — if not bitten — right near their tips. &lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p id="Rgh2PF"&gt;I don’t know if there’s a defect or if, say, my flywheels were just misaligned at the factory, but the company says it’s never seen this and will send me another unit. I wouldn’t personally buy one until this gets figured out. Modern Nerf wars typically use a shared pool of community darts (so there are no arguments about who owns which ones) and the community tends to frown on people who shred them. Plus, each dart can easily cost 10 cents even if you purchase in bulk.&lt;/p&gt;
&lt;p id="PY9jZK"&gt;I’m hoping the issue is a fluke, because this blaster plays nice with practically every kind of dart on the market — and I’m including Nerf’s own pitifully inaccurate Elite darts in that. While it’s &lt;em&gt;way&lt;/em&gt; too powerful to fire Elite darts in its semi-auto mode, which fly wildly off target like they do in most high-power blasters, the MK-3 loses enough velocity in full auto that Elites start working again. This is a blaster where you can literally flip a switch to play a lower-stakes game of Nerf with friends who don’t have a high-power kit. &lt;/p&gt;
&lt;p id="ccAArP"&gt;The MK-3 seems like a well-designed blaster in other ways as well. Every switch and lever has a satisfying and confident pull, there’s comfy rubber on the four-position stock and pistol grip, and you’ll find two built-in foregrip positions and an underslung rail to add your own — all while still looking like a sci-fi toy rather than a gun, which I appreciate. It comes with two 15-round magazines for long and short darts, a short magazine adapter, and a removable red-dot sight that looks cool but sadly has no windage adjustment and doesn’t line up with where my darts actually go.  &lt;/p&gt;
&lt;div class="c-wide-block"&gt;  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/eWbCDVG25QS4BX8oUsLjhpuD6G0=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22886342/vpavic_210928_4778_0051.jpg"&gt;
      &lt;cite&gt;Photo by Vjeran Pavic / The Verge&lt;/cite&gt;
      &lt;figcaption&gt;
&lt;em&gt;The MK-3’s compatible with practically every modern Nerf magazine, including the old 35-round drum and the &lt;/em&gt;&lt;a class="ql-link" href="https://outofdarts.com/products/29-round-short-dart-magazine-ood-original" target="_blank"&gt;&lt;em&gt;new 29-round Tachi stick mag&lt;/em&gt;&lt;/a&gt;. &lt;em&gt;Some are reporting issues with Talons feeding, though.&lt;/em&gt;
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p id="KDPQpf"&gt;Dart Zone says it’s upgraded its green “bamboo” darts alongside this blaster’s release, moving to a lighter green that’s easier to spot in grass. They’re also adopting the narrower dart head that helped improve the range and accuracy of the black darts that Dart Zone sells at Walmart alongside its Nexus Pro blaster. I don’t have enough of them to properly test, but so far I prefer the black darts’ performance.&lt;/p&gt;
&lt;p id="Hd5S1G"&gt;The Dart Zone Pro MK-3 should go on sale this Friday, October 1st at &lt;a href="https://goto.target.com/c/482924/81938/2092?u=http%3A%2F%2FTarget.com&amp;amp;sharedid=theverge.com" rel="sponsored nofollow noopener" target="_blank"&gt;Target.com&lt;/a&gt; (not in retail stores), and Dart Zone estimates it should arrive a week later at Amazon Canada. The company also plans to sell three-packs of the new 15-round magazines for roughly $30 at its own website closer to Christmas. &lt;/p&gt;
&lt;p id="Hh1oC7"&gt;After the Nexus Pro, Aeon Pro, Conquest Pro, the MK-3, and other ways Dart Zone has undercut Hasbro like building a better dart, I can’t wait to see what the company does next. And what, if anything, it might take to get the official Nerf brand to react. &lt;/p&gt;
&lt;p id="icREKl"&gt;&lt;em&gt;&lt;strong&gt;Update, 12:45PM ET: &lt;/strong&gt;&lt;/em&gt;&lt;em&gt;Want to see it in motion? You can find video impressions of the new blaster (and &lt;/em&gt;&lt;a href="https://youtu.be/EwqfprcYDzQ?t=947"&gt;&lt;em&gt;footage of its internals&lt;/em&gt;&lt;/a&gt;&lt;em&gt;) at &lt;/em&gt;&lt;a href="https://www.youtube.com/watch?v=EwqfprcYDzQ"&gt;&lt;em&gt;Out of Darts&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href="https://www.youtube.com/watch?v=3lL96_UCb3A"&gt;&lt;em&gt;Foamblast&lt;/em&gt;&lt;/a&gt;, &lt;a href="https://www.youtube.com/watch?v=WpQiYjhRU9A"&gt;&lt;em&gt;LordDraconical&lt;/em&gt;&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=b4PmEwPo2YI"&gt;&lt;em&gt;WalcomS7&lt;/em&gt;&lt;/a&gt;&lt;em&gt; on YouTube. Two of them noticed an issue with some Talon-style magazines, but Foamblast found &lt;/em&gt;&lt;a href="https://youtu.be/3lL96_UCb3A?t=589"&gt;&lt;em&gt;a possible solution&lt;/em&gt;&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;aside id="UO8vnw"&gt;&lt;div data-anthem-component="readmore" data-anthem-component-data='{"stories":[{"title":"Nerf Hyper review: where the rubber meets the foam ","url":"https://www.theverge.com/2021/9/8/22663549/nerf-hyper-rush-40-siege-50-mach-100-review"},{"title":"The engineers building ridiculous dart blasters that Nerf won’t touch","url":"https://www.theverge.com/22324389/nerf-gun-diy-homemade-blaster-out-of-darts-jupiter-caliburn-captain-slug-hasbro"}]}'&gt;&lt;/div&gt;&lt;/aside&gt;&lt;div class="p-fullbleed-block"&gt;  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/ZExnMFm0ETVcmx8heTl9iGBsPmA=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22886340/vpavic_210928_4778_0041.jpg"&gt;
      &lt;cite&gt;Photo by Vjeran Pavic / The Verge&lt;/cite&gt;
  &lt;/figure&gt;
&lt;/div&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/22699272/dart-zone-pro-mk-3-preview-blaster-prime-time-toys-nerf-price"/>
    <id>https://www.theverge.com/22699272/dart-zone-pro-mk-3-preview-blaster-prime-time-toys-nerf-price</id>
    <author>
      <name>Sean Hollister</name>
    </author>
  </entry>
  <entry>
    <published>2021-09-29T11:13:58-04:00</published>
    <updated>2021-09-29T11:13:58-04:00</updated>
    <title>The Book of Boba Fett premieres on Disney Plus December 29th</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/ycbOfoq5Yd7gcEDpV1P7BPTeUhw=/713x0:3128x1610/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69925957/huc2_ff_003556_19dc1374.0.jpeg" /&gt;
        &lt;figcaption&gt;The Mandalorian and Boba Fett in &lt;em&gt;The Mandalorian&lt;/em&gt;. | Image: Lucasfilm&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="JztUV6"&gt;Disney’s &lt;em&gt;The Book of Boba Fett&lt;/em&gt;, a new show focused on the iconic &lt;em&gt;Star Wars&lt;/em&gt; bounty hunter, will premiere on &lt;a href="https://disneyplus.bn5x.net/c/482924/1155720/9358?subId1=VergeBobaFett092921&amp;amp;u=https%3A%2F%2Fwww.disneyplus.com%2F" rel="sponsored nofollow noopener" target="_blank"&gt;Disney Plus&lt;/a&gt; on December 29th, &lt;a href="https://twitter.com/disneyplus/status/1443229083924672512"&gt;Disney announced Wednesday&lt;/a&gt;.&lt;/p&gt;
&lt;p id="d3m0MT"&gt;Disney revealed that the show was in the works after a post-credits scene during the season 2 finale of &lt;em&gt;The Mandalorian&lt;/em&gt;. Shortly after that big reveal, we learned that the show &lt;a href="https://www.theverge.com/2020/12/18/22188516/boba-fett-disney-plus-show-2021-release-date-mandalorian-finale"&gt;would premiere in December 2021&lt;/a&gt;, but now we have an official date.&lt;/p&gt;
&lt;p id="4VPsz4"&gt;Here’s the official synopsis, from Disney:&lt;/p&gt;
&lt;blockquote&gt;&lt;p id="yTZvRx"&gt;“The Book of Boba Fett,” a thrilling Star Wars adventure, finds legendary bounty hunter Boba Fett and mercenary Fennec Shand navigating the Galaxy’s underworld when they return to the sands of Tatooine to stake their claim on the territory once ruled by Jabba the Hutt and his crime syndicate.” &lt;/p&gt;&lt;/blockquote&gt;
&lt;p id="aUlDfm"&gt;Beyond that, though, we don’t know much, so we’ll have to wait for Disney to share more to get a better picture of what to expect. (Here’s hoping that the date announcement means a trailer is coming soon.)&lt;/p&gt;
&lt;p id="7I4Zga"&gt;The &lt;em&gt;Book of Boba Fett&lt;/em&gt; is just one of many &lt;em&gt;Star Wars&lt;/em&gt; shows on Disney Plus. This month saw the premiere of &lt;a href="https://www.theverge.com/star-wars/22684843/star-wars-visions-anime-anthology-disney-plus-interview"&gt;&lt;em&gt;Star Wars: Visions&lt;/em&gt;, an anime anthology&lt;/a&gt;, and there are many in the works, including &lt;em&gt;Obi-Wan Kenobi &lt;/em&gt;and &lt;em&gt;Andor.&lt;/em&gt;&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22700329/disney-plus-the-book-of-boba-fett-star-wars-release-date"/>
    <id>https://www.theverge.com/2021/9/29/22700329/disney-plus-the-book-of-boba-fett-star-wars-release-date</id>
    <author>
      <name>Jay Peters</name>
    </author>
  </entry>
  <entry>
    <published>2021-09-29T11:04:36-04:00</published>
    <updated>2021-09-29T11:04:36-04:00</updated>
    <title>Instagram Reels are now widely available on Facebook in the US</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="Facebook Reels" src="https://cdn.vox-cdn.com/thumbor/DJoms85O9a-ccp6N0HGMOoRcrY0=/454x0:2050x1064/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69925914/Facebook_Reels_Experience_20_0.0.jpg" /&gt;
        &lt;figcaption&gt;Image: Facebook&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="p5mg3d"&gt;A little over a month after Facebook announced &lt;a href="https://www.theverge.com/2021/8/19/22632223/facebook-reels-update-app-test-launch"&gt;a test&lt;/a&gt; blending Instagram Reels’ short-form video content with its main Facebook app, the company &lt;a href="https://about.fb.com/news/2021/09/launching-reels-on-facebook-us/"&gt;is ready to roll it out fully&lt;/a&gt; in the US. Now, the creators who use it can create reels on Facebook, and Facebook is testing suggestions that will push the videos to its users.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt="Facebook Reels recommendation" data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/SM7K0PkpHNNgOFQS_X-13LfxNYc=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22887512/NRP_Facebook_Reels_Recommendations_600.jpg"&gt;
      &lt;cite&gt;Image: Facebook&lt;/cite&gt;
      &lt;figcaption&gt;Facebook Reels recommendation&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="fODs46"&gt;A big part of the Reels effort is appealing to the people who make entertaining content, and with its news feed, Facebook can offer potential levels of exposure that are bigger than anyone else, including competition like TikTok and Snapchat. To drive the point home, Facebook announced a Reels Play bonus program, which will invite certain creators to get paid based on the performance of their reels. &lt;/p&gt;
&lt;p id="vEVs6P"&gt;Interested parties can sign up for the bonus program, but even if you’re not yet a pro at going viral, you can still create a Reel on Facebook. If you’re not familiar with the feature that Instagram &lt;a href="https://www.theverge.com/2020/8/5/21354117/instagram-reels-tiktok-vine-short-videos-stories-explore-music-effects-filters"&gt;added a year ago&lt;/a&gt;, it’s very similar to TikTok, with tools for you to easily edit videos with your own audio or popular song clips from the library, AR effects, and stitching.&lt;/p&gt;
&lt;aside id="WdPgPx"&gt;&lt;div data-anthem-component="readmore" data-anthem-component-data='{"stories":[{"title":"Instagram Reels’ biggest problem is replicating what TikTok does best ","url":"https://www.theverge.com/21362382/tiktok-reels-for-you-page-algorithm-instagram-paradox-choice-streaming"}]}'&gt;&lt;/div&gt;&lt;/aside&gt;&lt;div id="tGkjiW"&gt;&lt;iframe src="https://www.facebook.com/plugins/video.php?href=https%3A%2F%2Fwww.facebook.com%2Ffacebook%2Fvideos%2F4356148171148569%2F&amp;amp;show_text=0&amp;amp;width=267" width="267" height="476" style="border:none;overflow:hidden" scrolling="no" frameborder="0" allowfullscreen="true" allow="autoplay; clipboard-write; encrypted-media; picture-in-picture; web-share"&gt;&lt;/iframe&gt;&lt;/div&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22700289/facebook-reels-instagram-play-bonus-tiktok"/>
    <id>https://www.theverge.com/2021/9/29/22700289/facebook-reels-instagram-play-bonus-tiktok</id>
    <author>
      <name>Richard Lawler</name>
    </author>
  </entry>
</feed>
