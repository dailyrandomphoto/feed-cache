<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <title>The Verge -  Front Pages</title>
  <icon>https://cdn.vox-cdn.com/community_logos/52801/VER_Logomark_32x32..png</icon>
  <updated>2021-10-26T10:00:00-04:00</updated>
  <id>https://www.theverge.com/rss/front-page/index.xml</id>
  <link type="text/html" href="https://www.theverge.com/" rel="alternate"/>
  <entry>
    <published>2021-10-26T10:00:00-04:00</published>
    <updated>2021-10-26T10:00:00-04:00</updated>
    <title>Supernatural, the VR workout app, adds boxing for more variety</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/gckBdmcxmj74pIb1iFLy8kaAaas=/0x0:3240x2160/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/70046830/Boxing_MR.0.png" /&gt;
        &lt;figcaption&gt;You will punch your way to a much higher heart rate. | Supernatural&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="bCt7cJ"&gt;Supernatural, the &lt;em&gt;Beat Saber&lt;/em&gt;-like VR app for the Oculus Quest and Quest 2 that gets you to work up a sweat by beating targets with virtual bats, has added another way to wreck a target: your fists. Boxing is the latest workout introduced on Supernatural and like the original bat-oriented “Flow” workout, it works up a sweat while being fun.&lt;/p&gt;
&lt;p id="NJeiqr"&gt;Supernatural Flow is sort of like &lt;em&gt;Beat Saber&lt;/em&gt; crossed with high-intensity interval training (HIIT). Each workout can last from seven minutes to an hour, and there’s an enthusiastic coach shouting encouragements in your ear and guiding you through the session as you dance, lunge, squat, and murder targets with your VR bats.&lt;/p&gt;
&lt;p id="uD2SUm"&gt;The problem with Supernatural, at least for me, is that you can plateau. The workout, like HIIT, is hard on the knees and it's easy to get stuck doing workouts that get the heart rate up but don’t actually build your cardio health or muscle. I started cycling and lifting weights to push myself more and give my knees a rest. &lt;/p&gt;
&lt;aside id="UZnf3c"&gt;&lt;div data-anthem-component="readmore" data-anthem-component-data='{"stories":[{"title":"Is VR the next frontier in fitness?","url":"https://www.theverge.com/22379859/vr-virtual-reality-fitness-work-out-supernatural-chris-milk-interview"}]}'&gt;&lt;/div&gt;&lt;/aside&gt;&lt;p id="Fqc5R4"&gt;Crucially, neither of those things were happening in the Supernatural app. Since Supernatural launched in April 2020, it’s added two additional workouts — Guided Meditation and Recover (stretching) — but they felt less like a reason to strap on the headset and dive in than the original Flow workout. Boxing, which launches today, is the third new workout and the first intended to get the heart pumping like the original.&lt;/p&gt;
&lt;p id="hix5T5"&gt;VR boxing to get buff isn’t new. &lt;em&gt;Thrill of the Fight&lt;/em&gt; and &lt;em&gt;Knockout League&lt;/em&gt; on Oculus Quest both have enthusiastic fans who use them for cardio. But like &lt;em&gt;Beat Saber,&lt;/em&gt; those are games being used for working out. Boxing in Supernatural is very much a workout first. Black and white targets fly at your face and you’re meant to hit black targets with your left hand and white ones with your right. The targets look different depending on if you’re supposed to jab, uppercut, or toss a hook at them. Glowing bars twirl toward your face to force you to tilt left or right or do a full bob and weave to avoid them. The coach has you stretch for about thirty seconds to a minute both before and after the core workout. They also tell you when to change up your feet position to ensure you’re working out your left and right sides equally, and they tell you when to get ready to block a blow with both hands.&lt;/p&gt;
&lt;div id="w5Urgi"&gt;&lt;div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"&gt;&lt;iframe src="https://www.youtube.com/embed/mLBQWsX5bk4?rel=0" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute; border: 0;" allowfullscreen="" scrolling="no" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture;"&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p id="aZgWx5"&gt;The workouts available before launch were limited to just 13 minutes max. After that first one, I told my co-workers it had given me a good sweat but hadn’t done much else due to how short the workout was. Then I woke up the next morning and my arms and shoulders called me an idiot. Heart rate-wise, the average beats per minute in the boxing workout were about 10bpm higher than in the regular Supernatural workout — and my knees didn’t get as angry with me. I could definitely see using this regularly as a warm-up before weight training.&lt;/p&gt;
&lt;p id="Fkge4s"&gt;But I don’t know if this would be a good supplement for people who regularly box to keep fit. When I did a slower-paced low-intensity workout, I had no problem keeping great boxing form, but faster-paced workouts absolutely wrecked it. I went from feeling like I knew how to throw a punch to flailing like I was in a fight with my brother when I was 10. I didn’t miss a lot of targets, but I’m also positive my punches would have landed on a heavy bag like the gentle flutter of a butterfly wing. &lt;/p&gt;
&lt;p id="SKIsJh"&gt;Thankfully, I have no interest in going to a boxing gym — heck, I don’t really want to go to a regular gym either. And Supernatural, despite its inability to stop bad boxing habits, gives me the feeling of a gym without all the smells and potential for COVID-19 exposure. &lt;/p&gt;
&lt;p id="GVovFN"&gt;Boxing is available on Supernatural starting today. Supernatural is available for a free, month-long trial. After that, you’ll either need to pay $18.99 a month or $179 a year to get access to new workouts, which are available daily.&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/10/26/22746070/supernatural-vr-workout-app-adds-boxing-oculus-quest"/>
    <id>https://www.theverge.com/2021/10/26/22746070/supernatural-vr-workout-app-adds-boxing-oculus-quest</id>
    <author>
      <name>Alex Cranz</name>
    </author>
  </entry>
  <entry>
    <published>2021-10-26T09:31:39-04:00</published>
    <updated>2021-10-26T09:31:39-04:00</updated>
    <title>General Motors wants to help build 40,000 electric vehicle chargers in the US</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/qnIWmSwfK2BEUd_A9SAFyYk3ZzQ=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/70046707/ahawkins_20210503_4551_0002.0.jpg" /&gt;
        &lt;figcaption&gt;Photo by Andrew Hawkins / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="eciGy2"&gt;General Motors will work with its network of dealers to help spur the installation of 40,000 electric vehicle chargers in the US, the company announced today. It will also begin selling its own “Ultium” branded Level 2 chargers for home or business uses. &lt;/p&gt;
&lt;p id="XQSoSt"&gt;GM said that it hopes the new project will spur the creation of EV charging stations in urban and rural underserved areas. The automaker will give each of its dealers “up to 10 Ultium Level 2 destination charging stations” to install throughout their communities, but the automaker does not plan on paying for all 40,000 chargers. Rather, it will work with its dealers and community leaders to find the appropriate financing. &lt;/p&gt;
&lt;p id="8oAzBs"&gt;Unlike Tesla or Volkswagen, GM does not own its own EV charging network. Owners of GM’s electric vehicles must instead rely on a patchwork of third-party chargers, each with their own software and membership requirements.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="qfi44Z"&gt;&lt;q&gt;The automaker will give each of its dealers up to 10 EV chargers&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="lGV8ii"&gt;The dealer-community project, which kicks off in 2022, is part of GM’s $750 million commitment to EV charging infrastructure, which it announced during its recent Investor’s Day event. While it’s not the same kind of financial commitment as Tesla or Volkswagen —both of which have spent billions of dollars to create a nationwide network of public EV chargers — it does signal that GM is thinking holistically about EV ownership. &lt;/p&gt;
&lt;p id="IeDl7x"&gt;“When you think about the hill that we have to climb for customers to access charging in all communities, not just the ones that have been early adopters, we need a lot,” said Alex Keros, lead architect of EV infrastructure at GM. &lt;/p&gt;
&lt;p id="viljQc"&gt;There are approximately 41,000 public charging stations in the United States, with more than 100,000 outlets. But finding one that actually works or isn’t locked inside a gated parking garage can be a bit of a scavenger hunt. The charging experience in the US is almost &lt;a href="https://www.theverge.com/22419150/ev-charging-us-joe-biden-infrastructure-plan"&gt;comically fragmented&lt;/a&gt;, especially for non-Tesla owners. While Tesla’s Supercharger network has been praised for its seamless user experience and fast charging ability, the opposite appears to be true for pretty much everyone else.&lt;/p&gt;
&lt;aside id="CB0Lqk"&gt;&lt;div data-anthem-component="readmore" data-anthem-component-data='{"stories":[{"title":"EV charging in the US is broken — can Joe Biden fix it?","url":"https://www.theverge.com/22419150/ev-charging-us-joe-biden-infrastructure-plan"}]}'&gt;&lt;/div&gt;&lt;/aside&gt;&lt;p id="wCA9hE"&gt;Level 2 chargers use a 240-volt connection and can add between 12 and 37 miles of range per hour, &lt;a href="https://www.chargepoint.com/resources/how-choose-home-ev-charger/"&gt;according to ChargePoint&lt;/a&gt;. Installation of a Level 2 charger can cost upwards of several thousand dollars, including labor. Keros said the company will help GM dealers and local partners find the right financing. &lt;/p&gt;
&lt;p id="R60HaR"&gt;“In the dealer communities, we envision that it would be a collaboration, mostly between the dealer and the community partners that we would be working with,” he said. “But we also we recognize there’s a lot of variables. So we will work with the dealers and the community members to make sure, for example, if there’s grants or other types of things, that we’re trying to be as helpful as possible.”&lt;/p&gt;
&lt;p id="KcfrZr"&gt;GM isn’t planning on installing any DC Fast Chargers, which charge an EV’s battery much faster than a Level 2 charger but also wears the battery down faster too. The automaker has &lt;a href="https://www.theverge.com/2020/7/31/21349614/general-motors-evgo-fast-charging-network-investment"&gt;a partnership with charging company EVgo&lt;/a&gt; to make its DCFC stations available to GM customers. &lt;/p&gt;
&lt;p id="WKXt78"&gt;President Joe Biden recently introduced a $2 trillion infrastructure plan, &lt;a href="https://www.whitehouse.gov/briefing-room/statements-releases/2021/04/22/fact-sheet-biden-administration-advances-electric-vehicle-charging-infrastructure/"&gt;$174 billion of which is earmarked for electric cars&lt;/a&gt;. As part of that plan, &lt;a href="https://www.theverge.com/2021/1/25/22249237/biden-electric-vehicle-government-fleet-ev"&gt;Biden wants to build 500,000 new EV chargers by 2030&lt;/a&gt; — 11 times the current number of stations. That number is likely to be reduced as the legislation is being negotiated in Congress. &lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/ktuAeqjbd4I3BEA4zY9TPS5w3dg=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22955223/Ultium_Standard_Wall_Mounted_Charger_Wlogo1.jpg"&gt;
  &lt;/figure&gt;
&lt;p id="okhMm4"&gt;GM recently announced a new EV charging brand, &lt;a href="https://www.theverge.com/2021/4/28/22407789/gm-ev-charging-general-motors-ultium-charge-360"&gt;Ultium Charge 360&lt;/a&gt;, which will integrate GM’s own vehicle apps and software with a variety of third-party charging services, such as Blink, ChargePoint, EVgo, Flo, Greenlots, and SemaConnect. The company will also sell its own Ultium Charge 360-branded Level 2 chargers for home or business, Keros said. &lt;/p&gt;
&lt;p id="ouqKnz"&gt;GM will have three different chargers with two distinct power outputs: an 11.5kWh/48-amp smart charger, as well as a premium version with a customizable touchscreen and an embedded camera; and a 19.2kWh/48-amp premium charger. &lt;/p&gt;
&lt;p id="Sg6Ece"&gt;Each charging unit comes with WiFi and Bluetooth, as well as dynamic load balancing to handle multiple vehicles at once. GM is working with Ctek, a Swedish EV charging company, on the production of the Level 2 chargers. GM did not release pricing information for its Ultium chargers. &lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/10/26/22745575/gm-ev-charging-dealers-ultium-360-charge"/>
    <id>https://www.theverge.com/2021/10/26/22745575/gm-ev-charging-dealers-ultium-360-charge</id>
    <author>
      <name>Andrew J. Hawkins</name>
    </author>
  </entry>
  <entry>
    <published>2021-10-26T09:28:27-04:00</published>
    <updated>2021-10-26T09:28:27-04:00</updated>
    <title>YouTube gets serious about podcasting</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/-FFPKhg2jDb_7RWmEJjeom2mmGw=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/70046638/acastro_180403_1777_youtube_0001.0.jpg" /&gt;
        &lt;figcaption&gt;Illustration by Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="9jJYx8"&gt;Hello, hello, and welcome back to the workweek. I hope everyone had a lovely weekend, and shout out to those of you who attended On The Verge. It was so nice to see people IRL and actually meet face to face. I hope we can do it again soon! Anyway, today’s a busy one with a couple scoops and analyses. Off we go.&lt;/p&gt;
&lt;hr class="p-entry-hr" id="bInPpi"&gt;
&lt;p id="wF2z4W"&gt;&lt;strong&gt;SCOOP: Amazon is building a top-secret live audio app&lt;/strong&gt;&lt;/p&gt;
&lt;p id="d80Snq"&gt;Earlier today, I &lt;a href="https://www.theverge.com/e/22508626"&gt;published a story&lt;/a&gt; about Amazon’s work on a live audio app, codenamed Project Mic. The app takes bits from Anchor and bits from Clubhouse — users can host DJ sets, like they would a radio show, and intersperse songs with talk. The idea is to reinvent radio. (I feel like I should have put a trigger warning on that sentence for some of you — sorry!) &lt;/p&gt;
&lt;p id="nQUZ4G"&gt;The other highlights from my piece: the app will simultaneously stream content to Twitch, Amazon Music, and Alexa-equipped devices. Amazon is looking to bring A-list talent on board to help launch the app, along with regular people to keep the programming constant, and the app will be interactive, particularly through Alexa. The app will also be optimized for the car, whatever that means. Amazon didn’t comment on my story.&lt;/p&gt;
&lt;p id="wVnylN"&gt;I’m mostly thinking about this in relation to the flurry of tech companies now creating and distributing radio station-like content — Apple Music, Sonos, Spotify, and, soon, Amazon. Apple and Spotify, in particular, are thinking about how their platforms can dominate the car either through car maker partnerships or hardware, and so is Amazon, which will be critical if they want to eventually usurp radio. I own a 2009 Honda Fit with an aux cable, and personally, the ease of turning on the radio is something I’d struggle to give up. I currently can listen through Spotify or any of these other apps, but instead, pressing the power button and clicking through my presets is my default. That’s hard behavior to change! We’ll see if Amazon or anyone else can pull it off.&lt;/p&gt;
&lt;p id="pHya3W"&gt;&lt;strong&gt;SCOOP: YouTube finally has someone leading podcasting efforts&lt;/strong&gt;&lt;/p&gt;
&lt;p id="dg64rY"&gt;Another one exclusively for you &lt;em&gt;Hot Pod&lt;/em&gt; readers: Google has hired someone to lead its podcasting efforts. A company spokesperson confirms to me that Kai Chuk has been hired to “manage the large volume of existing podcasts and relationships across the YouTube platform.” I’ve heard Chuk’s title is Podcast Lead, but Google did not respond to my request for confirmation on that. Per Chuk’s LinkedIn, he’s been at YouTube for nearly 10 years, focusing mostly on media partnerships, so I can see why the podcasting role was a fit. &lt;/p&gt;
&lt;p id="XzwXlk"&gt;We here at &lt;em&gt;Hot Pod&lt;/em&gt; have been following YouTube’s move into podcasting, or at least it’s angling to take on a stronger role in the industry.&lt;em&gt; Bloomberg&lt;/em&gt; &lt;a href="https://www.bloomberg.com/news/articles/2021-10-05/youtube-looks-to-hire-its-first-executive-focused-on-podcasts?sref=ExbtjcSG"&gt;reported&lt;/a&gt; the YouTube team was looking to hire a podcast executive, which it seems is Chuk’s role, while the YouTube Music app in Canada is going to support background music listening for free users. The pieces are lining up, and we’ll presumably be hearing more.&lt;/p&gt;
&lt;p id="ZX4aDR"&gt;The thing I don’t fully understand about YouTube’s possible podcast plans is what exactly will be changed or managed — podcasters already upload and host videos on Google’s servers, and Google monetizes those videos for them based on user data. Yes, YouTube could also start hosting audio and doing different ad things there, but I once again have to ask whether podcasters want to either fully migrate hosting providers or start using many at the same time just to take advantage of the platforms. (YouTube also &lt;a href="https://blog.google/products/ads-commerce/youtube-music-audio-ads/"&gt;already sells&lt;/a&gt; audio advertising for when listeners are playing videos in the background.)&lt;/p&gt;
&lt;p id="Pg03AO"&gt;As for the listeners themselves, in 2020, Google &lt;a href="https://www.thinkwithgoogle.com/marketing-strategies/video/music-industry-changes/"&gt;said&lt;/a&gt; 15 percent of people listening to music on the platform did so with it on in the background, so I wonder how greatly that percentage might increase when YouTube starts allowing background listening for free.&lt;/p&gt;
&lt;p id="QPhjSh"&gt;Speaking of ads, we’ve got some deals to cover.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="hucIzC"&gt;&lt;div data-anthem-component="newsletter" data-anthem-component-data='{"slug":"podcasts"}'&gt;&lt;/div&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="rG6t0Q"&gt;&lt;strong&gt;SiriusXM will be the exclusive ad seller for &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;Crime Junkie&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; and Audiochuck&lt;/strong&gt;&lt;/p&gt;
&lt;p id="tO8TYZ"&gt;This is just breaking now: SiriusXM is partnering with Audiochuck, Ashley Flowers’ podcast company and maker of &lt;em&gt;Crime Junkie&lt;/em&gt;, as its exclusive global ad seller. Along with that, Audiochuck and Sirius will coordinate on developing new programming. The press release notes that all programming will continue to be widely available and not exclusive to any Sirius-owned property. (UTA brokered the deal, representing Flowers and Audiochuck.)&lt;/p&gt;
&lt;p id="ntQutX"&gt;“With three audio platforms as marketing vehicles that can reach the largest and most diverse audiences possible, [Audiochuck] can serve their most loyal fans and create new ones,” says Scott Greenstein, president and chief content officer at SiriusXM, in the release. &lt;/p&gt;
&lt;p id="gKOvuL"&gt;I’m flagging that quote because he notes marketing as an apparent selling point for Flowers’ decision to partner — Sirius owns its satellite radio business, of course, as well as Stitcher and Pandora, where Audiochuck shows will presumably get some love.&lt;/p&gt;
&lt;p id="ZOu6wR"&gt;Audiochuck &lt;a href="https://adlarge.com/post/press-adlarge-extends-partnership-with-top-podcast-crime-junkie-and-audiochuck-network"&gt;previously partnered&lt;/a&gt; with Cabana and the team at AdLarge Media, which, notably, doesn’t own a podcast player.&lt;/p&gt;
&lt;p id="OHgL14"&gt;On to the next one...&lt;/p&gt;
&lt;p id="T3KlzX"&gt;&lt;strong&gt;Acast and BBC re-up their monetization deal&lt;/strong&gt;&lt;/p&gt;
&lt;p id="FWTKcS"&gt;Acast and the BBC &lt;a href="https://investors.acast.com/press-releases/acast-and-bbc-renew-international-podcast-deal"&gt;are renewing&lt;/a&gt; a deal that gives Acast the exclusive rights to sell ads for BBC shows outside the UK. So the BBC has the UK covered, but everywhere else is fair game for Acast for the next three years. This seems convenient enough — the BBC doesn’t have to worry about staffing up and trying to sell ads for the global market but still benefits from its massive catalog and name recognition. The fight for ad dollars keeps on marching on...&lt;/p&gt;
&lt;p id="Cn5EbW"&gt;&lt;strong&gt;Your Kroger grocery store data is helping launch a programmatic ad marketplace&lt;/strong&gt;&lt;/p&gt;
&lt;p id="jAuEJG"&gt;This is arguably not podcast- or audio-related, at least not yet, but I had to share because, geez, grocery stores vacuuming up all the data to launch an ad network makes me sad! Last week, Kroger &lt;a href="https://go.krogerprecisionmarketing.com/privatemarketplace?utm_source=8451.com&amp;amp;utm_medium=pressrelease&amp;amp;utm_term=privatemarketplace&amp;amp;utm_content=pmplaunch&amp;amp;utm_campaign=privatemarketplace"&gt;announced&lt;/a&gt; its plan to launch a private programmatic ad marketplace where brands can buy ads and target them to consumers based on their purchasing habits. Kroger collects this info through its loyalty program and says it captures data on 96 percent of sales. Right now, it seems this marketplace is specifically for display advertising, but how long until a tech company tries buying up or using this type of data for podcast advertising? (Ehem, Amazon and Whole Foods.)&lt;/p&gt;
&lt;p id="IHNAJs"&gt;Okay, I’ll end this one on a small note...&lt;/p&gt;
&lt;p id="SoTU5x"&gt;&lt;strong&gt;Clubhouse launches pinned links, hints at monetization features &lt;/strong&gt;&lt;/p&gt;
&lt;p id="TlB3VM"&gt;I &lt;a href="https://twitter.com/verge/status/1451971798476673028?s=20"&gt;interviewed&lt;/a&gt; Paul Davison, Clubhouse’s CEO and co-founder, and Maya Watson, Clubhouse’s head of global marketing, over the weekend at the On The Verge event, and they announced a new feature on stage: pinned links. &lt;/p&gt;
&lt;p id="LbPNsa"&gt;Davison is billing this as a way for creators to monetize — they can now link out to their podcast, book landing page, Patreon, and so on — but it doesn’t yet help Clubhouse generate cash. Davison told me this is why the company raised private money, so they don’t have to worry about it. He also said we’d likely hear more about ways in which the platform itself will monetize, like through subscriptions and ticketed events, in the coming months. Now, for what it’s worth, Twitter already allows people to pin tweets to Spaces, and also, for what it’s worth, OnlyFans links aren’t allowed on Clubhouse. Will this feature move the needle for Clubhouse? I don’t think so.&lt;/p&gt;
&lt;hr class="p-entry-hr" id="MruayH"&gt;
&lt;p id="Dbw1AK"&gt;Okay, we’re in and out today, and I’ll leave you to catch up on the motherload of Facebook Papers content you have yet to read. As always, my DMs and email are open and accepting messages, as is Signal, if you ask nicely. Catch ya Thursday!&lt;/p&gt;
&lt;p id="cETik7"&gt;&lt;/p&gt;
&lt;p id="pciNI1"&gt;&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/10/26/22746466/youtube-podcasting-kai-chuk-amazon-project-mic-live-audio-hot-pod"/>
    <id>https://www.theverge.com/2021/10/26/22746466/youtube-podcasting-kai-chuk-amazon-project-mic-live-audio-hot-pod</id>
    <author>
      <name>Ashley Carman</name>
    </author>
  </entry>
  <entry>
    <published>2021-10-26T09:16:46-04:00</published>
    <updated>2021-10-26T09:16:46-04:00</updated>
    <title>How to get up-close photos with the iPhone 13 Pro’s hidden macro mode</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/tL1aGtV29riaCTMbo4h3mOG4H-c=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/70046601/vpavic_210916_untitled_0016.0.jpg" /&gt;
        &lt;figcaption&gt;Photo by Vjeran Pavic / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="A0o0mE"&gt;The &lt;a href="https://www.theverge.com/22684033/apple-iphone-13-pro-max-review"&gt;iPhone 13 Pro and Pro Max&lt;/a&gt; have great cameras, some of the best you can buy in a phone. They’ve also got an exclusive new feature Apple never offered before: a macro mode that lets you take extreme close-ups — just two centimeters away! — of intricate things you can barely see with the naked eye. &lt;a href="https://cdn.vox-cdn.com/uploads/chorus_asset/file/22864666/pro_IMG_022307.jpg"&gt;Your pet’s incredibly fine fur&lt;/a&gt;, the veins of leaves, and the sub-pixels in your computer monitor’s screen are all now within reach. &lt;/p&gt;
&lt;p id="A2YHZH"&gt;But Apple doesn’t give you a macro mode &lt;em&gt;button &lt;/em&gt;per se — it’s automatic. When you bring the phone within 10 cm of an object, it automatically switches to the ultrawide lens, which some users found jarring enough that Apple now lets you turn off auto-macro-switching in iOS 15.1. &lt;/p&gt;
&lt;p id="XUPRWh"&gt;But if you do that, and then manually switch to the ultrawide lens, you won’t get as close by default. Here’s a full-res example:&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/F3khy20uVZbOqn02gdw5R1DcbTo=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22955490/IMG_E0358.JPG"&gt;
      &lt;cite&gt;Photo by Sean Hollister / The Verge&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;iPhone 13 Mini screen, shot with the iPhone 13 Pro’s macro mode.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/e8Zy9PyIdAXCsdMEKkYqVXLW7j4=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22955489/IMG_E0355.JPG"&gt;
      &lt;cite&gt;Photo by Sean Hollister / The Verge&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Without the macro mode.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="xNdp4F"&gt;So how do you actually use macro mode?&lt;/p&gt;
&lt;h2 id="dNDgsl"&gt;How to use the iPhone 13 Pro’s macro mode&lt;/h2&gt;
&lt;p id="3Di0jk"&gt;Again, it’s automatic. You don’t need to look for a button, you just need to get close. Open the camera app and get very, very close to the thing you want to photograph — so close that it gets blurry. Then slowly back away until it looks crisp. If you want to be extra sure, pull back just far enough that the autofocus (yellow square) kicks in. &lt;/p&gt;
&lt;p id="hSdbFI"&gt;Hold incredibly still at that distance (two hands are good, braced is better, tripod is probably best!) and snap the shot. If you’re shooting handheld, you may want to take a few more so you can pick the clearest one — at that distance, any amount of motion might result in blur. &lt;/p&gt;
&lt;p id="3656IC"&gt;That’s it! Unless you’ve turned off the phone’s auto-macro mode, of course.&lt;/p&gt;
&lt;h2 id="8YXzwa"&gt;How to turn off (and on) the iPhone 13 Pro’s automatic lens swap&lt;/h2&gt;
&lt;div&gt;  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/2teQaBjs122XDXPidrRti-jFqH4=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22955484/iphone_13_pro_auto_macro.jpg"&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p id="n2Q8Ei"&gt;Hate how your iPhone 13 Pro or Pro Max automatically switches away from the (very good) normal lens when you get too close? You can now toggle it on and off as of iOS 15.1. &lt;/p&gt;
&lt;p id="STiEF6"&gt;Open the &lt;strong&gt;Settings&lt;/strong&gt; app, go to &lt;strong&gt;Camera&lt;/strong&gt;, then scroll all the way to the bottom&lt;strong&gt; &lt;/strong&gt;to find &lt;strong&gt;Auto Macro&lt;/strong&gt;. Toggle that off to disable the swap. &lt;/p&gt;
&lt;h2 id="cmKM4V"&gt;How to manually take macro photos with iPhone 13 Pro&lt;/h2&gt;
&lt;p id="kTRtFP"&gt;Hopefully, Apple will add a manual button soon, the same way you can manually jump to the ultrawide camera by tapping “0.5” or the zoom lens by tapping “3” (unless it’s too dark). Meanwhile, you can still take macro photos with Auto Macro toggled off — they just won’t necessarily be as close. &lt;/p&gt;
&lt;p id="XHRKRJ"&gt;&lt;strong&gt;Switch to your ultrawide lens&lt;/strong&gt; and follow the same instructions as earlier: get close, back away, hold still. Autofocus will still kick in, just not quite as close as it does with the automatic macro mode.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/oP70efVkA506BbQYFwjAAkAa3WU=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22955496/IMG_0371.JPG"&gt;
      &lt;figcaption&gt;&lt;em&gt;Again, here’s the ultrawide lens without auto-macro...&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/-eKwk2s5hxaenXRLGKQD376bRGo=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22955497/IMG_0374.JPG"&gt;
      &lt;figcaption&gt;&lt;em&gt;...and here’s how close you get with auto-macro on.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="x8YPuG"&gt;But you can also trick the iPhone 13 Pro and Pro Max into giving you nearly as good a result if you just zoom in yourself. Apple tells &lt;em&gt;The Verge&lt;/em&gt; that its auto-macro mode is effectively cropping out 3 megapixels’ worth of the 12-megapixel image, which is then upsampled to 12-megapixels once again, with a little bit of additional processing on top. So if you want to emulate that same jump from a 13mm-equivalent field of view to a 26mm-equivalent field of view, &lt;strong&gt;just hold down on the 0.5x button and drag the zoom wheel to about 0.9x magnification. &lt;/strong&gt;&lt;/p&gt;
&lt;p id="Q0hTIg"&gt;Here are three photos of a bamboo coaster I picked up in Maui with wood-burned letters, shot the closest I possibly could. The first is an auto-macro shot, the second a 0.5x shot that I cropped to roughly 0.9x on my Windows PC, and the third shot at 0.9x magnification on the iPhone itself. They’re almost the same, right? I think the auto-macro looks the sharpest with Apple’s processing on top, but none of them are great and all of them are fine.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/VeLErtC7kME4Ks5k-PKkOME0I-w=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22955973/IMG_0399.JPG"&gt;
      &lt;figcaption&gt;&lt;em&gt;Auto-macro.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/Wj8O61S-m1qD5ykE4TZdg-edWsY=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22955974/IMG_0400_crop.jpg"&gt;
      &lt;figcaption&gt;&lt;em&gt;Manually cropped on a PC.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/4gKeJnGFHqMmYm7-SAaXnbKow-8=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22955975/IMG_0402.JPG"&gt;
      &lt;figcaption&gt;&lt;em&gt;Shot at 0.9x crop on the iPhone 13 Pro itself.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="trQIjX"&gt;So now you know. If you take any particularly epic macro shots with a phone, &lt;a href="https://twitter.com/StarFire2258"&gt;hit me up on Twitter&lt;/a&gt;? &lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/22745578/iphone-13-pro-macro-mode-how-to"/>
    <id>https://www.theverge.com/22745578/iphone-13-pro-macro-mode-how-to</id>
    <author>
      <name>Sean Hollister</name>
    </author>
  </entry>
  <entry>
    <published>2021-10-26T09:00:00-04:00</published>
    <updated>2021-10-26T09:00:00-04:00</updated>
    <title>Adobe brings a simplified Photoshop to the web</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/seCPNjktkykmDBmzi9VD63tEO5g=/40x0:1240x800/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/70046433/Photoshop_on_the_web_Edititing.0.png" /&gt;
        &lt;figcaption&gt;The beta of Photoshop on the web. | Image: Adobe&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="dQ8l60"&gt;Adobe is bringing Photoshop and Illustrator to the web, letting you make changes to documents hosted in the cloud without having to download them and open up the app.&lt;/p&gt;
&lt;p id="tNNVw6"&gt;It’s a fascinating step forward for these two programs, but it’s also a very small one for now: these are not fully featured versions of Photoshop and Illustrator — or anywhere close to it. You can navigate layers, leave annotations and comments, and make basic edits using tools like the eraser, spot healing brush, and selection lasso. But you’ll still have to open the app for any substantial changes.&lt;/p&gt;
&lt;p id="L6QXLb"&gt;“We’re not bringing all the features on day one, but we really want to unlock all those basic edits that are just best done now in the browser with whoever you’re working with,” Scott Belsky, Adobe’s chief product officer, &lt;a href="https://www.theverge.com/e/22504483"&gt;said on our podcast &lt;em&gt;Decoder&lt;/em&gt;&lt;/a&gt;. Belsky describes the web version of Photoshop as offering a “light level of editing” that works with “real PSD” files.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt="A simplified Illustrator interface with tools on the left, an image in the center, and an expanded properties pane on the right. The app is inside a web browser." data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/eYCVRR4YrGPao7l1IgDTNJi023A=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22948110/Illustrator_on_the_web_Hero_Image.png"&gt;
      &lt;cite&gt;Image: Adobe&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Adobe Illustrator on the web.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="ksiYtn"&gt;The features play into one of the big themes of &lt;a href="https://blog.adobe.com/en/publish/2021/10/26/photoshop-ships-major-updates-across-desktop-ipad-apps-extends-light-editing-collaboration-features-web-beta.html"&gt;Adobe’s announcements&lt;/a&gt; today: making it easier for people to collaborate across its Creative Cloud services. The goal with Photoshop and Illustrator on the web is to make it easier for people you’re sharing files with for review — a client, an editor, a friend — to work with you on adjustments. Previously, they’d be able to make annotations and comments on the document. But now, if they’re given permission, they’ll also be able to jump in and make some basic changes, too.&lt;/p&gt;
&lt;p id="tvkjfr"&gt;Adobe is also adding a panel to desktop Photoshop to review comments that people have left. And it’s &lt;a href="https://www.theverge.com/e/22502564"&gt;adding a new hub on its website&lt;/a&gt; to let teams organize assets and create collaborative mood boards.&lt;/p&gt;
&lt;p id="G6o7qz"&gt;You’ll have to be a Creative Cloud subscriber to use Photoshop on the web, which will be available as a beta starting today. Illustrator for web is launching as an invite-only beta, &lt;a href="https://pages.adobe.com/creativecloud/en/collaboration/private-beta"&gt;accepting signups starting today&lt;/a&gt;.&lt;/p&gt;
&lt;p id="gC2DKU"&gt;A handful of other new features are coming to Photoshop, too. The app’s object selection tool is getting even more powerful, showing exactly what it can automatically highlight as you hover over objects within your scene. There are also new neural filters: a landscape mixer lets you remix your scene with a different setting or season; and new color transfer and harmonization filters let you apply the look of one image or layer to another. Adobe also says it’s improved last year’s Depth Blur filter (basically Photoshop’s version of portrait modes) to create a “more natural blurred background.”&lt;/p&gt;
&lt;p id="fI5G31"&gt;&lt;/p&gt;
&lt;div id="LaAfoT"&gt;&lt;iframe frameborder="0" height="200" scrolling="no" src="https://playlist.megaphone.fm?e=VMP3851352140" width="100%"&gt;&lt;/iframe&gt;&lt;/div&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/10/26/22738125/adobe-photoshop-illustrator-web-announced"/>
    <id>https://www.theverge.com/2021/10/26/22738125/adobe-photoshop-illustrator-web-announced</id>
    <author>
      <name>Jacob Kastrenakes</name>
    </author>
  </entry>
  <entry>
    <published>2021-10-26T09:00:00-04:00</published>
    <updated>2021-10-26T09:00:00-04:00</updated>
    <title>Adobe is adding a collaborative mood board to Creative Cloud</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="A web interface with a series of images neatly placed on a gray background." src="https://cdn.vox-cdn.com/thumbor/lLLaWsTjiMlfP4i04C5Yn4Zsopc=/82x0:1918x1224/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/70046437/media_159b13f6215d3c6ddfb408b50f83ebdc6f78da7a0.0.jpeg" /&gt;
        &lt;figcaption&gt;Creative Cloud Canvas is Adobe’s answer to services like Miro. | Image: Adobe&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="cazlab"&gt;Adobe is trying to make Creative Cloud’s website into more of a hub for collaborating across teams. So today, it’s &lt;a href="https://blog.adobe.com/en/publish/2021/10/26/creative-cloud-canvas-spaces-ps-ai-in-browser.html"&gt;announcing a few new tools&lt;/a&gt; headed to the platform: &lt;a href="https://www.theverge.com/e/22502166"&gt;basic web-based versions of Photoshop and Illustrator&lt;/a&gt;, a new feature called Canvas that lets you make mood boards, and a feature called Spaces that lets teams arrange and synchronize assets for projects.&lt;/p&gt;
&lt;p id="slgOV9"&gt;Canvas is similar to a bunch of tools that designers already use — there’s Miro, PureRef, you could even do something like this in Figma if you wanted to — but it comes with the perk of being integrated with Adobe’s ecosystem. You can pull Cloud documents onto a canvas, and they’ll link back to the original file, letting you open them up to make changes.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt="A web interface with navigation on the left and assets displayed in the main pane. It resembles Google Drive." data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/xtlo8iaBpxZyHj5XvP14cfSU0nI=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22945371/media_1f2c99845fc9ac79d5b20a654eb667f70bce06ab9.jpg"&gt;
      &lt;cite&gt;Image: Adobe&lt;/cite&gt;
      &lt;figcaption&gt;Creative Cloud Spaces lets users organize shared assets.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="EwxHk7"&gt;It also ties into Spaces, which is Adobe’s new interface for organizing files across teams. Creative Cloud’s website already serves as sort of a bare-bones take on Google Drive but for all your cloud PSDs and whatnot. Now, Adobe’s adding a way to group your stuff by teams and by projects, synchronized for everyone involved. Spaces can include Canvas files, asset libraries, and cloud files. For teams that are primarily using Adobe apps, it’s now even easier to never leave them.&lt;/p&gt;
&lt;p id="fzDuWZ"&gt;Canvas and Spaces won’t be available right away. Adobe plans to roll them out to everyone next year, but for now, they’re available in a limited beta.&lt;/p&gt;
&lt;p id="CqS6jh"&gt;The updates play into two of Adobe’s broader goals: making it easier to collaborate and owning the entire software chain so that no one ever has to leave Adobe’s apps. Adobe recently completed its acquisition of Frame.io, a popular web-based video collaboration tool; while it doesn’t have news to share about that service yet, you can imagine that playing a big role in this mission, too.&lt;/p&gt;
&lt;p id="8ByL5P"&gt;&lt;/p&gt;
&lt;div id="2oGkaK"&gt;&lt;iframe frameborder="0" height="200" scrolling="no" src="https://playlist.megaphone.fm?e=VMP3851352140" width="100%"&gt;&lt;/iframe&gt;&lt;/div&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/10/26/22738523/creative-cloud-canvas-spaces-adobe-announces"/>
    <id>https://www.theverge.com/2021/10/26/22738523/creative-cloud-canvas-spaces-adobe-announces</id>
    <author>
      <name>Jacob Kastrenakes</name>
    </author>
  </entry>
  <entry>
    <published>2021-10-26T09:00:00-04:00</published>
    <updated>2021-10-26T09:00:00-04:00</updated>
    <title>Oura adds period prediction and heart rate to its next-gen smart ring  </title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="Inside of the Oura ring, showing red and green LED lights." src="https://cdn.vox-cdn.com/thumbor/WsZBoCOS4PkIhevSbmBoUnsDgPc=/0x89:600x489/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/70046441/HERO_Oura_Ring_Gen3___LEDs_Zoomed.0.png" /&gt;
        &lt;figcaption&gt;Oura&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="1mQgLv"&gt;Oura’s Generation 3 smart ring will have a whole new slate of features, including period prediction, blood oxygen monitoring, and real-time heart rate tracking. The new ring will jump from three to seven temperature sensors and add a pulse oxygen sensor, Oura CEO Harpreet Rai told &lt;em&gt;The Verge&lt;/em&gt;. &lt;/p&gt;
&lt;p id="J7hI49"&gt;A new, green LED light will monitor heart rate throughout the day. Starting at the end of 2021, users will also be able to record their heart rate during exercise and see information about heart rate recovery after a workout is done. &lt;/p&gt;
&lt;p id="nXafAU"&gt;The upgrades underscore the company’s focus on health, Rai says, particularly the period prediction feature. The new ring will use shifts in temperature and user feedback to predict when a user might get their period up to 30 days in advance. Body temperature changes through the menstrual cycle, rising just before ovulation and falling as mensuration begins. &lt;/p&gt;
&lt;p id="eczwwS"&gt;Rai says period prediction is only the beginning of the company’s interest in menstruation and fertility. “It’s an underinvested area in wearables,” he says. &lt;a href="https://www.medrxiv.org/content/10.1101/2021.08.19.21262306v1.full.pdf"&gt;A study conducted&lt;/a&gt; in partnership with researchers at the University of California San Diego showed that the Oura ring can use temperature changes to identify pregnancy around nine days before an at-home pregnancy test, though pregnancy prediction is not part of the device at this point. &lt;/p&gt;
&lt;p id="UaE9T5"&gt;There’s already some precedent for the ring to be used in predicting periods. The FDA had already given the digital birth control company Natural Cycles clearance to use the Oura Ring to collect temperature data. Natural Cycles has an algorithm that integrates daily temperature readings and cycle tracking to tell users which days they’re most likely to get pregnant. Initially, users had to take their temperature using a thermometer, but the FDA signed off on the app’s integration with the Oura ring &lt;a href="https://www.theverge.com/2021/7/8/22568421/natural-cycles-birth-control-app-oura-ring-fda"&gt;in July&lt;/a&gt;.&lt;/p&gt;
&lt;p id="H9UpPQ"&gt;Oura did not have an active partnership with Natural Cycles, Rai says — Natural Cycles used Oura’s API to access data from the rings. Rai declined to comment on whether Oura was made aware of Natural Cycle’s work with the ring or whether the companies have spoken since the FDA’s clearance. &lt;/p&gt;
&lt;p id="AGf7BC"&gt;He did say, though, that the integration is a good proof of concept for the device. “It does honestly validate our technology and even the finger as being a really interesting place for some of these features,” Rai says. “That’s good for us and good for the ecosystem.” &lt;/p&gt;
&lt;p id="lCIMRa"&gt;With the new ring, Oura joins devices like the Apple Watch and Withings ScanWatch in offering blood oxygen monitoring. That feature will launch for Oura users in 2022. Oxygen monitoring is appealing to athletes, Rai says, and also has medical implications going forward — like with sleep apnea. “It doesn’t necessarily mean that we have to predict, diagnose, and treat on day one, but over time, does this mean potentially we can start saying, ‘Hey, something’s off, you may want to go see a doctor?’” he says. “I think there’s a way to do that.” It may take a similar form to the company’s approach to &lt;a href="https://www.theverge.com/2021/6/15/22535204/apple-watch-fitbit-oura-wearables-illness-prediction"&gt;illness prediction&lt;/a&gt;, which treats changes in temperature or other metrics that are changing in the body as “warning lights” rather than diagnostics, Rai says. &lt;/p&gt;
&lt;p id="SfrgH8"&gt;Oura is also moving to a membership model for its app, priced at $5.99 a month. Users who upgrade from an older ring to the new model will get a discount on the hardware and a free lifetime membership. &lt;/p&gt;
&lt;p id="aVuwb8"&gt;The Generation 3 ring will start shipping November 15th. &lt;/p&gt;
&lt;p id="X4EktZ"&gt;&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/10/26/22740187/oura-period-prediction-heart-rate-smart-ring"/>
    <id>https://www.theverge.com/2021/10/26/22740187/oura-period-prediction-heart-rate-smart-ring</id>
    <author>
      <name>Nicole Wetsman</name>
    </author>
  </entry>
  <entry>
    <published>2021-10-26T09:00:00-04:00</published>
    <updated>2021-10-26T09:00:00-04:00</updated>
    <title>Photoshop will get a ‘prepare as NFT’ option soon</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/lI_DP1QqJ0ifsh-FK-vds9R42lM=/0x13:1440x973/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/70046430/media_1dd91c848c1e0ed127468c10d0bd0646b4c3455d3.0.jpg" /&gt;
        &lt;figcaption&gt;&lt;em&gt;The Content Credentials panel in Photoshop.&lt;/em&gt; | Image: Adobe&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="d2ifBe"&gt;Adobe is launching a system built into Photoshop that can, among other things, help prove that the person selling an NFT is the person who made it. It’s called Content Credentials, and NFT sellers will be able to link the Adobe ID with their crypto wallet, allowing compatible NFT marketplaces to show a sort of verified certificate proving the art’s source is authentic. &lt;/p&gt;
&lt;p id="ayDfm6"&gt;According to &lt;a href="https://www.theverge.com/e/22504483"&gt;a &lt;em&gt;Decoder&lt;/em&gt; interview with Adobe’s chief product officer&lt;/a&gt; Scott Belsky, this functionality will be built into Photoshop with a “prepare as NFT” option, launching in preview by the end of this month. Belsky says attribution data created by the Content Credentials will live on an IPFS system. IPFS (InterPlanetary File System) is &lt;a href="https://www.theverge.com/2021/3/25/22349242/nft-metadata-explained-art-crypto-urls-links-ipfs"&gt;a decentralized way to host files&lt;/a&gt; where a network of people are responsible for keeping data safe and available, rather than a single company (somewhat similar to how torrent systems work). Adobe says that NFT marketplaces like OpenSea, Rarible, KnownOrigin, and SuperRare will be able to integrate with Content Credentials to show Adobe’s attribution information.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;div id="pYejpz"&gt;&lt;div data-anthem-component="aside:10374749"&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p id="KOZiwL"&gt;Art theft has been a Big Deal in the NFT world. There have been many examples of people minting art &lt;a href="https://www.theverge.com/tldr/2021/3/15/22332357/nft-art-jason-rohrer-castle-doctrine-selling-friend-art"&gt;they didn’t create&lt;/a&gt; or &lt;a href="https://www.theverge.com/2021/3/20/22334527/nft-scams-artists-opensea-rarible-marble-cards-fraud-art"&gt;don’t have the rights to&lt;/a&gt; on the blockchain. The reason is that anyone can mint an NFT, even if they don’t own the copyright to the content, and there’s not really anything the blockchain can do to stop that. Worse, the minting is enshrined on the blockchain, making the NFT’s creation seem authentic if you’re unaware of the original work. &lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="8nO84j"&gt;&lt;q&gt;This system doesn’t make it harder to mint an NFT of media you don’t own the rights to, but it could make that NFT less attractive to the market&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="my1hwn"&gt;In other words, I could right-click on an existing image of an NFT and mint it again myself, potentially fooling unaware buyers. While Adobe’s system won’t prevent art theft, it does offer a way to prove that the NFT you’re selling isn’t stolen — past that, it’s up to buyers to decide how much value they place on that. &lt;/p&gt;
&lt;aside id="Vir0ZT"&gt;&lt;div data-anthem-component="readmore" data-anthem-component-data='{"stories":[{"title":"How many layers of copyright infringement are in Emily Ratajkowski’s new NFT?","url":"https://www.theverge.com/2021/4/24/22399790/emily-ratajkowski-nft-christies-copyright-nightmare-richard-prince"}]}'&gt;&lt;/div&gt;&lt;/aside&gt;&lt;p id="ngP5Z8"&gt;Even Banksy, who gets a mention in &lt;em&gt;Decoder&lt;/em&gt;, has been caught up by NFT scammers. One NFT collector (ironically named Pranksy) paid $300K for an NFT attributed to the famous street artist, &lt;a href="https://www.theverge.com/2021/8/31/22650594/banksy-nft-scam-pranksy-ethereum-returned-duplicates-art"&gt;which was almost definitely fake&lt;/a&gt;. He ended up getting the money back, but there wouldn’t have been as much of a fuss if Banksy had digitally signed the NFT. As Adobe’s Belsky points out, Banksy probably wouldn’t want to link his name and Adobe ID to a crypto wallet, but the system is meant to be open-source — it’s possible the anonymous artist could figure out some way to provide Content Credentials verified by &lt;a href="https://pestcontroloffice.com/faq.asp"&gt;the company in charge of authenticating his work&lt;/a&gt;. &lt;/p&gt;
&lt;p id="fCEczH"&gt;NFTs aren’t the only thing that will benefit from Adobe’s Content Credentials, which are a result of its &lt;a href="https://contentauthenticity.org"&gt;Content Authenticity Initiative&lt;/a&gt;. The company is launching the system as a beta, and users can use it to show what edits were made to a file in Photoshop, tag their stock images on Adobe’s system, and more.&lt;/p&gt;
&lt;p id="Vto8iH"&gt;To hear more about Adobe’s view on NFTs, the impact of certified attribution on art and NFTs, and Photoshop on the web, check out &lt;a href="https://www.theverge.com/e/22504483"&gt;this week’s episode of &lt;em&gt;Decoder&lt;/em&gt;&lt;/a&gt; and the rest of our coverage from Adobe’s Max conference.&lt;/p&gt;
&lt;p id="gGlQdW"&gt;&lt;/p&gt;
&lt;div id="3f8Ky9"&gt;&lt;iframe frameborder="0" height="200" scrolling="no" src="https://playlist.megaphone.fm?e=VMP3851352140" width="100%"&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;aside id="DQTKdG"&gt;&lt;div data-anthem-component="actionbox" data-anthem-component-data='{"title":"Decoder with Nilay Patel","description":"A new podcast from The Verge about big ideas and other problems.","label":"Subscribe now!","url":"https://podcasts.apple.com/us/podcast/decoder-with-nilay-patel/id1011668648?i=1000496212371"}'&gt;&lt;/div&gt;&lt;/aside&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/10/26/22745506/adobe-nft-art-theft-content-credentials-opensea-rarible-photoshop"/>
    <id>https://www.theverge.com/2021/10/26/22745506/adobe-nft-art-theft-content-credentials-opensea-rarible-photoshop</id>
    <author>
      <name>Mitchell Clark</name>
    </author>
  </entry>
  <entry>
    <published>2021-10-26T09:00:00-04:00</published>
    <updated>2021-10-26T09:00:00-04:00</updated>
    <title>Honor 50 launches globally for €529, complete with Google apps and services</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/RHLf93jMTEA7AHh8OLhKGZ84Wsw=/1x0:5515x3676/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/70046449/HONOR_50_All_compressed.0.jpg" /&gt;
        &lt;figcaption&gt;&lt;em&gt;The Honor 50.&lt;/em&gt; | Image: Honor&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="Gteckz"&gt;The Honor 50 is releasing globally with prices starting at €529, the former Huawei sub-brand announced today. The smartphone was originally announced for the &lt;a href="https://www.theverge.com/2021/6/16/22536512/honor-50-series-pro-release-date-news-features-google-mobile-services-apps-play-store"&gt;Chinese market in June&lt;/a&gt;, but today sees the company confirm its global pricing and its early November release date. For €529 (about $615), you get 6GB of RAM and 128GB of storage, and there’s also a €599 (about $695) model with 8GB of RAM and 256GB of storage. Honor says the phone will be available in over 40 countries, but there are no immediate plans to release the phone in the US. &lt;/p&gt;
&lt;p id="rRGlIZ"&gt;The big news with the Honor 50 is that it’s the company’s first device to ship with Google’s apps and services since &lt;a href="https://www.theverge.com/2020/11/16/21570744/huawei-honor-sale-announced-us-sanctions"&gt;splitting from former parent company Huawei&lt;/a&gt;. &lt;a href="https://www.theverge.com/2019/5/19/18631558/google-huawei-android-suspension"&gt;Google pulled Huawei’s Android license&lt;/a&gt; in 2019 &lt;a href="https://www.theverge.com/2019/5/15/18216988/white-house-huawei-china-equipment-ban-trump-executive-order"&gt;due to US sanctions&lt;/a&gt;, meaning its phones have lacked essential Android apps and services like the Google Play Store, severely limiting its appeal outside of China. The return of Google means that the Honor 50 could once again be competitive with other Android handsets in the West.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="7GDlwK"&gt;&lt;q&gt;The Honor 50 is joined by the more affordable Honor 50 Lite&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="cLLlbl"&gt;The Honor 50 is otherwise pretty much the same device announced for China in June. It’s powered by a Snapdragon 778G processor and has a 4,300mAh battery that can be fast charged at 66W. It’s got a 6.57-inch curved 120Hz 1080p OLED display, and around back there are four cameras: a 108-megapixel main sensor, an 8-megapixel ultrawide, a 2-megapixel macro, and a 2-megapixel depth sensor.&lt;/p&gt;
&lt;p id="ZgTt21"&gt;As well as the Honor 50, Honor is also announcing a global launch for the more affordable Honor 50 Lite today, which starts at €299 (about $350) for the 6GB RAM model. Honor says the Honor 50 and Honor 50 Lite will be available in European markets such as the UK, France, Germany, Italy, and Spain, in addition to other markets like Egypt, the UAE, and South Africa.&lt;/p&gt;
&lt;p id="rCdoIq"&gt;&lt;/p&gt;
&lt;p id="GuE37G"&gt;&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/10/26/22746320/honor-50-huawei-google-apps-services-global-launch-price-release-date-countries"/>
    <id>https://www.theverge.com/2021/10/26/22746320/honor-50-huawei-google-apps-services-global-launch-price-release-date-countries</id>
    <author>
      <name>Jon Porter</name>
    </author>
  </entry>
  <entry>
    <published>2021-10-26T09:00:00-04:00</published>
    <updated>2021-10-26T09:00:00-04:00</updated>
    <title>Adobe’s Scott Belsky on how NFTs will change creativity</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/hAPA6lHBxueWLhXX5ljfApzUlfQ=/0x0:3000x2000/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/70046448/VRG_ILLO_Decoder_Adobe_s.0.jpg" /&gt;
    &lt;/figure&gt;


  &lt;p&gt;‘Prepare as NFT’ coming to Photoshop&lt;/p&gt; &lt;p class="p--has-dropcap p-large-text" id="1DVnOw"&gt;Adobe is one of those companies that I don’t think we pay enough attention to — it’s been around since 1982, and the entire creative economy runs through its software. You don’t just edit a photo, you Photoshop it. Premiere Pro and After Effects are industry-standard video production tools. Pro photographers all depend on Lightroom. We spend a lot of time on Decoder talking about the creator economy, but creators themselves spend all their time working in Adobe’s tools.&lt;/p&gt;
&lt;p id="wLYnPV"&gt;Adobe is in the middle of announcing new features for all those tools this week — at its annual conference, Adobe Max. On this episode, I’m talking to Scott Belsky, chief product officer at Adobe, about the new features coming to Adobe’s products, many of which focus on collaboration, and about creativity broadly — who gets to be a creative, where they might work, and how they get paid.&lt;/p&gt;
&lt;p id="bdcF9Q"&gt;Scott is a big proponent of NFTs — non-fungible tokens. &lt;a href="https://www.theverge.com/22310188/nft-explainer-what-is-blockchain-crypto-art-faq"&gt;You’ve probably heard about NFTs&lt;/a&gt;, but the quick version is that they allow people to buy and sell digital artwork and keep records of that ownership in a public blockchain. The idea is to create scarcity for digital goods, just like physical products — to definitively say you own a digital piece of art, just like you own a physical piece of art. Of course, the internet is a giant copy machine, so it’s a little more complicated than that — but a lot of people, including Scott, think it’s a revolution. In fact, Photoshop itself will be able to prepare an image to be an NFT very soon. I’m a little more skeptical — so we got into it.&lt;/p&gt;
&lt;p id="gvcOIy"&gt;Scott and I talk about all that. And file formats. And the future of local processing vs. cloud computing. And we squeezed it into just about an hour.&lt;/p&gt;
&lt;p id="1I3qth"&gt;&lt;/p&gt;
&lt;div id="Rz0mSy"&gt;&lt;iframe frameborder="0" height="200" scrolling="no" src="https://playlist.megaphone.fm?e=VMP3851352140" width="100%"&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;p id="zhKZdP"&gt;&lt;em&gt;This transcript has been lightly edited for clarity.&lt;/em&gt;&lt;/p&gt;
&lt;p id="L5UgRC"&gt;&lt;/p&gt;
&lt;p id="FuSou3"&gt;&lt;strong&gt;Scott Belsky, you’re the chief product officer at Adobe and the executive vice president of Creative Cloud. Welcome to &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;Decoder&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="dwsSyx"&gt;Thanks for having me.&lt;/p&gt;
&lt;p id="3UoQ3j"&gt;&lt;strong&gt;It’s been a while since we’ve talked, I’ve always enjoyed our conversations. We have a lot to talk about. This episode of the podcast is coming out alongside Adobe Max, your big conference, and you’re announcing a ton of new products there, including big features for Creative Cloud on the web. There’s news about the Content Authenticity Initiative.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="wFeJXm"&gt;Yes.&lt;/p&gt;
&lt;p id="E4WbOA"&gt;&lt;strong&gt;You’re very bullish on NFTs, which I really want to talk to you about, and I have some big questions about the future of computing. I was looking at these topics and I was like, “Man, I need like two hours.” But we’re going to try to get it all in.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="Xc5HDo"&gt;Let’s do it, a power hour.&lt;/p&gt;
&lt;p id="vflpGi"&gt;&lt;strong&gt;Yeah, exactly. But I want to start with what I have come to think of as the &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;Decoder&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; questions; the basics of how Adobe as a company works. I think Adobe, as a company, we take for granted in the best way. The products are ubiquitous, they’re famous, entire industries depend on them. But I feel like it’s a company we don’t know a lot about. So just start with the basics: you’re the chief product officer, how many people work on products at Adobe?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="SsyXTW"&gt;Probably somewhere around the 7,000-person range fall within the creative organization of engineering, product and design that I oversee. And then there is of course a group of product organization, the digital experience side of the business, which I don’t directly oversee. And the document cloud, which is the PDF, Acrobat business. So I don’t know the exact numbers, but we have quite a large product engineering and design organization at Adobe.&lt;/p&gt;
&lt;p id="mVktXB"&gt;&lt;strong&gt;When you talk about the difference between things you have to do in the future, trying to find the next turn, and then Photoshop — because when I say entire industries are organized around some of your software, entire industries are organized around Photoshop. How do you manage the split between making sure that product does what it needs to for its existing customers versus pivoting to what the next generation of customers might want?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="sj6NgZ"&gt;You’re getting into my everyday drama right now that I have to deal with.&lt;/p&gt;
&lt;p id="XN8CGW"&gt;&lt;strong&gt;This is the heart of &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;Decoder&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt;. I find it and I push the button.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="24cljC"&gt;It’s a great, great question. And there are various ways we go about this. So look at a product like Lightroom. Lightroom now has two variants. There’s Lightroom Classic, and there’s Lightroom for Creative Cloud, which is a more cloud-native photography organization and editing solution. Why did we do that? Because there was actually a legacy, incredible world-leading photography base that just couldn’t imagine ever doing anything on the cloud, and we really wanted to make sure that that product took the path in its evolution that was more towards local, on-premise photography management. And we had to honor that base. &lt;/p&gt;
&lt;p id="fN1cvi"&gt;But at the same time, we didn’t want to constrain the next-generation photographer that wants everything at her fingertips on mobile, desktop, web, and doesn’t even think about where the images are actually stored. And so sometimes we have to go that far and actually splinter and create two products. But we also recognize what is truly empowering for our customers is when everything works together. And so having Photoshop come to iPad, and now as we’ve just announced, coming to web. But Photoshop is Photoshop is Photoshop. You open it anywhere, it is full fidelity, truly interoperable across surfaces without any lossiness. That’s an important thing to deliver to our customer, and that’s kind of a promise, that we’re uncompromising on. In fact, the reason why we can’t port 30 years of features to a new surface like the iPad or the web on day one is because we just have to focus on the file format and the fidelity and trustworthiness of the file itself. Because the PSD is kind of like an iconic format, to your point, that industries standardize on, to some extent.&lt;/p&gt;
&lt;p id="sMAdDT"&gt;&lt;strong&gt;So I ask every executive that comes on the show, and maybe we can use the Lightroom example, how do you make decisions? How do you decide, “All right, we’ve got to actually split this product into two”?&lt;/strong&gt;&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="KuMhtH"&gt;&lt;q&gt;“Photoshop is Photoshop is Photoshop”&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="ua0eiU"&gt;It’s really about, it will sound cliche, but it’s very much anchoring [ourselves] in the customer and understanding where they are going. So, look back to the days when Photoshop was used to make every website. There was a subset of customers that were focusing so much on websites and only using a few specific tools in Photoshop. They struggled, amidst all the rest of the power of Photoshop, to be able to do that in a smooth and efficient way. And then products like Sketch came around that basically took those specific tools out of Photoshop and kind of flanked the product with a new product that was dedicated to screen design. Now we have Adobe XD, and of course others have emerged in the space as well. That’s an example of being very customer-centric.&lt;/p&gt;
&lt;p id="YELwhd"&gt;Now, we could have just said, “Oh, let’s make Photoshop better and better and better and better for the screen designer.” But actually, the customer was saying that they don’t want all the other stuff in there. They want a vector-centric editing capability that vertically integrates prototyping. At the time people were using third-party services to prototype the things that they made in Photoshop and places like Sketch. So we had to listen and say, “Okay, we need a vertically integrated screen design solution.” And now that is collaborative by default. That’s the playbook. And whenever we’re sitting in a meeting and we’re pontificating as people around a table, I’m like, all right, we got to end this meeting right now. What are customers struggling with and how are their behaviors going to impact our roadmap?&lt;/p&gt;
&lt;p id="vquTAP"&gt;&lt;strong&gt;If you ask customers, they would invent a faster horse, right? There’s what customers want, which is pretty narrow problem-solving for their needs right in the moment, and then there’s the next turn. I very much doubt a lot of your customers are saying, “I need to mint NFTs to multiple blockchains.” But I know that’s on your mind. How do you balance those two?&lt;/strong&gt;&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;div id="SNmZ7B"&gt;&lt;div data-anthem-component="aside:9545330"&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p id="e85XuU"&gt;It’s another great question, and it’s something that I think every leader of a company like Adobe needs to be very paranoid about. Because it’s very easy for us to build everything in the image of what we’ve done before. And I’ll ask myself this tough question since you’re not asking it, should XD have been on the web by default in the beginning? Hey, listen, we have a customer base that was never willing to trade performance and precision for ease of collaboration. And when we went to customers and said, “Well, if we bring some of this stuff to the web, you’re going to have a bit of a laggy experience and sometimes bandwidth is going to get in the way of you want to do, and you’re going to feel constrained by whatever happens to be the case at Verizon today.” They would’ve said, “Heck no, give me the power and precision. I want faster and faster and faster. And when Apple comes out with their new chips, I want it to be even faster.”&lt;/p&gt;
&lt;p id="TT2mQE"&gt;So the idea of going to the web was actually sort of crazy at first. And kudos to the companies that took the risk and also managed the years of frustration of customers because they weren’t delivering on the performance side that was required to be in business. But now bandwidth obviously is better and browsers are much more sophisticated. And partnering with our friends at the Chrome team and at Microsoft, at the Edge team, we were able to start to say, “Okay, what’s the future of web apps? And how can we actually take a product like Photoshop to the web and have that ease of collaboration coupled with performance?”&lt;/p&gt;
&lt;p id="ScmNM3"&gt;So in some cases it really does mean having some “burn the ship” moments where you’re like, okay, we are going to go all-in on the web right now, and we are going to make sure we nail this. But then again, it’s back to the customer. It’s knowing these are people who grew up in the age of Google Docs, they expect to be able to just share by clicking an icon. They don’t want to have to send an email and have a version control issue from day one.&lt;/p&gt;
&lt;p id="sbfGwe"&gt;&lt;strong&gt;There’s an elephant in the room. You keep talking about the other companies that have gotten there first. Obviously Figma is right there, they’re a very successful company. They’re a startup, they were web-based from the start, they now have a $10 billion valuation. Adobe’s a big company, do you wait for the small company to come and prove out the idea? Was that, oh man, we got to get there? Was that a competitive pressure for you? Or was it, man, I had this idea, but we had to serve the customers first, and now we can get there because customers are using Figma and they’re saying, “Why aren’t you doing this?”&lt;/strong&gt;&lt;/p&gt;
&lt;p id="OQn9Da"&gt;Yeah. Well, listen, Dylan’s a friend. I met [Figma co-founder] Dylan [Field] when I was still an independent entrepreneur running Behance back in probably 2010, when he was actually first cracking imaging on the web, which was not doable. And that’s where they kind of pivoted to screen design and vector-based creation. I think that when you are a market leader it is really helpful to make sure that, yes, you have to anchor on what the majority of your customers need, which is never something at the edge, it’s always what is at the center. And the folks that were willing to withstand frictions of web creation three to five years ago were a very small group of people.&lt;/p&gt;
&lt;p id="QUHss2"&gt;And so I try to have small teams exploring some of those things on the edge that may become the center someday. And do we always wish that we had started some of those things earlier? In some cases, yes. In some cases, no, because the technology’s changed and pivoted so many times that it’s almost easier sometimes to build on the modern stack today than to have started something three years ago, that now you have to re-platform. So there’s some advantage. I actually believe that by beginning our web journeys more recently, we’re going to be able to capitalize on some very fundamental new technology. But the market signals itself, right? Everyone sort of elevates, hopefully, and this all serves the customers at the end of the day. Everyone elevates everyone else’s game. What I like about the creative space right now is that there are so many new technologies coming in. So many smart people thinking, at the end of the day, this is going to serve customers regardless.&lt;/p&gt;
&lt;p id="y9NxPj"&gt;&lt;strong&gt;So let’s talk about the ultimate “moving from the edge to the center,” which is putting Photoshop on the web.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="DrQo1k"&gt;Yes.&lt;/p&gt;
&lt;p id="pHhd0n"&gt;&lt;strong&gt;That’s part of the announcement at Max, it’s &lt;/strong&gt;&lt;a href="https://www.theverge.com/e/22502564"&gt;&lt;strong&gt;Creative Cloud Web&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;. Tell me what the thinking is there. Photoshop is a classically heavy app. When &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;The Verge&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; reviews team does performance testing on laptops, we open Photoshop, we open Premiere. Bringing &lt;/strong&gt;&lt;a href="https://www.theverge.com/e/22502166"&gt;&lt;strong&gt;Photoshop to the web&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; seems like a big deal, Illustrator is coming along for the ride. How does that work?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="vqu7Ya"&gt;Well, first of all, what we know is that every Photoshop workflow is a collaborative one, to some extent. Whether you’re doing it for a client, you’re doing a project with a friend, whatever the case may be, you’re sharing it with somebody. When you’re sharing it with somebody, what do you want them to be able to do? Well, you want them to be able to review and comment on it, which we wanted to do first out of the gate.&lt;/p&gt;
&lt;p id="G7ob6p"&gt;Step number two is, if they want to jump in and make a slight tweak or change, make a copy edit, whatever — do they really have to go back to you and ask you and then have more back and forth? Can they just click in it and just start editing right away? And so the first phase that we’re also launching now is sort of a light level of editing, very nondestructive, full fidelity, real PSD in the cloud. We’re not bringing all the features on day one, but we really want to unlock all those basic edits that are just best done now in the browser with whoever you’re working with. We just want to knock that out of the park on day one.&lt;/p&gt;
&lt;p id="lANP2U"&gt;&lt;strong&gt;Do you have a button where the boss can just make something bigger?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="RuYZQk"&gt;Oh yeah, the scale functionality is shipping in Photoshop on the web. So, oh my goodness, we now are entering a generation of bigger logos as a result.&lt;/p&gt;
&lt;p id="ds4XTX"&gt;&lt;strong&gt;So you bring it to the web in this way to enable collaboration. You were talking about files and folders and PSDs and radically different expectations the different generations of consumers have. Once you get the PSD in the cloud, do you get to change that file format?&lt;/strong&gt; &lt;/p&gt;
&lt;div class="c-float-left"&gt;&lt;aside id="9t3eV5"&gt;&lt;q&gt;Having a PSD in the cloud is a Pandora’s box of oppertunity&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="NlLxPm"&gt;It’s a Pandora’s box of opportunity. When you have a PSD in the cloud, you can allow people to access it across any device. You can allow anyone to collaborate on it with you. You can approach the world of co-editing, where people can be in the same document at once. You can also do all kinds of fun integrations with third parties. I mean, imagine any image in the world that you’re working on, on any website, being able to right click, edit in Photoshop on web, jump into a new tab, make a change, click save, and go back to where you were. What sorts of creativity will that unlock as people are sharing content on social media websites all day, or on publishing to your blog an article, and I need to comp out something, or add a layer somewhere, add a little watermark? I mean, these things, it’s just such an unlock for so many more people to enter the funnel and to be able to be outfitted with creative capability.&lt;/p&gt;
&lt;p id="xwcmyQ"&gt;&lt;strong&gt;When you think about the architecture of Photoshop historically, you’ve got an app on your laptop that’s running on a local Intel, or AMD, or now an Apple chip. You’ve got a file, you’re operating a file, you send the file places. What is the architecture of Photoshop on the web? Are you running that on your data centers? Is it the same sort of x86 app that we’re used to with the web front end? How does that work?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="GCwxLD"&gt;No. So it’s all native in the browser. I mean, the client side work and the technologies we’ve leveraged are the latest for web apps, as you would see anywhere else. And the team can give you more specifics, but we have really tried to make sure that as much is done on the client side as possible, because performance is crucial. This is one reason why I’ve been trying to get the loading times down with the team — really the initial load, because there’s just so much. The first time you ever use Photoshop on the browser, we’ve got to bring a lot local for you to be successful and be nimble. But then beyond that, a lot of that stuff is happening locally, and then the cloud’s doing its job keeping things in sync.&lt;/p&gt;
&lt;p id="xOEFvI"&gt;And eventually [the cloud will bring] a lot of the power of AI at your fingertips as well. I mean already, when we have masking and stuff like that, a lot of that is algorithmically or AI-driven from the server side. And I think that’s part of the future of creativity, is allowing people to spend more of their time being creative in the exploratory process, as opposed to the mundane, repetitive stuff that we do, like applying marching ants around hair all day.&lt;/p&gt;
&lt;p id="LHIrv8"&gt;&lt;strong&gt;I’m curious about that. When I think of a web app like Photoshop or anything else, really, I think, okay, I’m looking at the front of an app, but all of the heavy lifting is being done on a computer somewhere else. And that lets you bring that app to many more kinds of devices than you would otherwise have. So I know a lot of designers use Figma, they literally work on Chromebooks, pretty midrange Chromebooks all day because it’s just a web app and they don’t need a ton of local processing power. Are you saying that Photoshop will still need a bunch of local processing power?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="VbVT0G"&gt;Well, Photoshop is being made to be able to take advantage of local processing power, right? But I think “need” is a good question. We want to bring Photoshop and some of these capabilities to everyone. We have tested on some Chromebooks, certainly the higher-end Chromebooks, and are pretty satisfied with some of the initial results. But there is the kind of headless Photoshop approach that we could have taken, which is basically streaming Photoshop from us running it in the server, which we did not do because we think that people need to be able to have agile, local operations.&lt;/p&gt;
&lt;p id="nuna3u"&gt;&lt;strong&gt;There’s a big split in computing architectures going on. There is the traditional Intel and AMD x86, there’s Apple’s new approach with its chips, and then there’s a huge push to just move it all to the cloud. Maybe most notably in the video game industry, where game streaming feels like the future, and people still have really big, heavy consoles sitting in their living rooms because that future’s not ready yet. When you think about architecting your apps, that’s three very different paths to go down. How do you pick between them?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="iI2vxW"&gt;We are very much back to the edge and center here. So at the center, we want to make sure that we bring the most powerful and creatively capable tools in the world to market. And so when it comes to the future of Photoshop on desktop, and Premiere Pro, and After Effects, and all the Substance 3D and immersive products, etc., we are in lockstep with Apple and with Microsoft on the absolute latest Apple Silicon and Arm chips, because we need to make sure that we’re always pushing that edge. I mean, think about the types of things that people are rendering and creating these days.&lt;/p&gt;
&lt;p id="5tCjs1"&gt;I think we get really excited about all the collaboration stuff, which I’m about to talk about, but there is this need, and that’s why it’s a cross-surface experience. There are things you want to do that are more collaboration-driven, and then there are things where you’re just, rock solid performance. If I can wait five seconds for this to be done instead of three minutes, all day, every day, I’m desktop, desktop, desktop. And to the metal. I want to make sure I get all the juice.&lt;/p&gt;
&lt;p id="znEhXP"&gt;But again, this is the insight on the web side, is that there is a new generation of people that are achieving better productivity as much through collaboration as they are through performance. And the web is just ruling the world there. I do think that the companies that win will be able to bridge both, and that’s very much our strategy.  Sometimes you just want Photoshop, Photoshop, Photoshop on desktop. And you just want to have all the optimized capabilities for your chipset as possible. And then you may want to open that on the web and do something with somebody else.&lt;/p&gt;
&lt;p id="tlv0Sc"&gt;&lt;strong&gt;Apple makes different GPU decisions than the Intel side. Famously, they do not support the very popular Nvidia GPUs. They have a different framework for GPUs called Metal. Now they’ve got an entirely new framework on their Pro machines in their new chips. When you’re trying to address both of those customer bases, how do you make sure that you’re optimizing as close to what the hardware will allow as possible?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="iH2eA4"&gt;Well, it kind of goes back to that principle of being platform-agnostic. We do want to meet customers where they are. That being said, we partner with these hardware partners of ours to make sure that we’re fully utilizing and embracing what they’re bringing to market. And sometimes one platform leapfrogs another for a period of time, then the other one leapfrogs them, and people keep kind of doing that. But it’s the customer’s choice. We really want them to be outfitted with the very best power possible. And obviously M1s have really exceeded our expectations when we first saw our products lit up on them. And we did all the work to optimize them for the M1, so that was super exciting. Our customers on those devices are thrilled.&lt;/p&gt;
&lt;p id="cMeR0I"&gt;&lt;strong&gt;So in the new machines with Apple’s new GPU structure, are you ready for that on day one, or is that going to take some time?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="NfAvPP"&gt;We’ve worked with them, and we are going to have performance improvements, for sure, right out of the gate. And then there will be more coming.&lt;/p&gt;
&lt;p id="tRmKBE"&gt;&lt;strong&gt;You’ve spent a lot of time talking about collaboration and what businesses need as they grow. There’s a pretty expanding gap between consumer creative tools and what I would now call enterprise creative tools. The most popular consumer tools are integrated directly into the distribution platforms: TikTok is a very powerful video editor, but it’s also a huge distribution platform. The same for Instagram. Photoshop with web collaboration, it’s expensive, it’s rapidly becoming a kind of enterprise tool. And the tool sets are not evolving together. They’re going in different directions because they have different constraints and different audiences. The way I would most simply phrase that is: being great at TikTok does not make you great at Premiere. It will teach you a lot of the language of video editing and give you instincts, but it will not actually make you good at the app. How do you bridge that gap for the younger generation of creators?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="CF6yOS"&gt;Well, our big bet is that the industry is moving, and has always moved, towards the side of being able to stand out creatively. And I actually shouldn’t even use the word industry, I should just say society. People want to stand out. And especially as we get replaced by algorithms when it comes to productivity stuff, we’re all going to want job security through our creative stuff. And whether it’s hosting podcasts or writing, or telling a narrative with data in a visually compelling way, or whatever the case may be, every small business needs to produce content.&lt;/p&gt;
&lt;p id="V06nfK"&gt;There’s a new movement of template-based creativity tools out there where people just take a template, edit it a little bit, and then post it on social media. I actually see the early signs of people starting to feel like they’re being generic now. And again, people want to go further. They want to add more creativity. So our bet is that everyone’s going to ultimately want to stand out. By the way, a huge amount of Premiere Pro customers export for use in TikTok. Why are they doing that if they have a local video editor? Because they want to do something that people look at on TikTok and they’re like, “Oh, how did they do that? You can’t do that on TikTok.”&lt;/p&gt;
&lt;p id="3fLv2v"&gt;Maybe it’s part of the human condition. Maybe it’s because it’s the only thing we, uniquely humans, can do, is create and transcend what’s been done before, and so we all have this innate desire to do that, to sell our products, to sell our ideas, to sell everything. And so that’s where we need to meet the customers at. Now, I think your point is right though, we can’t just make enterprise tools, in a sense, we can’t make creative tools that require huge learning curves. We have to make our products more accessible to more people. So that’s been a huge effort. In Creative Cloud what we’ve been doing is trying to make it easier to onboard, make it easier to learn tutorials, all that kind of stuff. But also, we are developing some stuff that we’re going to talk about in the next few months that’s going to reach a much broader audience, and in a more reimagined way that doesn’t require any learning curve at all. And I think that’s part of our mission for creativity for all.&lt;/p&gt;
&lt;p id="5LA6EF"&gt;&lt;strong&gt;Do you think that is just breaking out the tools? Like I think about content-aware fill, which is a revolutionary feature of Photoshop. It has just changed the landscape of photo editing in a real way. But to use it, you’ve got to know how to use Photoshop. And that’s the sort of tool where you could democratize it and bring it out to other places. Do you think about it that way? There are certain powerful features that you should just understand how to use, and that will ladder you into the bigger app and maybe a career?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="DNgILb"&gt;Listen, if I’m one of the billions of people that aren’t Creative Cloud customers and I saw things like content-aware fill and neural filters, where with a little slider, you can change someone’s frown, or you can change the landscape into summer from fall to winter, if I saw any of that, I would want to have access to that without having to learn that tool. And so that is kind of my charge to the teams, to say, “Hey, port some of this incredible — we call it Adobe magic internally — in a very easy to use, revolutionary interface form that everyone can access.” And that’s part of the challenge that I’m alluding to, that we’re going to start to make a dent in going into the new year.&lt;/p&gt;
&lt;p id="gWDJgP"&gt;But we already are trying to do it in our products that you see today. And shame on us, right? I mean, we should have this access for everyone. Everyone needs it now, is the point. Whereas 10 years ago, not everyone was creating content on social to make their business stand out. But now the creator economy is kind of the theme for this need.&lt;/p&gt;
&lt;p id="40aMSy"&gt;&lt;strong&gt;There was a tweet about a guy who &lt;/strong&gt;&lt;a href="https://twitter.com/ramonvanmeer/status/1448022919901335552"&gt;&lt;strong&gt;bought a business selling ramps for dogs to go up sofas&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;. Dog ramps. I’m going to get him on the show. And he was like, “I bought this business, it wasn’t doing any social marketing. I just made some great videos and bought the ads, and now my business is like 300x.” And that’s all marketing. The classic “the marketing made the business” is right in there. Is that your lane? That huge market of people with small businesses who see the marketing opportunity with social platforms? And if they want to make great content, it’s worth it for them to pay for the tools? In a way that, I don’t know, teenagers might not think it’s worth it to pay for the tools?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="lckOtO"&gt;Well, as I imagine the Adobe of tomorrow, I think that every student who’s making a history report, it’s not going to be a printed Word doc anymore, it’s going to be a visually compelling, animated or narrated, and video type of experience. And millions and millions of small businesses were started during the pandemic as people left their old day jobs and said, “Okay, I want to pursue my passion now.” From day one, it’s all about the content you’re representing across all these different platforms and different formats. And you want to test things, you want to make it creative and different than your competitors. Where are you going to go?&lt;/p&gt;
&lt;p id="XmIWA2"&gt;And then I think that for the big company too. I think about that moment when the lights went out during the Super Bowl and Oreo said, “You can still dunk in the dark.” That was a marketing moment that happened within 30 seconds. And whoever did that, it wasn’t a design team, it was a social media marketer who was empowered to just do it. They needed to have the brand assets, the fonts, all that stuff at their fingertips and just be able to execute and post it. That’s going to be the case across every brand in the world, big and small. A company needs to accommodate those workflows. &lt;/p&gt;
&lt;p id="VIa8Nx"&gt;So I look at Adobe, I’m like, well, we’ve got all the professional tools. We’ve got all that Adobe magic. We’ve got the collaboration services, like Creative Cloud libraries that make those fonts and assets available at your fingertips across mobile, web, and desktop. And then we’re going to have all these more consumer-focused creativity applications that make things more accessible to more people. But it’s all a unified system. To me, that’s the creative operating system of the future that people will need. And I just feel like, in that perspective, Adobe’s in its early days.&lt;/p&gt;
&lt;p id="PxRht1"&gt;&lt;strong&gt;Do you think about making some of these features just filters in other people’s apps? Have you thought about making a set of Snap filters? I don’t know if TikTok lets third parties play in their zone, but have you thought about going into the other apps and saying, “We’re bringing some of our technology here”?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="HILk5W"&gt;Well, it’s interesting. On the augmented reality side, we are making a lot of the tools. We’re always doing the picks and shovels of these mediums. And we’ve done a lot of work there, and we have approached partners who say to us, “Hey, we want your creators to create for our new mediums. Because otherwise our mediums are going to fall flat.” I mean, AR is never going to be interesting until it’s richly filled with interactive, amazing, engaging, entertaining content. And how are you going to do that unless you have millions of people who are the best creators in the world producing for that medium?&lt;/p&gt;
&lt;p id="dS5A5O"&gt;So we have had some conversations there, but I feel like, as a platform-agnostic player, our role is to say, “Hey, you’re a small business. You want to make your ad or your engaging content. You want it to be on TikTok and Instagram and Snapchat and YouTube and Facebook and Pinterest. You shouldn’t have to do it all over in each place. You should just be able to save it in all those formats. We should do the AI magic to just make that happen for you, and then you should just be able to publish directly from our product.” I think that would be the holy grail.&lt;/p&gt;
&lt;p id="0LebS9"&gt;&lt;strong&gt;So there’s a split there that I think is really interesting. You’re describing the small business owner themself or a creator themself. At the same time, right, you came to Adobe because you were the CEO of Behance. Adobe still runs Behance, it’s a networking platform for creatives. You sell subscriptions on it in a Patreon kind of way. You’re announcing some updates here at Max to make it easier to find jobs. What’s the split there between doing it yourself and then going on a platform like Behance, looking at a bunch of creatives, and hiring them?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="OjM34N"&gt;What we’re seeing increasingly is both. People go and they commission or they get UI kits, or they commission people to do original work for them, and then they use those as templates and starters for other derivations and evolutions of that content over time. I think that creativity will always be a collaborative discipline. And one of the things I love about Behance is just how many people in the far corners of the world have expertise in certain areas that just are superpowers for you wherever you are. Some of the best motion graphics designers I’ve ever found were in Central and Eastern Europe, in small little towns. And I don’t know how they became so great, but they are such a resource. And typically they would work for a headhunter, who’d work for an agency, who would work for a bigger agency, who worked for a brand, but now the brand can find that person directly and have them on retainer to do all kinds of cool stuff. So we’re seeing that happen all the time. I think you’ll always see a mixture of both.&lt;/p&gt;
&lt;p id="12mhcC"&gt;&lt;strong&gt;Do you foresee a world in which these specialized creators become an independent army of freelancers? Do you see creative moving out of the agency or the companies themselves?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="iKACAX"&gt;A hundred percent. Why? Because the natural inclination of all of us is to work for ourselves to some extent, and especially a creative, it’s like, “I want to choose my own work. I want to choose my own clients and work on my own terms.” And so the better and better you are, the more likely it is that you should have that future. In the old days when no one could find you and you couldn’t get attribution for your work, you had to work for an agency. You always had to be in that chain. But now, if you can get attribution directly for your work and the spotlight that you deserve, you can work directly for whoever and on your own terms. And by the way, I know we’re about to get into some of the NFT stuff, but it’s interesting to see the digital artist be in some ways at the mercy of circumstance and always at the end of that chain, to suddenly monetizing their work directly, both directly through relationships like we’re describing, as well as by minting their work and having it collected by others.&lt;/p&gt;
&lt;p id="sh7a9O"&gt;&lt;strong&gt;I want to make sure we spend some real time on NFTs, that’s where I was headed. But before I do that, that has a big implication for Adobe’s business, right? Adobe’s business right now is expensive; Creative Cloud subscriptions, I’m assuming CIOs are some of your biggest customers at big companies and they’re buying corporate enterprise licenses. As all those people move and they become freelance, or they start doing it themselves at smaller businesses, how are you thinking about Adobe’s model changing?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="C8opiT"&gt;It’s funny. I mean, I always think about our business as, our customers are creative professionals, the IT department will buy whatever tools they want to use.&lt;/p&gt;
&lt;p id="aYp0yW"&gt;&lt;strong&gt;That’s an optimistic read on the relationship between creative professionals and IT, but I buy it.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="0anGzt"&gt;But they do. No one’s going to tell their designer that we’re not going to pay for the tool you want to use — and that hurts us and helps us. It depends on what industry or what segment of the market we’re talking about. But the truth is, that we need to ultimately empower creative people and teams to work together. While you were saying that there will be more independent professionals and less people in design organizations or agencies, I think it’s more on the agency side. I actually think companies are realizing design is a competitive advantage, so they’re bringing people in-house, but nevertheless, they’re also working as teams. So a lot of the individuals of the world that are working on these incredible animations or editing projects with a product like Frame IO, they might be distributed freelancers, but they are working as a team. So they need enterprise-level collaboration capabilities, even if they are in fact individuals.&lt;/p&gt;
&lt;p id="iPY43U"&gt;&lt;strong&gt;All right, let’s talk about NFTs, I’ve made everybody wait long enough. We’ve been hinting at this conversation. You are very bullish on NFTs, non-fungible tokens. I have a quote here from a &lt;/strong&gt;&lt;a href="https://scottbelsky.medium.com/the-furry-lisa-cryptoart-the-new-economy-of-digital-creativity-6cb2300ea081"&gt;&lt;strong&gt;Medium post that you wrote&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;: “The NFT world is likely the greatest unlock of artist opportunity in a hundred-plus years. This isn’t a suboptimal or fringe version of the real-world art economy. It is a vastly improved one.” I would say I’m maybe less bullish on NFTs, but tell me why you think they’re so revolutionary.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="DktfsC"&gt;And let me be clear; I’m revolutionary on the technology of NFTs. I am not suggesting that the current boom of people trading them and buying them and selling them and these series and all that stuff is here to stay. In fact, my opinion would be that there’s going to be more crashes before more booms. However, I have just never seen a more empowering and better-aligned system for creativity than NFTs. You make an NFT and you not only get the primary sale revenue of it, but then you also, based on the contract you’re using, can get a percentage of every secondary sale forever. That blows out of the water any other form of art, in galleries and anything else for that matter — the attribution is always there for you. You always have a connection to your collectors.&lt;/p&gt;
&lt;p id="iUxprm"&gt;Again, it doesn’t exist in the real world with artists. It’s very good luck if you can even ever meet the artist that made your work. Just when you go down the line, it’s just better, better, better, better, better, better. And what it’s incentivizing is creativity. Artists are realizing, “Oh my goodness, I should make these NFTs that have this nature to them and I can airdrop new versions of this NFT to my collectors, just surprising them, delighting them, and I can have a relationship with them. They can even influence the future of my collection.” There’s a large rabbit hole that we won’t have time to go down, but suffice to say, NFTs represent a way of distributing and collectors owning creativity as a form of cultural flex, as a form of membership, as a form of patronage, and I think it’s early days.&lt;/p&gt;
&lt;p id="itZVxo"&gt;&lt;strong&gt;So let me offer you the pushback on that, because I buy it, and particularly the secondary sale thing, I think, has never been possible before. So that is all very interesting. The pushback I would give you is that NFTs aren’t actually the work, right? They are a pointer on the blockchain to someone else’s website where the work lives. I think it is very amusing that people are angry about right click and save as. That’s very funny to me, just if you take a step back, the fact that that is the problem in the NFT world is deeply funny.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="zXmysC"&gt;&lt;strong&gt;But, it’s still not the work, right? We’re still creating all the value around the work itself and not reasserting the value of the work, in a way that a painting is inherently worth something, or even a CD is inherently worth something, because the media and the art have merged — how do you solve that problem? Because I think that’s the thing that’s always going to be confusing for people. It’s always going to be the blocker.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="kNEU1B"&gt;Well, two comments on that. First, one is a philosophical one. And then one is very specifically what Adobe is doing to help solve this problem. But on the first side, I would just say that NFTs are really about identity. You are defined by the stuff you collect, by the art on your walls, by the clothes you wear. Any pair of shoes you buy is probably $3 in materials and $97 in virtual goods that are ascribed to the brand. Why are you paying that ridiculous premium? It’s because you’re buying a virtual good that helps define your identity. But the thing that we really want in our identity, everyone really wants, is authenticity. And so the idea of knowing and being able to demonstrate that whatever you have, those digital shoes you’re wearing in &lt;em&gt;Fortnite&lt;/em&gt; or whatever, are authentic, is absolutely hard-coded into identity, which is an age-old thing.&lt;/p&gt;
&lt;p id="lu1fZr"&gt;People have always wanted to be authentic and everyone’s always been afraid of being a fraud. So there’s that, that we’re capitalizing on in the NFT space. I think it helps address your comment. But from Adobe’s perspective, we’re seeing this right click and save and mint thing and saying, “Wow, if so many of these NFTs are made within our products, and if we can match the person who makes and actually pushes the pixels to the person who mints it, then we can actually solve that attribution gap.” You know who minted it forever on the blockchain, but you don’t know who created it forever on the blockchain. &lt;/p&gt;
&lt;p id="UTsjhy"&gt;It was this crazy thing, for the last few years we’ve been working on something called the Content Authenticity Initiative, which originally was intended to help people know if a piece of media that you saw, for example, on your guys’ website, if it was actually edited by someone on your staff, or if it was edited by some unattributed person. And that helps me determine whether I can trust the video or image. And so we’re going to use that same technology, but we are basically embedding it into our products when you’re minting the NFTs. And then we are putting it onto the blockchain in an open-source way, that is by no means DRM or anything like that. Anyone including competitors can do this, and then we’re working with the OpenSeas of the world to surface that information with the NFT forever more — wherever it’s powered on the blockchain. So in other words, you will be able to see an NFT and not only see who minted it, but also see some attribution for who created it. And I think that solves that problem.&lt;/p&gt;
&lt;p id="xjphsk"&gt;&lt;strong&gt;So I make my own CryptoPunk in Photoshop. I hit a button, and the &lt;/strong&gt;&lt;a href="https://www.theverge.com/2019/11/4/20948229/adobe-twitter-nyt-company-content-authenticity-initiative-attribution-misinformation-tool"&gt;&lt;strong&gt;technology you developed, I believe it was with Twitter and &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;The&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;New York Times&lt;/strong&gt;&lt;/em&gt;&lt;/a&gt;&lt;strong&gt;, to authenticate that a picture was a picture from &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;The&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;New York Times&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt;, comes into play, adds some verification to my punk and then mint the CryptoPunk. I sell it somewhere and as someone right clicks it and saves it and tries to remix it as a different NFT, my attribution comes along for the ride?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="9RIa1x"&gt;Well, actually what happens is if someone copies it, they will have been the minter of it, but they won’t have the cryptographic signature of being the creator of it. So what it will do is it will validate that you were the creator. It won’t validate that they weren’t the creator, but they can’t actually show that they are the creator. So imagine a world where you favor buying NFTs from artists with a cryptographic signature that you know that they actually made it, as opposed to one who doesn’t have that cryptographic signature. It’s like believing news from a Twitter account that has a verification badge versus not. Anyone can go on Twitter and repaste anything from anywhere, but if it doesn’t have a verification badge, you start to be skeptical of it. I think similarly of the NFT space. If I just get a CryptoPunk, but it doesn’t have a cryptographic signature of the creator of Larva Labs, I’m going to be like, “Well, wait a second. How do I really know that they made this?”&lt;/p&gt;
&lt;p id="4uoe2O"&gt;&lt;strong&gt;So is that the combination of NFTs and content authenticity? You’re trying to create a new set of customer norms in the art world, right? You’re checking for validation; that matches the traditional art world, where you have people who come in and say, “This painting is real.” You’re trying to create that more digitally.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="IkQ5Ik"&gt;It’s actually, in my own career, my quest since 2005 has been to help foster attribution in the creative world. I just simply believe that when people get credit for their work, they get opportunity, and it’s the best thing for creative meritocracy. So fast forward now, NFT boom, tons of people taking other people’s work and minting it and just trying to get away with it. And I’m saying, “Wow, this blockchain thing is great, but you can only track back to the original minter, not the creator.” If we can cryptographically signature the artists and the actual provenance of the object, like what layers, what pixels, where the sources came from and everything, that illuminates a massive gap in this new digital collectibles world, that I think could be very empowering to artists, could make sure that we flip the model and sort of say, “Hey, I only want NFTs that I know were created by the original artist.” And to your point, that’s what galleries and art authenticators are there for, but they’re not even able to do it with 100 percent precision. I think we can.&lt;/p&gt;
&lt;p id="4jtE5b"&gt;&lt;strong&gt;I buy it. But again, here’s the pushback that I see, that is a huge amount of control, and art is usually at its best when it’s pushing back on control and pushing back on the norms.&lt;/strong&gt; &lt;/p&gt;
&lt;p id="F5aBo3"&gt;I agree.&lt;/p&gt;
&lt;p id="uC767q"&gt;&lt;strong&gt;I’m going to tape a banana to the wall &lt;/strong&gt;&lt;a href="https://www.nytimes.com/2019/12/07/arts/art-basel-banana-eaten.html"&gt;&lt;strong&gt;and then someone’s going to eat it&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;. This is a real thing, if you’re listening to this, this is a real thing that happened at Art Basel in Miami.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="BUS3fl"&gt;I remember.&lt;/p&gt;
&lt;p id="w2uCYc"&gt;&lt;strong&gt;And that is breaking a norm and then another person breaking a norm. And that created a moment in the art world. You put computers in charge of everything, they don’t allow for norm breaking in that way. Right? They tend to enforce the rules very strictly. How do you see that dynamic? Because that’s the part that’s scary to me, basically we are creating a massive distributed DRM system that limits what people can do.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="NfDx63"&gt;What’s important here is that it is open source. And by the way, you can attribute it to anything, so any name, your pseudonym, can be your attribution point. So I think that you’ll actually continue to see creatives be very creative with how attribution is used. I don’t think Banksy would suddenly attribute it to his name and his Adobe ID. Right? I mean, it’s going to be like—&lt;/p&gt;
&lt;p id="1jZOkB"&gt;&lt;strong&gt;Do you think that Banksy has an Adobe ID? Do you know who Banksy is?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="JqW5ih"&gt;&lt;em&gt;(Pause)&lt;/em&gt; No. But—&lt;/p&gt;
&lt;p id="tof0A9"&gt;&lt;strong&gt;This is a radio show. Scott thought about it for a minute.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="y5ZAKv"&gt;I was like, “Do I know Banksy?”&lt;/p&gt;
&lt;p id="G3soCh"&gt;&lt;strong&gt;It was very telling.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="PFvjHJ"&gt;There was that time in a British pub…  but anyways, no, I think that it’s about making sure that people can get attribution if they want it. And in a way that is very consistent with the decentralized notion of no one single player, no single source of truth, but people just should have this form, and that’s why we made it open source. And we tried to get as many other folks involved as we can and listen, it’s early days, but I do believe it’s a problem that needs to be solved. And I haven’t seen a better solution yet, but this is why we’re working on it.&lt;/p&gt;
&lt;p id="CrsIPe"&gt;&lt;strong&gt;So do you think eventually Photoshop is going to have in its listed export options, like TIFF, JPEG, NFT?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="NAwtmp"&gt;We are going to have a “prepare as NFT” option by the end of this month.&lt;/p&gt;
&lt;p id="VJJHGJ"&gt;&lt;strong&gt;And what does that look like in practice? “Prepare as NFT” and then it goes where?&lt;/strong&gt;&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="t1NkEL"&gt;&lt;q&gt;“We are going to have a ‘prepare as NFT’ option by the end of this month.”&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="7f3g1l"&gt;It will be able to take whatever you’re working on and it will assist you in packaging it and preparing it along with the attribution capabilities that we just discussed, for some of the popular minting platforms and blockchains out there. And again, this is in preview, it’s not something that we’re gold-standard on yet in terms of readiness, we are just trying to respond to the customer’s desires. So a lot of our customers are like, “Listen, I make stuff in your tools, I mint it and I’m proud of that. But then other people mint the same stuff. I want the ability to show that I was the one who did it.” And we’re like, “Great. We’ll give you the ability to prepare as NFT, we’ll cryptographically sign it in an open-source way for you to be able to have that. We’ll work with the open marketplaces of NFTs, to surface that information alongside an NFT with any cryptographic signature around the actual creator. And hopefully that will help solve your problem.”&lt;/p&gt;
&lt;p id="7KKyMh"&gt;&lt;strong&gt;Have the marketplaces bought onto this? Are they going to support it right away?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="H32314"&gt;They’re very excited about it, because this is one of their biggest problems, is everyone’s right-clicking and minting other cool stuff. The blockchain starts from the moment of minting. So there’s just no way of knowing whether this was right clicked and saved or created from a product or not, from down to the pixels. So that’s something that they want us to help, themselves.&lt;/p&gt;
&lt;p id="lVoojj"&gt;&lt;strong&gt;I feel like we’re right back to where we started with file formats and PSDs. I feel like every time I talk to you we end up in the weeds of the PSD file format. How does this change that file format? Is this, once it’s signed, you can’t change it again? Do you have to change the format? Where does it live on the blockchain?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="wfe1YG"&gt;Yeah, no changes to the format, and the cryptographic signature points to an IPFS (InterPlanetary File System)-powered system that shows you the attribution data. But again, it’s a decentralized storage source and it’s an open-source framework. So that, again, anyone can cryptographically sign anything from within the tool that’s used to create something and then you leverage the same system. And that’s great, because we don’t want this to be anything that is proprietary to Adobe or part of one of our formats, that would negate the purpose.&lt;/p&gt;
&lt;p id="8oIcvG"&gt;&lt;strong&gt;What happens if I want to take one of these signed things that’s minted and remix it and post it to Instagram as a commentary on the art? Is there any place here where that stuff is prevented from happening? Because that’s just been happening with digital art in a variety of ways over time.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="Lv3ZUL"&gt;Listen, we’re so early days in this wild, wild West, I have no idea. I mean, I don’t think Instagram is surfacing attribution data—&lt;/p&gt;
&lt;p id="bpEdEc"&gt;&lt;strong&gt;But this is what I mean about computers being really good at this, right? Instagram has a copyright law obligation and people fight over it and they file the claims.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="baxtfx"&gt;Yeah. DMCA stuff or whatever else.&lt;/p&gt;
&lt;p id="eRGerx"&gt;&lt;strong&gt;I mean, I think just this week, Emily Ratajkowski and Dua Lipa got sued by paparazzi for posting photos that paparazzi had taken — which is a whole other conversation. But there’s already a set of laws that control what you can post or might get taken down from Instagram. Once you start creating this kind of parallel digitized system, that stuff gets automatic real fast. So I’m wondering if you see that line yet, or you’re saying it’s so early, we can’t know?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="i5D4vJ"&gt;Yeah, I think it’s too early to know. But I always go back to the primal motivations here. We just know that creatives’ opportunities are at the mercy of circumstance, unless they get attribution for their work. And we know that any form of monetization typically is taking advantage of the creative, and now we’re trying to shift the power into the creators’ hands. And I think that obviously the blockchain does that in all the ways we just discussed. I feel like there are also problems to be solved around attribution, which we’re trying to solve.&lt;/p&gt;
&lt;p id="ErukPf"&gt;And then as the desires come to remix and leverage and use, we want to play a role in that. I mean, we have a huge business with our Adobe Stock business. And this is people selling content that they make or shoot for other people to use with various levels of licensing. I think that blockchain is a great area to explore on that front as well. And we’re seeing multimedia types of creations come out now that require more types of stock from more sources, with more different various ways of compensating the artists. So it’s exciting. I mean, it’s really like a wild, wild West. I’m so happy we’re now beyond this traditional world of: either a gallery’s selling it on their wall, or it’s not valid. We’ve moved beyond this, and I think that’s very exciting.&lt;/p&gt;
&lt;p id="wAiYEw"&gt;&lt;strong&gt;Let me ask you about the Content Authenticity Initiative (CAI) in its original form. It was you and the &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;Times&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; and Twitter saying we want to make sure that what you see is real; a noble goal. You’re announcing some updates to that here at Max. How is it going? Is it working? Is it taking off? Are you ready to deploy it widely?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="JbK4yQ"&gt;The progress we’ve made is really around the partnership and the consortium of companies that are focused on this — trying to figure out what the standards should be and how to reveal that information in a very open-source, accessible way. What we have also been doing in tandem is using Photoshop as a reference app, to some degree, for how this should be done. We haven’t really launched that yet, that’s going to come out in preview at Max. So this month, and then we start to see how that works.&lt;/p&gt;
&lt;p id="hSNabp"&gt;Now, of course, it’s also a chicken and egg problem. You need enough people using CAI for there to be enough reason for a network, like Twitter or &lt;em&gt;New York Times&lt;/em&gt;, or Behance, to surface the attribution data from an asset that was made in one of our products using content credentials. And so what we’re also going to use as a reference app is Behance. So we’ll have our own two reference apps where it’s like, people can publish with content credentials in Photoshop, people can surface through content credentials in Behance, and then we can leverage that to show all these other players out there who want to solve this problem, hey, we’ve got the APIs and it’s ready to go. And so if you’re another creator tool in the market, leverage what we’re doing in Photoshop as a reference app. And if you’re Twitter or Facebook or someone else, leverage what we’re doing in Behance.&lt;/p&gt;
&lt;p id="msmSwL"&gt;&lt;strong&gt;So let me give you a really dumb example that I was thinking about last night. I’m a Packers fan. Yesterday Aaron Rodgers played the Bears, and the cameras on the sideline caught him screaming, “I own you” at Bears fans, which is very funny. And I encourage everyone to A, root for the Packers, and B, watch that clip. Fox caught the audio. They would’ve put out that clip and marked it with CAI. They would’ve said, “We’re the creators of this clip, it’s real.” But the ones that went viral were people pointing their iPhone at the screen, and then there’s definitely one where people went and tweaked the audio so you could hear it more clearly. Do they get to play in this world where someone said, “All I did was tweak the audio,” or “I just pointed my iPhone at the TV,” but it’s the real thing coming from Fox. Or do those kind of get secondary status here, and the one from Fox itself gets the tag?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="kN40RP"&gt;A reminder that the purpose of this is to help the viewer determine whether they can trust what they’re watching, In a perfect world, the piece that is captured by Fox actually says that: whatever Canon camera registered to Fox Incorporated captured this with this lens and whatever on this date at this location, and you’re seeing that footage. And it was edited a little bit, and it was cut down using Premiere Pro, and there was a filter added and the color correction was using DaVinci or whatever. And so you can sort of see the lineage of that asset. And you know as a news agency, as a viewer, that this is trustworthy content. Going with that logic, it would also say the same for me pointing my phone at the TV. It would say, “Scott Belsky captured this with whatever phone, with whatever tool, and edited it or didn’t edit it and posted it.” And again, this information is just helpful for the viewer to determine whether they can trust it or not.&lt;/p&gt;
&lt;p id="t7yL6Y"&gt;Imagine if that video also goes around, but someone dubs in something else that’s being yelled, right? And you’re like, “Whoa, can that be true?” And then you click on it and there’s no attribution data for it. And you’re like, “Hmm, well, everyone else, all the news outlets who posted this clip had attribution data with the content verification, and this person didn’t. I’m going to have some skepticism on this one.” I think that that’s where we’re trying to get. And I don’t see a better solution, unfortunately, than that. Obviously I’d love to have a true-and-false meter for every piece of content published in the world, but that’s not going to happen. And technology just keeps getting better. So people are going to have to start discerning based on whether they know the provenance of the asset or not.&lt;/p&gt;
&lt;p id="eI9eRV"&gt;&lt;strong&gt;I’m just going to ask this very simply, does that require a network connection, right? You’re saying this is happening on IPFS, it’s happening somewhere on a blockchain. Can I see that data if I don’t have Wi-Fi?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="QLH6hn"&gt;Well, any of the content I assume you’re consuming on any of these social platforms is being delivered to you through a network connection. So any information around authenticity, that a company made it, that media does require an internet connection.&lt;/p&gt;
&lt;p id="ZQOPUn"&gt;&lt;strong&gt;I’m just curious, because we’re completely disintermediating the file from the device at some point. I don’t know where the last step is, but this feels like closer to the last step.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="wl7JSr"&gt;Yeah. I mean, there are other technologies that are doing other kinds of things. They hide little things in the image and whatever else. But that becomes a cat-and-mouse game.&lt;/p&gt;
&lt;p id="rRJHJU"&gt;&lt;strong&gt;I used the Aaron Rodgers example because it is very low-stakes, right? It doesn’t really matter what he screamed at Bears fans. It happened. There are much higher-stakes examples I could use. There’s an election coming up. Do you think this stuff is going to be ready for the next election?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="GKjjjj"&gt;That’s a good question. What I do think will happen, unfortunately, is that there will be some specific things that happen that really diminish trust. I think we’ve seen a few of these examples before, but fortunately, countries haven’t gone to war yet. People haven’t been really traumatically affected yet by fake media on a large scale. And I just fear that it’s a when, not if, sort of scenario. And when that stuff happens, I think that everyone’s going to be grasping for ways to distinguish between true and false. And I think there’s going to be a need for it.&lt;/p&gt;
&lt;p id="qaniF4"&gt;This all started because our leadership team said, “Hey, we have an opportunity to be part of the solution.” Or a responsibility, rather, not just an opportunity to be part of the solution. The opportunity of creating things like content-aware fill — and I remember when I announced content-aware fill in After Effects three years ago on stage. I showed a video where you could literally remove an object or a person from a video and even remove the footsteps and the dust in their walk from an entire piece of video with AI. It sort of begs the question of, oh goodness, what are the implications for this?&lt;/p&gt;
&lt;p id="djZH0L"&gt;Now, that could have always been done. People could have done it the painstaking way. We just were trying to save creatives days and days of work by being able to do these sorts of things. And there are obviously many legitimate uses. I remember when the coffee cup was left in a &lt;em&gt;Game of Thrones&lt;/em&gt; episode and there was this moment of, how do you remove that? And I was like, “Hey, we have a feature for that. You can just remove the coffee cup from the entire scene and no one is going to ever notice the Starbucks cup.” So I think that there is a really great, legitimate use case for that. But with that opportunity comes the responsibility of helping people know what’s real. And that’s the type of stuff that this is meant to solve.&lt;/p&gt;
&lt;p id="ydYNkZ"&gt;&lt;strong&gt;Do you think that that affects your roadmap? When the next version of content-aware fill comes up, do you, as the decision-maker, say, “That’s too far until we build the trust tools”?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="p3rWp7"&gt;Everyone is working on these things. It’s not like we’re the only company trying to figure out how to remove stuff from video or imagery. This is a popular capability and we have to do it for our customers, just like our peers are doing it for their customers in the industry. I think that it’s a statement about Silicon Valley as a whole, that we tend to have teams that are very creative about what can go right in the future, and don’t spend time being creative about what could go wrong in the future. I think that if the early groups of product leaders and designers at Facebook, for example, sat around trying to brainstorm what could go wrong with their technology, years if not decades from then, maybe they would’ve built the platform differently.&lt;/p&gt;
&lt;div&gt;&lt;aside id="hrt6x3"&gt;&lt;q&gt;“I think that if the early groups of product leaders and designers at Facebook, for example, sat around trying to brainstorm what could go wrong with their technology, years if not decades from then, maybe they would’ve built the platform differently.”&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="wEODy7"&gt;I’m hoping that we have those conversations now. And I’m hoping that we spend the effort and cycles to innovate in ways that may or may not catch on in the industry. I think it’s important to do so in an open-source way, because then we can help the whole industry do this. And of course we benefit by being leaders in this. And I hope it’s something that the networks and other partners in this space start to prioritize as well. Because we can build it, but if companies like Twitter or Pinterest or whomever are not surfacing this information, it doesn’t really work either.&lt;/p&gt;
&lt;p id="jIdVz8"&gt;&lt;strong&gt;I think that’s a pretty excellent place to stop. Last question, it’s a softball everybody gets at the end. We’re obviously talking around Adobe Max, you’ve announced a lot of stuff, but what’s next for Adobe? Where do you see the next turn for the stuff you’re working on?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="B1iX2H"&gt;I’m very committed to making Creative Cloud as much about collaboration as it is about creativity. I feel like anyone working alone these days is working at a massive disadvantage. You want to leverage other people’s assets, you want to be able to frictionlessly collaborate with others. That’s why we are bringing our products to the web. That’s why we are building all these services like libraries, but also, as we just announced at Max, Creative Cloud spaces and the canvas, as new forms of collaboration for creative teams. I think we have an opportunity to make creativity a collaborative discipline that is far more inclusive and really transcends what we’ve ever seen before, which is the ultimate measure of the work that we do. So that’s what gets me excited every day these days.&lt;/p&gt;
&lt;p id="lOhfWX"&gt;&lt;strong&gt;Scott, it is always a pleasure to talk to you. I feel like I could definitely spend another hour on either NFTs or the future of computing, but we got to wrap it up. Thank you so much for being on &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;Decoder&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p class="c-end-para" id="3LVOdU"&gt;Thank you for having me.&lt;/p&gt;
&lt;aside id="wce1ZQ"&gt;&lt;div data-anthem-component="actionbox" data-anthem-component-data='{"title":"Decoder with Nilay Patel","description":"A new podcast from The Verge about big ideas and other problems.","label":"Subscribe now!","url":"https://podcasts.apple.com/us/podcast/decoder-with-nilay-patel/id1011668648?i=1000496212371"}'&gt;&lt;/div&gt;&lt;/aside&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/22740442/adobe-scott-belsky-interview-nft-photoshop-blockchain-creator-economy"/>
    <id>https://www.theverge.com/22740442/adobe-scott-belsky-interview-nft-photoshop-blockchain-creator-economy</id>
    <author>
      <name>Nilay Patel</name>
    </author>
  </entry>
</feed>
