<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <title>The Verge -  Front Pages</title>
  <icon>https://cdn.vox-cdn.com/community_logos/52801/VER_Logomark_32x32..png</icon>
  <updated>2021-09-29T15:00:00-04:00</updated>
  <id>https://www.theverge.com/rss/front-page/index.xml</id>
  <link type="text/html" href="https://www.theverge.com/" rel="alternate"/>
  <entry>
    <published>2021-09-29T15:00:00-04:00</published>
    <updated>2021-09-29T15:00:00-04:00</updated>
    <title>GM’s new software platform will enable over-the-air updates, in-car subscriptions, and maybe facial recognition </title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/U4zmTW2KLefLb9Xr3HeKjrD-DD0=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69927249/cadillac_2021_escalade_0422.0.jpg" /&gt;
        &lt;figcaption&gt;Photo by Sean O’Kane / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="0aCZyz"&gt;General Motors announced a new “end-to-end” software platform for its cars called “Ultifi” — a play on the name &lt;a href="https://www.theverge.com/2020/3/4/21164513/gm-ev-platform-architecture-battery-ultium-tesla"&gt;Ultium&lt;/a&gt;, which is the automaker’s new electric vehicle battery architecture. GM says the new software will enable &lt;a href="https://www.theverge.com/2019/5/21/18633000/gm-ota-software-updates-digital-platform-reuss"&gt;over-the-air (OTA) updates&lt;/a&gt;, in-car subscription services, and “new opportunities to increase customer loyalty.” &lt;/p&gt;
&lt;p id="JnIxym"&gt;The automaker envisions the new software powering everything from the mundane, like weather apps, to potentially controversial features like the use of in-car cameras for facial recognition or to detect children to automatically trigger the car’s child locks. The Linux-based system will also be available to third-party developers who may want to create apps and other features for GM customers. &lt;/p&gt;
&lt;p id="VObWr5"&gt;GM is currently undergoing a “transformation... from an automaker to a platform innovator,” said Scott Miller, vice president of software-defined vehicles at the company. Miller said he envisions Ultifi serving the role as a “powerful hub for all the vehicle’s systems.”&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="iiaOLJ"&gt;&lt;q&gt;GM is currently undergoing a “transformation... from an automaker to a platform innovator”&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="nMNtGu"&gt;Ultifi will start rolling out to GM vehicles, both electric and gas-powered, in 2023, the company said. By then, &lt;a href="https://www.theverge.com/2019/9/5/20851021/general-motors-android-auto-google-infotainment"&gt;GM will have many vehicles running Google’s embedded Android Automotive operating system&lt;/a&gt;, which Miller says will work “alongside” its new Ultifi software platform in certain vehicles. Not every GM vehicle will get Android Automotive, which is supposed to start rolling out to new vehicles this year, Miller added. &lt;/p&gt;
&lt;p id="n06fIK"&gt;“Android Automotive is a certain subset of functionality in the car,” he said. “Ultifi is more of an umbrella overall strategy.”&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/5TWxADy_ut67BJDGwYxEtkDWR00=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22888085/Ultifi_Graphic_Final.jpg"&gt;
  &lt;/figure&gt;
&lt;p id="8qrXFZ"&gt;The software will also be cloud-connected, meaning GM will be able to make decisions for the customers without their input. For example, if a vehicle’s owner left their sunroof open and weather services are predicting rain, the vehicle’s software can automatically close the sunroof. &lt;/p&gt;
&lt;p id="5B2Czl"&gt;Ultifi isn’t only anchored to systems within the car, Miller said. GM envisions the new platform interacting with other smart devices, such as a customer’s internet-enabled thermostat or a home security system. &lt;/p&gt;
&lt;p id="8labMU"&gt;GM also sees the potential for in-car purchases and subscriptions as a major new source of revenue for the company. Miller used the example of Super Cruise, GM’s “hands-free” advanced driver-assist system, which is installed in some of the automaker’s vehicles today. In the future, a driver may decide they want to test out Super Cruise on an upcoming road trip. Ultifi will enable that vehicle owner to subscribe to Super Cruise for the trip and then cancel the subscription when they’ve arrived at their destination. &lt;/p&gt;
&lt;p id="l5sDF1"&gt;A more common type of subscription that GM customers will face in the future is annual or monthly fees for data storage, similar to the fees that Apple and Google charge to their smartphone customers today. &lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="tcWCMr"&gt;&lt;q&gt;A more common type of subscription that GM customers will face in the future is annual or monthly fees for data storage&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="dG5P5P"&gt;GM isn’t the only automaker contemplating new ways to squeeze money out of its customers beyond the sale of its vehicle. Much like the tech sector, which over the years has figured out how a &lt;a href="https://www.theverge.com/2019/4/30/18522990/recurring-revenue-gadgets-hardware-profit-subscription"&gt;recurring revenue model&lt;/a&gt; allows them to rely on customers paying every month or year, automakers are increasingly turning to subscriptions and in-car fees to bring in more money. &lt;/p&gt;
&lt;p id="b2dCVy"&gt;Cars are more full of computers and software than ever before, which has made it possible for automakers to add new features or patch problems on the fly with over-the-air software updates. &lt;/p&gt;
&lt;p id="0NScRU"&gt;Automakers like &lt;a href="https://www.theverge.com/2012/9/24/3385506/tesla-model-s-over-the-air-car-firmware-update"&gt;Tesla&lt;/a&gt; and &lt;a href="https://www.theverge.com/2020/7/2/21311332/bmw-in-car-purchase-heated-seats-software-over-the-air-updates"&gt;BMW&lt;/a&gt; have proven that some customers are willing to pay to unlock certain vehicle features, like improved infotainment experiences or even longer range in their electric vehicles. Ford has said that its new &lt;a href="https://www.theverge.com/2020/6/25/21303479/ford-2021-f-150-active-drive-assist-hybrid-specs-features-photos"&gt;2021 F-150 will have over-the-air updates&lt;/a&gt; that cover the car from “bumper to bumper,” which could theoretically allow the company to charge for access to certain features.&lt;/p&gt;
&lt;p id="vCpydo"&gt;It’s also proven to be risky business, with &lt;a href="https://www.theverge.com/2019/12/4/20995630/bmw-apple-carplay-reverse-decision-annual-fee-vehicle"&gt;BMW ultimately reversing its controversial decision&lt;/a&gt; to charge customers an annual fee to use Apple’s CarPlay, which is free in most cars. &lt;/p&gt;
&lt;p id="CeHKYZ"&gt;Miller said GM is aware that too many subscriptions and fees can have the effect of repelling customers rather than keeping them loyal. “It has to be a win-win proposition for the customer and for us,” he said. &lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22700400/gm-ultifi-software-ota-update-subscription-facial-recognition"/>
    <id>https://www.theverge.com/2021/9/29/22700400/gm-ultifi-software-ota-update-subscription-facial-recognition</id>
    <author>
      <name>Andrew J. Hawkins</name>
    </author>
  </entry>
  <entry>
    <published>2021-09-29T14:31:02-04:00</published>
    <updated>2021-09-29T14:31:02-04:00</updated>
    <title>You can charge your iPhone 13 Pro Max much faster with the right power brick</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="The iPhone 13 Pro is a bit thicker because of a larger battery, so iPhone 12 Pro cases won’t fit." src="https://cdn.vox-cdn.com/thumbor/OnfHZ51hQMWJc91fwiQdne0eQIE=/0x0:2025x1350/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69927111/vpavic_210916_untitled_0027.0.jpg" /&gt;
        &lt;figcaption&gt;Photo by Vjeran Pavic / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="text-container"&gt;The iPhone 13 Pro Max can charge a bit faster than other models, &lt;a href="https://www.youtube.com/watch?v=byIt3SmVfyg"&gt;according to tests done by YouTuber ChargerLAB&lt;/a&gt; (and &lt;a href="https://twitter.com/duanrui1205/status/1441706758607552518"&gt;backed up&lt;/a&gt; by &lt;a href="https://news.mydrivers.com/1/785/785771.htm"&gt;other tests&lt;/a&gt;), which show that the phone sips up to 27 watts of power when plugged into a compatible charging brick. If you have a 13 Pro Max and want to fill up its &lt;a href="https://www.theverge.com/2021/9/17/22678607/iphone-13-reported-battery-sizes-increase-laptop-comparison"&gt;massive battery&lt;/a&gt; as fast as possible, you may want to look into getting a 30W charger that, according to &lt;a href="https://twitter.com/duanrui1205"&gt;tests cited by @duanrui1205&lt;/a&gt;, can charge it from zero to full in less than 90 minutes.&lt;/p&gt;
&lt;p id="id__aifbjt1xnnj"&gt;This appears to be an upgrade from the previous generation — &lt;a href="https://9to5mac.com/2021/04/02/what-can-i-use-to-charge-iphone-12/"&gt;&lt;em&gt;9to5Mac&lt;/em&gt; reports&lt;/a&gt; that the iPhone 12 caps out at around 22W charging speeds. That’s about a 5W boost, which is nothing to sneeze at. The benefit of a bigger charger is limited to the Pro Max, according to @duanrui1205, &lt;a href="https://twitter.com/duanrui1205/status/1442424092238352396?s=20"&gt;who said&lt;/a&gt; that the smaller iPhone 13 Pro could only charge at 20W. All of this goes out the window if you’re charging wirelessly or with Apple’s MagSafe puck — you’ll&lt;a href="https://www.theverge.com/2021/9/24/22691602/iphone-13-mini-slower-magsafe-charging-speeds-12w"&gt; be limited to 15W at most&lt;/a&gt;. &lt;/p&gt;
&lt;p id="dyzvo0"&gt;If you’re looking for a 30W brick to charge your Pro Max a bit faster, it’s probably worth looking around your house first: Apple includes 30W chargers with its new MacBook Air, and the 61W charger that comes with MacBook Pros could also provide the 27 watts your phone craves. You can check any other chargers in your house by reading the regulatory text that’s usually printed near the prongs — if it doesn’t outright say the wattage, you can figure it out by &lt;a href="https://en.wikipedia.org/wiki/Watt#Overview"&gt;multiplying the amps and voltage&lt;/a&gt;.&lt;/p&gt;
&lt;div id="nMrhUB"&gt;&lt;div data-anthem-component="gallery:10780536"&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p id="ADsxCo"&gt;If your search for a 30W charger didn’t turn up anything, &lt;a href="https://go.redirectingat.com?id=66960X1514734&amp;amp;xs=1&amp;amp;url=https%3A%2F%2Fwww.apple.com%2Fshop%2Fproduct%2FMY1W2AM%2FA%2F30w-usb-c-power-adapter&amp;amp;referrer=theverge.com&amp;amp;sref=https%3A%2F%2Fwww.theverge.com%2F2021%2F9%2F29%2F22700491%2Fiphone-13-pro-max-faster-charge-times-27w-charging-brick" rel="sponsored nofollow noopener" target="_blank"&gt;Apple will happily sell you one&lt;/a&gt;. You can also find plenty of third-party ones, like &lt;a href="https://www.amazon.com/Charger-Anker-Adapter-Compact-Foldable/dp/B091DS2M8X?tag=theverge02-20" rel="sponsored nofollow noopener" target="_blank"&gt;Anker’s tiny Nano II GaN brick&lt;/a&gt;. One thing to note when looking at chargers is the charge rates they support — to get the 27W charging, the iPhone is looking for 9V at three amps, according to ChargerLAB’s video (which shows one 30W charger &lt;a href="https://youtu.be/byIt3SmVfyg?t=309"&gt;that’s only charging the iPhone at 18W&lt;/a&gt;).&lt;/p&gt;
&lt;aside id="1COt6J"&gt;&lt;div data-anthem-component="readmore" data-anthem-component-data='{"stories":[{"title":"iPhone 13 Pro review: a better display, the best camera, and incredible battery life","url":"https://www.theverge.com/22684033/apple-iphone-13-pro-max-review"}]}'&gt;&lt;/div&gt;&lt;/aside&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22700491/iphone-13-pro-max-faster-charge-times-27w-charging-brick"/>
    <id>https://www.theverge.com/2021/9/29/22700491/iphone-13-pro-max-faster-charge-times-27w-charging-brick</id>
    <author>
      <name>Mitchell Clark</name>
    </author>
  </entry>
  <entry>
    <published>2021-09-29T14:27:51-04:00</published>
    <updated>2021-09-29T14:27:51-04:00</updated>
    <title>You can now buy a keyboard that only copies and pastes</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="The Key stood vertically in front of an orange background, connected to a black cord." src="https://cdn.vox-cdn.com/thumbor/MUJX1sTYIiG1ERH52jrjASssA8s=/0x1:1640x1094/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69927097/eTjj9DPQqeSSYp21FI01_2864_copy_pdp.0.jpg" /&gt;
        &lt;figcaption&gt;&lt;em&gt;Here it is.&lt;/em&gt; | Image: Drop&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="Xe27tl"&gt;Sure, why not. Stack Overflow has teamed up with tech retailer Drop to release this cute little keyboard called The Key. It’s programmed to do exactly two things: copy and paste. &lt;/p&gt;
&lt;p id="tB5hno"&gt;The product started as an &lt;a href="https://stackoverflow.blog/2021/03/31/the-key-copy-paste/"&gt;April Fools’ Day&lt;/a&gt; joke earlier this year — Stack Overflow users were shown alerts claiming that they had a limited number of copy / pastes and that they could only lift that cap by purchasing The Key. Users, as Stack Overflow’s &lt;a href="https://stackoverflow.blog/2021/09/28/become-a-better-coder-with-this-one-weird-click/"&gt;blog post&lt;/a&gt; notes, were disappointed to find out that The Key was not a real thing they could buy. &lt;/p&gt;
&lt;p id="eUpIma"&gt;So, you can buy it now. It’s &lt;a href="https://go.redirectingat.com?id=66960X1514734&amp;amp;xs=1&amp;amp;url=https%3A%2F%2Fdrop.com%2Fbuy%2Fstack-overflow-the-key-macropad%23overview&amp;amp;referrer=theverge.com&amp;amp;sref=https%3A%2F%2Fwww.theverge.com%2F2021%2F9%2F29%2F22700522%2Fstack-overflow-macropad-mechanical-keyboard-specs" rel="sponsored nofollow noopener" target="_blank"&gt;up for preorder&lt;/a&gt; on Drop’s website for $29. Per the company, the next batch is shipping on December 13th.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt="The Key laid horizontally on a white surface, connected to a black cord." data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/Ly1IBAU1kTqFIyCvAPHb5M6Kd78=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22887861/REMFbjgYSj2NKC4oK8s1_2676.jpg"&gt;
      &lt;cite&gt;Image: Drop&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;What more could you need?&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="6P7f3V"&gt;As you can see, we have three buttons here. We have C, and we have V. I’m not sure what the third button is supposed to do. That’s fine, of course, because the three keys are actually programmable — you can map them to do whatever task you want. &lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="kxU5JK"&gt;&lt;q&gt;“An ultra-smooth linear feel”&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="YHlHuD"&gt;So there are some legitimate pragmatic use cases. First of all, if you make the third key a delete key, you can force every writer to edit their pieces with this device, and our articles will probably end up a lot more concise and better for it. More realistically, you can imagine mapping the keys to raise and lower brightness, for example (a setting that’s a pain to adjust on some computers), or to end a Zoom call. &lt;/p&gt;
&lt;p id="uL2UoT"&gt;Plus, imagine how cool you’d look using this in on a plane or something. Can you really put a price on a gadget-savvy aesthetic?&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt="The Key’s three keys lined up on a white surface." data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/9gb7PFu-35WVT8RHgETtGA86V_0=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22887863/pqWbjKpQkulaQpK6SwoU_2688.jpg"&gt;
      &lt;cite&gt;Image: Drop&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;That’s Stack Overflow’s logo on the left.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="gUEBGj"&gt;Now, macropads like this aren’t actually a new concept (though three is a somewhat small number of keys). You can get them &lt;a href="https://go.redirectingat.com?id=66960X1514734&amp;amp;xs=1&amp;amp;url=https%3A%2F%2Fwww.etsy.com%2Fmarket%2Fmacropad&amp;amp;referrer=theverge.com&amp;amp;sref=https%3A%2F%2Fwww.theverge.com%2F2021%2F9%2F29%2F22700522%2Fstack-overflow-macropad-mechanical-keyboard-specs" rel="sponsored nofollow noopener" target="_blank"&gt;all over Etsy&lt;/a&gt; — there’s one that’s literally just an &lt;a href="https://go.redirectingat.com?id=66960X1514734&amp;amp;xs=1&amp;amp;url=https%3A%2F%2Fwww.etsy.com%2Flisting%2F625700003%2Fcherry-mx-mechanical-keyboard-switch%3Fga_order%3Dmost_relevant%26ga_search_type%3Dall%26ga_view_type%3Dgallery%26ga_search_query%3Dmacropad%26ref%3Dsr_gallery-1-37%26bes%3D1&amp;amp;referrer=theverge.com&amp;amp;sref=https%3A%2F%2Fwww.theverge.com%2F2021%2F9%2F29%2F22700522%2Fstack-overflow-macropad-mechanical-keyboard-specs" rel="sponsored nofollow noopener" target="_blank"&gt;Escape key&lt;/a&gt;.&lt;/p&gt;
&lt;p id="3QBaQ4"&gt;But in addition to its unique preprogramming, this one does appear particularly well-made. It’s outfitted with Kailh Box Black Switches, which Drop claims will deliver “an ultra-smooth linear feel.” The case is CNC-machined aluminum with custom XDA profile keycaps. I’m also seeing a USB-C connector on the back of the thing, and there are feet on the bottom to keep it from slipping. &lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt="The Key laid horizontally on a white surface next to a black cord." data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/51N7b66xyHCJ_UlYe9tV3saAfvU=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22887865/DXfOBQRgCJagyMf7rONQ_2647.jpg"&gt;
      &lt;cite&gt;Image: Drop&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;The cord is bigger than the keyboard, but sometimes that happens.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="fnJsOc"&gt;Oh, and you can also feel somewhat good about yourself while buying it. Stack Overflow is donating all of its proceeds to &lt;a href="https://www.digitalundivided.com/"&gt;digitalundivided&lt;/a&gt;, and Drop is donating five percent of its portion. &lt;/p&gt;
&lt;p id="DPFFXk"&gt;So, what are you waiting for? Copy and paste away, my friends.&lt;/p&gt;
&lt;p id="qxsxvI"&gt;&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22700522/stack-overflow-macropad-mechanical-keyboard-specs"/>
    <id>https://www.theverge.com/2021/9/29/22700522/stack-overflow-macropad-mechanical-keyboard-specs</id>
    <author>
      <name>Monica Chin</name>
    </author>
  </entry>
  <entry>
    <published>2021-09-29T13:42:17-04:00</published>
    <updated>2021-09-29T13:42:17-04:00</updated>
    <title>Google search’s next phase: context is king</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/xHowirLRwCuklvCPudSGDvMWE8w=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69926828/acastro_180427_1777_0003.6.jpg" /&gt;
        &lt;figcaption&gt;Illustration: Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;


  &lt;p&gt;Google says search is still far from solved&lt;/p&gt; &lt;p class="p--has-dropcap" id="R930gy"&gt;At its Search On event today, Google introduced several new features that, taken together, are its strongest attempts yet to get people to do more than type a few words into a search box. By leveraging its new Multitask Unified Model (MUM) machine learning technology in small ways, the company hopes to kick off a virtuous cycle: it will provide more detail and context-rich answers, and in return it hopes users will ask more detailed and context-rich questions. The end result, the company hopes, will be a richer and deeper search experience.&lt;/p&gt;
&lt;p id="lyLN12"&gt;Google SVP Prabhakar Raghavan oversees search alongside Assistant, ads, and other products. He likes to say — and repeated in an interview this past Sunday — that “search is not a solved problem.” That may be true, but the problems he and his team are trying to solve now have less to do with wrangling the web and more to do with adding context to what they find there. &lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="dzvXJE"&gt;&lt;q&gt;AI will help Google explore the questions people are asking&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="75lUXB"&gt;For its part, Google is going to begin flexing its ability to recognize constellations of related topics using machine learning and present them to you in an organized way. A coming redesign to Google search will begin showing “Things to know” boxes that send you off to different subtopics. When there’s a section of a video that’s relevant to the general topic — even when the video as a whole is not — it will send you there. Shopping results will begin to show inventory available in nearby stores, and even clothing in different styles associated with your search.&lt;/p&gt;
&lt;p id="gyV1eY"&gt;For your part, Google is offering — though perhaps “asking” is a better term — new ways to search that go beyond the text box. It’s making an aggressive push to get its image recognition software Google Lens into more places. It will be built into the Google app on iOS and also the Chrome web browser on desktops. And with MUM, Google is hoping to get users to do more than just identify flowers or landmarks, but instead use Lens directly to ask questions and shop.&lt;/p&gt;
&lt;p id="orhHOq"&gt;“It’s a cycle that I think will keep escalating,” Raghavan says. “More technology leads to more user affordance, leads to better expressivity for the user, and will demand more of us, technically.” &lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/BMVe7nxgCDRZMqzn0z4aOkpyvww=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22884248/google_lens_shirt_sock_search_wide_slow.gif"&gt;
      &lt;cite&gt;Image: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Google Lens will let users search using images and refine their query with text. &lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="312ACt"&gt;Those two sides of the search equation are meant to kick off the next stage of Google search, one where its machine learning algorithms become more prominent in the process by organizing and presenting information directly. In this, Google efforts will be helped hugely by recent advances in AI language processing. Thanks to systems known as large language models (MUM is one of these), machine learning has got much better at mapping the connections between words and topics. It’s these skills that the company is leveraging to make search not just more accurate, but more explorative and, it hopes, more helpful.&lt;/p&gt;
&lt;p id="QcnS2Q"&gt;One of Google’s examples is instructive. You may not have the first idea what the parts of your bicycle are called, but if something is broken you’ll need to figure that out. Google Lens can visually identify the derailleur (the gear-changing part hanging near the rear wheel) and rather than just give you the discrete piece of information, it will allow you to ask questions about fixing that thing directly, taking you to the information (in this case, the excellent &lt;a href="https://www.youtube.com/watch?v=BTYPHGWDE6E"&gt;Berm Peak Youtube&lt;/a&gt; channel).&lt;/p&gt;
&lt;div class="c-float-left"&gt;&lt;aside id="qFXrbc"&gt;&lt;q&gt;Multimodal search requires entirely new input from users&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="MLe91V"&gt;The push to get more users to open up Google Lens more often is fascinating on its own merits, but the bigger picture (so to speak) is about Google’s attempt to gather more context about your queries. More complicated, multimodal searches combining text and images demand “an entirely different level of contextualization that we the provider have to have, and so it helps us tremendously to have as much context as we can,” Raghavan says. &lt;/p&gt;
&lt;p id="nkHj6l"&gt;We are very far from the so-called “ten blue links” of search results that Google provides. It has been showing information boxes, image results, and direct answers for a long time now. Today’s announcements are another step, one where the information Google provides is not just a ranking of relevant information but a distillation of what its machines understand by scraping the web. &lt;/p&gt;
&lt;p id="ZderHj"&gt;In some cases — as with shopping — that distillation means you’ll likely be sending Google more page views. As with Lens, that trend is important to keep an eye on: Google searches increasingly push you to Google’s own products. But there’s a bigger danger here, too. The fact that Google is telling you more things directly increases a burden it’s always had: to speak with less bias. &lt;/p&gt;
&lt;p id="NCCmTM"&gt;By that, I mean bias in two different senses. The first is technical: the machine learning models that Google wants to use to improve search have well-documented problems with racial and gender biases. They’re trained by reading large swaths of the web, and, as a result, tend to pick up nasty ways of talking. Google’s troubles with its AI ethics team are also well documented at this point — it &lt;a href="https://www.theverge.com/2021/2/19/22292011/google-second-ethical-ai-researcher-fired"&gt;fired two lead researchers&lt;/a&gt; after they published a paper on this very subject. As Google’s VP of search, Pandu Nayak, told &lt;em&gt;The Verge&lt;/em&gt;’s &lt;a href="https://www.theverge.com/e/22460309"&gt;James Vincent in his article on today’s MUM announcements&lt;/a&gt;, Google knows that all language models have biases, but the company believes it can avoid “putting it out for people to consume directly.”&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/Vo4XOpVcCPykH4Q7FUe55J6_Hgg=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22885162/Untitled__2_.gif"&gt;
      &lt;cite&gt;Image: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;A new feature called “Things to know” will help users explore topics related to their searches. &lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="Fhuquw"&gt;Be that as it may (and to be clear, it may not be), it sidesteps another consequential question and another type of bias. As Google begins telling you more of its own syntheses of information directly, what is the point of view from which it’s speaking? As journalists, we often talk about how the so-called “view from nowhere” is an inadequate way to present our reporting. What is Google’s point of view? This is an issue the company has confronted in the past, sometimes known as the “one true answer” problem. When Google tries to give people short, definitive answers using automated systems, it often ends up &lt;a href="https://searchengineland.com/googles-one-true-answer-problem-featured-snippets-270549"&gt;spreading bad information&lt;/a&gt;. &lt;/p&gt;
&lt;p id="3a0rVn"&gt;Presented with that question, Raghavan responds by pointing to the complexity of modern language models. “Almost all language models, if you look at them, are embeddings in a high dimension space. There are certain parts of these spaces that tend to be more authoritative, certain portions that are less authoritative. We can mechanically assess those things pretty easily,” he explains. Raghavan says the challenge is then how to present some of that complexity to the user without overwhelming them. &lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="bqROVQ"&gt;&lt;q&gt;Can Google remain neutral if it’s delivering answers to users directly?&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="BvW70n"&gt;But I get the sense that the real answer is that, for now at least, Google is doing what it can to avoid facing the question of its search engine’s point of view by avoiding the domains where it could be accused of, as Raghavan puts it, “excessive editorializing.” Often when speaking to Google executives about these problems of bias and trust, they focus on easier-to-define parts of those high-dimension spaces like “authoritativeness.” &lt;/p&gt;
&lt;p id="ps3UWY"&gt;For example, Google’s new “Things to know” boxes won’t appear when somebody searches for things Google has identified as “particularly harmful/sensitive,” though a spokesperson says that Google is not “allowing or disallowing specific curated categories, but our systems are able to scalably understand topics for which these types of features should or should not trigger.”&lt;/p&gt;
&lt;p id="ocHpkr"&gt;Google search, its inputs, outputs, algorithms, and language models have all become almost unimaginably complex. When Google tells us that it is able to understand the contents of videos now, we take for granted that it has the computing chops to pull that off — but the reality is that even just indexing such a massive corpus is a monumental task that dwarfs the original mission of indexing the early web. (Google is only indexing audio transcripts of a subset of YouTube, for the record, though with MUM it aims to do visual indexing and other video platforms in the future). &lt;/p&gt;
&lt;p id="EHV15b"&gt;Often when you’re speaking to computer scientists, the &lt;a href="https://en.wikipedia.org/wiki/Travelling_salesman_problem"&gt;traveling salesman problem&lt;/a&gt; will come up. It’s a famous conundrum where you attempt to calculate the shortest possible route between a given number of cities, but it’s also a rich metaphor for thinking through how computers do their machinations. &lt;/p&gt;
&lt;p id="1SWuVC"&gt;“If you gave me all the machines in the world, I could solve fairly big instances,” Raghavan says. But for search, he says that it is unsolved and perhaps unsolvable by just throwing more computers at it. Instead, Google needs to come up with new approaches, like MUM, that take better advantage of the resources Google can realistically create. “If you gave me all the machines there were, I’m still bounded by human curiosity and cognition.”&lt;/p&gt;
&lt;p class="c-end-para" id="6UQaHl"&gt;Google’s new ways of understanding information are impressive, but the challenge is what it will do with the information and how it will present it. The funny thing about the traveling salesman problem is that nobody seems to stop and ask what exactly is in the case, what is he showing all his customers as he goes door to door?&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22698504/google-search-on-event-ai-mum-google-lens-update-changes"/>
    <id>https://www.theverge.com/2021/9/29/22698504/google-search-on-event-ai-mum-google-lens-update-changes</id>
    <author>
      <name>Dieter Bohn</name>
    </author>
  </entry>
  <entry>
    <published>2021-09-29T13:40:00-04:00</published>
    <updated>2021-09-29T13:40:00-04:00</updated>
    <title>Google Maps is making it easier to see wildfires and tree coverage</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="Google’s Tree Canopy Lab" src="https://cdn.vox-cdn.com/thumbor/kvH5CmxgHi-PBZ_NxZA9Jo617yQ=/79x0:922x562/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69926815/Tree_Canopy_Insights.0.png" /&gt;
        &lt;figcaption&gt;Google’s Tree Canopy Lab | Image: Google&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="VwAh1r"&gt;Google Maps has new features that should make it easier for users to see wildfires, tree canopy, and locations without formal addresses. It’s all aimed at helping communities be “safer, more sustainable, and discoverable,” according to the company.&lt;/p&gt;
&lt;p id="MJRZkH"&gt;A new wildfire layer on Maps will begin rolling out globally this week, Google announced today. It’ll show most major fires, those prompting evacuations, across the world. Red splotches and pins on the layer will indicate where blazes are and how far they’ve spread. By tapping on any single wildfire, users can see more information, like how many acres have burned, what percentage of the fire has been contained, and links to local emergency resources. &lt;/p&gt;
&lt;p id="09agYi"&gt;The map is updated about every hour. It uses data that’s gathered by satellites that are then processed by Google’s geospatial analysis platform, Earth Engine.  &lt;/p&gt;
&lt;p id="NQEPmX"&gt;It builds on a feature &lt;a href="https://www.theverge.com/2020/8/20/21376343/google-maps-search-wildfire-boundaries-california"&gt;Google piloted in California&lt;/a&gt; in 2019 and expanded to the rest of the US in 2020. That pilot allowed people in the US to see the shifting boundaries of a nearby wildfire in near real-time but didn’t allow for users to view more than one fire at a time. That was limiting since multiple blazes can break out simultaneously within the same region during fire season, which is growing longer and more intense in the Western US as a result of climate change. &lt;/p&gt;
&lt;div class="c-float-left"&gt;  &lt;figure class="e-image"&gt;
        &lt;img alt="Google rolled out a new wildfire layer on Maps." data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/rAGjPu9Aqc61IuxBXUN1QgulZBo=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22885246/wildfire_layer_2.gif"&gt;
      &lt;cite&gt;GIF: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Google plans to roll out a new wildfire layer on Maps.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p id="ReVAIE"&gt;Not only will users in the US now have a bird’s-eye view of major fires nearby; they’ll also even be able to see small fires on the map — a feature not yet available in other parts of the world. Australia and some other locations could have that capability “in the coming months,” Google says. &lt;/p&gt;
&lt;p id="pAGnJ2"&gt;There’s still one limitation, however, that Google has yet to hammer out with the wildfire layer. It can only be updated when the user is connected to the internet, which poses problems when wildfires &lt;a href="https://www.sfchronicle.com/california-wildfires/article/Wildfire-threat-Bay-Area-cell-phone-internet-15495592.php"&gt;take out power lines and cell towers&lt;/a&gt;.&lt;/p&gt;
&lt;p id="FZlghR"&gt;The other two updates Google announced are primarily designed to give local governments better planning and mapping tools. Google &lt;a href="https://www.theverge.com/2020/11/18/21573081/google-new-tool-hot-cities-trees-climate-change-temperature"&gt;launched a Tree Canopy Lab&lt;/a&gt; last year in Los Angeles to help the city identify spots in need of more trees. Getting more greenery to those neighborhoods is one way to prevent heat-related illnesses and death in places that are the most vulnerable during heatwaves. Temperatures can actually differ &lt;a href="https://www.theverge.com/2021/7/14/22575481/nyc-urban-heat-island-effect-thermal-camera-surface-temperature"&gt;from neighborhood to neighborhood&lt;/a&gt; based on how much greenery they have, a trend that is &lt;a href="https://www.theverge.com/2021/5/25/22452777/hot-summer-weather-race-urban-heat-island-us-cities"&gt;made worse by a history of racial segregation&lt;/a&gt; in the US. &lt;/p&gt;
&lt;p id="hZNWro"&gt;Google says Tree Canopy will be available in over 100 more cities in 2022, including Guadalajara, London, Sydney, and Toronto. It charts out the density of tree cover using images taken by plane for Google Maps.&lt;/p&gt;
&lt;div class="c-float-right"&gt;  &lt;figure class="e-image"&gt;
        &lt;img alt="Google’s new Address Maker app makes it easier for users to chart routes to locations without a formal address." data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/xp94RbXGKXvm48CyM4MoxomfvVo=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22885339/Address_Maker__1_.gif"&gt;
      &lt;cite&gt;GIF: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Google’s new Address Maker app makes it easier for users to chart routes to locations without a formal address.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p id="pSXyTR"&gt;Using Google Maps has always been more challenging for people who live in places without formal addresses. That’s made it more difficult for packages and emergency services like ambulances to reach many people living in rural areas and developing nations where addresses aren’t commonplace. So Google today launched a new app called Address Maker to make it easier for governments and NGOs to build out their own maps to fill in the blanks. &lt;/p&gt;
&lt;p id="XoJE2V"&gt;Address Maker is an expansion to Google’s existing &lt;a href="https://www.theverge.com/2020/5/28/21272954/google-maps-plus-code-geolocation-digital-address-six-digit"&gt;Plus Codes&lt;/a&gt;, which allow people to share their location using a six-digit code in lieu of an address. (There are also Plus Codes for places with traditional addresses. The Plus Code for Google’s headquarters, for example, is ‘CWC8+R9 Mountain View, California.’)&lt;/p&gt;
&lt;p id="bkReJE"&gt;While a Plus Code for places without addresses will show up on Google Maps, a route to reach it might not. Address Maker aims to make it faster and easier for officials and local advocates to assign Plus Codes to locations and chart previously unmapped roads for an entire community. It uses an open-source algorithm, and Plus Codes are accessible offline.&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22698533/google-maps-wildfires-tree-coverage-tracker"/>
    <id>https://www.theverge.com/2021/9/29/22698533/google-maps-wildfires-tree-coverage-tracker</id>
    <author>
      <name>Justine Calma</name>
    </author>
  </entry>
  <entry>
    <published>2021-09-29T13:31:15-04:00</published>
    <updated>2021-09-29T13:31:15-04:00</updated>
    <title>Google’s Search On fall 2021 event: news and announcements</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/nYDhE-z3bLPg6F5xmQxa1KlxSZw=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69926764/acastro_180427_1777_0001.5.jpg" /&gt;
        &lt;figcaption&gt;Illustration by Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p&gt;All the new features coming to the search giant’s core product&lt;/p&gt; &lt;p id="kXiPpK"&gt;Google is announcing several new updates and features to its core search functions for its &lt;a href="https://searchon.withgoogle.com/"&gt;Search On fall 2021 event&lt;/a&gt;. The new changes to search utilize AI through Google Lens visual searches in new ways; allow for easier window shopping online through local stores, checking what’s in stock at a nearby shop right from a product search; and introduce improvements to tracking wildfires through Google Maps, among other things.&lt;/p&gt;
&lt;p id="RtLxTM"&gt;We have all the info on the latest features to come to Google search and will continue to monitor how these new updates take shape. Stay tuned to check out all the coverage of Search On 2021 and Google’s new announcements.&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22699191/google-search-on-fall-2021-news-announcements"/>
    <id>https://www.theverge.com/2021/9/29/22699191/google-search-on-fall-2021-news-announcements</id>
    <author>
      <name>Dieter Bohn</name>
      <name>Justine Calma</name>
      <name>Antonio G. Di Benedetto</name>
      <name>James Vincent</name>
    </author>
  </entry>
  <entry>
    <published>2021-09-29T13:30:11-04:00</published>
    <updated>2021-09-29T13:30:11-04:00</updated>
    <title>Google expands shopping searches with Lens and in-store inventory checks</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/EWprwDNQAGM1IAQg-57T1fcZMp4=/141x0:1659x1012/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69926747/Search_On_Lens_Desktop_screenshot.0.jpg" /&gt;
        &lt;figcaption&gt;Image: Google&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="woIlY1"&gt;Shopping online isn’t always a convenience. If you enjoy window shopping or browsing curated collections at a brick-and-mortar store for inspiration, finding something online you don’t yet know you want or are unaware of is tricky if you start with a text search. Google is announcing new shopping search tools to try to alleviate this, with features that utilize Google Lens for finding products to buy from pictures online, broader search terms to help you browse clothing, and the ability to check in-store inventory from home. It claims the new tools will help shoppers “find what they’re looking for in a more visual way.” This comes after &lt;a href="https://www.theverge.com/2020/4/21/21228741/google-shopping-free-listing-ads-search-coronavirus-covid"&gt;Google allowed all businesses to create listings on Google Shopping for free&lt;/a&gt; last year. Now, it wants more window shopping to be done right from Google search.&lt;/p&gt;
&lt;p id="vXNJqK"&gt;Google Lens has been around since 2017, &lt;a href="https://www.theverge.com/2018/8/17/17715048/google-goggles-app-update-lens-ai"&gt;replacing Google Goggles&lt;/a&gt; that came before it, with &lt;a href="https://www.theverge.com/2018/5/8/17333154/google-lens-ai-ar-live-demo-hands-on-io-2018"&gt;the ability to use a smartphone camera to conduct visual searches&lt;/a&gt; based on the identification of objects found in the real world. Those image searches have allowed users to learn more about the things around them, even finding the same or similar item to buy without looking for a label or barcode to scan. Now, Google wants to make it possible to shop for any product you see in an image or video on the web with nothing more than the picture itself. Soon, iOS users will have a new dedicated button in the Google app, allowing a Google Lens search of any image on a page to bring up Google Shopping listings for purchase through a visual match. The feature will also be coming to Chrome on the desktop. &lt;/p&gt;
&lt;p id="HJBNhK"&gt;Google did not give specific dates for this feature launching on iOS or desktop, stating that it hopes to roll them out by the end of the year. There was no initial mention of if or when this feature may come to Android, but Google has clarified that it plans to extend this functionality to Android at a later date, after the iOS and desktop versions.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/jV9ZIZu0xgGoDD9LE6cp4IyvlGg=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22885615/Search_On_Lens_Mode.gif"&gt;
      &lt;cite&gt;GIF: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Google Lens search in the Google app on iOS.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/6IMBomtnwiJfFffKLOs5F8VPJ70=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22885621/Search_On_Lens_Desktop.gif"&gt;
      &lt;cite&gt;GIF: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Desktop Google Lens search on Chrome.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="x8M8Tu"&gt;Another shopping-focused feature coming from Google, which has surely been spurred on by the boom in e-commerce since the beginning of the pandemic, is easier browsing of clothing, accessories, and shoes via search results based on general terms. Google says that if you search for a generic article on mobile, for example, “cropped jackets,” you will see a visual feed of that type of clothing in a variety of colors and designs. These visual results will be accompanied by relevant videos, style guides, or local shops that carry those styles. From there, you can filter your search further according to brand, style, or department; check ratings and reviews; or compare prices on the results that appeal to you most.&lt;/p&gt;
&lt;p id="3Gtmdk"&gt;Google calls this window shopping, which is one of the challenges of shopping for clothes online compared to going to a physical store to see what’s on display. It claims the dataset is pulled from over 24 billion product listings. The new feature is available only on mobile and is usable right from a Google search beginning today.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/9pSNZFTXk45QIDYEBu3b3oCDkK8=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22885624/Shoppable_Search.gif"&gt;
      &lt;cite&gt;GIF: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Shoppable search within the Google app.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="MK3OxF"&gt;The third and final Google Shopping update allows users to remotely check in-store inventory directly within a Google search. Shoppers searching for a product are able to filter by “in stock.” This selection should show nearby stores that have the item available. Google claims the new feature can help a small business attract new customers, though it remains to be seen how accurate it might be across a variety of retailers and how one might ensure a product is there for them once they arrive — particularly at small businesses that do not have curbside or in-store pickup. &lt;/p&gt;
&lt;p id="VDIDtP"&gt;Google has indicated that it does rely on data from the retailer to determine stock status and claims it will only indicate an item is in stock when there is high confidence; otherwise, it may show limited stock.&lt;/p&gt;
&lt;p id="NAnntE"&gt;The new “in stock” filter is available today across mobile web browsers and the Google app on both iOS and Android.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/J0k_JYSB5tKG7oj6AYQxQV4YdL0=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22883129/SWIS_in_stock_filter.gif"&gt;
      &lt;cite&gt;GIF: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Google’s in-stock filtering when shopping nearby locations for an item.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;


</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22696646/google-shopping-lens-search-inventory-check-ios-chrome"/>
    <id>https://www.theverge.com/2021/9/29/22696646/google-shopping-lens-search-inventory-check-ios-chrome</id>
    <author>
      <name>Antonio G. Di Benedetto</name>
    </author>
  </entry>
  <entry>
    <published>2021-09-29T13:23:06-04:00</published>
    <updated>2021-09-29T13:23:06-04:00</updated>
    <title>Google Lens will soon search for words and images combined</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/iQI8hANhfsaYapnfkDq449xSta8=/0x0:1800x1200/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69926700/google_lens_socks_with_this_pattern_pic.0.jpg" /&gt;
    &lt;/figure&gt;

  &lt;p id="jEnRgj"&gt;Google is updating its visual search tool Google Lens with new AI-powered language features. The update will let users further narrow searches using text. So, for example, if you snap a photo of a paisley shirt in order to find similar items online using Google Lens, you can add the command “socks with this pattern” to specify the garments you’re looking for. &lt;/p&gt;
&lt;p id="ahaSzW"&gt;Additionally, Google is launching a new “Lens mode” option in its iOS Google app, allowing users to search using any image that appears while searching the web. This will be available “soon,” but it’ll be limited to the US. Google is also launching Google Lens on desktop within the Chrome browser, letting users select any image or video when browsing the web to find visual search results without leaving their tab. This will be available globally “soon.”	&lt;/p&gt;
&lt;p id="wpZIUW"&gt;These updates are part of Google’s latest push to improve its search tools using AI language understanding. The updates to Lens are powered by a machine learning model that the company unveiled at I/O earlier this year named MUM. In addition to these new features, Google is also introducing new AI-powered tools to its web and mobile searches.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/1NsWwk0iw40qIuXHElj4LhkdBG4=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22884223/google_lens_derailleur.gif"&gt;
      &lt;cite&gt;Image: Google&lt;/cite&gt;
      &lt;figcaption&gt;
&lt;em&gt;Using the updated Google Lens to identify a bike’s derailleur&lt;/em&gt;. &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="cUlfhG"&gt;The changes to Google Lens show the company hasn’t lost interest in this feature, which has always shown promise but seemed to appeal more as a novelty. Machine learning techniques have made object and image recognition features relatively easy to launch at a basic level, but, as today’s updates show, they require a little finesse on the part of the users to be properly functional. Enthusiasm may be picking up, though — Snap &lt;a href="https://www.theverge.com/2021/8/26/22642017/snapchat-scan-feature-ar-camera-visual-search"&gt;recently upgraded its own Scan feature&lt;/a&gt;, which functions pretty much identically to Google Lens. &lt;/p&gt;
&lt;p id="JAiBQc"&gt;Google wants these Lens updates to turn its world-scanning AI into a more useful tool. It gives the example of someone trying to fix their bike but not knowing what the mechanism on the rear wheel is called. They snap a picture with Lens, add the search text “how to fix this,” and Google pops up with the results that identify the mechanism as a “derailleur.” &lt;/p&gt;
&lt;p id="URUqLp"&gt;As ever with these demos, the examples Google is offering seem simple and helpful. But we’ll have to try out the updated Lens for ourselves to see if AI language understanding is really making visual search more than just a parlor trick. &lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22698014/google-lens-update-text-search-desktop-chrome"/>
    <id>https://www.theverge.com/2021/9/29/22698014/google-lens-update-text-search-desktop-chrome</id>
    <author>
      <name>James Vincent</name>
    </author>
  </entry>
  <entry>
    <published>2021-09-29T13:22:40-04:00</published>
    <updated>2021-09-29T13:22:40-04:00</updated>
    <title>Google is using AI to help users explore the topics they’re searching for — here’s how</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/OXKtcKb_23TFAF1Du1aHDeGJ6Gw=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69926694/acastro_180508_1777_google_IO_0003.0.jpg" /&gt;
        &lt;figcaption&gt;Illustration by Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="M03k9t"&gt;“Can you get medicine for someone at the pharmacy?”&lt;/p&gt;
&lt;p id="B5vnm2"&gt;It’s a simple enough question for humans to understand, says Pandu Nayak, vice president of search at Google, but such a query represents the cutting-edge of machine comprehension. You and I can see that the questioner is asking if they can fill out a subscription for &lt;em&gt;another person&lt;/em&gt;, Nayak tells &lt;em&gt;The Verge&lt;/em&gt;. But until recently, if you typed this question into Google, it would direct you to websites explaining how to fill out &lt;em&gt;your&lt;/em&gt; prescription. “It missed the subtlety that the prescription was for someone else,” he says.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="xH4SyC"&gt;&lt;q&gt;MUM is Google’s biggest, brightest AI language model&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="rrwnss"&gt;The key to delivering the right answer, says Nayak, is AI, which Google is using today to improve its search results. The prescription query was solved in 2019, when Google integrated a machine learning model &lt;a href="https://www.theverge.com/2019/10/25/20931657/google-bert-search-context-algorithm-change-10-percent-langauge"&gt;called BERT&lt;/a&gt; into search. As part of a new generation of AI language systems known as large language models (the most famous of which is OpenAI’s GPT-3), BERT was able to parse the nuances of our prescription query correctly and return the right results. Now, in 2021, Google is updating its search tools yet again, using another acronymized AI system that’s BERT’s successor: MUM.&lt;/p&gt;
&lt;p id="yjWpoX"&gt;Originally&lt;a href="https://blog.google/products/search/introducing-mum/"&gt; revealed&lt;/a&gt; at Google I/O in May, MUM is at least 1,000 times bigger than BERT, says Nayak; on the same order of magnitude as GPT-3, which has 175 billion parameters. (Parameters being a measure of a model’s size and complexity.) MUM is also multimodal, meaning it processes visual data as well as text. And it’s been trained on 75 languages, which allows the system to “generalize from languages where there’s a lot of data, like English, to languages where there’s less data, like Hindi,” says Nayak. That helps in ensuring that any upgrades it provides are spread across Google’s many markets.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/Vo4XOpVcCPykH4Q7FUe55J6_Hgg=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22885162/Untitled__2_.gif"&gt;
      &lt;cite&gt;Image: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;A new feature rolling out in the coming months named “Things to know” will use AI to help users explore topics related to their searches. &lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="GUJBlL"&gt;Nayak speaks of MUM with pride, as the latest AI wunderkind trained in Google’s labs. But the company is also cautious. Large language models are controversial for a number of reasons. They’re prone to lying, for example — as happy writing fiction as fact. And they’ve been shown time and time again to encode racial and gender biases. This is a problem that Google’s own researchers have highlighted and been shot down for doing so. Notably, Google fired two of its top ethics researchers, &lt;a href="https://www.theverge.com/2020/12/3/22150355/google-fires-timnit-gebru-facial-recognition-ai-ethicist"&gt;Timnit Gebru&lt;/a&gt; and &lt;a href="https://www.theverge.com/2021/2/19/22292011/google-second-ethical-ai-researcher-fired"&gt;Margaret Mitchell&lt;/a&gt;, after &lt;a href="https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/"&gt;they co-authored a paper&lt;/a&gt; highlighting problems with exactly this technology.&lt;/p&gt;
&lt;p id="378AJV"&gt;For these reasons, perhaps, the changes to search that Google is launching are relatively restrained. The company is introducing three new features “in the coming months,” some powered by MUM, each of which is ancillary to its search engine’s primary function — ranking web results. But Nayak says they’re just the tip of the iceberg when it comes to Google’s ambitions to improve its products with AI. “To me, this is just the start,” he says.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="1ANHUv"&gt;&lt;q&gt;“Things to know” will be automatically generated based on users’ searches&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="sZcWPG"&gt;First, though, the features. Number one is called “Things to know” and acts as an advanced snippet function, pulling out answers to predicted questions based on user’s searches. Type in “acrylic painting,” for example, and “Things to know” will automatically generate new queries, like “How do you use household items in acrylic painting.” Nayak says there are certain “sensitive queries” that &lt;em&gt;won’t&lt;/em&gt; trigger this response (like “bomb making”) but that most topics are automatically covered. It will be rolling out in the “coming months.”&lt;/p&gt;
&lt;p id="o6n8Xp"&gt;The second new feature suggests further searches that might help users broaden or refine their queries. So, with the “acrylic painting” search above, Google might now suggest a narrower focus, like “acrylic painting techniques,” or a broader remit, like “different styles of painting.” As Nayak puts it, Google wants to use AI’s ability to recognize “the space of possibilities within [a] topic” and help people explore variants of their own searches. This feature will be available immediately, though it is not powered by MUM. &lt;/p&gt;
&lt;p id="VBAKnC"&gt;The third new feature is more straightforward and based on video transcription. When users are searching for video content, Google will use MUM to suggest new searches based on what it hears within the video. Nayak gives the example of watching a video about Macaroni penguins and Google suggesting a new search of “Macaroni penguin life story.” Again, it’s about suggesting new areas of search for users. This feature will launch on September 29th in English in the US.&lt;/p&gt;
&lt;p id="6YiNuE"&gt;In addition to these AI-based changes, Google is also expanding its “&lt;a href="https://www.theverge.com/2021/7/22/22587197/google-search-about-this-result-feature-context-explanation-terms-language"&gt;About This&lt;/a&gt;” feature in search, which will give new information about the source of results. It’s also bringing its MUM-powered AI smarts to its visual search tech, Google Lens.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/OjI7isUaz_bNGkDo-LY1WErv5Eo=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22885141/Untitled.png"&gt;
      &lt;cite&gt;Image: Google&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Google will give users new option to “refine” or “broaden” their search — using MUM to explore related topics. &lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="8abtQD"&gt;The change to search is definitely the main focus, but what’s interesting is also what Google &lt;em&gt;isn’t&lt;/em&gt; launching. When it demoed MUM at I/O earlier this year, it showed off ambitious features where users could literally talk to the subjects of their searches, like the dwarf planet Pluto, and &lt;a href="https://www.theverge.com/2021/5/18/22442328/google-io-2021-ai-language-model-lamda-pluto"&gt;ask them questions&lt;/a&gt;. In another, users asked expansive questions, like “I just hiked Mt. Adams, I want to hike Mt. Fuji in the fall. What should I do differently?” before being directed to relevant snippets and web pages.&lt;/p&gt;
&lt;p id="ZJDMN8"&gt;It seems these sorts of searches, which are rooted deeply in the functionality of large language models, are too free-form for Google to launch publicly. Most likely, the reason for this is that the language models could easily say the wrong thing. That’s when those bias problems come into play. For example, when GPT-3 is &lt;a href="https://www.vox.com/future-perfect/22672414/ai-artificial-intelligence-gpt-3-bias-muslim"&gt;asked to complete a sentence&lt;/a&gt; like “Audacious is to boldness as Muslim is to …,” nearly a quarter of the time, it finishes the sentence with the word “terrorism.” These aren’t problems that are easy to navigate.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="ZGE2Qo"&gt;&lt;q&gt;“Even if the model has biases, we’re not putting it out for people to consume directly”&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="ASR31v"&gt;When questioned about these difficulties, Nayak reframes the problems. He says it’s obvious that language models suffer from biases but that this isn’t necessarily the challenge for Google. “Even if the model has biases, we’re not putting it out for people to consume directly,” he says. “We’re launching products. And what matters is, are the products serving our users? Are they surfacing undesirable things or not?”&lt;/p&gt;
&lt;p id="grzlvU"&gt;But the company can’t completely stamp out these problems in its finished products either. Google’s Photo app infamously&lt;a href="https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai"&gt; tagged Black people as “gorillas”&lt;/a&gt; in one well-known incident, and the sort of racial and gender-based discrimination present in language AI is often much more subtle and difficult to detect.&lt;/p&gt;
&lt;p id="1XxEC5"&gt;There’s also the problem of what the shift to AI-generated answers might mean for the wider future of Google search. In a&lt;a href="https://arxiv.org/pdf/2105.02274.pdf"&gt; speculative paper&lt;/a&gt; published earlier this year, Google’s researchers considered the question of replacing search altogether with large language models and &lt;a href="https://www.theverge.com/2021/5/26/22454513/google-future-of-search-conversation-ai-mum-lamda"&gt;highlighted a number of difficulties&lt;/a&gt; with the approach. (Nayak is definitive that this is not a serious prospect for the company: “That is absolutely not the plan.”)&lt;/p&gt;
&lt;p id="DKOiYb"&gt;And there’s also the consistent grumbling that Google continues to take up more space in search results with its own product, shunting searches to Google Shopping, Google Maps, and so on. The new MUM-powered “Things to know” feature certainly seems to be part of this trend: filleting out the most informative search results from web pages, and potentially stopping users from clicking through, and therefore sustaining the creator of that data.&lt;/p&gt;
&lt;p id="GJ9cCC"&gt;Nayak’s response to this is that Google delivers more traffic to the web each year and that if it doesn’t “build compelling experiences” for users, then the company “will not be around to send traffic to the web” in the future. It’s not a wholly convincing answer. Google may deliver more traffic each year, but how much of that is just a function of increasing web use? And even if Google does disappear from search, wouldn’t other search engines pick up the slack in sending people traffic?&lt;/p&gt;
&lt;p id="T9mP4x"&gt;Whatever the case, it’s clear that the company is putting AI language understanding at the heart of its search tools — at the heart of Google, indeed. There are many open questions about the challenges of integrating this tech, but for now, Google is happy to continue the search for answers of its own.&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22696268/google-search-on-updates-ai-mum-explained"/>
    <id>https://www.theverge.com/2021/9/29/22696268/google-search-on-updates-ai-mum-explained</id>
    <author>
      <name>James Vincent</name>
    </author>
  </entry>
  <entry>
    <published>2021-09-29T13:16:46-04:00</published>
    <updated>2021-09-29T13:16:46-04:00</updated>
    <title>Ah fuck: YouTube reinstates classic ‘I can’t believe you’ve done this’ meme</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/uSJbf66JxNC5YwGRsrlZ-9sMhfE=/154x0:825x447/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69924627/Screen_Shot_2021_09_29_at_10.19.08_AM.0.png" /&gt;
    &lt;/figure&gt;

  &lt;p id="JP9Z5p"&gt;Whether it wants to be or not, YouTube is a guardian of internet history, with countless classic videos sitting in its archives. That makes it jarring when the company acts like what it is: a multinational corporation with no real understanding of this value. Case in point, this week, YouTube removed the original upload of the “&lt;a href="https://knowyourmeme.com/memes/i-cant-believe-youve-done-this"&gt;Ah fuck, I can’t believe you’ve done this&lt;/a&gt;” meme, initially rejecting an appeal from its creator (and the guy who can’t believe this was done) and claiming that the clip violates the company’s “violent or graphic content policy.”&lt;/p&gt;
&lt;p id="ADM4jv"&gt;In the words of &lt;a href="http://paulweedon.co.uk/"&gt;Paul Weedon&lt;/a&gt; himself, star of and uploader of the original video: Ah, fuck. “It’s completely out of the blue,” Weedon told &lt;em&gt;The Verge&lt;/em&gt; regarding the takedown. “I made the case that it’s been online going on 15 years and is basically part of internet culture.” &lt;/p&gt;
&lt;p id="iGSbp7"&gt;Weedon &lt;a href="https://twitter.com/Twotafkap/status/1442952265552187394"&gt;tweeted&lt;/a&gt; out the news of the takedown and his unsuccessful appeal, noting that the video had been on YouTube for 14 years and racked up 12 million views in that time with “no issues whatsoever.” &lt;/p&gt;
&lt;p id="iIdHJI"&gt;However, &lt;a href="https://www.youtube.com/watch?v=wKbU8B-QVZk"&gt;now the video is available again&lt;/a&gt;. In a &lt;a href="https://twitter.com/TeamYouTube/status/1443247423250444295"&gt;response tweet&lt;/a&gt; from its TeamYouTube account, the video hoster now says “this was a mistake on our end and your video is back up.”&lt;/p&gt;
&lt;div id="lVy7jq"&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p lang="en" dir="ltr"&gt;So this has happened. &lt;a href="https://t.co/O2baPp0ybq"&gt;pic.twitter.com/O2baPp0ybq&lt;/a&gt;&lt;/p&gt;— Paul Weedon (@Twotafkap) &lt;a href="https://twitter.com/Twotafkap/status/1442952265552187394?ref_src=twsrc%5Etfw"&gt;September 28, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id="cypzMC"&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p lang="en" dir="ltr"&gt;Following up: this was a mistake on our end and your video is back up. We’re so sorry about the frustration this caused &amp;amp; have shared feedback with the team to prevent similar mistakes in the future. Look out for an email from us with more soon.&lt;/p&gt;— TeamYouTube (@TeamYouTube) &lt;a href="https://twitter.com/TeamYouTube/status/1443247423250444295?ref_src=twsrc%5Etfw"&gt;September 29, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;p id="fhvPe6"&gt;The clip is a classic: low-res, contextless, and instantly funny. And, of course, &lt;a href="https://youtu.be/O7lRV1VHv1g"&gt;countless re-uploads&lt;/a&gt; still exist (alongside the other, legitimately horrible content that YouTube is happy to leave up). &lt;/p&gt;
&lt;p id="TYDRV4"&gt;The video has been &lt;a href="https://knowyourmeme.com/memes/i-cant-believe-youve-done-this"&gt;remixed and re-memed&lt;/a&gt; in countless ways since it first went viral in the mid-2010s, and Weedon himself has an interesting relationship with the content. In an &lt;a href="https://www.vice.com/en/article/4avnqj/my-life-as-a-meme-i-cant-believe-youve-done-this-revisited"&gt;article for &lt;em&gt;Vice&lt;/em&gt;&lt;/a&gt; published earlier this year, he describes how the clip was part of a series of “stunts” he and his friends filmed in the vein of &lt;em&gt;Jackass&lt;/em&gt; and how he sold the original rights for the video to the now-defunct &lt;a href="http://Break.com"&gt;Break.com&lt;/a&gt; and isn’t now sure who even owns the IP. &lt;/p&gt;
&lt;p id="RmVoXM"&gt;“At the time, going viral wasn’t really comparable to any other experience and it certainly wasn’t something I could discuss in solidarity with my friends,” writes Weedon. “All of a sudden you’re everywhere and it’s out of your control. You either try to fight it and get destroyed or embrace it and try to cash in. After yanking down several other videos on my YouTube channel, I opted for the latter.”&lt;/p&gt;
&lt;p id="ieeNQv"&gt;To add insult to injury, Weedon is currently exploring the possibility of making a documentary about the meme, so the takedown added a little twist to the proceedings. “This takes things in a completely different direction,” he tells &lt;em&gt;The Verge&lt;/em&gt;. “It says a lot about where YouTube is going. On Twitter, people seem to be saying the same thing: that this is a video that represents what YouTube used to be, and they’ve torched it.”&lt;/p&gt;
&lt;p id="VXLG9y"&gt;Weedon says he’s found all the footage from the day the meme was shot and that he and a group of friends are still exploring how to tell the story. “We’re still figuring out what we want to do with it,” he says. “Though, I definitely regret promising the documentary would be coming ‘soon’ in the teaser. I forgot how impatient people on the internet can be.”&lt;/p&gt;
&lt;div id="FUUguD"&gt;&lt;div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"&gt;&lt;iframe src="https://www.youtube.com/embed/o1yfyzT3Q0c?rel=0" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute; border: 0;" allowfullscreen="" scrolling="no" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture;"&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p id="AbL5kb"&gt;&lt;em&gt;&lt;strong&gt;Update, September 29th, 6:13AM ET: &lt;/strong&gt;&lt;/em&gt;&lt;em&gt;Updated the story with comment from Weedon. &lt;/em&gt;&lt;/p&gt;
&lt;p id="1rQDKS"&gt;&lt;em&gt;&lt;strong&gt;Update, September 29th, 1:15PM ET: &lt;/strong&gt;&lt;/em&gt;&lt;em&gt;Updated the story to reflect the video’s reinstatement and added a statement from YouTube.&lt;/em&gt;&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/9/29/22699955/youtube-takes-down-ah-fuck-cant-believe-youve-done-this-meme"/>
    <id>https://www.theverge.com/2021/9/29/22699955/youtube-takes-down-ah-fuck-cant-believe-youve-done-this-meme</id>
    <author>
      <name>James Vincent</name>
    </author>
  </entry>
</feed>
