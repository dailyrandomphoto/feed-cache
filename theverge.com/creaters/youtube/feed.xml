<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <title>The Verge -  YouTubes</title>
  <icon>https://cdn.vox-cdn.com/community_logos/52801/VER_Logomark_32x32..png</icon>
  <updated>2020-11-19T11:13:59-05:00</updated>
  <id>https://www.theverge.com/rss/youtube/index.xml</id>
  <link type="text/html" href="https://www.theverge.com/youtube" rel="alternate"/>
  <entry>
    <published>2020-11-19T11:13:59-05:00</published>
    <updated>2020-11-19T11:13:59-05:00</updated>
    <title>YouTube’s biggest child star is getting his own show on Amazon</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/3JjjAClcZNSPigL7WcOD3lEHjeQ=/58x0:634x384/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/67817241/image002__6_.0.jpg" /&gt;
    &lt;/figure&gt;

  &lt;p id="F89pgV"&gt;Ryan Kaji, the nine-year-old star of YouTube’s &lt;a href="https://www.youtube.com/channel/UChGJGhZ9SOOHvBB0Y4DOO_w"&gt;popular Ryan’s World channel&lt;/a&gt;, is expanding his empire with a new show on Amazon Kids Plus. This marks the first time that a YouTube creator has partnered with Amazon for an original series. The show will debut on November 27th.&lt;/p&gt;
&lt;p id="95dsek"&gt;&lt;em&gt;Super Spy Ryan &lt;/em&gt;is a 30-minute live-action / animation hybrid series that follows Kaji as he’s transported to an animated world and tasked with recovering the Golden Console from a “nefarious hamster” named Packrat, according to a press release. The live-action sequences will feature Kaji and his family, who also appear frequently on his YouTube channel. The channel currently boasts close to 30 million subscribers.  &lt;/p&gt;
&lt;p id="lwfxHs"&gt;“Bringing &lt;em&gt;Super Spy Ryan&lt;/em&gt; as the first original on Amazon Kids Plus is a dream come true for us,” Shion Kaji, Ryan’s father, said in a statement. &lt;/p&gt;
&lt;p id="seWOKE"&gt;Amazon Kids Plus is a subscription &lt;a href="https://www.amazon.com/ftu/home?tag=theverge02-20" rel="sponsored nofollow noopener" target="_blank"&gt;package that costs $3 a month&lt;/a&gt; and gives kids an assortment of videos, books, games, and other entertainment items. The package also includes original content, and Amazon, like other companies, is looking to YouTube creators as a way to try to get kids to spend more time on its platform. &lt;/p&gt;
&lt;p id="5IOUzz"&gt;For Kaji and his family, expanding onto one of the world’s biggest websites is just another step in growing their business. Kaji currently has a show deal with Nickelodeon — a part of ViacomCBS’s plans to try to make Nickelodeon relevant again. His team also launched a Ryan and Friends OTT channel in September. &lt;/p&gt;
&lt;p id="mxlJK3"&gt;Other popular YouTube kids channels are in the process of expanding to streamers and networks. Cocomelon &lt;a href="https://www.theverge.com/2020/10/19/21519095/cocomelon-netflix-moonbug-blippi-hulu-streaming-youtube-children-entertainment"&gt;is now on Netflix&lt;/a&gt;, thanks to deals made by its parent company, Moonbug, and HBO Max is partnering with YouTubers for original series. For streamers and networks, it makes sense: kids already know these characters and can watch new programming featuring their favorite YouTube channels on more secure platforms. Amazon will likely hope to bring over some of the audience from Ryan’s World and keep them on the site with other Amazon Kids Plus offerings. &lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2020/11/19/21575169/youtube-ryan-world-amazon-show-animated-nickelodeon-ott-streaming"/>
    <id>https://www.theverge.com/2020/11/19/21575169/youtube-ryan-world-amazon-show-animated-nickelodeon-ott-streaming</id>
    <author>
      <name>Julia Alexander</name>
    </author>
  </entry>
  <entry>
    <published>2020-11-18T17:23:32-05:00</published>
    <updated>2020-11-18T17:23:32-05:00</updated>
    <title>YouTube will run ads on some creator videos, but it won’t give them any of the revenue</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/yMv4O-RqEFMYTWWWEoV47RyPH0Q=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/67813613/acastro_180321_1777_youtube_0003.0.jpg" /&gt;
        &lt;figcaption&gt;Illustration by Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="j4ghXb"&gt;Starting today, YouTube will begin running ads on some creators’ videos, but it won’t give them a portion of the ad revenue because they’re not big enough to be enrolled in its Partner Program. &lt;/p&gt;
&lt;p id="jOJhzj"&gt;When advertisements run on YouTube videos, those creators typically receive a portion of the revenue through their role in YouTube’s Partner Program. With the new monetization rules, a creator who is not in the partner program “may see ads on some of your videos,” &lt;a href="https://support.google.com/youtube/thread/83733719?hl=en&amp;amp;msgid=83733719"&gt;according to an update to the platform’s Terms of Service&lt;/a&gt;. &lt;/p&gt;
&lt;p id="Kpb50x"&gt;Prior to the update, YouTube says these videos only received ads in limited circumstances, like if they were monetized by a record label as part of a copyright claim. The update will mostly affect smaller creators without a huge viewership; &lt;a href="https://support.google.com/youtube/answer/72851?hl=en"&gt;YouTube’s Partner Program&lt;/a&gt; requires creators to have accrued 4,000 total hours of watch time over the last 12 months and have more than 1,000 subscribers. &lt;/p&gt;
&lt;p id="dlQrbg"&gt;Advertising is big business for YouTube and its parent company, Google, with the video site generating $5 billion &lt;a href="https://abc.xyz/investor/static/pdf/2020Q3_alphabet_earnings_release.pdf?cache=514fb58"&gt;in the last quarter alone&lt;/a&gt;. Advertising is also a big deal for creators, who may rely on the site’s payouts to support themselves. Now, YouTube will be able to run more ads on its platform and won’t have to pay a number of creators in the process. The company confirmed to &lt;em&gt;The Verge &lt;/em&gt;that ads will still not run on videos from non-partnered creators that center on sensitive topics. These include politics, religion, alcohol, and gambling. &lt;/p&gt;
&lt;p id="SCfs0m"&gt;The news &lt;a href="https://twitter.com/search?q=https%3A%2F%2Ftwitter.com%2FTeamYouTube%2Fstatus%2F1329107787817750530&amp;amp;src=typed_query"&gt;did not go over well&lt;/a&gt; with members of the YouTube community. The creator community’s relationship with YouTube over advertising revenue has been fraught for years. In late 2016 and early 2017, YouTube creators who were in the Partner Program were hit by a &lt;a href="https://www.theverge.com/2019/4/5/18287318/youtube-logan-paul-pewdiepie-demonetization-adpocalypse-premium-influencers-creators"&gt;sudden drop in advertising revenue&lt;/a&gt; as the platform struggled to contain &lt;a href="https://www.theverge.com/2017/12/8/16751206/elsagate-youtube-kids-creepy-conspiracy-theory"&gt;disturbing children’s videos&lt;/a&gt; and other harmful content. Then in 2018, the Logan Paul incident led to changes to the Partner Program and more difficulty for &lt;a href="https://www.theguardian.com/technology/2018/jan/18/youtube-creators-vloggers-ads-logan-paul"&gt;creators to start earning revenue&lt;/a&gt;. &lt;/p&gt;
&lt;p id="ixkhwb"&gt;YouTube didn’t say how many creators will see ads run on their videos without paying out to them, but the company confirmed channels of all sizes may see ads appear. The company will monitor the impact on creators.&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2020/11/18/21573937/youtube-ads-creators-partner-program-revenue-terms-of-service-update"/>
    <id>https://www.theverge.com/2020/11/18/21573937/youtube-ads-creators-partner-program-revenue-terms-of-service-update</id>
    <author>
      <name>Julia Alexander</name>
    </author>
  </entry>
  <entry>
    <published>2020-11-18T07:57:26-05:00</published>
    <updated>2020-11-18T07:57:26-05:00</updated>
    <title>YouTube is adding information about COVID-19 vaccines to its fact-check panels</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/fj1VKD23_l-PB8Qpeyuj9RGFqPo=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/67809774/acastro_180403_1777_youtube_0002.0.jpg" /&gt;
        &lt;figcaption&gt;Illustration by Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="KZxyXM"&gt;Earlier this year, YouTube started adding “&lt;a href="https://uk.pcmag.com/social-networking/126730/youtube-adding-fact-check-information-panels-to-counter-covid-19-misinformation"&gt;information panels&lt;/a&gt;” to videos and searches about COVID-19, directing viewers to authoritative sources in an attempt to combat misinformation about the disease. Now, as work on a COVID-19 vaccine &lt;a href="https://www.theverge.com/2020/11/16/21569345/moderna-covid-19-vaccine-coronavirus-efficacy-announcement"&gt;begins to show early results&lt;/a&gt;, the company is tweaking this panel to also link to info about vaccination.&lt;/p&gt;
&lt;p id="IqOs8n"&gt;It’s a small change to what is already a small intervention in the battle against online misinformation. For relevant videos, the panel will now prompt viewers to “learn about vaccine progress,” linking to sources like the CDC and WHO. The panels have already begun appearing in searches and under videos in the US, &lt;a href="https://www.cnet.com/news/youtube-tweaks-its-covid-19-fact-check-panel-to-add-vaccine-info-too/"&gt;reports &lt;em&gt;CNET&lt;/em&gt;&lt;/a&gt;, and should be rolling out globally in the next couple of days. &lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/5HdciWr3UjqlQhOU1CrvUGZvBa8=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22050000/vaccinealert.jpg"&gt;
      &lt;cite&gt;Image: YouTube&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;The new vaccine alert panel. &lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="YyKEE3"&gt;YouTube certainly has the right idea here. As work on various COVID-19 vaccines progresses, it’s guaranteed that anti-vax conspiracy theorists will try to discourage people from being vaccinated — a trend that will directly harm the public’s health and prolong the damaging effects of the coronavirus pandemic. YouTube will certainly be a prime vector for this sort of misinformation, allowing conspiracy theorists to easily reach viewers.&lt;/p&gt;
&lt;p id="hI5RCr"&gt;However, the Google-owned company has not inspired confidence in its ability to handle misinformation of late. It’s declined to take down videos&lt;a href="https://www.theverge.com/2020/11/12/21562910/youtube-2020-election-trump-misinformation-fake-news-recommendations"&gt; spreading falsehoods about the outcome of the US election&lt;/a&gt;, for example; behavior that is in marked contrast to that of Twitter and Facebook, which have been more active in labelling and removing false claims. Once the anti-vax movement gets started on any viable COVID-19 vaccine, tiny information boxes under conspiracy theory videos will be all too easy to ignore. &lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2020/11/18/21573019/youtube-information-panels-fact-check-boxes-covid-19-vaccination-misinformation"/>
    <id>https://www.theverge.com/2020/11/18/21573019/youtube-information-panels-fact-check-boxes-covid-19-vaccination-misinformation</id>
    <author>
      <name>James Vincent</name>
    </author>
  </entry>
  <entry>
    <published>2020-11-12T16:42:40-05:00</published>
    <updated>2020-11-12T16:42:40-05:00</updated>
    <title>YouTube defends choice to leave up videos with false election claims</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/g3iHDm1BcbCR_CTK0RxF0RlyDA0=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/67781140/acastro_180321_1777_youtube_0003.0.jpg" /&gt;
        &lt;figcaption&gt;Illustration by Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="TYN3Ll"&gt;YouTube is pushing back against claims its platform is helping promote and spread misinformation surrounding the 2020 US election, saying its most popular videos related to the election are from “authoritative” sources. YouTube also claims it takes measures to stop the spread of videos containing false or misleading claims by not surfacing them in search results or through its recommendation engine. &lt;/p&gt;
&lt;p id="qbY1Yg"&gt;“Like other companies, we’re allowing these videos because discussion of election results &amp;amp; the process of counting votes is allowed on YT. These videos are not being surfaced or recommended in any prominent way,” YouTube wrote from its YouTubeInsider account in response to a &lt;a href="https://twitter.com/mhbergen/status/1326934206866321408"&gt;tweet from &lt;em&gt;Bloomberg&lt;/em&gt; journalist Mark Bergen&lt;/a&gt;, who criticized the company’s slow and inconsistent moderation of election content. “The most popular videos about the election are from authoritative news organizations. On average, 88 percent of the videos in top-10 results in the U.S. come from high-auth sources when people search for election-related content.” &lt;/p&gt;
&lt;div id="JN60P9"&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p lang="en" dir="ltr"&gt;The most popular videos about the election are from authoritative news organizations. On average, 88% of the videos in top-10 results in the U.S. come from high-auth sources when people search for election-related content.&lt;/p&gt;— YouTubeInsider (@YouTubeInsider) &lt;a href="https://twitter.com/YouTubeInsider/status/1326967696567586816?ref_src=twsrc%5Etfw"&gt;November 12, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;p id="9gIwXV"&gt;YouTube did not disclose what it considers authoritative, nor did it break down what percentage of views of election content come from users typing in phrases into the search box instead of following certain channels, seeking out those channels, or finding them via Facebook, Reddit, or other social networks. Even though its top-10 results for election content may contain mainstream media sources, YouTube does not appear to be acknowledging how often users seek out videos from untrustworthy sources or find them online through other means. &lt;/p&gt;
&lt;p id="QK30CP"&gt;YouTube has &lt;a href="https://www.bloomberg.com/news/articles/2020-11-10/youtube-election-loophole-lets-some-false-trump-win-videos-spread"&gt;come under fire in the run-up to and after Election Day&lt;/a&gt; for allowing videos from organizations like One America News Network that falsely say President Donald Trump won reelection and that mass voter fraud is responsible for his loss to President-elect Joe Biden. &lt;/p&gt;
&lt;p id="FDnt71"&gt;Unlike Facebook and Twitter, which have been aggressively labeling and removing links and posts that spread false information surrounding the election, YouTube says it &lt;a href="https://www.nytimes.com/2020/11/10/technology/election-misinformation-continues-staying-up-on-youtube.html"&gt;allows people to discuss the outcome of the election and processes like vote counting&lt;/a&gt;, even if they do so in ways that spread unproven conspiracies or peddle false or misleading claims. YouTube claims it counteracts the spread of such content by limiting how discoverable these videos are using search and its recommendation engine. &lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="rYZ6H9"&gt;&lt;q&gt;YouTube permits videos of people repeating fake claims about the election&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="ks5vTV"&gt;However, YouTube appears to be struggling with how to contain the spread of videos uploaded to its platform on other social networks like Facebook, where the videos often go viral too fast before either company is able to slow down their spread. &lt;/p&gt;
&lt;p id="CXRgOk"&gt;In an example displaying the process by which YouTube helps amplify misinformation, &lt;a href="https://www.vice.com/en/article/z3vxdx/youtube-is-doing-basically-nothing-to-stop-election-misinformation-from-spreading"&gt;&lt;em&gt;Vice &lt;/em&gt;reported&lt;/a&gt; on a false claim alleging that RealClearPolitics had rescinded its Pennsylvania call in favor of Biden, which was then circulated by Trump lawyer Rudy Giuliani until right-wing YouTube channel The Next News Network published a video repeating the claim. The video was then circulated on Facebook mainly through links posted to private groups, which makes it hard for Facebook moderators to clamp down on its spread. All the while, The Next News Network is racking up views and even selling merchandise under the video, revenue cuts of which go to YouTube, &lt;em&gt;Vice &lt;/em&gt;reports. &lt;/p&gt;
&lt;p id="Qrt4cB"&gt;YouTube says it’s using an election panel pinned to the top of election-related searches pointing users toward its Google webpage with verified election results. It’s also removing advertising from certain videos that undermine “confidence in elections with demonstrably false information,” &lt;a href="https://www.nytimes.com/2020/11/10/technology/election-misinformation-continues-staying-up-on-youtube.html"&gt;according to &lt;em&gt;The New York Times&lt;/em&gt;&lt;/a&gt;, but YouTube is not removing the videos outright. &lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2020/11/12/21562910/youtube-2020-election-trump-misinformation-fake-news-recommendations"/>
    <id>https://www.theverge.com/2020/11/12/21562910/youtube-2020-election-trump-misinformation-fake-news-recommendations</id>
    <author>
      <name>Nick Statt</name>
    </author>
  </entry>
  <entry>
    <published>2020-11-12T13:34:58-05:00</published>
    <updated>2020-11-12T13:34:58-05:00</updated>
    <title>YouTube is canceling Rewind this year because 2020 has been too much </title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/ZWUArueaNKlWWewThk_l5oGChHk=/224x0:1124x600/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/67779340/rewind.0.jpeg" /&gt;
    &lt;/figure&gt;

  &lt;p id="aKFZpR"&gt;YouTube Rewind, the company’s annual end-of-year tribute to creators, trends, and moments that defined the platform, will not happen this year. &lt;/p&gt;
&lt;p id="oFXHGY"&gt;The company &lt;a href="https://twitter.com/YouTube/status/1326947861305696263"&gt;issued a statement&lt;/a&gt; acknowledging that “2020 has been different,” adding that “it doesn’t feel right to carry on as if it weren’t.” This marks the first time Rewind won’t happen since YouTube started the annual celebration in 2010. Although Rewind has become somewhat of a joke to the &lt;a href="https://www.theverge.com/2019/12/5/20995497/youtube-rewind-trending-videos-james-charles-shane-dawson-jeffree-star-tati-westbrook-beauty-drama"&gt;creator community in recent years&lt;/a&gt; — it became the most &lt;a href="https://www.theverge.com/2018/12/13/18137894/youtube-rewind-2018-dislike-shane-dawson-logan-paul-pewdiepie-mkbhd-philip-defranco"&gt;disliked video of all time in 2018&lt;/a&gt; — it’s still a staple of YouTube’s culture, even if people are just using it to dunk on their own videos. To be fair, even CEO Susan Wojcicki admitted &lt;a href="https://www.theverge.com/2019/2/5/18212519/youtube-rewind-susan-wojcicki-pewdiepie-mkbhd-ninja"&gt;they’re a little cringe-inducing&lt;/a&gt;. &lt;/p&gt;
&lt;p id="ARwLtZ"&gt;“We know that so much of the good that did happen in 2020 was created by all of you,” YouTube’s statement reads. “You’ve found ways to lift people up, help them cope, and make them laugh. You made a hard year genuinely better.” &lt;/p&gt;
&lt;p id="U36dVS"&gt;While many people were stuck at home because of the pandemic, YouTube saw surges in usage as more people spent time streaming video online. By the end of March, views on videos that &lt;a href="https://www.theverge.com/2020/3/27/21197642/youtube-with-me-style-videos-views-coronavirus-cook-workout-study-home-beauty"&gt;centered on “with me”&lt;/a&gt;-stylized content (get ready with me, work out with me, study with me) saw massive increases in viewership, according to the company. Live-streaming &lt;a href="https://www.theverge.com/2020/5/13/21257227/coronavirus-streamelements-arsenalgg-twitch-youtube-livestream-numbers"&gt;also picked up&lt;/a&gt; on YouTube as well as on other platforms like Twitch and Facebook.&lt;/p&gt;
&lt;p id="gKTBoP"&gt;YouTube did not say if Rewind will return for 2021. Like everything right now, I imagine it all depends on what the future brings. All I know is I’ll miss YouTube’s annual attempt to try to celebrate the good on its platform (especially when there’s so much bad), and the inevitable reaction videos from creators explaining in detail just &lt;a href="https://www.theverge.com/2018/12/6/18129044/youtube-rewind-shane-dawson-logan-paul-jake-pewdiepie-vlog-squad-david-dobrik-boxing-ksi"&gt;how wrong YouTube got it again&lt;/a&gt;.&lt;/p&gt;
&lt;p id="cywxNA"&gt;&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2020/11/12/21562697/youtube-rewind-canceled-2020-pandemic-creators-community"/>
    <id>https://www.theverge.com/2020/11/12/21562697/youtube-rewind-canceled-2020-pandemic-creators-community</id>
    <author>
      <name>Julia Alexander</name>
    </author>
  </entry>
  <entry>
    <published>2020-11-11T19:45:09-05:00</published>
    <updated>2020-11-11T19:45:09-05:00</updated>
    <title>YouTube went down around the world, but it’s now fixed</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/ovXufO_IiDC4xokB02URjTXXEx4=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/67775196/wjoel_1777_180403_youtube_006.0.jpg" /&gt;
        &lt;figcaption&gt;Illustration by William Joel / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="94i5uX"&gt;YouTube has recovered from a seemingly worldwide outage that prevented videos from loading for roughly an hour. During the outage, many &lt;em&gt;Verge&lt;/em&gt; staffers were unable to watch videos, and &lt;a href="https://twitter.com/TeamYouTube/status/1326681978037444608"&gt;YouTube confirmed&lt;/a&gt; at 7:23PM ET that something was going on:&lt;/p&gt;
&lt;div id="jsFkmr"&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p lang="en" dir="ltr"&gt;If you’re having trouble watching videos on YouTube right now, you’re not alone – our team is aware of the issue and working on a fix. We’ll follow up here with any updates.&lt;/p&gt;— TeamYouTube (@TeamYouTube) &lt;a href="https://twitter.com/TeamYouTube/status/1326681978037444608?ref_src=twsrc%5Etfw"&gt;November 12, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;p id="X95HsB"&gt;The issue appeared to affect other services that use the YouTube infrastructure too, including YouTube TV and the movies and TV shows you’d purchase through Google TV (formerly known as Google Play Movies &amp;amp; TV). We couldn’t load them.&lt;/p&gt;
&lt;p id="jQg4LO"&gt;Early in the outage, the YouTube website itself seemed to load just fine, but videos themselves would continuously show the loading wheel. One &lt;em&gt;Verge&lt;/em&gt; staffer got a video to load after about a minute. As of about 8:00PM ET, though, we saw error screens like this whenever we tried to watch a video:&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt=" " data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/Nx7QQBCQ4LrfzslCIitSZ67jwiI=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22030908/Screen_Shot_2020_11_11_at_5.02.58_PM.png"&gt;
      &lt;figcaption&gt;&lt;em&gt;A YouTube error message.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="m93cVb"&gt;At 9:13 PM ET, YouTube gave the all-clear:&lt;/p&gt;
&lt;div id="mf9ffQ"&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p lang="en" dir="ltr"&gt;...And we’re back – we’re so sorry for the interruption. This is fixed across all devices &amp;amp; YouTube services, thanks for being patient with us ❤️ &lt;a href="https://t.co/1s0qbxQqc6"&gt;https://t.co/1s0qbxQqc6&lt;/a&gt;&lt;/p&gt;— TeamYouTube (@TeamYouTube) &lt;a href="https://twitter.com/TeamYouTube/status/1326709701526974464?ref_src=twsrc%5Etfw"&gt;November 12, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;p id="fffdxs"&gt;Things seemed to be back as early as 8:30PM ET, but you might have hit a few quirks. At that point, videos played on YouTube’s website seemed to be working as they normally do. On the mobile app, one &lt;em&gt;Verge&lt;/em&gt; staffer saw a few error messages, but those would clear with a refresh. YouTube TV worked on mobile for another &lt;em&gt;Verge&lt;/em&gt; staffer at that point after he force closed the app. &lt;/p&gt;
&lt;p id="QO7QA1"&gt;DownDetector showed a truly tremendous number of user reports of problems with YouTube, indicating the problem was widespread — &lt;a href="https://downdetector.com/status/youtube/"&gt;the DownDetector graph&lt;/a&gt; peaked with more than 280,000 user reports in less than an hour. Numerous users on Twitter reported that YouTube wasn’t working for them, either, and &lt;a href="https://t.co/faRH0nk7wj"&gt;searches spiked&lt;/a&gt; for “is YouTube down.”&lt;/p&gt;
&lt;p id="aAtGHd"&gt;When reached for comment, YouTube pointed us to the tweet we included in this story.&lt;/p&gt;
&lt;p id="VOOUFm"&gt;&lt;em&gt;&lt;strong&gt;Update November 11th, 8:51PM ET&lt;/strong&gt;&lt;/em&gt;&lt;em&gt;: YouTube seems to be recovering, so we have made numerous changes throughout.&lt;/em&gt;&lt;/p&gt;
&lt;p id="zBohwQ"&gt;&lt;em&gt;&lt;strong&gt;Update, 9:16PM ET: &lt;/strong&gt;&lt;/em&gt;&lt;em&gt;Added YouTube’s all-clear.&lt;/em&gt;&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2020/11/11/21561764/youtube-down-outage-loading-videos"/>
    <id>https://www.theverge.com/2020/11/11/21561764/youtube-down-outage-loading-videos</id>
    <author>
      <name>Jay Peters</name>
    </author>
  </entry>
  <entry>
    <published>2020-11-10T13:56:22-05:00</published>
    <updated>2020-11-10T13:56:22-05:00</updated>
    <title>Google is giving free Stadia gaming kits to YouTube premium subscribers</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/TcMQqKZESa1mKmiOcLvvsul-GUs=/0x43:1024x726/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/67767012/google_stadia_chromecast.0.jpg" /&gt;
    &lt;/figure&gt;

  &lt;p id="YSvpJ7"&gt;Google has added another promotion to help push its cloud gaming service Stadia into more households: now YouTube Premium subscribers can get its &lt;a href="https://stadia.google.com/SdFGljSduV5z"&gt;$100 Stadia Premiere Edition bundle for free&lt;/a&gt;. A Google spokesperson told &lt;em&gt;The Verge&lt;/em&gt; that supply for the US promotion has run out, but is still available for eligible YouTube Premium subscribers based in the UK. &lt;/p&gt;
&lt;p id="Vp6tt1"&gt;Google’s Stadia Premiere Edition features one Stadia controller in addition to a Chromecast Ultra. However, it is important to note that the Chromecast Ultra included in this package is the older model, not the recently-released Chromecast with Google TV. Google has said that &lt;a href="https://www.theverge.com/2020/9/30/21492682/new-chromecast-google-tv-stadia-support-launch"&gt;the new Chromecast with Google TV will not support the cloud gaming service&lt;/a&gt; until the first half of 2021.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="iuErbI"&gt;&lt;q&gt;With Stadia’s one-year anniversary coming up next week, the service has come a long way over the last year&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="gAMITr"&gt;Unfortunately, if you plan to subscribe to YouTube Premium now, you will not have the option to redeem this offer. The promotion is only available if you subscribed to YouTube Premium since November 6th as &lt;a href="https://twitter.com/RandomNoobYT/status/1326089055990394882?s=20"&gt;shared by Twitter user @RandomNoobYT&lt;/a&gt;, who posted a screenshot and shared the terms and conditions, which detail which subscribers are eligible for the promotion.&lt;/p&gt;
&lt;p id="v476l3"&gt;With Stadia’s one-year anniversary coming up next week, the service has come a long way over the last year. Stadia has increased its library with many of the hottest blockbuster titles, including &lt;a href="https://community.stadia.com/t5/Stadia-Community-Blog/This-Week-on-Stadia-Viking-invasions-dance-parties-and-the/ba-p/38384"&gt;&lt;em&gt;Assassin’s Creed: Valhalla&lt;/em&gt; and &lt;em&gt;Destiny 2: Beyond Light&lt;/em&gt; arriving at the same time&lt;/a&gt; as other platforms. Stadia has also released various features to its service, such as &lt;a href="https://www.theverge.com/2020/11/5/21546194/google-stadia-family-sharing-digital-games-purchase-share"&gt;Family Sharing&lt;/a&gt;, so that you can share your Stadia library with a friend or family member at no additional cost.&lt;/p&gt;
&lt;p id="qPp2dU"&gt;&lt;em&gt;&lt;strong&gt;Update, November 12th 6:36PM ET&lt;/strong&gt;&lt;/em&gt;&lt;em&gt;: A Google spokesperson reached out to The Verge to confirm the promotion is out of stock in the US. &lt;/em&gt;&lt;/p&gt;
&lt;p id="SEyk6C"&gt;&lt;em&gt;&lt;strong&gt;Correction&lt;/strong&gt;&lt;/em&gt;&lt;em&gt;: An earlier version of this post incorrectly identified the new Chromecast with Google TV as a Chromecast Ultra. We regret the error. &lt;/em&gt;&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2020/11/10/21558820/youtube-premium-subscribers-free-stadia-premiere-edition-bundle-deals"/>
    <id>https://www.theverge.com/2020/11/10/21558820/youtube-premium-subscribers-free-stadia-premiere-edition-bundle-deals</id>
    <author>
      <name>Taylor Lyles</name>
    </author>
  </entry>
  <entry>
    <published>2020-11-06T12:56:01-05:00</published>
    <updated>2020-11-06T12:56:01-05:00</updated>
    <title>Social networks solved their 2016 election problems, but their 2020 problems are bigger</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/ysDIKs-HXiDVd62rfQvDdAGL_go=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/67749278/acastro_20200727_1777_facebookGroups_0001.0.jpg" /&gt;
        &lt;figcaption&gt;Illustration by Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="J6l34v"&gt;As many predicted it would, &lt;a href="https://www.nytimes.com/live/2020/11/05/us/us-election-results"&gt;the outcome of the 2020 election is taking many days to determine&lt;/a&gt;. A surge of misinformation on social networks, &lt;a href="https://www.washingtonpost.com/technology/2020/11/04/election-results-misinformation/"&gt;led by President Trump&lt;/a&gt; and &lt;a href="https://www.buzzfeednews.com/article/craigsilverman/eric-trump-election-disinformation"&gt;his family&lt;/a&gt;, has challenged social networks’ content policies and drawn fresh attention to their enforcement capabilities. It’s too early to answer whether the platforms have risen fully to the 2020 challenge. Given the bellicose nature of the Trumps’ pronouncements this week, their hardest work is very likely still ahead of them.&lt;/p&gt;
&lt;p id="1SdkgF"&gt;But in the meantime, I think it’s not too early to ask a related question: how did platforms handle the &lt;em&gt;2016&lt;/em&gt; challenge in 2020?&lt;/p&gt;
&lt;p id="TTervO"&gt;After all, it was the aftermath of Trump’s election win that &lt;a href="https://www.nbcnews.com/business/consumer/trust-facebook-has-dropped-51-percent-cambridge-analytica-scandal-n867011"&gt;reshaped public opinion about tech platforms&lt;/a&gt;, drew broad regulatory scrutiny around the world, and spurred the companies to reshape their policy and content moderation efforts. Now that all of the ballots have been cast, if not quite tallied, how did platforms rise to the challenge?&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="SHx1Ny"&gt;&lt;q&gt;The News Feed is no longer hospitable to Macedonian teens looking to turn a quick profit by exploiting America’s partisan divide&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="R0MV0m"&gt;There were two major problems that became visible in the immediate aftermath of 2016. (A third — data privacy — popped up about a year later in the form of Cambridge Analytica. But data privacy wasn’t really a campaign story this year.)&lt;/p&gt;
&lt;p id="AnksSB"&gt;The first problem is that what came to be called “fake news” — false and hyper-partisan stories, published both by political operatives and profit-hungry spammers — had overwhelmed Facebook. These stories often outperformed real news stories, &lt;a href="https://www.cnbc.com/2016/12/30/read-all-about-it-the-biggest-fake-news-stories-of-2016.html"&gt;generating millions of interactions throughout the campaign&lt;/a&gt;. Subsequent research found that while fake news likely did not change many votes, &lt;a href="https://www.cnbc.com/2016/12/30/read-all-about-it-the-biggest-fake-news-stories-of-2016.html"&gt;it may have accelerated polarization in the United States&lt;/a&gt;.&lt;/p&gt;
&lt;p id="yDrWTO"&gt;Perhaps worse, from Facebook’s perspective, fake news created a brand problem. If the company’s chief cash cow, the News Feed, came to be seen as a welcome home for hoaxes and spam, the company’s long-term fortunes could be imperiled. And so Facebook &lt;a href="https://www.facebook.com/journalismproject/programs/third-party-fact-checking/how-it-works"&gt;developed partnerships with a bipartisan network of fact-checkers&lt;/a&gt;, added prominent labels to disputed stories, and changed the News Feed algorithm &lt;a href="https://www.theverge.com/2016/6/29/12055124/facebook-news-feed-algorithm-changes"&gt;to favor posts from friends over links from publishers&lt;/a&gt;.&lt;/p&gt;
&lt;p id="01Cdgr"&gt;The result? The News Feed is no longer hospitable to &lt;a href="https://www.buzzfeednews.com/article/craigsilverman/how-macedonia-became-a-global-hub-for-pro-trump-misinfo"&gt;Macedonian teens&lt;/a&gt; looking to turn a quick profit by exploiting America’s partisan divide. And whatever the outcome of the 2020 election, it seems unlikely that “fake news” will be identified as a primary reason anybody won — at least, not of the troll-farm variety that got us all so worked up in 2016.&lt;/p&gt;
&lt;p id="r6E78n"&gt;In the 2020 election, misinformation may have been &lt;a href="https://www.npr.org/2020/10/24/927300432/robocalls-rumors-and-emails-last-minute-election-disinformation-floods-voters"&gt;more likely to spread via robocall, text message, and email than it was via social network&lt;/a&gt;. That suggests bad actors increasingly find Facebook and Twitter too expensive or time-consuming to use to spread hoaxes, which is just what those platforms had hoped for when they began working on integrity efforts.&lt;/p&gt;
&lt;p id="hqFOJs"&gt;That’s the good news. The bad news is that Facebook is far from a healthy information environment. &lt;em&gt;New York Times&lt;/em&gt; journalist Kevin Roose has documented that &lt;a href="https://www.theverge.com/interface/2020/7/22/21332774/facebook-crowdtangle-kevin-roose-nyt-tweets-interactions-reach-engagement"&gt;conservative outrage bait often dominates the top stories of the day in total number of interactions&lt;/a&gt;. And while Facebook argues that those interactions don’t accurately reflect what most people see in the News Feed, the company also hasn’t offered any ways for the public or researchers to see that data for themselves. And so what we do know of news on Facebook is that the popular stuff tends to be more partisan than not. Basically everything else is still a question mark.&lt;/p&gt;
&lt;p id="G84SmH"&gt;At the same time, we also now know that whatever false claims social networks amplify, the claims typically do not &lt;em&gt;originate&lt;/em&gt; there. As I wrote in the first edition of &lt;a href="https://www.platformer.news/"&gt;&lt;em&gt;Platformer&lt;/em&gt;&lt;/a&gt;, research on disinformation about mail-in voting shows that &lt;a href="https://www.platformer.news/p/the-covid-presidency-arrives-on-platforms"&gt;conspiracy theories travel much more effectively via mass media than they do on tech platforms&lt;/a&gt;. (Though the platforms play an important supporting role.)&lt;/p&gt;
&lt;p id="x8Cf0v"&gt;Meanwhile, tens of thousands of journalists have lost their jobs around the country over the past decade. (Something that platforms’ success in disrupting the digital advertising market had a crucial hand in.) It’s hard to think of a more banal conclusion to a topic like this complicated than to say, with a furrowed brow, “a more holistic approach is needed.” But it is!&lt;/p&gt;
&lt;p id="eIhEa5"&gt;The second major problem that Facebook uncovered in 2016 was, famously, foreign interference on the platform. A Kremlin-linked troll farm known as the Internet Research Agency &lt;a href="https://www.nytimes.com/2020/09/22/us/politics/russia-disinformation-election-trump.html"&gt;sought to sow division in America&lt;/a&gt;, using fake accounts to promote pages encouraging Texas to secede, Black voters to stay home, and protesters and counter-protesters to show up at the same real-world rallies. Those moves were part of a larger Russian campaign to interfere with the election, which also included hacking the Democratic National Committee and disseminating their emails to media outlets.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="ACKacW"&gt;&lt;q&gt;But in 2020, those issues can now be recognized as minor symptoms of much larger problems&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="S4XQpu"&gt;In response, Facebook dramatically expanded its teams working on what it called “platform integrity.” Between full-time employees and contractors working on content moderation, the team swelled to more than 30,000 people by the end of 2018. Among its hires were former US intelligence agency workers who helped Facebook root out influence operations. They began regularly reporting on takedowns at home and abroad. By September of this year, Facebook &lt;a href="https://www.nytimes.com/2020/09/01/technology/facebook-russia-disinformation-election.html"&gt;had taken down a dozen new campaigns from Russia’s IRA alone&lt;/a&gt;.&lt;/p&gt;
&lt;p id="VkVyQM"&gt;Again: this is good news. But by the 2020 campaign, there was no longer any need for a Russian influence operation to sow discord here. The discord was all homegrown, and the president issued a series of autocratic social media posts demanding that the vote count be stopped before it was complete and baselessly asserting that he had been the victim of fraud.&lt;/p&gt;
&lt;p id="wyw3Ac"&gt;“Nothing that Russia or Iran or China could say is anywhere near as wild as what the president is saying,” Clint Watts, a former FBI agent who tracks foreign disinformation, &lt;a href="https://www.nbcnews.com/politics/2020-election/russians-have-no-need-spread-misinformation-trump-his-allies-are-n1246653"&gt;told NBC&lt;/a&gt;. “We cannot say this time that Russia, Iran or China interfered in a significant way. They don’t need to write fake news this time — we’re making plenty of fake news of our own.”&lt;/p&gt;
&lt;p id="rZSDqU"&gt;In short, Facebook and Twitter — and, to a lesser extent, YouTube — met two of the key challenges revealed by the 2016 election. But in 2020, those issues can now be recognized as minor symptoms of much larger problems. And it’s not at all clear that the platforms are in a position to solve them.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="gv0MEp"&gt;&lt;q&gt;Take your lies to live television, which will air your baseless claims in real time&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="6weMQk"&gt;It’s one thing to stop misinformation when it’s coming from Macedonian teenagers; it’s quite another to stop it when it’s coming from the president of the United States. Despite heavy political pressure, both Facebook and Twitter have &lt;a href="https://www.theverge.com/2020/11/5/21550991/twitter-trump-stop-the-vote-tweet-restricted-election-interference"&gt;intervened repeatedly&lt;/a&gt; this week to remove and label the president’s false tweets claiming he already won the election. But his message has spread nonetheless, particularly on YouTube, &lt;a href="https://www.platformer.news/p/how-youtube-got-played-on-election"&gt;which has no policy against claiming premature victory&lt;/a&gt;.&lt;/p&gt;
&lt;p id="fp6SDa"&gt;Trump’s message spread because disinformation is an effective political strategy. If you lie constantly — and are supported by a network of enablers, cable news networks, talk radio, and platform amplification and recommendation algorithms — you can amass a huge following. Take your lies to live television, which will air your baseless claims in real time, and you can build it even more.&lt;/p&gt;
&lt;p id="WXxxAh"&gt;And so while neither fake news nor foreign influence operations decided the 2020 election, another kind of influence operation made and continues to make use of Facebook, Twitter, and YouTube every day. The platforms are handmaidens in the spread of hyper-partisan information — indeed, in many ways, they optimize for it.&lt;/p&gt;
&lt;p id="62hZcy"&gt;And on that front, 2016 and 2020 don’t look very different at all.  &lt;/p&gt;
&lt;p id="Kb9Z6v"&gt;&lt;em&gt;This column was co-published with &lt;/em&gt;&lt;a href="https://www.platformer.news/"&gt;&lt;em&gt;Platformer&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, a daily newsletter about Big Tech and democracy.&lt;/em&gt;&lt;/p&gt;
&lt;p id="AYKHrk"&gt;&lt;/p&gt;
&lt;p id="SpsSt1"&gt;&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2020/11/6/21552763/election-2020-tech-social-platforms-facebook-twitter-youtube-misinformation"/>
    <id>https://www.theverge.com/2020/11/6/21552763/election-2020-tech-social-platforms-facebook-twitter-youtube-misinformation</id>
    <author>
      <name>Casey Newton</name>
    </author>
  </entry>
  <entry>
    <published>2020-11-05T17:50:29-05:00</published>
    <updated>2020-11-05T17:50:29-05:00</updated>
    <title>YouTube’s lax misinformation rules are letting election lies spread</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/iLXehTL83Zm_YOvCAYh8kqC87ew=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/67745154/acastro_180321_1777_youtube_0002.0.jpg" /&gt;
        &lt;figcaption&gt;Illustration by Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="iMEgtm"&gt;For people who stumbled their way onto One American News Network (OANN)’s YouTube channel over the last few days, it might be easy to think President Trump won the 2020 election. &lt;/p&gt;
&lt;p id="hnBhBf"&gt;Videos titled “Trump won. Dems try to pull a fast one” and “Trump Won. MSM hopes you don’t believe your eyes,” started appearing on OANN’s YouTube channel on November 4th — one day after people cast their votes in the federal election. The videos are full of lies predicated on people’s fears that would trigger moderation on another network, including: &lt;/p&gt;
&lt;p id="cJIjJ3"&gt;“It appears that Trump won by such a large margin, now they’re actually pumping out illegal ballots into the battleground states to actually beat him.”&lt;/p&gt;
&lt;p id="RJx3gC"&gt;“They think that they can add 100,000 votes that no one gets to see or review, and all 100,000 votes are for Joe Biden and that Republicans will just accept those votes as fact? They think they can just play Republicans.”&lt;/p&gt;
&lt;p id="hPEizu"&gt;“Joe Biden didn’t campaign much — he’s senile, and is expected to be removed from office if elected.” &lt;/p&gt;
&lt;p id="37QQ9k"&gt;To be clear: there has been no public evidence of voter fraud so far, and even less evidence to support that Biden will be removed from office after the election. The videos have more than 500,000 views combined, at the time of this writing.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="ZFhNmP"&gt;&lt;q&gt;Even after high-profile coverage of the videos’ demonstrably false claims, YouTube has decided to leave the videos live&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="4OVq8C"&gt;OANN has become a poster child for Trump-focused misinformation on YouTube, but the problem is much larger than a single video or a single channel. It’s a network-wide issue, the result of YouTube staking out a position as one of the most permissive major social networks. That stance has served YouTube well for years, but it’s become a liability in the face of the Trump campaign’s ongoing scramble to delegitimize the results of the 2020 election. While Facebook and Twitter tighten their moderation rules and supercharge enforcement, YouTube is still struggling with how to respond to misinformation around the election.&lt;/p&gt;
&lt;p id="LJ39KS"&gt;YouTube’s response to the OANN videos shows the company’s policy in practice: even after high-profile coverage of the videos’ demonstrably false claims, YouTube has decided to leave the videos live. The videos simply don’t violate YouTube’s content policy, a spokesperson told &lt;em&gt;The Verge&lt;/em&gt;. Instead, the company will remove ads from them, because alleging who won an election before the election is called is “in the scope of our demonstrably false policy.” YouTube will also include an information box noting that the election results aren’t final, and pointing people to Google’s main election hub — but that’s only when the video is clicked. &lt;/p&gt;
&lt;p id="7ziE04"&gt;It’s not just OANN using YouTube to spread misinformation, either. Steven Crowder, a controversial right-wing pundit who lost &lt;a href="https://www.theverge.com/2019/6/5/18654196/steven-crowder-demonetized-carlos-maza-youtube-homophobic-language-ads"&gt;his ability to monetize his channel&lt;/a&gt; for &lt;a href="https://www.theverge.com/2020/8/12/21365601/youtube-steven-crowder-monetization-reinstated-harassment-carlos-maza"&gt;more than a year&lt;/a&gt; after he incited harassment against another YouTube creator, spread misinformation about ballot counting during one of his livestreams. &lt;/p&gt;
&lt;p id="sQthXe"&gt;During the livestream, which resulted in millions of views, Crowder and his fellow hosts talked about suspicious-looking activity outside of a polling center in Detroit. The theory Crowder helped spread insinuated that a man wheeling in a device was carrying in ballots, &lt;a href="https://www.buzzfeednews.com/article/janelytvynenko/election-rumors-debunked#125906181"&gt;according to &lt;em&gt;BuzzFeed&lt;/em&gt;&lt;/a&gt;. When the man in question loaded a box onto a moving device, Crowder said, “you wouldn’t be able to trust it because some ballots could fly off the back,” &lt;em&gt;BuzzFeed &lt;/em&gt;noted. &lt;/p&gt;
&lt;p id="KR63wz"&gt;The man in question ended up being a photographer for a local media organization. He was not handling ballots, but carrying equipment, according to &lt;em&gt;BuzzFeed&lt;/em&gt;.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="ehBIRY"&gt;&lt;q&gt;OANN has become a poster child for Trump-focused misinformation on YouTube, but the problem is much larger than a single video or a single channel&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="dz6mp8"&gt;Elsewhere, former White House adviser and right-wing commentator Seb Gorka used a different YouTube livestream to spread misinformation about Trump winning certain states. In Gorka’s livestream, entitled “They’re trying to steal the President’s victory,” Gorka suggests that stations stopped counting votes in Democrat-controlled states while trying to find new mail-in ballots to help Biden win. At one point early on, Gorka notes that “every single one of those states saw the president victorious as the snapshot was taken yesterday evening.” No plausible evidence has been produced for any of these claims. &lt;/p&gt;
&lt;p id="PG18qa"&gt;In a livestream earlier today about a number of topics related to the current election, former White House strategist and former executive chairman of Breitbart News, Steve Bannon, said he’d not only fire FBI Director Christopher Wray and Dr. Anthony Fauci, the director of the National Institute of Allergy and Infectious Diseases, but “put the heads on pikes.” He added he’d put them “at the two corners of the White House as a warning to federal bureaucrats — you either get with the program, or you’re gone.” &lt;/p&gt;
&lt;p id="iMk8gn"&gt;YouTube later removed the video for “violating our policy against inciting violence,” a spokesperson told &lt;em&gt;The Verge&lt;/em&gt;. Around the same time as the video came down, Twitter banned Bannon’s podcast-specific Twitter account for “violating the Twitter Rules, specifically our &lt;a href="https://help.twitter.com/en/rules-and-policies/glorification-of-violence"&gt;policy on the glorification of violence&lt;/a&gt;,” a spokesperson told &lt;em&gt;The Verge&lt;/em&gt;. &lt;/p&gt;
&lt;p id="8011Su"&gt;YouTube has policies about misinformation pertaining to the election, but organizations like OANN have become adept at evading them. The company will step in when a video misleads people about voting. This can include “content aiming to mislead voters about the time, place, means or eligibility requirements for voting, or false claims that could materially discourage voting,” according to a spokesperson. By that definition, it appears content that misled viewers about premature wins in certain states, outright incorrect information about who won the election, or insinuations about one party tampering with another party’s votes while those ballots are being counted, would not violate YouTube’s policies. &lt;/p&gt;
&lt;p id="dgpKsQ"&gt;YouTube’s team prepared for Election Day — but weren’t ready for what came after, says Angelo Carusone, president of Media Matters for America, a watchdog group that has been reporting on YouTube’s misinformation struggles&lt;em&gt;. &lt;/em&gt;&lt;/p&gt;
&lt;p id="bI7Nxr"&gt;“That’s been their biggest failure,” Carusone said. “How they choose to label content really comes down to what’s inside the video, but the most critical part of a video oftentimes is that straight thumbnail, and how it’s framed in both the headline and the description, because that’s going to give people an interpretive lens for the rest of what they see.”&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="Z820vb"&gt;&lt;q&gt;“The most critical part of a video oftentimes is that straight thumbnail, and how it’s framed in both the headline and the description”&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="Ool3Xy"&gt;YouTube tends to be strictest about titles and thumbnails, which aren’t permitted to mislead users about the nature of the video. The &lt;a href="https://support.google.com/youtube/answer/2801973?hl=en&amp;amp;ref_topic=9282365"&gt;policy states&lt;/a&gt; that creators and organizations are prohibited from “using the title, thumbnails, description, or tags to trick users into believing the content is something it is not.” Under those rules, you might think that videos with titles claiming point blank that Trump won should violate YouTube’s policy, since a winner has not yet been declared. But YouTube has not been enforcing the policy by that logic, and could not tell &lt;em&gt;The Verge &lt;/em&gt;why the video’s title didn’t trigger moderation.&lt;/p&gt;
&lt;p id="1Ghtbm"&gt;YouTube has faced criticism for its &lt;a href="https://www.theverge.com/2019/2/6/18213648/shane-dawson-conspiracy-video-youtube-demoneitzation-community-guidelines"&gt;lax policies in the past&lt;/a&gt; — but never during such a high-stakes political crisis. Controversial videos like anti-vaccination content, &lt;a href="https://www.theverge.com/2019/12/3/20992018/youtube-borderline-content-recommendation-algorithm-news-authoritative-sources"&gt;are often classified as&lt;/a&gt; “borderline content”: ineligible for advertising and often downgraded in search results, but still present on platforms for audiences that search them out. But as a result, outright bans on the platform are still rare, and content like OANN’s recent videos can gain as much as 500,000 views even while struggling against the limitations placed on borderline content.&lt;/p&gt;
&lt;p id="lFe3L6"&gt;YouTube’s lack of action is particularly noticeable because other social networks like Facebook and Twitter have taken a more aggressive approach to try and prevent the spread of misinformation. Since election night, Twitter has flagged &lt;a href="https://twitter.com/verge/status/1324410466815430658"&gt;more than a third of the president’s tweets&lt;/a&gt; for misleading readers about the ongoing election, although it has resisted calls to ban the president from the platform entirely. More recently, Facebook shut down &lt;a href="https://www.theverge.com/2020/11/5/21551551/facebook-stop-the-steal-group-misinformation-election-2020"&gt;a 300,000-person group&lt;/a&gt; called “Stop the Steal,” aimed at halting the ongoing vote-counting efforts, after observing “worrying calls for violence from members of the group.”&lt;/p&gt;
&lt;p id="h5YfTP"&gt;In contrast, YouTube’s response has critics like Carusone wondering if the election caught them off guard. “I can’t tell how much of it is that they didn’t do the homework or want to put the work in,” he says. “I think they made an intentional decision not to do this. They were taking a temporary approach to a limited set of content on their platform.”&lt;/p&gt;
&lt;p id="h99Ccn"&gt;&lt;em&gt;&lt;strong&gt;Update November 5th, 6:37pm ET: &lt;/strong&gt;&lt;/em&gt;&lt;em&gt;The story has been updated to note that YouTube removed a video where Steve Bannon made comments that violated the company’s policy against inciting violence.&lt;/em&gt;&lt;/p&gt;
&lt;p id="PLkxFR"&gt;&lt;em&gt;&lt;strong&gt;Update 2 November 5th, 7:25pm ET: &lt;/strong&gt;&lt;/em&gt;&lt;em&gt;The story has been updated to include Twitter’s statement. &lt;/em&gt;&lt;/p&gt;
&lt;p id="RPwHAR"&gt;&lt;/p&gt;
&lt;p id="m5W2t8"&gt;&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2020/11/5/21551790/youtube-election-misinformation-oann-live-stream-mail-in-vote-count-ballot"/>
    <id>https://www.theverge.com/2020/11/5/21551790/youtube-election-misinformation-oann-live-stream-mail-in-vote-count-ballot</id>
    <author>
      <name>Julia Alexander</name>
    </author>
  </entry>
  <entry>
    <published>2020-11-04T18:47:08-05:00</published>
    <updated>2020-11-04T18:47:08-05:00</updated>
    <title>YouTube says video claiming Trump won does not violate its election misinformation policies</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/W0dENoxyiuAFpL8Rekg9moEl5Uk=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/67739797/acastro_180806_1777_youtube_cancel_0001.0.jpg" /&gt;
        &lt;figcaption&gt;Illustration by Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="ek8q3e"&gt;YouTube says a video claiming President Donald Trump won the election does not violate any of its policies and has allowed it to stay on the platform, despite the election not being called by major news outlets for either candidate yet. The decision stands in contrast to Twitter and Facebook’s more aggressive attempts to clamp down on misleading claims and misinformation over election results.&lt;/p&gt;
&lt;p id="fbXbyM"&gt;The video, titled “Trump Won. MSM hopes you don’t believe your eyes,” was published by pro-Trump network One American News Network (OANN) on Wednesday. YouTube says the video violates its advertising guidelines but not its content policies, so it can stay online but will run without ads, the company confirmed to &lt;em&gt;The Verge. &lt;/em&gt;The story was &lt;a href="https://www.cnbc.com/2020/11/04/youtube-refuses-to-remove-video-that-appears-to-violate-its-policies.html"&gt;first reported by &lt;em&gt;CNBC&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p id="Ehn3Ta"&gt;In the video, an OANN anchor says Trump won another term, baselessly claims that Trump would win a number of swing states if it weren’t for voter fraud, and then equates counting outstanding mail-in ballots with an attempt to steal the election. Despite this information being incorrect, the video doesn’t cross the line on YouTube. &lt;/p&gt;
&lt;p id="RhWROp"&gt;YouTube’s current policies regarding misinformation and the election pertain to content “aiming to mislead voters about the time, place, means or eligibility requirements for voting, or false claims that could materially discourage voting,” according to a company spokesperson. YouTube took down several livestreams related to the election between yesterday and today for violating its spam policies, according to the company, although &lt;em&gt;Bloomberg &lt;/em&gt;reporter Mark Bergen &lt;a href="https://twitter.com/mhbergen/status/1324125455377559552?s=21"&gt;has tracked instances&lt;/a&gt; where misleading content has managed to appear on the site via livestreams. &lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="TNt2UQ"&gt;&lt;q&gt;Instead, YouTube has removed ads from the specific OANN video&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="OLFs9Z"&gt;A panel appears below the OANN video stating that election results “may not be final,” with a link to Google’s main search page that shows updated election coverage. &lt;em&gt;“&lt;/em&gt;All search results and videos about this election — including this video — surface an information panel noting that election results may not be final and we are continuing to raise up authoritative content in search results and recommendations,” the spokesperson added. &lt;/p&gt;
&lt;p id="7yZOLx"&gt;YouTube &lt;a href="https://support.google.com/youtube/answer/2801973"&gt;also has a policy&lt;/a&gt; that says creators or organizations can not use “thumbnails, description, or tags to trick users into believing the content is something it is not,” but declined to tell &lt;em&gt;The Verge &lt;/em&gt;why this video is allowed to remain up if Trump, in fact, hadn’t won the election at the time the video was posted.&lt;/p&gt;
&lt;p id="XBFWU4"&gt;Instead, YouTube has removed ads from the specific OANN video. The company does not allow ads to run on videos undermining election confidence through demonstrably false information. Since the election has not been called at this time, the video is in violation of YouTube’s advertising policy. YouTube has routinely in the past &lt;a href="https://www.theverge.com/2019/12/3/20992018/youtube-borderline-content-recommendation-algorithm-news-authoritative-sources"&gt;kept borderline content&lt;/a&gt; (videos that YouTube doesn’t promote, but won’t take down as they don’t violate any policies) on its platform but removed the ability for creators and organizations to &lt;a href="https://www.theverge.com/2019/6/5/18654196/steven-crowder-demonetized-carlos-maza-youtube-homophobic-language-ads"&gt;monetize said content.&lt;/a&gt; &lt;/p&gt;
&lt;p id="tw2QFU"&gt;While YouTube is known for its confusing policies, its handling of election misinformation looks even worse against the backdrop of Twitter and Facebook’s more aggressive actions. &lt;/p&gt;
&lt;p id="TWKnhd"&gt;Facebook removed a specific feature within Instagram’s hashtags to try and slow down the spread of misinformation that could result in voter suppression, &lt;a href="https://www.nytimes.com/2020/11/02/technology/facebook-twitter-youtube-election-day.html"&gt;according to the &lt;em&gt;New York Times&lt;/em&gt;.&lt;/a&gt; The company has also turned off political group recommendations for similar reasons. On election day, Facebook also added a notification at the top of people’s news feeds to try and prevent users seeing false claims that one party won the election prematurely. Facebook relied on verified news results from publications like &lt;em&gt;Reuters&lt;/em&gt; and the &lt;em&gt;Associated Press&lt;/em&gt;. As election results come in, Facebook has also labeled false claims from parties that they’ve won specific states.&lt;/p&gt;
&lt;p id="msiOg2"&gt;Twitter has aggressively &lt;a href="https://www.theverge.com/2020/11/4/21549859/twitter-suspend-donald-trump-election-votes-misleading-information"&gt;labeled tweets from people&lt;/a&gt;, including President Trump, falsely claiming that mail-in voting has led to inaccurate results. False claims about &lt;a href="https://www.theverge.com/2020/11/4/21550017/trump-pennsylvania-premature-victory-twitter-rules-tweets-biden-election-2020"&gt;winning specific states&lt;/a&gt; and the election at large have also been amended with warning labels saying that final results have yet to be counted and the election is not over.&lt;/p&gt;
&lt;p id="lzdAJe"&gt;YouTube claims that it’s also doing its part through information boxes that relate back to Google search results, which also relies on authoritative voices like &lt;em&gt;Reuters&lt;/em&gt; and the &lt;em&gt;Associated Press&lt;/em&gt;. It just won’t remove this one specific OANN video claiming that Trump won the election. &lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2020/11/4/21550180/youtube-oann-video-election-trump-misinformation-voting-final-results-facebook-twitter"/>
    <id>https://www.theverge.com/2020/11/4/21550180/youtube-oann-video-election-trump-misinformation-voting-final-results-facebook-twitter</id>
    <author>
      <name>Julia Alexander</name>
    </author>
  </entry>
</feed>
