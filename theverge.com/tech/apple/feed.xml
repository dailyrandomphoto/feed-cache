<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <title>The Verge -  Apples</title>
  <icon>https://cdn.vox-cdn.com/community_logos/52801/VER_Logomark_32x32..png</icon>
  <updated>2021-08-10T17:43:03-04:00</updated>
  <id>https://www.theverge.com/rss/apple/index.xml</id>
  <link type="text/html" href="https://www.theverge.com/apple" rel="alternate"/>
  <entry>
    <published>2021-08-10T17:43:03-04:00</published>
    <updated>2021-08-10T17:43:03-04:00</updated>
    <title>Apple’s controversial new child protection features, explained</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/rxZvrW0rpUQ9VAnQVwCTqnsA-zM=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69706816/acastro_180604_1777_apple_wwdc_0003.0.jpg" /&gt;
        &lt;figcaption&gt;Illustration by Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="Qvb3Xg"&gt;Apple stakes its reputation on privacy. The company has promoted encrypted messaging across its ecosystem, encouraged limits on how mobile apps can gather data, and fought law enforcement agencies looking for user records. For the past week, though, Apple has been fighting accusations that its upcoming iOS and iPadOS release will weaken user privacy.&lt;/p&gt;
&lt;p id="zVWpBi"&gt;The debate stems from an &lt;a href="https://www.theverge.com/2021/8/5/22611305/apple-scan-photos-iphones-icloud-child-abuse-imagery-neuralmatch"&gt;announcement Apple made&lt;/a&gt; on Thursday. In theory, the idea is pretty simple: Apple wants to fight child sexual abuse, and it’s taking more steps to find and stop it. But critics say &lt;a href="https://www.theverge.com/22617554/apple-csam-child-safety-features-jen-king-riana-pfefferkorn-interview-decoder"&gt;Apple’s strategy could weaken&lt;/a&gt; users’ control over their own phones, leaving them reliant on Apple’s promise that it won’t abuse its power. And Apple’s response has highlighted just how complicated — and sometimes downright confounding — the conversation really is.&lt;/p&gt;
&lt;h2 id="oxykkz"&gt;&lt;strong&gt;What did Apple announce last week?&lt;/strong&gt;&lt;/h2&gt;
&lt;p id="dc6dr1"&gt;Apple has &lt;a href="https://go.redirectingat.com?id=66960X1514734&amp;amp;xs=1&amp;amp;url=https%3A%2F%2Fwww.apple.com%2Fchild-safety%2F&amp;amp;referrer=theverge.com&amp;amp;sref=https%3A%2F%2Fwww.theverge.com%2F2021%2F8%2F10%2F22613225%2Fapple-csam-scanning-messages-child-safety-features-privacy-controversy-explained" rel="sponsored nofollow noopener" target="_blank"&gt;announced three changes&lt;/a&gt; that will roll out later this year — all related to curbing child sexual abuse but targeting different apps with different feature sets.&lt;/p&gt;
&lt;p id="ztjL4D"&gt;The first change affects Apple’s Search app and Siri. If a user searches for topics related to child sexual abuse, Apple will direct them to resources for reporting it or getting help with an attraction to it. That’s rolling out later this year on iOS 15, watchOS 8, iPadOS 15, and macOS Monterey, and it’s largely uncontroversial.&lt;/p&gt;
&lt;p id="DhX8SA"&gt;The other updates, however, have generated far more backlash. One of them adds a parental control option to Messages, obscuring sexually explicit pictures for users under 18 and sending parents an alert if a child 12 or under views or sends these pictures.&lt;/p&gt;
&lt;p id="9ZwidK"&gt;The final new feature scans iCloud Photos images to find child sexual abuse material, or CSAM, and reports it to Apple moderators — who can pass it on to the National Center for Missing and Exploited Children, or NCMEC. Apple says it’s designed this feature specifically to protect user privacy while finding illegal content. Critics say that same designs amounts to a security backdoor.&lt;/p&gt;
&lt;h2 id="XeULNq"&gt;&lt;strong&gt;What is Apple doing with Messages?&lt;/strong&gt;&lt;/h2&gt;
&lt;p id="YemRv3"&gt;Apple is introducing a Messages feature that’s meant to protect children from inappropriate images. If parents opt in, devices with users under 18 will scan incoming and outgoing pictures with an image classifier trained on pornography, looking for “sexually explicit” content. (Apple says it’s not technically limited to nudity but that a nudity filter is a fair description.) If the classifier detects this content, it obscures the picture in question and asks the user whether they really want to view or send it.&lt;/p&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt="A screenshot of Apple’s Messages filter for sexually explicit content." data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/DcBS2sGWzUDR2RWMe6AIhKN4IWY=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22774655/CCEA6371_9191_46CC_BDE4_B2DAB4CC4409.jpeg"&gt;
      &lt;cite&gt;Image: Apple&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;A screenshot of Apple’s Messages filter for sexually explicit content.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="n4evJQ"&gt;The update — coming to accounts set up as families in iCloud on iOS 15, iPadOS 15, and macOS Monterey — also includes an additional option. If a user taps through that warning and they’re under 13, Messages will be able to notify a parent that they’ve done it. Children will see a caption warning that their parents will receive the notification, and the parents won’t see the actual message. The system doesn’t report anything to Apple moderators or other parties.&lt;/p&gt;
&lt;p id="A9Rr8f"&gt;The images are detected on-device, which Apple says protects privacy. And parents are notified if children actually confirm they want to see or send adult content, not if they simply receive it. At the same time, critics like Harvard Cyberlaw Clinic instructor Kendra Albert have &lt;a href="https://twitter.com/KendraSerra/status/1423365222841135114?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1423367106972852228%7Ctwgr%5E%7Ctwcon%5Es2_&amp;amp;ref_url=https%3A%2F%2Fwww.theverge.com%2F2021%2F8%2F6%2F22613365%2Fapple-icloud-csam-scanning-whatsapp-surveillance-reactions"&gt;raised concerns&lt;/a&gt; about the notifications — saying they could end up outing queer or transgender kids, for instance, by encouraging their parents to snoop on them.&lt;/p&gt;
&lt;h2 id="04Xy16"&gt;&lt;strong&gt;What does Apple’s new iCloud Photos scanning system do?&lt;/strong&gt;&lt;/h2&gt;
&lt;p id="OfIeOI"&gt;The iCloud Photos scanning system is focused on finding child sexual abuse images, which are illegal to possess. If you’re a US-based iOS or iPadOS user and you sync pictures with iCloud Photos, your device will locally check these pictures against a list of known CSAM. If it detects enough matches, it will alert Apple’s moderators and reveal the details of the matches. If a moderator confirms the presence of CSAM, they’ll disable the account and report the images to legal authorities.&lt;/p&gt;
&lt;h2 id="mmwusD"&gt;&lt;strong&gt;Is CSAM scanning a new idea?&lt;/strong&gt;&lt;/h2&gt;
&lt;p id="Qo0T8L"&gt;Not at all. Facebook, Twitter, Reddit, and many other companies scan users’ files against hash libraries, often using a Microsoft-built tool called PhotoDNA. They’re also legally required to report CSAM to the National Center for Missing and Exploited Children (NCMEC), a nonprofit that works alongside law enforcement.&lt;/p&gt;
&lt;p id="0ZjID1"&gt;Apple has limited its efforts until now, though. The company has said previously that it uses image matching technology to find child exploitation. But in a call with reporters, it said it’s never scanned iCloud Photos data. (It confirmed that it already scanned iCloud Mail but didn’t offer any more detail about scanning other Apple services.)&lt;/p&gt;
&lt;h2 id="AaZIXm"&gt;&lt;strong&gt;Is Apple’s new system different from other companies’ scans?&lt;/strong&gt;&lt;/h2&gt;
&lt;p id="OOjY2L"&gt;A typical CSAM scan runs remotely and looks at files that are stored on a server. Apple’s system, by contrast, checks for matches locally on your iPhone or iPad.&lt;/p&gt;
&lt;p id="EuJuhx"&gt;The system works as follows. When iCloud Photos is enabled on a device, the device uses a tool called NeuralHash to break these pictures into hashes — basically strings of numbers that identify the unique characteristics of an image but can’t be reconstructed to reveal the image itself. Then, it compares these hashes against a stored list of hashes from NCMEC, which compiles millions of hashes corresponding to known CSAM content. (Again, as mentioned above, there are no actual pictures or videos.)&lt;/p&gt;
&lt;p id="h8ucZY"&gt;If Apple’s system finds a match, your phone generates a “safety voucher” that’s uploaded to iCloud Photos. Each safety voucher indicates that a match exists, but it doesn’t alert any moderators and it encrypts the details, so an Apple employee can’t look at it and see which photo matched. However, if your account generates a certain number of vouchers, the vouchers all get decrypted and flagged to Apple’s human moderators — who can then review the photos and see if they contain CSAM.&lt;/p&gt;
&lt;p id="b52TDh"&gt;Apple emphasizes that it’s exclusively looking at photos you sync with iCloud, not ones that are only stored on your device. It tells reporters that disabling iCloud Photos will completely deactivate all parts of the scanning system, including the local hash generation. “If users are not using iCloud Photos, NeuralHash will not run and will not generate any vouchers,” Apple privacy head Erik Neuenschwander &lt;a href="https://techcrunch.com/2021/08/10/interview-apples-head-of-privacy-details-child-abuse-detection-and-messages-safety-features/?tpcc=ECTW2020"&gt;told &lt;em&gt;TechCrunch&lt;/em&gt; in an interview&lt;/a&gt;.&lt;/p&gt;
&lt;p id="pJqada"&gt;Apple has used on-device processing to bolster its privacy credentials in the past. iOS can &lt;a href="https://www.theverge.com/2016/6/13/11924080/apple-ai-on-device-privacy-wwdc-2016"&gt;perform a lot of AI analysis&lt;/a&gt; without sending any of your data to cloud servers, for example, which means fewer chances for a third party to get their hands on it.&lt;/p&gt;
&lt;p id="4wSx4y"&gt;But the local / remote distinction here is hugely contentious, and following a backlash, Apple has spent the past several days drawing extremely subtle lines between the two.&lt;/p&gt;
&lt;h2 id="rKzkRU"&gt;&lt;strong&gt;Why are some people upset about these changes?&lt;/strong&gt;&lt;/h2&gt;
&lt;p id="cAxm1C"&gt;Before we get into the criticism, it’s worth saying: Apple has gotten praise for these updates from some privacy and security experts, including the prominent cryptographers and computer scientists Mihir Bellare, David Forsyth, and Dan Boneh. “This system will likely significantly increase the likelihood that people who own or traffic in [CSAM] are found,” said Forsyth in an endorsement provided by Apple. “Harmless users should experience minimal to no loss of privacy.”&lt;/p&gt;
&lt;p id="M6nWSV"&gt;But other experts and advocacy groups have come out against the changes. They say the iCloud and Messages updates have the same problem: they’re creating surveillance systems that work directly from your phone or tablet. That could provide a blueprint for breaking secure end-to-end encryption, and even if its use is limited right now, it could open the door to more troubling invasions of privacy.&lt;/p&gt;
&lt;p id="kF34kp"&gt;&lt;a href="https://appleprivacyletter.com/"&gt;An August 6th open letter&lt;/a&gt; outlines the complaints in more detail. Here’s its description of what’s going on:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p id="b50GTi"&gt;While child exploitation is a serious problem, and while efforts to combat it are almost unquestionably well-intentioned, Apple’s proposal introduces a backdoor that threatens to undermine fundamental privacy protections for all users of Apple products.&lt;/p&gt;
&lt;p id="GC2dGn"&gt;Apple’s proposed technology works by continuously monitoring photos saved or shared on the user’s iPhone, iPad, or Mac. One system detects if a certain number of objectionable photos is detected in iCloud storage and alerts the authorities. Another notifies a child’s parents if iMessage is used to send or receive photos that a machine learning algorithm considers to contain nudity.&lt;/p&gt;
&lt;p id="drY3RR"&gt;Because both checks are performed on the user’s device, they have the potential to bypass any end-to-end encryption that would otherwise safeguard the user’s privacy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p id="wR3KIb"&gt;Apple has disputed the characterizations above, particularly the term “backdoor” and the description of monitoring photos saved on a user’s device. But as we’ll explain below, it’s asking users to put a lot of trust in Apple, while the company is facing government pressure around the world.&lt;/p&gt;
&lt;h2 id="7F22ax"&gt;&lt;strong&gt;What’s end-to-end encryption, again?&lt;/strong&gt;&lt;/h2&gt;
&lt;p id="Su1Rpy"&gt;To massively simplify, end-to-end encryption (or E2EE) makes data unreadable to anyone besides the sender and receiver; in other words, not even the company running the app can see it. Less secure systems can still be encrypted, but companies may hold keys to the data so they can scan files or grant access to law enforcement. Apple’s iMessages uses E2EE; iCloud Photos, like many cloud storage services, doesn’t.&lt;/p&gt;
&lt;p id="esnJuH"&gt;While E2EE can be incredibly effective, it doesn’t necessarily stop people from seeing data on the phone itself. That leaves the door open for specific kinds of surveillance, including a system that Apple is now accused of adding: client-side scanning.&lt;/p&gt;
&lt;h2 id="YEyzOe"&gt;&lt;strong&gt;What is client-side scanning?&lt;/strong&gt;&lt;/h2&gt;
&lt;p id="8jPj5E"&gt;The Electronic Frontier Foundation &lt;a href="https://www.eff.org/deeplinks/2019/11/why-adding-client-side-scanning-breaks-end-end-encryption"&gt;has a detailed outline&lt;/a&gt; of client-side scanning. Basically, it involves analyzing files or messages in an app before they’re sent in encrypted form, often checking for objectionable content — and in the process, bypassing the protections of E2EE by targeting the device itself. In a phone call with &lt;em&gt;The Verge&lt;/em&gt;, EFF senior staff technologist Erica Portnoy compared these systems to somebody looking over your shoulder while you’re sending a secure message on your phone.&lt;/p&gt;
&lt;h2 id="PQK32h"&gt;&lt;strong&gt;Is Apple doing client-side scanning?&lt;/strong&gt;&lt;/h2&gt;
&lt;p id="5x75cC"&gt;Apple vehemently denies it. In &lt;a href="https://go.redirectingat.com?id=66960X1514734&amp;amp;xs=1&amp;amp;url=https%3A%2F%2Fwww.apple.com%2Fchild-safety%2Fpdf%2FExpanded_Protections_for_Children_Frequently_Asked_Questions.pdf&amp;amp;referrer=theverge.com&amp;amp;sref=https%3A%2F%2Fwww.theverge.com%2F2021%2F8%2F10%2F22613225%2Fapple-csam-scanning-messages-child-safety-features-privacy-controversy-explained" rel="sponsored nofollow noopener" target="_blank"&gt;a frequently asked questions document&lt;/a&gt;, it says Messages is still end-to-end encrypted and absolutely no details about specific message content are being released to anybody, including parents. “Apple never gains access to communications as a result of this feature in Messages,” it promises.&lt;/p&gt;
&lt;p id="Cks3Ug"&gt;It also rejects the framing that it’s scanning photos &lt;em&gt;on your device &lt;/em&gt;for CSAM. “By design, this feature only applies to photos that the user chooses to upload to iCloud,” its FAQ says. “The system does not work for users who have iCloud Photos disabled. This feature does not work on your private iPhone photo library on the device.” The company later clarified to reporters that Apple could scan iCloud Photos images synced via third-party services as well as its own apps.&lt;/p&gt;
&lt;p id="8QKiFs"&gt;As Apple acknowledges, iCloud Photos doesn’t even have any E2EE to break, so it could easily run these scans on its servers — just like lots of other companies. Apple argues its system is actually more secure. Most users are unlikely to to have CSAM on their phone, and Apple claims only around 1 in 1 trillion accounts could be incorrectly flagged. With this local scanning system, Apple says it won’t expose any information about anybody else’s photos, which wouldn’t be true if it scanned its servers.&lt;/p&gt;
&lt;h2 id="L7G048"&gt;&lt;strong&gt;Are Apple’s arguments convincing?&lt;/strong&gt;&lt;/h2&gt;
&lt;p id="0BGcyZ"&gt;Not to a lot of its critics. As Ben Thompson &lt;a href="https://stratechery.com/2021/apples-mistake/"&gt;writes at &lt;em&gt;Stratechery&lt;/em&gt;&lt;/a&gt;, the issue isn’t whether Apple is only sending notifications to parents or restricting its searches to specific categories of content. It’s that the company is searching through data before it leaves your phone.&lt;/p&gt;
&lt;blockquote&gt;&lt;p id="rdr527"&gt;Instead of adding CSAM scanning to iCloud Photos in the cloud that they own and operate, Apple is compromising the phone that you and I own and operate, without any of us having a say in the matter. Yes, you can turn off iCloud Photos to disable Apple’s scanning, but that is a policy decision; the capability to reach into a user’s phone now exists, and there is nothing an iPhone user can do to get rid of it.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p id="7FEaCM"&gt;CSAM is illegal and abhorrent. But as the open letter to Apple notes, many countries have pushed to compromise encryption in the name of fighting terrorism, misinformation, and other objectionable content. Now that Apple has set this precedent, it will almost certainly face calls to expand it. And if Apple later rolls out end-to-end encryption for iCloud — something it’s reportedly &lt;a href="https://www.reuters.com/article/us-apple-fbi-icloud-exclusive/exclusive-apple-dropped-plan-for-encrypting-backups-after-fbi-complained-sources-idUSKBN1ZK1CT"&gt;considered doing, albeit never implemented&lt;/a&gt; — it’s laid out a possible roadmap for getting around E2EE’s protections.&lt;/p&gt;
&lt;p id="rH0qDp"&gt;Apple says it will refuse any calls to abuse its systems. And it boasts a lot of safeguards: the fact that parents can’t enable alerts for older teens in Messages, that iCloud’s safety vouchers are encrypted, that it sets a threshold for alerting moderators, and that its searches are US-only and strictly limited to NCMEC’s database.&lt;/p&gt;
&lt;blockquote&gt;&lt;p id="j65XiD"&gt;Apple’s CSAM detection capability is built solely to detect known CSAM images stored in iCloud Photos that have been identified by experts at NCMEC and other child safety groups. We have faced demands to build and deploy government-mandated changes that degrade the privacy of users before, and have steadfastly refused those demands. We will continue to refuse them in the future. Let us be clear, this technology is limited to detecting CSAM stored in iCloud and we will not accede to any government’s request to expand it.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p id="aOwkc0"&gt;The issue is, Apple has the power to modify these safeguards. “Half the problem is that the system is so easy to change,” says Portnoy. Apple has stuck to its guns in some clashes with governments; it &lt;a href="https://www.theverge.com/2016/2/17/11036306/apple-fbi-iphone-encryption-backdoor-tim-cook"&gt;famously defied&lt;/a&gt; a Federal Bureau of Investigation demand for data from a mass shooter’s iPhone. But it’s &lt;a href="https://www.theverge.com/2018/7/18/17587304/apple-icloud-china-user-data-state-run-telecom-privacy-security"&gt;acceded to other requests&lt;/a&gt; like storing Chinese iCloud data locally, even if it insists it hasn’t compromised user security by doing so.&lt;/p&gt;
&lt;p id="6AmFSw"&gt;Stanford Internet Observatory professor Alex Stamos &lt;a href="https://twitter.com/alexstamos/status/1424054555394727939"&gt;also questioned&lt;/a&gt; how well Apple had worked with the larger encryption expert community, saying that the company had declined to participate in a series of discussions about safety, privacy, and encryption. “With this announcement they just busted into the balancing debate and pushed everybody into the furthest corners with no public consultation or debate,” he tweeted.&lt;/p&gt;
&lt;h2 id="AqTnwc"&gt;&lt;strong&gt;How do the benefits of Apple’s new features stack up against the risks?&lt;/strong&gt;&lt;/h2&gt;
&lt;p id="EAtRi1"&gt;As usual, it’s complicated — and it depends partly on whether you see this change as a limited exception or an opening door.&lt;/p&gt;
&lt;p id="2jEzpt"&gt;Apple has legitimate reasons to step up its child protection efforts. In late 2019, &lt;a href="https://www.nytimes.com/interactive/2019/09/28/us/child-sex-abuse.html"&gt;&lt;em&gt;The New York Times&lt;/em&gt; published reports&lt;/a&gt; of an “epidemic” in online child sexual abuse. It blasted American tech companies for failing to address the spread of CSAM, and in a later article, &lt;a href="https://www.nytimes.com/2020/02/07/us/online-child-sexual-abuse.html"&gt;NCMEC singled out Apple&lt;/a&gt; for its low reporting rates compared to peers like Facebook, something the &lt;em&gt;Times&lt;/em&gt; attributed partly to the company not scanning iCloud files.&lt;/p&gt;
&lt;p id="Fo9lbl"&gt;Meanwhile, internal Apple documents have said that iMessage has a sexual predator problem. In documents revealed by the recent &lt;em&gt;Epic v. Apple&lt;/em&gt; trial, an Apple department head listed “child predator grooming” as an under-resourced “active threat” for the platform. Grooming often includes sending children (or asking children to send) sexually explicit images, which is exactly what Apple’s new Messages feature is trying to disrupt.&lt;/p&gt;
&lt;p id="9KQCtc"&gt;At the same time, Apple itself has &lt;a href="https://www.theverge.com/2018/3/28/17172718/apple-ceo-tim-cook-privacy-facebook-cambridge-analytica"&gt;called privacy&lt;/a&gt; a “human right.” Phones are intimate devices full of sensitive information. With its Messages and iCloud changes, Apple has demonstrated two ways to search or analyze content directly on the hardware rather than after you’ve sent data to a third party, even if it’s analyzing data that you &lt;em&gt;have&lt;/em&gt; consented to send, like iCloud photos.&lt;/p&gt;
&lt;p id="dWs0I6"&gt;Apple has acknowledged the objections to its updates. But so far, it hasn’t indicated plans to modify or abandon them. On Friday, &lt;a href="https://9to5mac.com/2021/08/06/apple-internal-memo-icloud-photo-scanning-concerns/"&gt;an internal memo&lt;/a&gt; acknowledged “misunderstandings” but praised the changes. “What we announced today is the product of this incredible collaboration, one that delivers tools to protect children, but also maintain Apple’s deep commitment to user privacy,” it reads. “We know some people have misunderstandings, and more than a few are worried about the implications, but we will continue to explain and detail the features so people understand what we’ve built.”&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/8/10/22613225/apple-csam-scanning-messages-child-safety-features-privacy-controversy-explained"/>
    <id>https://www.theverge.com/2021/8/10/22613225/apple-csam-scanning-messages-child-safety-features-privacy-controversy-explained</id>
    <author>
      <name>Adi Robertson</name>
    </author>
  </entry>
  <entry>
    <published>2021-08-10T13:44:04-04:00</published>
    <updated>2021-08-10T13:44:04-04:00</updated>
    <title>Here’s why Apple’s new child safety features are so controversial</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/L8aonSnsyj7CRHujfw7YFDDk_38=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69705443/acstro_190902_apple_event_0004.0.0.jpg" /&gt;
        &lt;figcaption&gt;Illustration by Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;


  &lt;p&gt;Encryption and consumer privacy experts break down Apple’s plan for child safety&lt;/p&gt; &lt;p class="p--has-dropcap p-large-text" id="IsDfmF"&gt;Last week, Apple, without very much warning at all, announced &lt;a href="https://www.theverge.com/2021/8/5/22611721/apple-csam-child-abuse-scanning-hash-system-ncmec"&gt;a new set of tools built into the iPhone designed to protect children from abuse&lt;/a&gt;. Siri will now offer resources to people who ask for child abuse material or who ask how to report it. iMessage will now flag nudes sent or received by kids under 13 and alert their parents. Images backed up to iCloud Photos will now be matched against a database of known child sexual abuse material (CSAM) and reported to the National Center for Missing and Exploited Children (NCMEC) if more than a certain number of images match. And that matching process doesn’t just happen in the cloud — part of it happens locally on your phone. That’s a big change from how things normally work.&lt;/p&gt;
&lt;p id="kGCO04"&gt;Apple claims it designed what it says is a much more private process that involves scanning images on your phone. And that is a very big line to cross — basically, the iPhone’s operating system now has the capability to look at your photos and match them up against a database of illegal content, and you cannot remove that capability. And while we might all agree that adding this capability is justifiable in the face of child abuse, there are huge questions about what happens when governments around the world, from the UK to China, ask Apple to match up other kinds of images — terrorist content, images of protests, pictures of dictators looking silly. These kinds of demands are routinely made around the world. And until now, no part of that happened on your phone in your pocket. &lt;/p&gt;
&lt;div&gt;  &lt;figure class="e-image"&gt;
        &lt;img alt="Riana Pfefferkorn and Jen King, from Stanford, in the the Decoder art style" data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/sci2ZRqVPfK84REZCRLuSAbN_FM=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22773381/VRG_ILLO_Decoder_King_Apple_alt_s.jpg"&gt;
      &lt;cite&gt;Photo Illustration by Grayson Blackmon / The Verge&lt;/cite&gt;
      &lt;figcaption&gt;Riana Pfefferkorn and Jen King&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p id="cvYmzL"&gt;To unpack all of this, I asked Riana Pfefferkorn and Jennifer King to join me on the show. They’re both researchers at Stanford: Riana specializes in encryption policies, while Jen specializes in privacy and data policy. She’s also worked on child abuse issues at big tech companies in the past. &lt;/p&gt;
&lt;p id="6bjvBx"&gt;I think for a company with as much power and influence as Apple, rolling out a system that changes an important part of our relationship with our personal devices deserves thorough and frequent explanation. I hope the company does more to explain what it’s doing, and soon.&lt;/p&gt;
&lt;div id="UP7DUJ"&gt;
&lt;iframe frameborder="0" height="200" scrolling="no" src="https://playlist.megaphone.fm?e=VMP7739805133" width="100%"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p id="5wAaCG"&gt;&lt;em&gt;The following transcript has been lightly edited for clarity.&lt;/em&gt;&lt;/p&gt;
&lt;p id="KxKLBU"&gt;&lt;strong&gt;Jen King and Riana Pfefferkorn, you are both researchers at Stanford. Welcome to &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;Decoder&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="9sFZPq"&gt;&lt;strong&gt;Jen King:&lt;/strong&gt; Thanks for having us.&lt;/p&gt;
&lt;p id="08rIGv"&gt;&lt;strong&gt;Riana Pfefferkorn:&lt;/strong&gt; Thank you.&lt;/p&gt;
&lt;p id="t6omtz"&gt;&lt;strong&gt;Let’s start with some introductions. Riana, what’s your title and what do you work on at Stanford?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="sdjgPF"&gt;&lt;strong&gt;RP:&lt;/strong&gt; My name is Riana Pfefferkorn. I’m a research scholar at the Stanford Internet Observatory. I’ve been at Stanford in various capacities since late 2015, and I primarily focus on encryption policies. So this is really a moment in the sun for me, for better or for worse.&lt;/p&gt;
&lt;p id="wmcb35"&gt;&lt;strong&gt;Welcome to the light. Jen, what about you? What’s your title, what do you work on?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="vqfdMT"&gt;&lt;strong&gt;JK: &lt;/strong&gt;I am a fellow on privacy and data policy at the Stanford Institute for Human-Centered Artificial Intelligence. I’ve been at Stanford since 2018, and I focus primarily on consumer privacy issues. And so, that runs the gamut across social networks, AI, you name it. If it involves data and people and privacy, it’s kind of in my wheelhouse.&lt;/p&gt;
&lt;p id="zKYsvx"&gt;&lt;strong&gt;I asked both of you to come on the show because of a very complicated new set of tools from Apple, designed to protect children from harm. &lt;/strong&gt;&lt;a href="https://go.redirectingat.com?id=66960X1514734&amp;amp;xs=1&amp;amp;url=https%3A%2F%2Fwww.apple.com%2Fchild-safety%2F&amp;amp;referrer=theverge.com&amp;amp;sref=https%3A%2F%2Fwww.theverge.com%2F22617554%2Fapple-csam-child-safety-features-jen-king-riana-pfefferkorn-interview-decoder" rel="sponsored nofollow noopener" target="_blank"&gt;&lt;strong&gt;The announcement of those tools&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;, the tools themselves, how they’ve been announced, how they’ve been communicated about, have generated a tremendous amount of confusion and controversy, so I’m hoping you can help me understand the tools, and then understand the controversy.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="xINx0r"&gt;&lt;strong&gt;There’s three of them. Let’s go through them from simplest to most complicated. The simplest one actually seems totally fine to me. Correct me if I’m wrong. If you ask Siri on the iPhone for information on how to report child abuse, or much more oddly, if you ask it for child abuse material, it will give you resources to help you report it, or tell you to get support for yourself. This does not seem very controversial at all. It also frankly seems very strange that Apple realized that it was getting this many inquiries to Siri. But, there it is.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="pgwMzW"&gt;&lt;strong&gt;That seems fine to me.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="1Gc5yr"&gt;&lt;strong&gt;JK:&lt;/strong&gt; It doesn’t really raise any red flags for me, I don’t know about you, Riana.&lt;/p&gt;
&lt;p id="tGhHk3"&gt;&lt;strong&gt;RP: &lt;/strong&gt;This seems like something that I’m not sure if this was part of their initial announcement, or if they’d hurriedly added this after the fact, once people started critiquing them or saying, oh my God, this is going to have such a terrible impact on trans and queer and closeted youth. &lt;/p&gt;
&lt;p id="K2rMtS"&gt;As it stands, I don’t think it’s controversial, I just am not convinced that it’s going to be all that helpful. Because what they are saying is, if you ask Siri, “Siri, I’m being abused at home, what can I do?” Siri will basically tell you, according to their documentation, go report it somewhere else. Apple still doesn’t want to know about this. &lt;/p&gt;
&lt;p id="pAqKdm"&gt;Note that they are not making any changes to the abuse reporting functionality of iMessage, which, as I understand it, is limited basically to like, spam. They could’ve added that directly in iMessage, given that iMessage is the tool where all of this is happening. Instead, they’re saying, if you just happen to go and talk to Siri about this, we will point you to some other resources that are not Apple.&lt;/p&gt;
&lt;p id="AX6rXI"&gt;&lt;strong&gt;I think that question about overall effectiveness pervades this entire conversation. But in terms of, here’s the thing, the controversy is pretty small. This one to me feels simple and seemingly the least important to focus on.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="1hqgvR"&gt;&lt;strong&gt;The next one does have some meaningful controversy associated with it, which is, if you are a child who is [12 years old] or younger, and you’re on your family’s iCloud plan, and you send or receive nudes in iMessage, the Messages app on your phone will detect it, and then tell your parents if you view it. And if you’re sending it, it will detect it, say, “do you really want send it?” and then tell your parents if you choose to send it. This has a wide variety of privacy implications for children; a wide variety of implications particularly for queer youth, and transgender youth.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="ID21At"&gt;&lt;strong&gt;At the same time, it feels to me like the controversy around this one is just: how is this deployed? Who will get to use it? Will they always be operating with their children’s best interests at heart? But there’s no technical controversy here. This is a policy controversy, as near as I understand. Is that right, Jen?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="5y0o4Q"&gt;&lt;strong&gt;JK: &lt;/strong&gt;I think so. I say that with a small hesitation, because I am not sure, and Riana may know the answer to this. where they’re doing that real-time scanning to determine whether the image itself, how much, I guess — the proportion of skin it probably contains. I assume that’s happening on the client side, on the phone itself. And I don’t know if Riana has any particular concerns about how that’s being done.&lt;/p&gt;
&lt;p id="ZqIfTS"&gt;Most of the criticisms I’ve heard raised about this are some really good normative questions around what type of family and what type of parenting structure does this really seek to help? I’m a parent, I have my kid’s best interests at heart. But not every family operates in that way. And so I think there’s just been a lot of concerns that just assuming that reporting to parents is the right thing to do won’t always yield the best consequences for a wide variety of reasons.&lt;/p&gt;
&lt;p id="sGXeBS"&gt;&lt;strong&gt;Riana, do you have any concerns on the technical side that are not policy concerns? That’s how I keep thinking about it. There’s a bunch of technical stuff: we’re creating capabilities. And there’s a bunch of policy stuff: how we’re using those capabilities. And obviously the third one, which is the scanning iCloud photos, contains both of those controversies. This one, it really feels like, as Jen called it, a normative controversy.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="q6b2J7"&gt;&lt;strong&gt;RP: &lt;/strong&gt;So, yeah — their documentation is clear that they are analyzing images on the device, and I know that there has been some concern that because it’s not transparent from their documentation exactly how this is happening, how accurate is this image analysis going to be. What else is going to get ensnared in this, that might not actually be as accurate as Apple is saying it’s going to be? That’s definitely a concern that I’ve seen from some of the people who work on the issue of trying to help people who have been abused, in their family life or by intimate partners.&lt;/p&gt;
&lt;p id="GQh91j"&gt;And it’s something that honestly, I don’t understand the technology well enough, and I also don’t think that Apple has provided enough documentation to enable reasoned analysis, and thoughtful analysis. That seems to be one of the [things] they’ve tripped over, is not providing sufficient documentation to enable people to really inspect and test out their claims.&lt;/p&gt;
&lt;p id="cKKre3"&gt;&lt;strong&gt;That is absolutely a theme that runs right into the third announcement, which is this very complicated cryptographic system to check images that are uploaded to iCloud photos for known child sexual abuse material. I’m not even going try to explain this one. Riana, I’m just going defer to you. Explain how Apple says this system works.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="diLvbW"&gt;&lt;strong&gt;RP: &lt;/strong&gt;This will be done on the client baked into the operating system and deployed for every iPhone running iOS 15, once that comes out around the world. But this will only be turned on within the United States at least, so far. There is going to be an on-device attempt to try and make a hash of the photos you have uploaded to iCloud Photos, and check the hash against the hash database that is maintained by the National Center for Missing and Exploited Children, or NCMEC, that contains known child sex abuse material, or CSAM for short.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;div id="GIl3Ux"&gt;&lt;div data-anthem-component="aside:9545330"&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p id="C7PTLz"&gt;There is not going to be a hash of actual CSAM on your phone. There’s not going to be a search of everything so far on your camera roll, only if [the photos] are going into iCloud photos. If you have one image that is in the NCMEC database, that will not trigger review by Apple, where they will have a human in the loop to take a look. It will be some unspecified threshold number of images that have to be triggered by their system, which is more complex than I want to try and explain.&lt;/p&gt;
&lt;p id="4IHH8K"&gt;So, if there is a collection of CSAM material sufficient to cross the threshold, then there will be the ability for a human reviewer at Apple to review and confirm that these are images that are part of the NCMEC database. They’re not going be looking at unfiltered, horrific imagery. There is going to be some degraded version of the image, so that they aren’t going to be exposed to this. Really, it’s very traumatic for people who have to review this stuff.&lt;/p&gt;
&lt;p id="aliaOc"&gt;And then if they confirm that it is in fact, known as CSAM, then that report goes to NCMEC, pursuant to Apple’s duties under federal law, and then NCMEC will involve law enforcement.&lt;/p&gt;
&lt;p id="RxOtoO"&gt;&lt;strong&gt;One of the things that’s very challenging to understand here is that Apple has built it this way so they’re not scanning iCloud data in the cloud, from what I understand. What they don’t want to do is have people upload their photo libraries to iCloud, and then scan a bunch of information in the cloud. &lt;/strong&gt;&lt;/p&gt;
&lt;p id="iOx6jS"&gt;&lt;strong&gt;That other way of doing it, which is in the cloud, is what the other major tech companies do, and that is kind of our expectation of what they do.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="WGHoDT"&gt;&lt;strong&gt;JK:&lt;/strong&gt; Right, although I think the use case is potentially quite different. It’s one of the interesting questions why Apple is doing this in such an aggressive and public way, given that they were not a major source of child sexual violence imagery reporting to begin with. But when you think about these different products, in the online ecosystem, a lot of what you’re seeing are pedophiles who are sharing these things on these very public platforms, even if they carve out little small spaces of them.&lt;/p&gt;
&lt;p id="b10waX"&gt;And so they’re usually doing it on a platform, right? Whether it’s something like Facebook, WhatsApp, Dropbox, whatever it might be. And so, yes, in that case, you’re usually uploading imagery to the platform provider, it’s up to them whether they want to scan it in real time to see what you are uploading. Does it match one of these known images, or known videos that NCMEC maintains a database of?&lt;/p&gt;
&lt;div class="c-float-left"&gt;&lt;aside id="of2hyG"&gt;&lt;q&gt;“The idea that I was going to have the entire NCMEC hash database sitting on my phone...the idea that we’re pushing that to everybody’s individual devices was kind of shocking to me.”&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="lcmnTo"&gt;That they’re doing it this way is just a really interesting, different use case than what we often see. And I’m not sure if Riana has any kind of theory behind why they’ve decided to take this particular tactic. I mean, when I first heard about it, the idea that I was going to have the entire NCMEC hash database sitting on my phone — I mean, obviously, hashes are extremely small text files, so we’re talking about just strings of characters that to the human eye, it just looks like garbage, and they don’t take up a lot of memory, but at the same time, the idea that we’re pushing that to everybody’s individual devices was kind of shocking to me. I’m still kind of in shock about it. Because it’s just such a different use case than what we’ve seen before.&lt;/p&gt;
&lt;p id="T7YGkq"&gt;&lt;strong&gt;RP:&lt;/strong&gt; One of the concerns that has been raised with having this kind of client-side technology being deployed is that once you’re pushing it to people’s devices, it is possible — this is a concern of researchers in this space — for people to try and reverse-engineer that, basically, and figure out what is in the database. There’s a lot of research that’s done there. There are fears on one side about, well what if something that is not CSAM gets slipped into this database?&lt;/p&gt;
&lt;p id="yIlT1X"&gt;The fear on the other side is, what if people who have really strong motivations to continue trading CSAM try to defeat the database by figuring out what’s in it, figuring out how they can perturb an image, so that it slips past the hash matching feature.&lt;/p&gt;
&lt;p id="yoxbKP"&gt;And that’s something that I think is a worry, that once this is put onto people’s devices — rather than happening server-side as currently happens with other technologies such as PhotoDNA — that you are opening up an avenue for malicious reverse engineering to try and figure out how to continue operating, unimpeded and uncaught.&lt;/p&gt;
&lt;p id="uBvCn8"&gt;&lt;a href="https://www.theverge.com/2021/8/6/22613365/apple-icloud-csam-scanning-whatsapp-surveillance-reactions"&gt;&lt;strong&gt;I read some strident statements&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; from the EFF (Electronic Frontier Foundation) and Edward Snowden, and others, calling this a backdoor into the iPhone. Do you think that is a fair characterization, Riana?&lt;/strong&gt; &lt;/p&gt;
&lt;p id="1ODviq"&gt;&lt;strong&gt;RP: &lt;/strong&gt;I don’t like using the word backdoor because it’s a very loaded term and it means different things to different people. And I don’t know that I agree with that because this is all still happening on the client. Right? Apple is very careful to not mention that there are end-to-end encryption for iMessage. And I agree gives an insight into what people are doing on their phone that was not there before. But I don’t know whether that means that you could characterize it as a backdoor.&lt;/p&gt;
&lt;p id="7MeDcb"&gt;I’ve heard a lot of people talking about, like, “Does this mean it’s not end-to-end encryption anymore? Does this mean it’s a backdoor?” I don’t care. I don’t care what we’re calling it. That’s a way of distracting from the main things that we’re actually trying to talk about here, which I think are: what are the policy and privacy and free expression data security impacts that will result from Apple’s decision here? And how will that go out beyond the particular CSAM context? And will what they’re doing work to actually protect children better than what they’ve been doing to date? So quibbling over labels is just not very interesting to me, frankly.&lt;/p&gt;
&lt;p id="SkDq43"&gt;&lt;strong&gt;This comes back to that efficacy question that we’re talking about with Siri. Right now, in order to detect CSAM material, you have to A, be somebody who has it, B, be putting it into your camera roll, and then C, uploading that to iCloud photos. I feel like if criminals are dumb, maybe they’re going to get caught. But it seems very easy for anybody with even a moderate amount of interest to avoid this system, thus reducing the need for this controversy at all.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="1HsMAp"&gt;&lt;strong&gt;JK:&lt;/strong&gt; There’s a couple things here. One is that you could take the position that Apple’s being extremely defensive here and saying, essentially, “Hey, pedophile community, we don’t want you here, so we’re going to, in a very public way, work to defeat your use of our products for that purpose.” Right? And that might be quite effective. &lt;/p&gt;
&lt;p id="MIZBXc"&gt;I want to actually add a little context here for why I’m in this conversation. Before I worked in academia, I used to work in [the tech] industry. I worked for about two years building a tool to review CSAM material and detect it. And when I worked on this project, it was very clear from the beginning that the goal was to get it off the servers of the company I was working for. Like — there was no higher goal. We were not going to somehow solve the child pornography problem.&lt;/p&gt;
&lt;p id="j7w5Dp"&gt;That’s where I have a particular insight. One of the reasons Apple could be taking this stand could be a moral issue — it could be that they’ve decided that they just simply do not want their products associated with this type of material, and in a very public way they’re going to take a stand against it. I think you’re right. I think that there are people for whom, if you’re going to get caught using an Apple product, it’s probably because you weren’t necessarily well-versed in all the ways to try to defeat this type of thing.&lt;/p&gt;
&lt;p id="DGlRxU"&gt;[But] I think it’s really important to remember [that] when you talk about these issues and you think about this group of people, that they are a &lt;em&gt;community&lt;/em&gt;. And there are a lot of different ways that you can detect this content. I would feel a lot better about this decision if I felt like what we were hearing is that all other methods have been exhausted, and this is where we are at.&lt;/p&gt;
&lt;p id="VIizA0"&gt;And I am in no way of the belief that all other methods have been exhausted, by Apple or by kind of the larger tech community et al, who I think has really failed on this issue, given I worked on it from 2002 to 2004 and it’s gotten tremendously worse since that time. A lot more people have joined the internet since then, so it is kind of a question of scale. But I would say industry across the board has really been bad at really trying to defeat this as an issue.&lt;/p&gt;
&lt;p id="DUhJjk"&gt;&lt;strong&gt;What are the other methods?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="U7MBzm"&gt;&lt;strong&gt;JK:&lt;/strong&gt;  It’s important to understand that this is a community of users, and different communities use different products in different ways. When you’re in product design, you’re designing a product with particular users in mind. You kind of have your optimal user groups that you want to privilege the product for, who you want to attract, how you want to design the features for.&lt;/p&gt;
&lt;p id="rkgkbl"&gt;The kind of work I did to try to understand this community, it became very clear that this group of users know what they’re doing is illegal. They don’t want to get caught, and they use things very materially different than other users. And so if you’re willing to put in the time to understand how they operate and put in the resources to detect them, and to really see how they differ from other users — because they don’t use these products the same way that you and I probably do. Right? They’re not loading up photos to share with friends and family. They’re operating under subterfuge. They know what they’re doing is highly illegal. &lt;/p&gt;
&lt;p id="qPoAGZ"&gt;There’s often a great deal of pressure in terms of timing, for example. One of the things I witnessed in the work I did was that people often would create accounts and basically have an upload party. They would use the service at an extremely high rate for an extremely short amount of time and then ditch it, ditch whatever product they were working in. Because they knew that they only had a limited amount of time before they would get caught.&lt;/p&gt;
&lt;p id="nSAQg8"&gt;To just assume that you can’t potentially put in more work to understand how these people use your product, and that they may be detectable in ways that don’t require the types of work that we’re seeing Apple do — if I had more reassurance they’d actually kind of done that level of research and really exhausted their options I would probably feel more confident about what they’re doing. &lt;/p&gt;
&lt;p id="KPs5fx"&gt;I don’t want to just point the finger at Apple. I think this is an industry-wide problem, with a real lack of devotion to resources behind it. &lt;/p&gt;
&lt;p id="e0qcIW"&gt;&lt;strong&gt;RP: &lt;/strong&gt;The trouble with this particular context is how extremely unique CSAM is compared to any other kind of abusive content that a provider might encounter. It is uniquely opaque in terms of how much outside auditability or oversight or information anybody can have.&lt;/p&gt;
&lt;p id="vyPGTF"&gt;I mentioned earlier that there’s a risk that people might be able to try and reverse-engineer what’s in the database of hashed values to try and figure out how they could  subvert and sneak CSAM around the database.&lt;/p&gt;
&lt;p id="FKNFVN"&gt;The other thing is that it’s hard for us to know exactly what it is that providers are doing. As Jen was saying, there’s a bunch of different techniques that they could take and different approaches that they can employ. But when it comes to what they are doing on the backend about CSAM, they are not very forthcoming because everything that they tell people to explain what it is they’re doing is basically a roadmap to the people who want to abuse that process, who want to evade it.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="GJk6oD"&gt;&lt;q&gt;CSAM databases are a black box&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="PR5PMG"&gt;So it is uniquely difficult to get information about this on the outside, as a researcher, as a user, as a policymaker, as a concerned parent, because of this veil of secrecy that hangs over everything to do with this whole process, from what is in the database, to what are different providers doing. Some of that sometimes comes out a little bit in prosecutions of people who get caught, by providers, for uploading and sharing CSAM on their services. There will be depositions and testimony and so forth. But it’s still kind of a black box. And that makes it hard to critique the suggested improvements, to have any kind of oversight. &lt;/p&gt;
&lt;p id="ipSGIP"&gt;And that’s part of the frustration here, I think, is that it’s very difficult to, say, “You just have to trust us and trust everything all the way down from every point, from NCMEC on down,” and simultaneously, “Just know that what we’re doing is not something that has other collateral harms,” because for anything outside of CSAM, you have more ambiguity and legitimate use cases and context where it matters. &lt;/p&gt;
&lt;p id="f4STsa"&gt;When it comes to CSAM, context does not matter. Something that I’ve been saying in recent days is: there’s no fair use for CSAM the way that there is for using copyrighted work. There’s this lack of information that makes it really difficult for folks like Jen or me or other people in civil society, other researchers, to be able to comment. And Jen, I’m so glad that you have this background, that you at least have both the privacy and the understanding from working on this from the provider’s side.&lt;/p&gt;
&lt;p id="bKk2N0"&gt;&lt;strong&gt;If you take that and you view it from Apple’s side, most charitably: well, at least Apple announced &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;something&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt;. Right? They are being transparent, to a degree. We went and asked Google, “Hey, do you do this scanning in Google Photos?” And there’s no way to know. We just don’t know the answer to that question.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="bznAW4"&gt;&lt;strong&gt;I think if you went to Dropbox and asked them they would just not tell you. We assume that they are. But at least here, Apple is saying, “We’re doing it. Here’s the method by which we’re doing it.” That method, that addition of capability to the iPhone, is problematic in various ways. But they’re copping to it and they’re explaining how it works. Do they get points for that?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="TG24gM"&gt;&lt;strong&gt;RP: &lt;/strong&gt;They certainly learned that they won’t get any plaudits for that. You’ve identified that. This might be a point where they say other organizations scan using PhotoDNA in the cloud, and they do so over email. And I don’t know how well understood that is by the general public, that, for most of the services that you use, if you are uploading photos, they are getting scanned to look for CSAM for the most part. If you’re using webmail, if you’re using a cloud storage provider — Dropbox absolutely does. &lt;/p&gt;
&lt;p id="T4PYwb"&gt;But you’re right that they are not necessarily that forthcoming about it in their documentation. And that’s something that might kind of redound to the benefit of those who are trying to track and catch these offenders, is that there may be some misunderstanding or just lack of clarity about what is happening. That trips up people who trade in this stuff and share and store this stuff because they don’t realize that.&lt;/p&gt;
&lt;p id="4twvGr"&gt;I guess there’s almost some question about whether Apple is kind of ensuring that there will be less CSAM on iCloud Photos three months from now than there is today, because they’re being more transparent about this and about what they are doing.&lt;/p&gt;
&lt;p id="fuoH5l"&gt;&lt;strong&gt;JK:&lt;/strong&gt; There is a really complicated relationship here between the companies and law enforcement that I think bears mentioning, which is that, the companies, broadly, are the source of all this material. You know? Hands down. I don’t even know if you see offline CSAM these days. It’s all online, and it’s all being traded on the backs of these large organizations.&lt;/p&gt;
&lt;p id="O4q4iS"&gt;Holding CSAM is illegal. Every copy the platforms hold is a felony, essentially, a criminal felony. At the same time that they are the source of this material and law enforcement wants to crack down, law enforcement needs the platforms to report it. So there’s this tension at play that I think is not necessarily well understood from the outside. &lt;/p&gt;
&lt;p id="LA7MoL"&gt;There’s a bit of a symbiotic relationship here where, if the companies crack down too much and force it all off their services, it all ends up on the dark web, completely out of the reach of law enforcement without really heavy investigative powers. In some ways, that disadvantages law enforcement. One could argue that they need the companies to not crack down so much that it completely disappears off their services because it makes their job much harder. So there is a very weird tension here that I think needs to be acknowledged.&lt;/p&gt;
&lt;p id="qSQvww"&gt;&lt;strong&gt;It feels like one enormous aspect of this entire controversy is the fact that the scanning is being done on the device. That’s the Rubicon that’s been crossed: up until now, your local computer has not scanned your local storage in any way. But once you hit the cloud, all kinds of scanning happens. That’s problematic, but it happens.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="Ro4Fdn"&gt;&lt;strong&gt;But we have not yet entered the point where law enforcement is pushing a company to do local scanning on your phone, or your computer. Is that the big bright line here that’s causing all the trouble?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="Msc4V9"&gt;&lt;strong&gt;RP: &lt;/strong&gt;I view this as a paradigm shift, to take where the scanning is happening from in the cloud, where you are making the &lt;em&gt;choice&lt;/em&gt; to say, “I’m going to upload these photos into iCloud.” It’s being held in third parties’ hands. You know, there’s that saying that “it’s not the cloud; it’s just somebody else’s computer,” right?&lt;/p&gt;
&lt;p id="hah3C8"&gt;You’re kind of assuming some level of risk in doing that: that it might be scanned, that it might be hacked, whatever. Whereas moving it down onto the device — even if, right now, it’s only for photos that are in the cloud — I think is very different and is intruding into what we consider a more private space that, until now, we could take for granted that it would stay that way. So I do view that as a really big conceptual shift.&lt;/p&gt;
&lt;div class="c-float-left"&gt;&lt;aside id="1TAk9e"&gt;&lt;q&gt;“The illusion that you’ve been able to control the data on your phone has been nothing more than an illusion for most people for quite a while now. “&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="Bbp4Ml"&gt;Not only is it a conceptual shift in how people might think about this, but also from a legal standpoint. There is a big difference between data that you hand over to a third party and assume the risk that they’re going to turn around and report to the cops, versus what you have in the privacy of your own home or in your briefcase or whatever.&lt;/p&gt;
&lt;p id="jfbQLE"&gt;I do view that as a big change.&lt;/p&gt;
&lt;p id="qwY2AD"&gt;&lt;strong&gt;JK:&lt;/strong&gt; I would add that some of the dissonance here is the fact that we just had Apple come out with the &lt;a href="https://www.theverge.com/2021/4/27/22405474/apple-app-tracking-transparency-ios-14-5-privacy-update-facebook-data"&gt;“asks apps to not track” feature&lt;/a&gt;, which was already in existence before, but they actually made that dialog box prominent to ask you when you were using an app if you want the app to track you. It seems a bit dissonant that they just rolled out that feature, and then suddenly, we have this thing that seems almost more invasive on the phone.&lt;/p&gt;
&lt;p id="INPE1K"&gt;But I would say, as someone who’s been studying privacy in the mobile space for almost a decade, there is already an extent to which these phones aren’t ours, especially when you have third-party apps downloading your data, which has been a feature of this ecosystem for some time. This is a paradigm shift. But maybe it’s a paradigm shift in the sense that we had areas of the phone that we maybe thought were more off-limits, and now they are less so than they were before.&lt;/p&gt;
&lt;p id="dnpaI0"&gt;The illusion that you’ve been able to control the data on your phone has been nothing more than an illusion for most people for quite a while now.&lt;/p&gt;
&lt;p id="TCiVLq"&gt;&lt;strong&gt;The idea that you have a local phone that has a networking stack, that then goes to talk to the server and comes back — that is almost a 1990s conception of connected devices, right? In 2021, everything in your house is always talking to the internet, and the line between the client and the server is extremely blurry to the point where we market the networks. We market 5G networks, not just for speed but for capability, whether or not that’s true.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="Tri5Jn"&gt;&lt;strong&gt;But that fuzziness between client and server and network means that the consumer might expect privacy on local storage versus cloud storage, but I’m wondering if this is &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;actually&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; a line that we crossed  — or if just because Apple announced this feature, we’re now perceiving that there &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;should&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; be a line.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="N6Pcvc"&gt;&lt;strong&gt;RP:&lt;/strong&gt; It’s a great point because there are a number of people who are kind of doing the equivalent of “If the election goes the wrong way, I’m going to move to Canada” by saying “I’m just going to abandon Apple devices and move to Android instead.” But Android devices are basically just a local version of your Google Cloud. I don’t know if that’s better.&lt;/p&gt;
&lt;p id="pX2a8m"&gt;And at least you can fork Android, [although] I wouldn’t want to run a forked version of Android that I sideloaded from some sketchy place. But we’re talking about a possibility that people just don’t necessarily understand the different ways that the different architectures of their phones work.&lt;/p&gt;
&lt;div class="c-float-right c-float-hang"&gt;&lt;aside id="JtbfcW"&gt;&lt;q&gt;“People’s rights, people’s privacy, people’s free expression — that shouldn’t depend upon a consumer choice.”&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="PoLQ9s"&gt;A point that I’ve made before is that people’s rights, people’s privacy, people’s free expression, that shouldn’t depend upon a consumer choice that they made at some point in the past. That shouldn’t be path-dependent for the rest of time on whether or not their data that they have on their phone is really theirs or whether it actually is on the cloud.&lt;/p&gt;
&lt;p id="QNiqNT"&gt;But you’re right that, as the border becomes blurrier, it becomes both harder to reason about these things from arm’s length, and it also becomes harder for just average people to understand and make choices accordingly.&lt;/p&gt;
&lt;p id="x57xbG"&gt;&lt;strong&gt;JK: &lt;/strong&gt;Privacy shouldn’t be a market choice. I think it’s a market failure, for the most part, across industry. A lot of the assumptions we had going into the internet in the early 2000s was that privacy could be a competitive value. And we do see a few companies competing on it. DuckDuckGo comes to mind, for example, on search. But bottom line, privacy shouldn’t be left up to... or at least many aspects of privacy shouldn’t be left up to the market.&lt;/p&gt;
&lt;p id="RP3xQ6"&gt;&lt;strong&gt;There’s another tension that I want to explore with both of you, which is the sort of generalized surveillance tension around encryption and Apple specifically. Apple famously will not unlock iPhones for law enforcement, or at least they say they won’t do it here. They say they don’t do it in other countries like China. They have wanted to encrypt the whole of iCloud, &lt;/strong&gt;&lt;a href="https://www.theverge.com/2020/1/21/21075033/apple-icloud-end-to-end-encryption-scrapped-fbi-reuters-report"&gt;&lt;strong&gt;and famously the FBI talked them out of it&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;. And in China, they’ve &lt;/strong&gt;&lt;a href="https://www.theverge.com/2018/7/18/17587304/apple-icloud-china-user-data-state-run-telecom-privacy-security"&gt;&lt;strong&gt;handed over the iCloud data centers&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt; to the Chinese government. The Chinese government holds those keys.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="fECxjW"&gt;&lt;strong&gt;I believe what they want to do is encrypt everything and just wash their hands of it, and walk away, and say, “It’s our customers’ data. It’s private. It’s up to them.” They cannot, for various reasons. Do you think that tension has played into this system as it is currently architected, where they &lt;/strong&gt;&lt;em&gt;&lt;strong&gt;could&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; just say, “We’re scanning all the data in the cloud directly and handing it over to the FBI or NCMEC or whoever,” but instead they want to encrypt that data, so they’ve now built this other ancillary system that does a little bit of local hashing comparison against the table in the cloud, it generates these complicated security vouchers, and then it reports to NCMEC if you pass a threshold. &lt;/strong&gt;&lt;/p&gt;
&lt;p id="dWf7DD"&gt;&lt;strong&gt;All of that seems like at some point they’re going to want to encrypt the cloud, and this is the first step towards a deal with law enforcement, at least in this country.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="JF5yTZ"&gt;&lt;strong&gt;RP:&lt;/strong&gt; I have heard that idea from someone else I talked to about this and mentioned it to my colleague at SIO, Alex Stamos. Alex is convinced that this is a prelude to announcing end-to-end encryption for iCloud later on. It seems to be the case that, however it is that they are encrypting iCloud data for photos, that they have said it is “too difficult to decrypt everything that’s in the cloud, scan it for CSAM, and do that at scale.”  So it’s actually more efficient and, in Apple’s opinion, more privacy-protective, to do this on the client side of the architecture instead.&lt;/p&gt;
&lt;div class="c-float-left"&gt;&lt;aside id="mWSocq"&gt;&lt;q&gt;Child safety features could be a prelude to a totally encrypted iCloud&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="nTfG2y"&gt;I don’t know enough about the different ways that Dropbox encrypts their cloud, that Apple encrypts their cloud, that Microsoft encrypts its cloud, versus how iCloud does it, to know whether Apple is in fact doing something different that makes it uniquely hard for them to scan in the cloud the way that other entities do. But certainly, I think that looming over all of this is that there has been several years’ worth of encryption files, not just here in the US, but around the world, primarily focused in the last couple of years on child sex abuse material. Prior to that, it was terrorism. And there’s always concerns about other types of material as well.&lt;/p&gt;
&lt;p id="DUO4mX"&gt;One thing that’s a specter looming over this move by Apple is that they may see this as something where they can provide some kind of a compromise and hopefully preserve the legality of device encryption and of end-to-end encryption, writ large, and maybe try and rebuff efforts that we have seen, including in the US, even just last year, to effectively ban strong encryption. This might be, “If we give an inch, maybe they won’t take a mile.”&lt;/p&gt;
&lt;p id="m1Bwao"&gt;&lt;strong&gt;I’ve seen a lot of pushback against that idea. Just to be honest, personally, if the outcome is the same — there’s scanning done of stuff you put on the cloud — I think that is the consumer expectation. Once you upload something to somebody else’s server, they can look at it. They can, I don’t know, copyright strike it. They can scan it for CSAM. That stuff is going to happen once you give your data away to a cloud provider. That does feel like a consumer expectation in 2021, whether that is good or bad. I just think it’s the expectation.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="NKEFwp"&gt;&lt;strong&gt;It seems like this is a very complicated mechanism to accomplish the same goal of just scanning in the cloud. But because it is this very complicated mechanism, that is “give an inch so they won’t take a mile,” the controversy seems to be they’re not just going take the inch. &lt;/strong&gt;&lt;/p&gt;
&lt;p id="wi3FSZ"&gt;&lt;strong&gt;Governments around the world will now ask you to expand this capability in various ways that maybe the United States government won’t do, but certainly the Chinese government or the Indian government or other more oppressive governments would certainly take advantage of. Is there a backstop here for Apple to not expand the capability beyond CSAM?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="tChyNf"&gt;&lt;strong&gt;RP:&lt;/strong&gt; This is my primary concern. The direction I think this is going is that we don’t have, ready to go, hashed databases or hashes of images of other types of abusive content besides CSAM, with the exception of terrorist and violent extremist content. There is a database called GIFCT that is an industry collaboration, to collaboratively contribute imagery to a database of terror and violent extremist content, largely arising out of the Christchurch shooting a few years back, which really woke up a new wave of concern around the world about providers hosting terrorists and violent extremist material on their services.&lt;/p&gt;
&lt;p id="M7bg8d"&gt;So my prediction is that the next thing that Apple will be pressured to do will be to deploy the same thing for GIFCT as they are currently doing for the NECMC database of hashes of CSAM. And from there on, I mean, you can put anything you’d like into a hashed image database. &lt;/p&gt;
&lt;p id="lBd94c"&gt;Apple just said, “If we’re asked to do this for anything but CSAM, we simply will not.” And, that’s fine, but why should I believe you? Previously, their slogan was, “What happens on your iPhone stays on your iPhone.” And now that’s not true, right?&lt;/p&gt;
&lt;div class="c-float-right c-float-hang"&gt;&lt;aside id="QsAmmW"&gt;&lt;q&gt;“For a large enough market, like China, I think that they will fold.”&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="EkBK9f"&gt;They might abide by that, where they think that the reputational trade off is not worth the upside. But if there’s a distinction with choices between either you implement this hashed database of images that this particular government doesn’t like, or you lose access to our market, and you will never get to sell a Mac or an iPhone in this country again? For a large enough market, like China, I think that they will fold.&lt;/p&gt;
&lt;p id="xGCrbO"&gt;India is one place that a lot of people have pointed to. India has a billion people. They actually are not that big of a market for iPhones, at least commensurate with the size of the market that currently exists in China. But the EU is. The European Union is a massive market for Apple. And the EU just barely got talked off the ledge from having &lt;a href="https://www.theverge.com/2019/3/26/18280726/europe-copyright-directive"&gt;an upload filter mandate for copyright-infringing material&lt;/a&gt; pretty recently. And there are rumblings that they are going to introduce a similar plan for CSAM at the end of this year.&lt;/p&gt;
&lt;p id="w9DQmm"&gt;For a large enough market, basically, it’s hard to see how Apple, thinking of their shareholders, not just of their users’ privacy or of the good of the world, continues taking that stand and says, “No, we’re not going to do this,” for whatever it is they’re confronted with. Maybe if it’s lese majeste laws in Thailand that say, “You are banned from letting people share pictures of the king in a crop top” — which is a real thing — maybe they’ll say, “Eh, this market isn’t worth the hit that we would take on the world stage.” But if it’s the EU, I don’t know. &lt;/p&gt;
&lt;p id="cMbomq"&gt;&lt;strong&gt;Let’s say the EU was going implement this upload filter. If they say, “We need an upload filter for CSAM,” and Apple’s already built it, and it preserves encryption, isn’t that the correct trade-off?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="t4sHj8"&gt;&lt;strong&gt;RP:&lt;/strong&gt; I think that there are absolutely a lot of folks that you could talk to who would quietly admit that they might think — if this really did get limited only ever to CSAM for real — that that might be a compromise that they could live with. Even though we’re talking about moving surveillance down into your device. And, really, there’s no limitation on them for only doing this for iCloud photos. It could be on your camera roll next. If we really believe that this would not move beyond CSAM, there are a lot of folks who might be happy with that trade-off.&lt;/p&gt;
&lt;p id="a1Or7B"&gt;Going back to your question about what a backstop might be, though, to keep it from going up beyond CSAM, this goes back to what I mentioned earlier about how CSAM is really unique among types of abuse. And once you’re talking about literally any other type of content, you’re necessarily going to have an impact on free expression, values on news, commentary, documentation of human rights abuses, all of these things. &lt;/p&gt;
&lt;p id="64VzWS"&gt;And that’s why there’s already a lot of criticism of the GIFCT database that I mentioned, and why it would be supremely difficult to build out a database of images that are hate speech, whatever that means. Much less something that is copyright infringing. There is nothing that is only ever illegal and there’s no legitimate context, except for CSAM.&lt;/p&gt;
&lt;p id="ydIgHW"&gt;So I think that this is a backstop that Apple could potentially try to point to. But just because it would trample free expression and human rights to deal with this for anything else — I don’t necessarily know that that’s something that’s going to stop governments from demanding it.&lt;/p&gt;
&lt;p id="NM5epQ"&gt;&lt;strong&gt;For CSAM, there is a database of images that exist that are just illegal. You can’t have them, you can’t look at them. And there’s no value towards even pointing them out and saying, “look at this” for things like scholarship or research.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="R8GbG0"&gt;&lt;strong&gt;But a database of images of terrorism, &lt;/strong&gt;&lt;a href="https://www.theverge.com/2019/3/15/18266859/new-zealand-shooting-video-social-media-manipulation"&gt;&lt;strong&gt;the video of the Christchurch shooting&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;, there are fuzzier boundaries there. Right? There are legitimate reasons for some people to have that video or to have other terrorism-related content: to report on it, to talk about it, to analyze it. And because that is a fuzzier set, it’s inherently more dangerous to implement these kinds of filters.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="0OTPrW"&gt;&lt;strong&gt;JK:&lt;/strong&gt; I would argue that your example points to one of the easiest examples of that whole genre, and that it’s much harder from those extreme examples to work backwards to “what is terrorism” versus “what are groups engaging in rightful protests on terrorism-related issues,” for example?  The line-drawing becomes much, much harder.&lt;/p&gt;
&lt;p id="xHVQOH"&gt;To kind add some context to what Riana was saying, we are very much talking about the US and the fact that this content is illegal in the US. In Europe, those boundaries, I think, are much broader because they’re not operating under the First Amendment. I’m not a lawyer, so I’m definitely speaking a little bit outside my lane, but there isn’t the same free speech absolutism in the EU because they don’t have the First Amendment we have here in the US. The EU has been much more willing to try to draw lines around particular content that we don’t do here.&lt;/p&gt;
&lt;p id="cFaHHB"&gt;&lt;strong&gt;RP:&lt;/strong&gt; I think that there are different regimes in different countries for the protection of fundamental rights that look a little different from our Constitution. But they exist. And so, when there have been laws or surveillance regimes that would infringe upon those, there are other mechanisms, where people have brought challenges and where some things have been struck down as being incompatible with people’s fundamental rights as recognized, in other countries. &lt;/p&gt;
&lt;p id="YSOUoU"&gt;And it’s very difficult to engage in that line-drawing. I have a side hustle talking about deepfakes. There is absolutely a lot of interest in trying to figure out, okay, how do we keep mis- and disinformation from undermining democracy, from hurting vaccine rollout efforts, and also from having deepfakes influence an election. And it would be real easy — this is what law professors Danielle Citron and Bobby Chesney call &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3213954"&gt;“the liar’s dividend”&lt;/a&gt; — for a government that does not like evidence of something that actually happened, something that is true and authentic but inconvenient for them, to say, “That’s fake news. That is a deepfake. This is going in our database of hashes of deepfakes that we’re gonna make you implement in our country.”&lt;/p&gt;
&lt;p id="HDVdmC"&gt;So there’s all of these different issues that get brought up on on the free expression side once you’re talking about anything other than child sex abuse material. Even there, it takes a special safe harbor under the federal law that applies to make it okay for providers to have this on their services. As Jen was saying, otherwise that is just a felony, and you have to report it. If you don’t report it, you don’t get the safe harbor, and that provider is also a felon.&lt;/p&gt;
&lt;p id="7pMqyS"&gt;The National Center for Missing and Exploited Children is the only entity in America that is allowed to have this stuff. There are some debates going on in different places right now about whether there are legitimate applications for using CSAM to train AI and ML models. Is that a permissible use? Is that re-victimizing the people who are depicted? Or would it have an upside in helping better detect other images? Because the more difficult side of this is detecting &lt;em&gt;new&lt;/em&gt; imagery, rather than detecting known imagery that’s in a hashed database.&lt;/p&gt;
&lt;p id="HW3kw7"&gt;So even there, that’s a really hot button issue. But it gets back to Jen’s point: if you start from the fuzzy cases and work backwards, Apple could say “We’re not going to do this for anything other than CSAM because there’s never going to be agreement on anything else other than this particular database.”&lt;/p&gt;
&lt;p id="7Kg5Gq"&gt;Apple has also said they are not compiling the hashed databases, the image databases themselves. They’re taking what is handed to them, with the hashes, that NCMEC provides or that other child safety groups in other countries provide. If they don’t have visibility into what is in those databases, then again, it’s just as much of a black box to them as it is to anybody else. Which has been a problem with GIFCT: we don’t know what’s in it. We don’t know if it contains human rights documentation or news or commentary or whatever. Rather than just something that everybody can agree nobody should ever get to look at ever, not even consenting adults.&lt;/p&gt;
&lt;p id="BJB5eb"&gt;&lt;strong&gt;So you’re saying the danger there is, there’s a child safety organization in some corrupt country. And, the dictator of that country says, “There’s eight photos of me sneezing, and I just want them to not exist anymore. Add them to the database.” Apple will never know that it’s being used in that way, but the photos will be detected and potentially reported to the authorities.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="sElEcg"&gt;&lt;strong&gt;RP: &lt;/strong&gt;Well, Apple is saying one of the protections against non-CSAM uses of this is that they have a human in the loop who reviews matches, if there is a hit for a sufficiently large collection of CSAM. They will take a look and be like, “Yep, that matches the NCMEC databases.” If what they’re looking at is the Thai king in a crop top, then they can say, “What the heck? No, this isn’t CSAM.” And supposedly, that’s going to be another further layer of protection.&lt;/p&gt;
&lt;p id="5JZULq"&gt;I think that I have already started seeing some concerns, though, about, “Well, what if there’s a secret court order that tells NCMEC to stick something in there? And then NCMEC employees have to just go along with it somehow?”  That seems like something that could be happening now, given that PhotoDNA is based off of hashes that NCMEC provides even now for scanning Dropbox and whatever.&lt;/p&gt;
&lt;p id="O5EqCY"&gt;This is really highlighting how it’s just trust all the way down. You have to trust the device. You have to trust the people who are providing the software to you. You have to trust NCMEC. And it’s really kind of revealing the feet of clay that I think is kind of underpinning the whole thing. We thought our devices were ours, and Apple had taken pains during &lt;em&gt;Apple v. FBI&lt;/em&gt; to say, “Your device is yours. It doesn’t belong to us.” Now it looks like, well, maybe the device really is still Apple’s after all, or at least the software on it.&lt;/p&gt;
&lt;p id="exRIWQ"&gt;&lt;strong&gt;This brings me to just the way they’ve communicated about this, which we were talking about briefly before we started recording. You both mentioned big meaty debates happening in civil society organizations, with policymakers, with academics, with researchers, about how to handle these things, about the state of encryption, about the various tradeoffs.&lt;/strong&gt;&lt;/p&gt;
&lt;p id="sBREE2"&gt;&lt;strong&gt;It does not appear that Apple engaged those debates in any substantive way before rolling this out. Do you think if they had, or if they had been more transparent with members of that community, that the reaction wouldn’t have been quite so heated?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="BP2C3t"&gt;&lt;strong&gt;RP: &lt;/strong&gt;The fact that Apple rolled this out with maybe a one day’s heads up to some people in civil society orgs and maybe some media, isn’t helpful. Nobody was brought into this process while they were designing this, to tell them, “Here are the concerns that we have for queer 12-year-olds. Here are the concerns for privacy. Here are the civil liberties and the human rights concerns,” all of that. It looks like this was just rolled out as a fait accompli with no notice. &lt;/p&gt;
&lt;p id="aiJ8md"&gt;With, I have to say, really confusing messaging, given that there are these three different components and it was easy to conflate two of them and get mixed up about what was happening. That has further caused a lot of hammering and wailing and gnashing of teeth. &lt;/p&gt;
&lt;p id="abgrvS"&gt;But if they had involved elements of civil society other than, presumably, NCMEC itself and probably law enforcement agencies, maybe some of the worst could have been averted. Or maybe they would have ignored everything that we would have said and just gone forth with the thing that they’re doing it as-is.&lt;/p&gt;
&lt;p id="dto65l"&gt;But, as Jen and I can tell you — Jen and I have both been consulted before by tech companies who have something that impacts privacy. And they’ll preview that for us in a meeting and take our feedback. And that’s standard practice for tech companies, at least at some points. If you don’t really care what people’s feedback is, then you roll out where you get feedback from people later and later in the process,&lt;/p&gt;
&lt;p id="YlAzuP"&gt;But if they had really wanted to minimize the free expression and privacy concerns, then they should have consulted with outsiders, even if there are voices they thought that would be “too screechy,” as the executive director of NCMEC called everybody who expressed any kind of reservation about this. Even if they didn’t want to talk to what Apple might think is somehow the lunatic fringe or whatever, they could have talked to more moderate voices. They could have talked to academics. They could have talked to me, although I’m probably too screechy for them, and at least taken those concerns back and thought about them. But they didn’t.&lt;/p&gt;
&lt;p id="chUWwR"&gt;&lt;strong&gt;We’ve heard about the controversy, we’ve heard about the criticism. Do you think Apple responds to that in any meaningful way? Do you think they back off this plan, or is this just shipping in iOS 15, as they’ve said?&lt;/strong&gt;&lt;/p&gt;
&lt;p id="unQYZb"&gt;&lt;strong&gt;JK:&lt;/strong&gt; I think image hashing match ships. I don’t know about the “nanny cam,” again, for lack of a better word.&lt;/p&gt;
&lt;p id="ZAKiiC"&gt;I predict that they will double down on the CSAM image scanning for all of the different reasons we’ve talked about today. I think Riana really hit the nail on the head — I think there’s some kind of political strategizing going on behind the scenes here. If they are trying to take a bigger stand on encryption overall, that this was the piece that they had to give up to law enforcement in order to do so.&lt;/p&gt;
&lt;p id="rVGzAQ"&gt;&lt;strong&gt;RP:&lt;/strong&gt; I think certainly for the stuff about Siri that is uncontroversial, they’ll keep rolling that out. I’m not certain, but it seems like the iMessage stuff either wasn’t messaged clearly at the beginning, or maybe they really did change over the course of the last few days in terms of what they said they were going to do. If that’s true, and I’m not sure whether it is, that then indicates that maybe there is some room to at least make some tweaks. &lt;/p&gt;
&lt;p id="7zdEmH"&gt;However, the fact that they rolled out this whole plan as a fait accompli, that’s going to be put into iOS 15 at the very end, without any consultations, suggests to me that they are definitely going to go forward with these plans. With that said, there may be some silver lining in the fact that civil society was not consulted at any point in this process, that now, maybe there’s an opportunity to use this concerted blowback as a way to try and get pushback in that might not have been possible, had civil society been looped in all along the way, and incorporated and neutralized, almost.&lt;/p&gt;
&lt;p id="VAT51A"&gt;So, I’m not sanguine about the odds of them just not deploying this CSAM thing at all. Don’t get me wrong, I would love to be wrong with the slippery slope arguments, that the next thing will be demanding this for GIFCT and then it’ll be not as much to say in deepfakes and copyright infringement. I would love to be proved wrong about that, even as silly as it would make me look. But I’m not sure that that’s going to be the case.&lt;/p&gt;
&lt;aside id="jA4WOx"&gt;&lt;div data-anthem-component="actionbox" data-anthem-component-data='{"title":"Decoder with Nilay Patel","description":"A podcast from &amp;lt;em&amp;gt;The Verge&amp;lt;/em&amp;gt; about big ideas and other problems.","label":"Subscribe now!","url":"https://podcasts.apple.com/us/podcast/decoder-with-nilay-patel/id1011668648"}'&gt;&lt;/div&gt;&lt;/aside&gt;&lt;p id="OnXzp8"&gt;&lt;em&gt;&lt;strong&gt;Update August 10th, 5:53PM ET: &lt;/strong&gt;&lt;/em&gt;&lt;em&gt;Added full transcript.&lt;/em&gt;&lt;/p&gt;
&lt;p id="K5cfwt"&gt;&lt;/p&gt;
&lt;p id="Q2om1A"&gt;&lt;/p&gt;
&lt;p id="flYo2u"&gt;&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/22617554/apple-csam-child-safety-features-jen-king-riana-pfefferkorn-interview-decoder"/>
    <id>https://www.theverge.com/22617554/apple-csam-child-safety-features-jen-king-riana-pfefferkorn-interview-decoder</id>
    <author>
      <name>Nilay Patel</name>
    </author>
  </entry>
  <entry>
    <published>2021-08-10T09:15:00-04:00</published>
    <updated>2021-08-10T09:15:00-04:00</updated>
    <title>The new Beats Studio Buds are already $20 off at Amazon and Walmart</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/vcoTzDBjvW18DhQigmIWvGClqe4=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69703765/DSCF4152_Edited.0.jpg" /&gt;
        &lt;figcaption&gt;Typically $150, Amazon and Walmart are both taking $20 off the recently-released earbuds. | Photo by Chris Welch / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="qh5ECz"&gt;The Beats name carries quite a bit of clout, which is perhaps the reason Apple is able to sell headphones and earbuds that fall under the Beats umbrella for a premium, regardless of their performance. The &lt;a href="https://www.theverge.com/2021/6/14/22533158/beats-studio-buds-features-price"&gt;Beat Studio Buds&lt;/a&gt; are a great case in point. The fitness-focused earbuds lack wireless charging and suffer from lackluster noise cancellation, however, they typically command a higher price than some of the like-minded competition.&lt;/p&gt;
&lt;p id="YJri1v"&gt;Thankfully, they’re currently on sale at &lt;a href="https://www.amazon.com/dp/B096SV8SJG/?tag=theverge02-20" rel="sponsored nofollow noopener" target="_blank"&gt;Amazon&lt;/a&gt; and &lt;a href="http://goto.walmart.com/c/1141873/565706/9383?subId1=Verge&amp;amp;veh=aff&amp;amp;sourceid=imp_000011112222333344&amp;amp;u=https%3A%2F%2Fwww.walmart.com%2Fip%2FBeats-Studio-Buds-True-Wireless-Noise-Cancelling-Bluetooth-Earbuds-Black%2F643659699&amp;amp;partnerpropertyid=1065598" rel="sponsored nofollow noopener" target="_blank"&gt;Walmart&lt;/a&gt; for $20 off, the first discount the colorful earbuds have received since making their debut in June. Noted flaws aside, the Beats Studio Buds still tout a comfortable, stem-less design and satisfying sound for the price, along with USB-C charging and support for a handful of Android-specific features — a hallmark we hope Apple brings to more devices in the future. &lt;a href="https://www.theverge.com/22532970/beats-studio-buds-review"&gt;&lt;strong&gt;Read our review&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id="RN8hwK"&gt;&lt;div data-anthem-component="productcard:10693512"&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p id="LmGDy1"&gt;The latest iPad Air is, arguably, the best tablet for most people right now. The well-built, midrange device adopts some design elements from the &lt;a href="https://www.theverge.com/2018/11/5/18062612/apple-ipad-pro-review-2018-screen-usb-c-pencil-price-features"&gt;2018 iPad Pro&lt;/a&gt; and is compatible with many of the same devices, yet it’s not nearly as big or expensive as its larger sibling. Nonetheless, it comes with a terrific display, USB-C connectivity, and Apple’s A14 Bionic processor, a chip that promises to provide ample speed for years to come. Normally $600, &lt;a href="https://www.amazon.com/gp/product/B08J61FCVN?tag=theverge02-20" rel="sponsored nofollow noopener" target="_blank"&gt;Amazon&lt;/a&gt; and &lt;a href="https://shop-links.co/1748634717956796230#donotlink" rel="sponsored nofollow noopener" target="_blank"&gt;Best Buy&lt;/a&gt; are selling the 10.9-inch iPad Air with Wi-Fi and 64GB of storage for $500, matching the best price we’ve seen on the 2020 iPad Air this year. &lt;a href="https://www.theverge.com/21525780/apple-ipad-air-2020-review" rel="sponsored nofollow noopener" target="_blank"&gt;&lt;strong&gt;Read our review&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id="ACt7lS"&gt;&lt;div data-anthem-component="productcard:9546269"&gt;&lt;/div&gt;&lt;/div&gt;
&lt;aside id="CbVvYe"&gt;&lt;div data-anthem-component="readmore" data-anthem-component-data='{"stories":[{"title":"iOS 15 and iPadOS 15 preview: a first look at Apple’s latest software","url":"https://www.theverge.com/2021/6/30/22556236/ios-15-ipados-iphone-ipad-software-update-public-beta-preview-apple"}]}'&gt;&lt;/div&gt;&lt;/aside&gt;&lt;p id="MJQRQT"&gt;If you prefer a laptop over a tablet, Google’s Pixelbook Go is a standout that also happens to be discounted at Amazon for a limited time. While still occasionally pricier than similarly specced Chromebooks, Google’s sensible 13.3-inch laptop remains one of the &lt;a href="https://www.theverge.com/21296102/best-chromebooks" rel="sponsored nofollow noopener" target="_blank"&gt;best Chromebooks&lt;/a&gt; you can buy, with good battery life, an excellent keyboard, and an attractive, understated design that makes the most of its sturdy magnesium chassis. This particular model — which is currently &lt;a href="https://www.amazon.com/dp/B07YMM4YC1/?tag=theverge02-20" rel="sponsored nofollow noopener" target="_blank"&gt;on sale at Amazon&lt;/a&gt; and &lt;a href="https://shop-links.co/1748639044897168537#donotlink" rel="sponsored nofollow noopener" target="_blank"&gt;Best Buy for $749&lt;/a&gt; — also packs in a Core i5 processor and 8GB of RAM, while remaining plenty light at 2.3 pounds. &lt;a href="https://www.theverge.com/2019/10/25/20931476/google-pixelbook-go-review-the-price-of-simplicity" rel="sponsored nofollow noopener" target="_blank"&gt;&lt;strong&gt;Read our review&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id="x7lVfd"&gt;&lt;div data-anthem-component="productcard:9508145"&gt;&lt;/div&gt;&lt;/div&gt;
&lt;aside id="8ORaE4"&gt;&lt;div data-anthem-component="readmore" data-anthem-component-data='{"stories":[{"title":"New Chromebooks will now have Google Meet installed by default","url":"https://www.theverge.com/2021/8/3/22607620/google-meet-app-chromebooks-chrome-os-now-preinstalled"}]}'&gt;&lt;/div&gt;&lt;/aside&gt;&lt;h2 id="JWkcqL"&gt;Other deals of note&lt;/h2&gt;
&lt;ul&gt;
&lt;li id="QOYZLp"&gt;The digital, Nintendo Switch edition of &lt;a href="https://www.amazon.com/dp/B076TK4M96/?tag=theverge02-20" rel="sponsored nofollow noopener" target="_blank"&gt;&lt;em&gt;Stardew Valley &lt;/em&gt;is available for $10&lt;/a&gt;, a healthy 33 percent off its typical retail price.&lt;/li&gt;
&lt;li id="G2eP0a"&gt;Sony’s WH-1000XM3 are down to $200 at &lt;a href="https://api.narrativ.com/api/v0/client_redirect/?url=https%3A%2F%2Fbestbuy.7tiv.net%2Fc%2F376373%2F633495%2F10014%3Fprodsku%3D6280544%26u%3Dhttp%253A%252F%252Fwww.bestbuy.com%252Fsite%252F-%252F6280544.p%253Fcmp%253DRMX%26nrtv_cid%3D.nrtv_plchldr.%26subId2%3Dnymag%26subId3%3D1748627482240752605%26subId1%3Dgeneral&amp;amp;a=1743780474724360893&amp;amp;uuid=d3c8488d-ecbb-4a74-be6e-3f754f83bd98&amp;amp;uid_bam=1742974147761218429&amp;amp;ar=1748627482240752605" rel="sponsored nofollow noopener" target="_blank"&gt;Best Buy&lt;/a&gt; and &lt;a href="https://www.amazon.com/Sony-Noise-Cancelling-Headphones-WH1000XM3/dp/B07G4MNFS1?tag=theverge02-20" rel="sponsored nofollow noopener" target="_blank"&gt;Amazon&lt;/a&gt;, the second-best price we’ve seen on Sony’s last-gen, noise-canceling headphones. &lt;a href="https://www.theverge.com/2018/9/11/17844914/sony-1000x-m3-review-noise-canceling-headphones" rel="sponsored nofollow noopener" target="_blank"&gt;&lt;strong&gt;Read our review&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li id="VoPV7G"&gt;Amazon’s second-gen Echo Show 8 is $100 at &lt;a href="https://www.amazon.com/All-new-Echo-Show-8-2nd-Gen-2021-release/dp/B084DC4LW6?tag=theverge02-20" rel="sponsored nofollow noopener" target="_blank"&gt;Amazon&lt;/a&gt; and &lt;a href="https://api.narrativ.com/api/v0/client_redirect/?url=https%3A%2F%2Fbestbuy.7tiv.net%2Fc%2F376373%2F633495%2F10014%3Fprodsku%3D6461328%26u%3Dhttp%253A%252F%252Fwww.bestbuy.com%252Fsite%252F-%252F6461328.p%253Fcmp%253DRMX%26nrtv_cid%3D.nrtv_plchldr.%26subId2%3Dverge%26subId3%3D1748627482219940052%26subId1%3Dgeneral&amp;amp;a=1742759409782754841&amp;amp;uuid=d3c8488d-ecbb-4a74-be6e-3f754f83bd98&amp;amp;uid_bam=1742974147761218429&amp;amp;ar=1748627482219940052" rel="sponsored nofollow noopener" target="_blank"&gt;Best Buy&lt;/a&gt;, the best price to date on Amazon’s midsize smart display. &lt;a href="https://www.theverge.com/22521948/amazon-echo-show-8-2nd-gen-2021-review" rel="sponsored nofollow noopener" target="_blank"&gt;&lt;strong&gt;Read our review&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li id="QYGFO1"&gt;
&lt;a href="https://www.amazon.com/gp/product/B07ZPC9QD4/?tag=theverge02-20" rel="sponsored nofollow noopener" target="_blank"&gt;Apple’s AirPods Pro&lt;/a&gt; are currently $190 at Amazon, one of the better prices we’ve seen on the iconic, noise-canceling earbuds in recent months. &lt;a href="https://www.theverge.com/2019/11/1/20942472/apple-airpods-pro-review-design-price-specs-features-noise-cancellation" rel="sponsored nofollow noopener" target="_blank"&gt;&lt;strong&gt;Read our review&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li id="HDbvZE"&gt;
&lt;em&gt;The Last of Us Part II&lt;/em&gt;, Naughty Dog’s harrowing sequel to the PlayStation 2 hit, is on sale at &lt;a href="https://shop-links.co/1748629708216196460#donotlink" rel="sponsored nofollow noopener" target="_blank"&gt;Best Buy&lt;/a&gt; and &lt;a href="https://www.amazon.com/Last-Us-Part-II-PlayStation-4/dp/B07DJRFSDF?tag=theverge02-20" rel="sponsored nofollow noopener" target="_blank"&gt;Amazon&lt;/a&gt; for $20, matching its best-ever price. &lt;a href="https://www.theverge.com/21286964/the-last-of-us-part-2-review-ps4" rel="sponsored nofollow noopener" target="_blank"&gt;&lt;strong&gt;Read our review&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li id="CXo9ei"&gt;
&lt;a href="https://www.amazon.com/dp/B00N1YPXW2/?tag=theverge02-20" rel="sponsored nofollow noopener" target="_blank"&gt;Blue’s Yeti Microphone&lt;/a&gt; is $100 at Amazon, one of the steeper discounts we’ve seen this year on the popular USB mic.&lt;/li&gt;
&lt;/ul&gt;
&lt;aside id="WwpjXI"&gt;&lt;div data-anthem-component="newsletter" data-anthem-component-data='{"slug":"deals"}'&gt;&lt;/div&gt;&lt;/aside&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/good-deals/2021/8/10/22617041/apple-beats-studio-buds-2020-ipad-air-google-pixelbook-go-deal-sale"/>
    <id>https://www.theverge.com/good-deals/2021/8/10/22617041/apple-beats-studio-buds-2020-ipad-air-google-pixelbook-go-deal-sale</id>
    <author>
      <name>Brandon Widder</name>
    </author>
  </entry>
  <entry>
    <published>2021-08-10T08:02:29-04:00</published>
    <updated>2021-08-10T08:02:29-04:00</updated>
    <title>Apple’s 2021 iPhones will reportedly have a video portrait mode</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="The iPhone 12, in blue." src="https://cdn.vox-cdn.com/thumbor/RTk5pBi9ZINbC-QvxznUJ4R-7BE=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69703347/vpavic_4243_20201018_0121.0.0.jpg" /&gt;
        &lt;figcaption&gt;&lt;em&gt;Last year’s iPhone 12. &lt;/em&gt; | Photo by Vjeran Pavic / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="tdeYaK"&gt;Apple’s upcoming flagship iPhones, likely to be called the iPhone 13 line, will be able to automatically blur the backgrounds of footage as part of a new video portrait mode, according to a &lt;a href="https://www.bloomberg.com/news/articles/2021-08-10/apple-readies-new-iphones-with-pro-focused-camera-video-updates?sref=ExbtjcSG"&gt;new report from &lt;em&gt;Bloomberg&lt;/em&gt;&lt;/a&gt;. This “Cinematic Video” feature is said to be one of three major new camera features coming to this year’s iPhones. The other two are support for ProRes video recording, and new editing options for photographs. &lt;/p&gt;
&lt;p id="ZJAMmG"&gt;The improvements join another camera upgrade that’s believed to be on the way for this year’s phones. Last November, reliable Apple analyst Min-Chi Kuo said that the phones’ &lt;a href="https://www.theverge.com/2020/11/6/21553255/apple-iphone-13-camera-2021-ming-chi-kuo-analyst-prediction"&gt;ultrawide cameras would be improved&lt;/a&gt;, with a larger aperture for better low-light photography. &lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="hhgM34"&gt;&lt;q&gt;A new filters-style photo editing feature is also rumored&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="sxY4Zx"&gt;ProRes support will be especially helpful for video editors, giving them more control over footage once it’s already been shot. Its rumored arrival follows the addition of ProRAW support to Apple’s iPhone lineup last year, which allows more flexibility while editing photos. When it comes to photos, &lt;em&gt;Bloomberg&lt;/em&gt; reports that this year’s iPhones will have a new filters-style editing feature, which will let users choose a style to apply to their photos. Unlike a filter, the new feature will apply changes to specific elements within photos, rather than uniformly across the whole shot. &lt;/p&gt;
&lt;p id="smqPZZ"&gt;&lt;em&gt;Bloomberg &lt;/em&gt;re-iterates some previous reporting about this year’s devices. It says the phones could have faster refresh rates, echoing a previous rumor that this year’s Pro models could have &lt;a href="https://www.theverge.com/2021/3/1/22307312/iphone-13-smaller-notch-high-refresh-rate-screen-hole-punch-foldable-se-5g"&gt;120Hz LTPO displays&lt;/a&gt;. All iPhone 13 models could have smaller display notches, as well as the traditional boost to processing power with a new A15 chip. However, &lt;em&gt;Bloomberg&lt;/em&gt; says this year’s updates will be “modest,” and there will be the same variety of models and screen sizes as what we saw last year. &lt;/p&gt;
&lt;p id="FaqAMf"&gt;The report doesn’t indicate when the new iPhones might be announced, but Apple tends to announce its flagship phones in September each year (the exception was last year, when they were announced in October due to the pandemic). Other new Apple products thought to be on the way include new Apple Silicon-powered MacBook Pros, Apple Watches, AirPods, and iPads.&lt;/p&gt;
&lt;p id="YM6tug"&gt;&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/8/10/22618070/apple-2021-iphone-13-camera-features-video-portrait-prores-ai-filters"/>
    <id>https://www.theverge.com/2021/8/10/22618070/apple-2021-iphone-13-camera-features-video-portrait-prores-ai-filters</id>
    <author>
      <name>Jon Porter</name>
    </author>
  </entry>
  <entry>
    <published>2021-08-10T00:01:00-04:00</published>
    <updated>2021-08-10T00:01:00-04:00</updated>
    <title>The new Parallels 17 officially lets you run Windows 11 on your Mac</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/SnQXhOAzBK-ZuMh8YD-8Oh_0D7o=/206x0:3562x2237/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69702570/1_Windows_on_Macbook_Pro_Parallels_Desktop_17_for_Mac.0.png" /&gt;
    &lt;/figure&gt;

  &lt;p id="t6r4zg"&gt;Windows 11 is coming to Macs, even those without Boot Camp. &lt;a href="https://go.redirectingat.com?id=66960X1514734&amp;amp;xs=1&amp;amp;url=https%3A%2F%2Fwww.parallels.com%2Fproducts%2Fdesktop%2Fwhats-new&amp;amp;referrer=theverge.com&amp;amp;sref=https%3A%2F%2Fwww.theverge.com%2F2021%2F8%2F10%2F22617544%2Fparallels-17-mac-windows-11-preview-emulation-performance-upgrades" rel="sponsored nofollow noopener" target="_blank"&gt;Parallels Desktop 17&lt;/a&gt; will allow Mac users to experience &lt;a href="https://www.theverge.com/2021/6/24/22546791/microsoft-windows-11-announcement-features-updates"&gt;Microsoft’s next version of Windows&lt;/a&gt; in a window on their Mac desktop. Parallels supports both Intel and M1 Macs (though there’s a catch for those running Arm-based machines), and can even be used to run &lt;a href="https://www.theverge.com/2021/7/29/22600261/windows-11-beta-release-insider-microsoft-download-test"&gt;the Windows 11 preview&lt;/a&gt; for those who can’t wait.&lt;/p&gt;
&lt;p id="vJA6go"&gt;The catch for M1 users is &lt;a href="https://www.theverge.com/22383598/parallels-desktop-mac-windows-10-install-m1-macbook"&gt;the same as when Parallels first added support&lt;/a&gt; for Apple’s latest machines — you’ll only be able to emulate Arm-based operating systems, which means you’ll be limited to Windows on Arm. While &lt;a href="https://www.youtube.com/watch?v=7BE-Ca7OX5o"&gt;it does seem possible&lt;/a&gt; to install a Windows 11 preview for Arm machines, you’ll probably want to proceed with caution. Windows on Arm’s x86 emulation has been a bit of a rocky road, and the x64 app emulation &lt;a href="https://www.theverge.com/2020/12/10/22168542/x64-emulation-windows-on-arm-surface-pro-x"&gt;is still a work in progress.&lt;/a&gt; Basically, if you’re looking to run a virtualized version of Windows on your M1, you’ll still have to deal with the same caveats that would come with running Windows on any other Arm machines.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="3UHJhK"&gt;&lt;q&gt;Windows on M1 will likely have the same caveats as Windows on any other Arm machines&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="Im90r8"&gt;While M1 users have to deal with Windows on Arm, they also get some performance improvements if they’re coming from Parallels 16: Parallels says that the new version will let M1 Macs get up to 28 percent better DirectX 11 performance, and up to 33 percent faster start times for Windows 10 on Arm Insider Preview VMs. This comes alongside the up to 25 percent faster 2D graphics and up to 6 times faster OpenGL performance that Parallels says will be coming to Windows VMs on all supported Macs, Intel and M1 alike. M1 users will also be able to use BitLocker and Secure Boot thanks to a virtualized TPM.&lt;/p&gt;
&lt;p id="FkEY3O"&gt;There are other under-the-hood improvements with Parallels 17 (for example, it’s now a universal app, which should make IT departments’ lives easier), and it’s also getting support for &lt;a href="https://www.theverge.com/2021/6/7/22458628/apple-macos-12-monterey-features-updates-wwdc-2021"&gt;macOS Monterey&lt;/a&gt; — the virtualization software will be able to run on macOS 12 computers, as well as create virtual ones. &lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="1OSvCU"&gt;&lt;q&gt;If you’re someone who keeps a foot in both the Windows and Mac worlds, Parallels is worth a look&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
  &lt;figure class="e-image"&gt;
        &lt;img alt="A chart describing pricing and availability. Parallels Desktop 17 - $79.99 per year. Parallels Desktop 17 Perpetual license - $99.99. Parallels Desktop Pro or Business Edition: $99.99 per year. Upgrade from any previous version of Parallels Desktop to Desktop 17: $49.99. Upgrade from any previous version to Parallels Desktop Pro Edition: $49.99 per year." data-mask-text="false" src="https://cdn.vox-cdn.com/thumbor/F_NgUH730q0kbKcmh3nBILHUVeo=/400x0/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/22772698/Pricing_and_Availability_Parallels_Desktop_17.png"&gt;
      &lt;cite&gt;Image: Parallels&lt;/cite&gt;
      &lt;figcaption&gt;&lt;em&gt;Parallels has kept the pricing the same this year.&lt;/em&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;p id="NGmNGV"&gt;If you want the regular version of Parallels Desktop 17, you have the choice of getting a subscription for $79.99 a year, or a perpetual license for $99.99. If you had a perpetual license for a previous version of Parallels, you can upgrade to 17 for $49.99. There’s also Pro and Business editions that cost $99.99 a year. Parallels sells the software on its website, but before you plunk down any cash, it may be worth waiting until Windows 11 launches (&lt;a href="https://www.theverge.com/2021/6/28/22553666/microsoft-windows-11-october-20th-release-date-hint-rumor"&gt;potentially in October&lt;/a&gt;) to see how well it fares on Parallels — or if Windows 11 is even worth jumping to in the first place.&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/8/10/22617544/parallels-17-mac-windows-11-preview-emulation-performance-upgrades"/>
    <id>https://www.theverge.com/2021/8/10/22617544/parallels-17-mac-windows-11-preview-emulation-performance-upgrades</id>
    <author>
      <name>Mitchell Clark</name>
    </author>
  </entry>
  <entry>
    <published>2021-08-09T17:09:00-04:00</published>
    <updated>2021-08-09T17:09:00-04:00</updated>
    <title>Apple keeps shutting down employee-run surveys on pay equity — and labor lawyers say it’s illegal</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/ePHQrieoO2mNGruhek063Lp3-RU=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69701173/acastro_210809_4703_0001.0.jpg" /&gt;
        &lt;figcaption&gt;Illustration by Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;


  &lt;p&gt;The company bans surveys that include diversity data&lt;/p&gt; &lt;p class="p--has-dropcap p-large-text" id="auIHVh"&gt;Apple &lt;a href="https://www.apple.com/diversity/"&gt;insists&lt;/a&gt; it does not have a problem with pay inequality. Skeptical Apple employees have been trying to verify that claim by sending out informal surveys on how much people make, particularly as it relates to women and underrepresented minorities. But the company has shut down three of those surveys, citing stringent rules on how employees can collect data. Now, multiple labor lawyers tell &lt;em&gt;The Verge&lt;/em&gt; the company may be violating worker protections: the surveys can be considered a form of labor organizing — &lt;a href="https://www.nlrb.gov/about-nlrb/rights-we-protect/the-law/employees/concerted-activity"&gt;under US law&lt;/a&gt;, employees have the right to discuss pay. &lt;/p&gt;
&lt;p id="W7CVA1"&gt;“Apple cannot bar its employees from discussing pay equity as it relates to protected classes,” says Vincent P. White, a labor lawyer with White, Hilferty &amp;amp; Albanese. “If they were, they could tell people not to talk about pronouns. The logical outgrowth of that doesn’t even track. I view their effort to shut this down as an act of retaliation.”&lt;/p&gt;
&lt;p id="rsN1Lf"&gt;The first known survey began in the spring and asked people to volunteer salary information in addition to how they identify in terms of race, ethnicity, gender, and disability. After about 100 responses, Apple’s people team — the company’s name for what is commonly called human resources — asked employees to take the survey down, saying the demographic questions constituted personally identifying information, or PII.&lt;/p&gt;
&lt;p id="cQiVKd"&gt;Last week, employees tried to start another pay equity survey but were again told to take it down because it included a question on gender. When they created a new survey without the gender question, the Apple people team allegedly said it had to be shut down because it was hosted on the company’s corporate Box account.&lt;/p&gt;
&lt;p id="At1WUd"&gt;“This is like a 2021 version of a foreman on the docks telling people they can’t compare their wages way back in the 1800s,” says White. “This isn’t new. It’s just the newest version of ‘you can’t talk about your pay.’” &lt;/p&gt;
&lt;p id="bIIAhw"&gt;The people team also sent employees the following information on “prohibited surveys”:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p id="OXjvmq"&gt;&lt;strong&gt;Prohibited Surveys&lt;/strong&gt;&lt;br&gt;The following employee surveys are prohibited in all cases and may not be conducted.&lt;/p&gt;
&lt;p id="Oa604m"&gt;&lt;strong&gt;Surveys as Data Collection&lt;/strong&gt;&lt;/p&gt;
&lt;p id="JtT1LC"&gt;Surveys are not permitted to be used as a means of collecting identifiable employee data without following the usual process to obtain this data from the People team. This includes any questions about an employee’s address, demographics, and so on, except for collecting country or region, which is permitted.&lt;/p&gt;
&lt;p id="RQfYC6"&gt;Using surveys as a tool to collect health information — including but not limited to health reports, testing results, and vaccination status — is also prohibited.&lt;/p&gt;
&lt;p id="wQWnQK"&gt;All requests for identifiable employee data must be submitted to the People team via the People Report Request Form. If approved, the People team will provide the employee data directly from their systems.&lt;/p&gt;
&lt;p id="GcK28s"&gt;&lt;strong&gt;Surveys Requesting Diversity Data&lt;/strong&gt;&lt;br&gt;Diversity data is highly sensitive personal data. If you have a need for such information, you must work with your I&amp;amp;D Business Partner and the I&amp;amp;D Insights and Solutions team before collecting any data. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p id="jwlBUn"&gt;“Those rules may themselves violate the protected right to concerted activity — while [Apple] might point to these handbook type rules that you’ve agreed to not do this as a condition of employment, that doesn’t mean they can legally prevent employees from doing what they’re doing,” says Veena Dubal, a law professor at UC Hastings.&lt;/p&gt;
&lt;p id="mhGsbO"&gt;Now, Apple engineer Cher Scarlett has started a new survey on Typeform, which she is paying for out of pocket. “I was looking at levels.fyi (a website that lets people compare salary data across companies) and noticed a few very low salaries in a certain geographic area that were 10 to 15 percent lower compared to other people on the team,” Scarlett says. “Every time I looked at gender, they were women. I’m not going to say that’s a definitive issue, but it’s a prompt for anyone to ask if this is a widespread problem. We should be able to easily find out whether or not that’s the case so we can know whether people are truly being paid fairly.”&lt;/p&gt;
&lt;div id="Y2ANx0"&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p lang="en" dir="ltr"&gt;Two pay transparency surveys have been shut down in the past 6 months at Apple. I won't be intimidated. We have the right to collect this data amongst ourselves.&lt;br&gt;&lt;br&gt;There's a new survey, voluntary and totally anonymous. &lt;br&gt;&lt;br&gt;The password is my status in Slack.&lt;a href="https://t.co/fUr1DZ5Df1"&gt;https://t.co/fUr1DZ5Df1&lt;/a&gt;&lt;/p&gt;— Cher Scarlett (@cherthedev) &lt;a href="https://twitter.com/cherthedev/status/1423918155185811460?ref_src=twsrc%5Etfw"&gt;August 7, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;p id="9N3Xhw"&gt;The new survey, which has nearly 500 entries, asks employees to volunteer information on their salary, level, team, latest RSU grant, tenure at Apple, geographic location, signing bonus, relevant work experience, as well as whether they’re permanently remote, and a member of an underrepresented race or gender. &lt;/p&gt;
&lt;p id="o4b7VL"&gt;“We want our colleagues and industry peers to have the knowledge of Apple’s pay bands and to give minoritized employees and prospective employees the confidence to negotiate fair wages and bonuses,” the survey reads.&lt;/p&gt;
&lt;p id="GbIp9s"&gt;Grace Reckers, an organizer with the Office and Professional Employees International Union, says the fact that this information is all voluntary should protect employees. “This is protected activity — because you’re opting into the survey, I don’t even know how the PII excuse would work or matter.”&lt;/p&gt;
&lt;p id="YMUxCt"&gt;Scarlett says Apple’s response to the surveys has only made employees more suspicious: “I don’t think anyone is going into this saying there for sure is a wage gap, whether that’s gender or race or disability. But it is concerning to everyone that every single time someone tries to create more transparency, Apple shuts it down. It makes it feel like maybe there is a problem, and they’re already aware of it.”&lt;/p&gt;
&lt;p id="Nlg7KS"&gt;In 2018, Apple’s mean and median gender pay gap for employees in the UK was &lt;a href="https://www.apple.com/legal/more-resources/docs/uk-gender-pay-gap-report-2019.pdf"&gt;12 percent in favor of men&lt;/a&gt;. That’s 5 percentage points below the overall gender pay gap in the UK. The company has to publish this data under UK law, but it does not have the same requirements in the US. &lt;/p&gt;
&lt;p id="y32n17"&gt;Two years earlier, &lt;a href="https://www.nytimes.com/2016/02/27/technology/apple-shareholders-show-their-support-for-tim-cook.html"&gt;Apple CEO Tim Cook told investors&lt;/a&gt; that women at Apple made 99.6 cents for every dollar men made, while underrepresented minorities made 99.7 cents for every dollar white employees made. That same year, the company released a diversity report &lt;a href="https://www.theverge.com/2016/8/3/12371204/apple-diversity-report-pay-gap-hiring-equal-pay"&gt;saying it had fixed the problem&lt;/a&gt;. &lt;/p&gt;
&lt;p class="c-end-para" id="tUEk5i"&gt;Apple did not respond to a request for comment from &lt;em&gt;The Verge&lt;/em&gt;. &lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/8/9/22609687/apple-pay-equity-employee-surveys-protected-activity"/>
    <id>https://www.theverge.com/2021/8/9/22609687/apple-pay-equity-employee-surveys-protected-activity</id>
    <author>
      <name>Zoe Schiffer</name>
    </author>
  </entry>
  <entry>
    <published>2021-08-09T06:49:08-04:00</published>
    <updated>2021-08-09T06:49:08-04:00</updated>
    <title>Apple pushes back against child abuse scanning concerns in new FAQ </title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/3UQv-Ep-FwzGk_VnzIk8s6KW_1M=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69698169/acastro_180604_1777_apple_wwdc_0002.0.jpg" /&gt;
        &lt;figcaption&gt;Illustration by Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="ZGzBGm"&gt;In a &lt;a href="https://www.apple.com/child-safety/pdf/Expanded_Protections_for_Children_Frequently_Asked_Questions.pdf"&gt;new FAQ&lt;/a&gt;, Apple has attempted to assuage concerns that its new &lt;a href="https://www.theverge.com/2021/8/5/22611721/apple-csam-child-abuse-scanning-hash-system-ncmec"&gt;anti-child abuse measures&lt;/a&gt; could be turned into surveillance tools by authoritarian governments. “Let us be clear, this technology is limited to detecting CSAM [child sexual abuse material] stored in iCloud and we will not accede to any government’s request to expand it,” the company writes. &lt;/p&gt;
&lt;p id="XeChzT"&gt;Apple’s new tools, &lt;a href="https://www.theverge.com/2021/8/5/22611721/apple-csam-child-abuse-scanning-hash-system-ncmec"&gt;announced last Thursday&lt;/a&gt;, include two features designed to protect children. One, called “communication safety,” uses on-device machine learning to identify and blur sexually explicit images received by children in the Messages app, and can notify a parent if a child age 12 and younger decides to view or send such an image. The second is designed to detect known CSAM by scanning users’ images if they choose to upload them to iCloud. Apple is notified if CSAM is detected, and it will alert the authorities when it verifies such material exists.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="G0Hzjw"&gt;&lt;q&gt;“Apple’s CSAM detection capability is built solely to detect known CSAM images”&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="QqqbpC"&gt;The plans met with a swift backlash from digital &lt;a href="https://www.theverge.com/2021/8/6/22613365/apple-icloud-csam-scanning-whatsapp-surveillance-reactions"&gt;privacy groups and campaigners&lt;/a&gt;, who argued that these introduce a backdoor into Apple’s software. These groups note that once such a backdoor exists there is always the potential for it to be expanded to scan for types of content that go beyond child sexual abuse material. Authoritarian governments could use it to scan for politically dissent material, or anti-LGBT regimes could use it to crack down on sexual expression.&lt;/p&gt;
&lt;p id="1A2zHQ"&gt;“Even a thoroughly documented, carefully thought-out, and narrowly-scoped backdoor is still a backdoor,” the &lt;a href="https://www.eff.org/deeplinks/2021/08/apples-plan-think-different-about-encryption-opens-backdoor-your-private-life"&gt;Electronic Frontier Foundation wrote&lt;/a&gt;. “We’ve already seen this mission creep in action. One of the technologies originally built to scan and hash child sexual abuse imagery has been repurposed to create a database of ‘terrorist’ content that companies can contribute to and access for the purpose of banning such content.”&lt;/p&gt;
&lt;p id="ArY2kw"&gt;However, Apple argues that it has safeguards in place to stop its systems from being used to detect anything other than sexual abuse imagery. It says that its list of banned images is provided by the National Center for Missing and Exploited Children (NCMEC) and other child safety organizations, and that the system “only works with CSAM image hashes provided by NCMEC and other child safety organizations.” Apple says it won’t add to this list of image hashes, and that the list is the same across all iPhones and iPads to prevent individual targeting of users.&lt;/p&gt;
&lt;p id="e5ce9I"&gt;The company also says that it will refuse demands from governments to add non-CSAM images to the list. “We have faced demands to build and deploy government-mandated changes that degrade the privacy of users before, and have steadfastly refused those demands. We will continue to refuse them in the future,” it says. &lt;/p&gt;
&lt;p id="KkMQzE"&gt;It’s worth noting that despite Apple’s assurances, the company has made concessions to governments in the past in order to continue operating in their countries. It sells &lt;a href="https://support.apple.com/en-us/HT204170#:~:text=FaceTime%20isn%27t%20available%20or,iPod%20touch%20in%20Saudi%20Arabia.&amp;amp;text=Search%20for%20the%20FaceTime%20app%20in%20Spotlight%20or%20using%20Siri."&gt;iPhones without FaceTime&lt;/a&gt; in countries that don’t allow encrypted phone calls, and in China it’s removed &lt;a href="https://www.theverge.com/2020/8/18/21374246/apple-china-chinese-operations-restrictions"&gt;thousands of apps from its App Store&lt;/a&gt;, as well as moved to &lt;a href="https://www.theverge.com/2018/7/18/17587304/apple-icloud-china-user-data-state-run-telecom-privacy-security"&gt;store user data on the servers of a state-run telecom&lt;/a&gt;. &lt;/p&gt;
&lt;p id="c1f0jU"&gt;The FAQ also fails to address some concerns about the feature that scans Messages for sexually explicit material. The feature does not share any information with Apple or law enforcement, the company says, but it doesn’t say how it’s ensuring that the tool’s focus remains solely on sexually explicit images.&lt;/p&gt;
&lt;p id="RXy2yk"&gt;“All it would take to widen the narrow backdoor that Apple is building is an expansion of the machine learning parameters to look for additional types of content, or a tweak of the configuration flags to scan, not just children’s, but anyone’s accounts,” wrote the EFF. The EFF also notes that machine-learning technologies frequently classify this content incorrectly, and cites &lt;a href="https://www.eff.org/tossedout/tumblr-ban-adult-content"&gt;Tumblr’s attempts to crack down on sexual content&lt;/a&gt; as a prominent example of where the technology has gone wrong.&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/8/9/22616381/apple-child-sexual-abuse-material-scanning-icloud-faq-pushback-privacy"/>
    <id>https://www.theverge.com/2021/8/9/22616381/apple-child-sexual-abuse-material-scanning-icloud-faq-pushback-privacy</id>
    <author>
      <name>Jon Porter</name>
    </author>
  </entry>
  <entry>
    <published>2021-08-07T15:55:22-04:00</published>
    <updated>2021-08-07T15:55:22-04:00</updated>
    <title>WhatsApp lead and other tech experts fire back at Apple’s Child Safety plan</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/TGtA_nWVkDJWCy43HTsdInDqf6Q=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69691082/acastro_170731_1777_0006_v4.0.jpg" /&gt;
        &lt;figcaption&gt;Illustration by Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="ai81Iq"&gt;The chorus of voices expressing concern and dismay over &lt;a href="https://www.apple.com/child-safety/"&gt;Apple’s new Child Safety measures&lt;/a&gt; grew louder over the weekend, as an open letter with more than 4,000 signatures made the rounds online. &lt;a href="https://appleprivacyletter.com/"&gt;The Apple Privacy Letter&lt;/a&gt; asked the iPhone maker to “reconsider its technology rollout,” lest it undo “decades of work by technologists, academics and policy advocates” on privacy-preserving measures.&lt;/p&gt;
&lt;p id="UPWw1g"&gt;Apple’s plan, &lt;a href="https://www.theverge.com/2021/8/5/22611721/apple-csam-child-abuse-scanning-hash-system-ncmec"&gt;which it announced on Thursday&lt;/a&gt;, involves taking hashes of images uploaded to iCloud and comparing them to a database that contains hashes of known CSAM images. According to Apple, this allows it to keep user data encrypted and &lt;a href="https://twitter.com/reneritchie/status/1423726172849033216?s=20"&gt;run the analysis on-device&lt;/a&gt; while still allowing it to report users to the authorities if they’re found to be sharing child abuse imagery. Another prong of Apple’s Child Safety strategy involves optionally warning parents if their child &lt;a href="https://twitter.com/rsgnl/status/1423389211542032384"&gt;under 13 years old&lt;/a&gt; sends or views photos containing sexually explicit content. &lt;a href="https://www.theverge.com/2021/8/6/22612934/apple-vp-memo-concerns-privacy-new-scanning-photos-images-ios"&gt;An internal memo at Apple&lt;/a&gt; acknowledged that people would be “worried about the implications” of the systems.&lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="1dseaO"&gt;&lt;q&gt;Cathcart calls Apple’s approach “very concerning,” and he’s not alone&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="yIGcHp"&gt;&lt;a href="https://twitter.com/wcathcart/status/1423701473624395784?s=20"&gt;WhatsApp’s head Will Cathcart&lt;/a&gt; said in a Twitter thread that his company wouldn’t be adopting the safety measures, &lt;a href="https://twitter.com/wcathcart/status/1423701476841385988?s=20"&gt;calling Apple’s approach&lt;/a&gt; “very concerning.” Cathcart said WhatsApp’s system to fight child exploitation, which partly utilizes user reports, preserves encryption like Apple’s and has led to the company &lt;a href="https://twitter.com/wcathcart/status/1423701475595755524?s=20"&gt;reporting over 400,000 cases&lt;/a&gt; to the National Center for Missing and Exploited Children in 2020. (Apple is also working with the Center for its CSAM detection efforts.)&lt;/p&gt;
&lt;p id="iuhKXJ"&gt;WhatsApp’s owner, Facebook, has reasons to pounce on Apple for privacy concerns. Apple’s &lt;a href="https://www.theverge.com/2021/4/27/22405474/apple-app-tracking-transparency-ios-14-5-privacy-update-facebook-data"&gt;changes to how ad tracking works in iOS 14.5&lt;/a&gt; started a fight between the two companies, with Facebook &lt;a href="https://www.theverge.com/2020/12/16/22178068/facebook-apple-newspaper-ads-ios-privacy-changes"&gt;buying newspaper ads&lt;/a&gt; criticizing Apple’s privacy changes as harmful to small businesses. &lt;a href="https://www.theverge.com/2020/12/16/22179721/apple-defends-upcoming-privacy-changes-standing-up-for-users-facebook-data"&gt;Apple fired back&lt;/a&gt;, saying that the change “simply requires” that users be given a choice on whether to be tracked.&lt;/p&gt;
&lt;p id="7HaVEp"&gt;The list of people and organizations raising concerns about Apple’s policy includes Edward Snowden, the Electronic Frontier Foundation, professors, and more. We’ve collected some of those reactions here to act as an overview of some of the criticisms levied against Apple’s new policy.&lt;/p&gt;
&lt;hr class="p-entry-hr" id="TJSuYM"&gt;
&lt;p id="ZZcaWR"&gt;Matthew Green, an associate professor at Johns Hopkins University, pushed back on the feature before it was publicly announced. He tweeted about Apple’s plans and about how the hashing system could be abused by governments and malicious actors.&lt;/p&gt;
&lt;div id="My9zJN"&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p lang="en" dir="ltr"&gt;These tools will allow Apple to scan your iPhone photos for photos that match a specific perceptual hash, and report them to Apple servers if too many appear.&lt;/p&gt;— Matthew Green (@matthew_d_green) &lt;a href="https://twitter.com/matthew_d_green/status/1423072476888805376?ref_src=twsrc%5Etfw"&gt;August 5, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;p id="AKpR2F"&gt;The EFF &lt;a href="https://www.eff.org/deeplinks/2021/08/apples-plan-think-different-about-encryption-opens-backdoor-your-private-life"&gt;released a statement&lt;/a&gt; that blasted Apple’s plan, more or less calling it a “thoroughly documented, carefully thought-out, and narrowly-scoped backdoor.” The EFF’s press release goes into detail on how it believes Apple’s Child Safety measures could be abused by governments and how they decrease user privacy.&lt;/p&gt;
&lt;div id="jacoNW"&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p lang="en" dir="ltr"&gt;Apple's filtering of iMessage and iCloud is not a slippery slope to backdoors that suppress speech and make our communications less secure. We’re already there: this is a fully-built system just waiting for external pressure to make the slightest change. &lt;a href="https://t.co/f2nv062t2n"&gt;https://t.co/f2nv062t2n&lt;/a&gt;&lt;/p&gt;— EFF (@EFF) &lt;a href="https://twitter.com/EFF/status/1423375818693038084?ref_src=twsrc%5Etfw"&gt;August 5, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;p id="Yuo1qD"&gt;Kendra Albert, an instructor at Harvard’s Cyberlaw Clinic, has a thread on the potential dangers to queer children and Apple’s initial lack of clarity around age ranges for the parental notifications feature.&lt;/p&gt;
&lt;div id="6bBV1n"&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p lang="en" dir="ltr"&gt;The idea that parents are safe people for teens to have conversations about sex or sexting with is admirable, but in many cases, not true. (And as far as I can tell, this stuff doesn't just apply to kids under the age for 13.)&lt;/p&gt;— Kendra Albert (@KendraSerra) &lt;a href="https://twitter.com/KendraSerra/status/1423367106972852228?ref_src=twsrc%5Etfw"&gt;August 5, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id="p5xtGT"&gt;
&lt;blockquote class="twitter-tweet" data-conversation="none"&gt;
&lt;p lang="en" dir="ltr"&gt;EFF reports that the iMessage nudity notifications will not go to parents if the kid is between 13-17 but that is not anywhere in the Apple documentation that I can find. &lt;a href="https://t.co/Ma1BdyqZfW"&gt;https://t.co/Ma1BdyqZfW&lt;/a&gt;&lt;/p&gt;— Kendra Albert (@KendraSerra) &lt;a href="https://twitter.com/KendraSerra/status/1423436593696854018?ref_src=twsrc%5Etfw"&gt;August 6, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;p id="CEqw4k"&gt;Edward Snowden retweeted the &lt;em&gt;Financial Time&lt;/em&gt;s article about the system, giving his own characterization of what Apple is doing.&lt;/p&gt;
&lt;div id="CCaMt8"&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p lang="en" dir="ltr"&gt;Apple plans to modify iPhones to constantly scan for contraband: &lt;br&gt;&lt;br&gt;“It is an absolutely appalling idea, because it is going to lead to distributed bulk surveillance of our phones and laptops,” said Ross Anderson, professor of security engineering. &lt;a href="https://t.co/rS92HR3pUZ"&gt;https://t.co/rS92HR3pUZ&lt;/a&gt;&lt;/p&gt;— Edward Snowden (@Snowden) &lt;a href="https://twitter.com/Snowden/status/1423387232963022848?ref_src=twsrc%5Etfw"&gt;August 5, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;p id="sceSPL"&gt;Politician Brianna Wu called the system “the worst idea in Apple History.”&lt;/p&gt;
&lt;div id="wB779u"&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p lang="en" dir="ltr"&gt;This is the worst idea in Apple history, and I don't say that lightly.&lt;br&gt;&lt;br&gt;It destroys their credibility on privacy. It will be abused by governments. It will get gay children killed and disowned. This is the worst idea ever. &lt;a href="https://t.co/M2EIn2jUK2"&gt;https://t.co/M2EIn2jUK2&lt;/a&gt;&lt;/p&gt;— Brianna Wu (@BriannaWu) &lt;a href="https://twitter.com/BriannaWu/status/1423384759858774026?ref_src=twsrc%5Etfw"&gt;August 5, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id="H0QULK"&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p lang="en" dir="ltr"&gt;Just to state: Apple's scanning does not detect photos of child abuse. It detects a list of known banned images added to a database, which are initially child abuse imagery found circulating elsewhere. What images are added over time is arbitrary. It doesn't know what a child is.&lt;/p&gt;— SoS (@SwiftOnSecurity) &lt;a href="https://twitter.com/SwiftOnSecurity/status/1423383256003747840?ref_src=twsrc%5Etfw"&gt;August 5, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;p id="r1g6Co"&gt;Writer Matt Blaze also tweeted about the concerns that the technology could be abused by overreaching governments, trying to prevent content other than CSAM.&lt;/p&gt;
&lt;div id="PouZDU"&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p lang="en" dir="ltr"&gt;In other words, not only does the policy have to be exceptionally robust, so does the implementation.&lt;/p&gt;— matt blaze (@mattblaze) &lt;a href="https://twitter.com/mattblaze/status/1423476875817635840?ref_src=twsrc%5Etfw"&gt;August 6, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;p id="rvN3WU"&gt;Epic CEO Tim Sweeney also criticized Apple, saying that the company “vacuums up everybody’s data into iCloud by default.” He also promised to share more thoughts specifically about Apple’s Child Safety system.&lt;/p&gt;
&lt;div id="wfglq3"&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p lang="en" dir="ltr"&gt;It’s atrocious how Apple vacuums up everybody’s data into iCloud by default, hides the 15+ separate options to turn parts of it off in Settings underneath your name, and forces you to have an unwanted email account. Apple would NEVER allow a third party to ship an app like this.&lt;/p&gt;— Tim Sweeney (@TimSweeneyEpic) &lt;a href="https://twitter.com/TimSweeneyEpic/status/1423728945225211908?ref_src=twsrc%5Etfw"&gt;August 6, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id="kuuOuE"&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p lang="en" dir="ltr"&gt;I will share some very detailed thoughts on this related topic later.&lt;/p&gt;— Tim Sweeney (@TimSweeneyEpic) &lt;a href="https://twitter.com/TimSweeneyEpic/status/1423730378234376206?ref_src=twsrc%5Etfw"&gt;August 6, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;p id="7YCGmI"&gt;Not every reaction has been critical, however. Ashton Kutcher (who has done &lt;a href="https://thecnnfreedomproject.blogs.cnn.com/2011/04/14/moore-kutcher-join-our-crusade-to-end-child-sex-trafficking/"&gt;advocacy work to end child sex trafficking since 2011&lt;/a&gt;) calls Apple’s work “a major step forward” for efforts to eliminate CSAM.&lt;/p&gt;
&lt;div id="SFDxLg"&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p lang="en" dir="ltr"&gt;I believe in privacy - including for kids whose sexual abuse is documented and spread online without consent. These efforts announced by &lt;a href="https://twitter.com/Apple?ref_src=twsrc%5Etfw"&gt;@Apple&lt;/a&gt; are a major step forward in the fight to eliminate CSAM from the internet. &lt;a href="https://t.co/TQIxHlu4EX"&gt;https://t.co/TQIxHlu4EX&lt;/a&gt;&lt;/p&gt;— ashton kutcher (@aplusk) &lt;a href="https://twitter.com/aplusk/status/1423387418451922950?ref_src=twsrc%5Etfw"&gt;August 5, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/8/6/22613365/apple-icloud-csam-scanning-whatsapp-surveillance-reactions"/>
    <id>https://www.theverge.com/2021/8/6/22613365/apple-icloud-csam-scanning-whatsapp-surveillance-reactions</id>
    <author>
      <name>Mitchell Clark</name>
    </author>
  </entry>
  <entry>
    <published>2021-08-06T18:47:42-04:00</published>
    <updated>2021-08-06T18:47:42-04:00</updated>
    <title>Spotify says it plans to add AirPlay 2 to its iOS app — eventually</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/DDD20yR_WaPMtNUO2rsOrGSinnc=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69690480/acastro_180213_1777_0004.0.jpg" /&gt;
        &lt;figcaption&gt;Illustration by Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="qolgag"&gt;Spotify still hasn’t added AirPlay 2 support to its iOS app — but despite the delay, it’s still “working to make [it] a reality,” the company tells &lt;em&gt;The Verge&lt;/em&gt;. Some doubt was cast on AirPlay 2 inclusion when &lt;a href="https://www.macrumors.com/2021/08/06/spotify-pauses-airplay-2-support-for-ios-app/"&gt;&lt;em&gt;MacRumors&lt;/em&gt; spotted a forum post&lt;/a&gt; where a Spotify forum moderator claimed that &lt;a href="https://community.spotify.com/t5/Closed-Ideas/iOS-Airplay-2-Support-for-iOS/idc-p/5244350/highlight/true#M235171"&gt;“audio driver compatibility issues”&lt;/a&gt; might mean the feature wouldn’t be added for the foreseeable future. Spotify now claims that’s wrong.&lt;/p&gt;
&lt;p id="lIlb4R"&gt;&lt;em&gt;The Verge&lt;/em&gt; received the following statement from Spotify regarding AirPlay 2:&lt;/p&gt;
&lt;blockquote&gt;&lt;p id="l6Ose4"&gt;A post on one of Spotify’s Community pages contained incomplete information regarding our plans for AirPlay2. Spotify will support AirPlay2 and we’re working to make that a reality.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p id="5hM8lP"&gt;AirPlay 2, &lt;a href="https://www.theverge.com/2018/5/29/17403684/airplay-2-ios-114-stereo-homepod-available-now"&gt;added as part of iOS 11 update&lt;/a&gt;, introduced multiroom audio, Siri voice control, and fairly broad support across a wide swath of speakers, &lt;a href="https://www.theverge.com/2019/1/8/18173637/tv-airplay-2-apple-lg-samsung-sony-vizio-ces-2019"&gt;televisions&lt;/a&gt;, and streaming services. It was a real first for Apple’s “casting” feature, which had previously been somewhat poorly supported outside of Apple’s own devices. &lt;/p&gt;
&lt;div class="c-float-right"&gt;&lt;aside id="BIY2Ct"&gt;&lt;q&gt;I’m not going to call it petty, but...&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id="w59IH1"&gt;Spotify has its own way to get audio from its service to other devices in the form of &lt;a href="https://www.spotify.com/us/connect/"&gt;Spotify Connect&lt;/a&gt;, but considering Spotify already supports Google Cast, skipping Apple’s newest streaming protocol would seem like an odd omission. As &lt;em&gt;MacRumors&lt;/em&gt; notes, Apple provides a seemingly simple &lt;a href="https://developer.apple.com/documentation/avfoundation/media_playback_and_selection/getting_airplay_2_into_your_app"&gt;four step developer document&lt;/a&gt; explaining how to enable the feature. However, &lt;a href="https://twitter.com/marcoarment/status/1423744958541058052?s=20"&gt;developer Marco Arment points&lt;/a&gt; out that the fourth step (adopting a new API that supports enhanced buffering) is a bigger hurdle than it appears. &lt;/p&gt;
&lt;div id="BhtYf6"&gt;
&lt;blockquote class="twitter-tweet" data-conversation="none"&gt;
&lt;p lang="en" dir="ltr"&gt;…and that new API:&lt;br&gt;&lt;br&gt;- is barely documented&lt;br&gt;&lt;br&gt;- has no public sample code&lt;br&gt;&lt;br&gt;- is full of major gotchas&lt;br&gt;&lt;br&gt;- can’t change speeds seamlessly&lt;br&gt;&lt;br&gt;- doesn’t provide precise timing&lt;br&gt;&lt;br&gt;- requires much more complex logic&lt;br&gt;&lt;br&gt;- is less efficient, which can cause background CPU-overage terminations&lt;/p&gt;— Marco Arment (@marcoarment) &lt;a href="https://twitter.com/marcoarment/status/1423744958541058052?ref_src=twsrc%5Etfw"&gt;August 6, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;p id="d6ALoR"&gt;But add in that Spotify has a less-than-friendly relationship with Apple, even going as far as &lt;a href="https://www.theverge.com/2019/3/13/18263453/spotify-apple-app-store-antitrust-complaint-ec-30-percent-cut-unfair"&gt;filing an antitrust complaint&lt;/a&gt; and &lt;a href="https://www.theverge.com/22457400/spotify-horacio-gutierrez-apple-app-store-interview-decoder"&gt;publicly calling it a bully&lt;/a&gt;, and a choice to not prioritize incorporating AirPlay 2 makes even more sense. It’s good that it’s still happening, but for any Spotify subscriber on iOS, it’s hard to not feel like you’re caught in the crossfire between two tech giants on the outs. &lt;/p&gt;
&lt;p id="XlsirN"&gt;&lt;em&gt;&lt;strong&gt;Update August 6th, 6:47PM ET:&lt;/strong&gt;&lt;/em&gt;&lt;strong&gt; &lt;/strong&gt;&lt;em&gt;Changed headline and added statement from Spotify confirming it is working on AirPlay2 support.&lt;/em&gt;&lt;/p&gt;
&lt;p id="2tmCRK"&gt;&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/8/6/22613420/spotify-airplay2-support-audio-issues-drivers"/>
    <id>https://www.theverge.com/2021/8/6/22613420/spotify-airplay2-support-audio-issues-drivers</id>
    <author>
      <name>Ian Carlos Campbell</name>
    </author>
  </entry>
  <entry>
    <published>2021-08-06T12:01:35-04:00</published>
    <updated>2021-08-06T12:01:35-04:00</updated>
    <title>Apple VP acknowledges concerns about new scanning feature in internal memo</title>
    <content type="html">  

    &lt;figure&gt;
      &lt;img alt="" src="https://cdn.vox-cdn.com/thumbor/VSGhQl7q43Qpftf2KZ6WSgH_o0I=/0x0:2040x1360/1310x873/cdn.vox-cdn.com/uploads/chorus_image/image/69688995/acstro_190902_apple_event_0004.0.0.jpg" /&gt;
        &lt;figcaption&gt;Illustration by Alex Castro / The Verge&lt;/figcaption&gt;
    &lt;/figure&gt;

  &lt;p id="OrjUON"&gt;Apple’s &lt;a href="https://go.redirectingat.com?id=66960X1514734&amp;amp;xs=1&amp;amp;url=https%3A%2F%2Fwww.apple.com%2Fchild-safety%2F&amp;amp;referrer=theverge.com&amp;amp;sref=https%3A%2F%2Fwww.theverge.com%2F2021%2F8%2F6%2F22612934%2Fapple-vp-memo-concerns-privacy-new-scanning-photos-images-ios" rel="sponsored nofollow noopener" target="_blank"&gt;forthcoming feature&lt;/a&gt; that will scan iOS devices for images of child abuse is an “important mission,” a software vice president at the company wrote in an internal memo. First reported by&lt;a href="https://9to5mac.com/2021/08/06/apple-internal-memo-icloud-photo-scanning-concerns/"&gt; &lt;em&gt;9to5 Mac&lt;/em&gt;&lt;/a&gt;, the memo by Sebastian Marineau-Mes acknowledges that the new protections have some people “worried about the implications” but that the company will “maintain Apple’s deep commitment to user privacy.”&lt;/p&gt;
&lt;p id="8Ig9yv"&gt;As part of its Expanded Protections for Children, &lt;a href="https://www.theverge.com/2021/8/5/22611721/apple-csam-child-abuse-scanning-hash-system-ncmec"&gt;Apple plans to scan images&lt;/a&gt; on iPhones and other devices before they are uploaded to iCloud. If it finds an image that matches one in the database of the National Center for Missing and Exploited Children (NCMEC), a human at Apple will review the image to confirm whether it contains child pornography. If it’s confirmed, NCMEC will be notified and the user’s account will be disabled. &lt;/p&gt;
&lt;p id="3HB3oD"&gt;The announcement raised concerns among privacy advocates who questioned how Apple could prevent the system from being exploited by bad actors. The Electronic Frontier Foundation &lt;a href="https://www.eff.org/deeplinks/2021/08/apples-plan-think-different-about-encryption-opens-backdoor-your-private-life"&gt;said in a statement&lt;/a&gt; that “it’s impossible to build a client-side scanning system that can only be used for sexually explicit images sent or received by children” and that the system, however well-intended, “will break key promises of the messenger’s encryption itself and open the door to broader abuses.”&lt;/p&gt;
&lt;p id="7BFvDx"&gt;According to &lt;em&gt;9to5Mac&lt;/em&gt;, Marineau-Mes wrote in the memo that the project involved “deep cross-functional commitment” across the company that “delivers tools to protect children, but also maintain Apple’s deep commitment to user privacy.”&lt;/p&gt;
&lt;p id="uGfQad"&gt;Apple did not immediately reply to a request for comment Friday.&lt;/p&gt;

</content>
    <link rel="alternate" type="text/html" href="https://www.theverge.com/2021/8/6/22612934/apple-vp-memo-concerns-privacy-new-scanning-photos-images-ios"/>
    <id>https://www.theverge.com/2021/8/6/22612934/apple-vp-memo-concerns-privacy-new-scanning-photos-images-ios</id>
    <author>
      <name>Kim Lyons</name>
    </author>
  </entry>
</feed>
